# Comparing `tmp/bert4keras3-1.3.0-py3-none-any.whl.zip` & `tmp/bert4keras3-1.3a0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,30 +1,30 @@
-Zip file size: 77632 bytes, number of entries: 28
--rw-rw-rw-  2.0 fat      300 b- defN 24-Apr-28 17:13 bert4keras3/__init__.py
--rw-rw-rw-  2.0 fat    23146 b- defN 24-Apr-28 17:13 bert4keras3/backend.py
--rw-rw-rw-  2.0 fat    11419 b- defN 24-Apr-28 17:13 bert4keras3/layers.py
--rw-rw-rw-  2.0 fat     5211 b- defN 24-Apr-28 17:13 bert4keras3/models.py
--rw-rw-rw-  2.0 fat    30152 b- defN 24-Apr-28 17:13 bert4keras3/ops.py
--rw-rw-rw-  2.0 fat    27955 b- defN 24-Apr-28 17:13 bert4keras3/snippets.py
--rw-rw-rw-  2.0 fat    15509 b- defN 24-Apr-28 17:13 bert4keras3/tokenizers.py
--rw-rw-rw-  2.0 fat    28576 b- defN 24-Apr-28 17:13 bert4keras3/transformers.py
--rw-rw-rw-  2.0 fat    19920 b- defN 24-Apr-28 17:13 bert4keras3/Layers_add/Attentions.py
--rw-rw-rw-  2.0 fat    17887 b- defN 24-Apr-28 17:13 bert4keras3/Layers_add/Embeddings.py
--rw-rw-rw-  2.0 fat     4546 b- defN 24-Apr-28 17:13 bert4keras3/Layers_add/FFN.py
--rw-rw-rw-  2.0 fat     4233 b- defN 24-Apr-28 17:13 bert4keras3/Layers_add/GP.py
--rw-rw-rw-  2.0 fat     7422 b- defN 24-Apr-28 17:13 bert4keras3/Layers_add/LayerNorms.py
--rw-rw-rw-  2.0 fat      114 b- defN 24-Apr-28 17:13 bert4keras3/Layers_add/__init__.py
--rw-rw-rw-  2.0 fat     3374 b- defN 24-Apr-28 17:13 bert4keras3/Layers_add/sampler.py
--rw-rw-rw-  2.0 fat     6078 b- defN 24-Apr-28 17:13 bert4keras3/Models/Alberts.py
--rw-rw-rw-  2.0 fat    23829 b- defN 24-Apr-28 17:13 bert4keras3/Models/Berts.py
--rw-rw-rw-  2.0 fat     2775 b- defN 24-Apr-28 17:13 bert4keras3/Models/GAUs.py
--rw-rw-rw-  2.0 fat    15439 b- defN 24-Apr-28 17:13 bert4keras3/Models/GPTs.py
--rw-rw-rw-  2.0 fat     8857 b- defN 24-Apr-28 17:13 bert4keras3/Models/LLamas.py
--rw-rw-rw-  2.0 fat    12053 b- defN 24-Apr-28 17:13 bert4keras3/Models/Roformers.py
--rw-rw-rw-  2.0 fat    33076 b- defN 24-Apr-28 17:13 bert4keras3/Models/T5models.py
--rw-rw-rw-  2.0 fat      152 b- defN 24-Apr-28 17:13 bert4keras3/Models/__init__.py
--rw-rw-rw-  2.0 fat    11358 b- defN 24-Apr-28 17:21 bert4keras3-1.3.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat      188 b- defN 24-Apr-28 17:21 bert4keras3-1.3.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-28 17:21 bert4keras3-1.3.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       12 b- defN 24-Apr-28 17:21 bert4keras3-1.3.0.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2365 b- defN 24-Apr-28 17:21 bert4keras3-1.3.0.dist-info/RECORD
-28 files, 316038 bytes uncompressed, 73846 bytes compressed:  76.6%
+Zip file size: 77730 bytes, number of entries: 28
+-rw-rw-rw-  2.0 fat      300 b- defN 24-Apr-30 09:03 bert4keras3/__init__.py
+-rw-rw-rw-  2.0 fat    23146 b- defN 24-Apr-30 09:03 bert4keras3/backend.py
+-rw-rw-rw-  2.0 fat    11419 b- defN 24-Apr-30 09:03 bert4keras3/layers.py
+-rw-rw-rw-  2.0 fat     5506 b- defN 24-Apr-30 09:03 bert4keras3/models.py
+-rw-rw-rw-  2.0 fat    30152 b- defN 24-Apr-30 09:03 bert4keras3/ops.py
+-rw-rw-rw-  2.0 fat    27955 b- defN 24-Apr-30 09:03 bert4keras3/snippets.py
+-rw-rw-rw-  2.0 fat    15509 b- defN 24-Apr-30 09:03 bert4keras3/tokenizers.py
+-rw-rw-rw-  2.0 fat    28576 b- defN 24-Apr-30 09:03 bert4keras3/transformers.py
+-rw-rw-rw-  2.0 fat    19920 b- defN 24-Apr-30 09:03 bert4keras3/Layers_add/Attentions.py
+-rw-rw-rw-  2.0 fat    17887 b- defN 24-Apr-30 09:03 bert4keras3/Layers_add/Embeddings.py
+-rw-rw-rw-  2.0 fat     4546 b- defN 24-Apr-30 09:03 bert4keras3/Layers_add/FFN.py
+-rw-rw-rw-  2.0 fat     4233 b- defN 24-Apr-30 09:03 bert4keras3/Layers_add/GP.py
+-rw-rw-rw-  2.0 fat     7422 b- defN 24-Apr-30 09:03 bert4keras3/Layers_add/LayerNorms.py
+-rw-rw-rw-  2.0 fat      384 b- defN 24-Apr-30 09:03 bert4keras3/Layers_add/__init__.py
+-rw-rw-rw-  2.0 fat     3374 b- defN 24-Apr-30 09:03 bert4keras3/Layers_add/sampler.py
+-rw-rw-rw-  2.0 fat     6078 b- defN 24-Apr-30 09:03 bert4keras3/Models/Alberts.py
+-rw-rw-rw-  2.0 fat    23889 b- defN 24-Apr-30 09:03 bert4keras3/Models/Berts.py
+-rw-rw-rw-  2.0 fat     2775 b- defN 24-Apr-30 09:03 bert4keras3/Models/GAUs.py
+-rw-rw-rw-  2.0 fat    15439 b- defN 24-Apr-30 09:03 bert4keras3/Models/GPTs.py
+-rw-rw-rw-  2.0 fat     8857 b- defN 24-Apr-30 09:03 bert4keras3/Models/LLamas.py
+-rw-rw-rw-  2.0 fat    12053 b- defN 24-Apr-30 09:03 bert4keras3/Models/Roformers.py
+-rw-rw-rw-  2.0 fat    33118 b- defN 24-Apr-30 09:03 bert4keras3/Models/T5models.py
+-rw-rw-rw-  2.0 fat      152 b- defN 24-Apr-30 09:03 bert4keras3/Models/__init__.py
+-rw-rw-rw-  2.0 fat    11358 b- defN 24-Apr-30 09:04 bert4keras3-1.3a0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat      188 b- defN 24-Apr-30 09:04 bert4keras3-1.3a0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-30 09:04 bert4keras3-1.3a0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       12 b- defN 24-Apr-30 09:04 bert4keras3-1.3a0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2365 b- defN 24-Apr-30 09:04 bert4keras3-1.3a0.dist-info/RECORD
+28 files, 316705 bytes uncompressed, 73944 bytes compressed:  76.7%
```

## zipnote {}

```diff
@@ -63,23 +63,23 @@
 
 Filename: bert4keras3/Models/T5models.py
 Comment: 
 
 Filename: bert4keras3/Models/__init__.py
 Comment: 
 
-Filename: bert4keras3-1.3.0.dist-info/LICENSE
+Filename: bert4keras3-1.3a0.dist-info/LICENSE
 Comment: 
 
-Filename: bert4keras3-1.3.0.dist-info/METADATA
+Filename: bert4keras3-1.3a0.dist-info/METADATA
 Comment: 
 
-Filename: bert4keras3-1.3.0.dist-info/WHEEL
+Filename: bert4keras3-1.3a0.dist-info/WHEEL
 Comment: 
 
-Filename: bert4keras3-1.3.0.dist-info/top_level.txt
+Filename: bert4keras3-1.3a0.dist-info/top_level.txt
 Comment: 
 
-Filename: bert4keras3-1.3.0.dist-info/RECORD
+Filename: bert4keras3-1.3a0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## bert4keras3/models.py

```diff
@@ -137,23 +137,32 @@
         transformer.model.predict(inputs,verbose=3)
         if keras_weights_path is not None:
             transformer.model.load_weights(keras_weights_path, skip_mismatch=True)
         if lora_model:
             
             def enable_lora(t):
                 if isinstance(t,keras.layers.Embedding) :
-                    t.enable_lora(kwargs['attention_head_size']*2)
+                    t.enable_lora(configs['attention_head_size']*2)
                 elif isinstance(t,keras.layers.Dense):
-                    t.enable_lora(kwargs['attention_head_size'])
+                    t.enable_lora(configs['attention_head_size'])
+                else:
+                    return True
+                return False
+                    
             for layer in transformer.model.layers:
-                layer.trainable=False
-                enable_lora(layer)
+                if 'norm' in layer.name.lower():
+                    continue
+                flag = enable_lora(layer)
+                if flag:
+                    layer.trainable=False
                 for kid in dir (layer):
                     t = getattr(layer,kid)
                     enable_lora(t)
+                    if flag:
+                        layer.trainable=False
     if checkpoint_path is not None:
         transformer.load_weights_from_checkpoint(checkpoint_path)
 
     if return_keras_model:
         return transformer.model
     else:
         return transformer
```

## bert4keras3/Layers_add/__init__.py

```diff
@@ -1,3 +1,9 @@
 #! -*- coding: utf-8 -*-
 from bert4keras3.Layers_add import sampler, Embeddings, GP, LayerNorms, Attentions, FFN
+from bert4keras3.Layers_add.sampler import *
+from bert4keras3.Layers_add.Embeddings import *
+from bert4keras3.Layers_add.GP import *
+from bert4keras3.Layers_add.LayerNorms import *
+from bert4keras3.Layers_add.Attentions import *
+from bert4keras3.Layers_add.FFN import *
```

## bert4keras3/Models/Berts.py

```diff
@@ -36,31 +36,32 @@
     def get_custom_position_ids(self):
         return True
     def get_inputs(self):
         """BERT的输入是token_ids和segment_ids
         （但允许自行传入位置id，以实现一些特殊需求）
         """
         x_in = self.apply(
-            layer=Input, shape=(self.sequence_length,), name='Input-Token'
+            layer=Input, shape=(self.sequence_length,), name='Input-Token',dtype='int32'
         )
         inputs = [x_in]
 
         if self.segment_vocab_size > 0:
             s_in = self.apply(
                 layer=Input,
                 shape=(self.sequence_length,),
-                name='Input-Segment'
+                name='Input-Segment',dtype='int32'
+                
             )
             inputs.append(s_in)
 
         if self.custom_position_ids:
             p_in = self.apply(
                 layer=Input,
                 shape=(self.sequence_length,),
-                name='Input-Position'
+                name='Input-Position',dtype='int32'
             )
             inputs.append(p_in)
 
         return inputs
     
     def apply_main_cache_layers(self, inputs, index,self_cache_update_index,
                                 cross_cache_update_index=None,
```

## bert4keras3/Models/T5models.py

```diff
@@ -131,21 +131,21 @@
 
     def get_inputs(self):
         """T5的Encoder的输入只有token_ids
         """
         x_in = self.apply(
             layer=Input,
             shape=(self.sequence_length,),
-            name='Encoder-Input-Token'
+            name='Encoder-Input-Token',dtype='int32'
         )
         if self.segment_vocab_size > 0:
             s_in = self.apply(
                 layer=Input,
                 shape=(self.sequence_length,),
-                name='Segment-Input-Token'
+                name='Segment-Input-Token',dtype='int32'
             )
             return [x_in,s_in]
         return x_in
 
     def apply_embeddings(self, inputs):
         """T5的embedding只有token embedding，
         并把relative position embedding准备好，待attention使用。
@@ -342,15 +342,15 @@
             layer=Input,
             shape=(self.sequence_length, self.hidden_size),
             name='Input-Context'
         )
         x_in = self.apply(
             layer=Input,
             shape=(self.decoder_sequence_length,),
-            name='Decoder-Input-Token'
+            name='Decoder-Input-Token',dtype='int32'
         )
         return [c_in, x_in]
 
     def apply_embeddings(self, inputs):
         """T5的embedding只有token embedding，
         并把relative position embedding准备好，待attention使用。
         """
```

## Comparing `bert4keras3-1.3.0.dist-info/LICENSE` & `bert4keras3-1.3a0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `bert4keras3-1.3.0.dist-info/RECORD` & `bert4keras3-1.3a0.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 bert4keras3/__init__.py,sha256=29y0RGIvNTBhSGTAOdNCMCUhlL3_s7x05sX_fdcjjhA,300
 bert4keras3/backend.py,sha256=D-9WqJu_Xu37hoHecwQyZmhMgIWrli38_ny8SyW9ZXE,23146
 bert4keras3/layers.py,sha256=IbVd3zYbsXhjeVOjcA0TSpuIbfa3eAJQ6f9N_eXnYO4,11419
-bert4keras3/models.py,sha256=o7DHSr3Dvj7IIa5oVBqyO92fIm2oLU-pPoo5YsSlNaU,5211
+bert4keras3/models.py,sha256=Iz6HP22lfNEbcXMlBBxeYdCAqu-t6w_pczkYsrZ6qIM,5506
 bert4keras3/ops.py,sha256=H1sbMv6WOJ5iGAB0rosPz9LoqdT0jS8knLRW_6XkFRU,30152
 bert4keras3/snippets.py,sha256=YwdYRvrewhM6g8ax9_Ath0HsJATzuyZksL5Dow9l65A,27955
 bert4keras3/tokenizers.py,sha256=PCxtC4L9tF2w-GUDujdPK8Z5WlnbaL8mYhRfHn1S_Qg,15509
 bert4keras3/transformers.py,sha256=g0-dYhls5_Og9bWCBkIgGpofK-sScFnqZO0kqAKmbkE,28576
 bert4keras3/Layers_add/Attentions.py,sha256=413rnVV4mSw_1dM9hynnYwBICx0aQM5l77Z9xR-kytA,19920
 bert4keras3/Layers_add/Embeddings.py,sha256=ouDdsbL0MCPjbl7rHyWzjSKy-U9U1a18BrMjCPqKn6o,17887
 bert4keras3/Layers_add/FFN.py,sha256=AtDY4woLD71m7KqswqV8HbK-9qOAjpayjt9NIMO1k6s,4546
 bert4keras3/Layers_add/GP.py,sha256=fpKU4XhEZMTwb-LS9ApSQR8icbtstIZZaJ41jeH-qGY,4233
 bert4keras3/Layers_add/LayerNorms.py,sha256=dnV1qk2dwOLeCU8GKvil3Y0KaZxRU1LEvzDMDYTeJVg,7422
-bert4keras3/Layers_add/__init__.py,sha256=rvdQom205l23httVOX8RCVaV3yY3xxpFDsOl6Vdh2Q0,114
+bert4keras3/Layers_add/__init__.py,sha256=24feh_P-5Hw1jbrfLJJo54DNtF-eV08tXPHNCmJh17Y,384
 bert4keras3/Layers_add/sampler.py,sha256=9n_d3p-RRJMqLyGAHqb2CarkGMGp4mGm-h7tBej-_I4,3374
 bert4keras3/Models/Alberts.py,sha256=uDue4rNrGgIWo02PubpeVaBL3x5-WckwBsP6QC8Cgvo,6078
-bert4keras3/Models/Berts.py,sha256=KGEQqkkHfblPqr7xsmenOjg0jaZSuRcEqkNkVwMb5Aw,23829
+bert4keras3/Models/Berts.py,sha256=to92hpqZcEvwIVBH43oDx9P5yWcVfk6bvgorcCwFM6k,23889
 bert4keras3/Models/GAUs.py,sha256=KdYfhoADsVTIUKQ2loxeyChPFyCsjOfyO6w2Gr8EQrM,2775
 bert4keras3/Models/GPTs.py,sha256=OOqoHUpGatYDEbX3KyYPd9kqw0EA4fw0PeoX456ixEg,15439
 bert4keras3/Models/LLamas.py,sha256=-ppR72_JXRjOQaEC9gAnIfsRjRECTv2wXu8hCt-AvE4,8857
 bert4keras3/Models/Roformers.py,sha256=3KWBOfJksIxddQmnK6_6WzyqhFJ8DcR_1eTXKLmXcIQ,12053
-bert4keras3/Models/T5models.py,sha256=v0O6NTLB7AQNCdHJKWszpND8Z3IT59SISwVuLbWu8UY,33076
+bert4keras3/Models/T5models.py,sha256=QrzcCQ6rIEcfYakTJgrzeyqlmj5aUz1gehWkZhqRhxU,33118
 bert4keras3/Models/__init__.py,sha256=mU4QuR6NHq05HAT9a2kzC_pLvCysRYZ_u1H4OjcdYHQ,152
-bert4keras3-1.3.0.dist-info/LICENSE,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
-bert4keras3-1.3.0.dist-info/METADATA,sha256=P3IMTnD0YxiTvVw0a0PZi-iKMrdND3Q7ZAhOQmlAlRI,188
-bert4keras3-1.3.0.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
-bert4keras3-1.3.0.dist-info/top_level.txt,sha256=cPWwwxSejEMpgAZFzYQc0WmhPrF3XwP3k1IwRO93gwQ,12
-bert4keras3-1.3.0.dist-info/RECORD,,
+bert4keras3-1.3a0.dist-info/LICENSE,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
+bert4keras3-1.3a0.dist-info/METADATA,sha256=gsDH442EwkT2lOXu51LcVlDpAVqWCYtY1rMJGl4Hj-A,188
+bert4keras3-1.3a0.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+bert4keras3-1.3a0.dist-info/top_level.txt,sha256=cPWwwxSejEMpgAZFzYQc0WmhPrF3XwP3k1IwRO93gwQ,12
+bert4keras3-1.3a0.dist-info/RECORD,,
```

