# Comparing `tmp/pystarburst-0.7.0-py3-none-any.whl.zip` & `tmp/pystarburst-0.8.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,50 +1,50 @@
-Zip file size: 130653 bytes, number of entries: 48
--rw-r--r--  2.0 unx    12104 b- defN 80-Jan-01 00:00 pystarburst/__init__.py
--rw-r--r--  2.0 unx       72 b- defN 80-Jan-01 00:00 pystarburst/_internal/__init__.py
--rw-r--r--  2.0 unx       72 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/__init__.py
--rw-r--r--  2.0 unx      598 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/analyzer.py
--rw-r--r--  2.0 unx     1638 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/analyzer_utils.py
--rw-r--r--  2.0 unx      354 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/base_model.py
--rw-r--r--  2.0 unx     3780 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/dataframe_api_client.py
--rw-r--r--  2.0 unx     1620 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/__init__.py
--rw-r--r--  2.0 unx     1840 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/binary.py
--rw-r--r--  2.0 unx     5640 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/general.py
--rw-r--r--  2.0 unx      789 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/grouping_set.py
--rw-r--r--  2.0 unx      686 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/sort.py
--rw-r--r--  2.0 unx     1129 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/table.py
--rw-r--r--  2.0 unx     1415 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/table_function.py
--rw-r--r--  2.0 unx     1591 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/unary.py
--rw-r--r--  2.0 unx     3227 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/window.py
--rw-r--r--  2.0 unx       72 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/__init__.py
--rw-r--r--  2.0 unx     1101 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/__init__.py
--rw-r--r--  2.0 unx     2787 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/binary.py
--rw-r--r--  2.0 unx     1048 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/leaf.py
--rw-r--r--  2.0 unx     1779 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/table.py
--rw-r--r--  2.0 unx      594 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/table_function.py
--rw-r--r--  2.0 unx     3039 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/unary.py
--rw-r--r--  2.0 unx     1355 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/trino_plan.py
--rw-r--r--  2.0 unx     5652 b- defN 80-Jan-01 00:00 pystarburst/_internal/error_message.py
--rw-r--r--  2.0 unx     6878 b- defN 80-Jan-01 00:00 pystarburst/_internal/server_connection.py
--rw-r--r--  2.0 unx    12795 b- defN 80-Jan-01 00:00 pystarburst/_internal/type_utils.py
--rw-r--r--  2.0 unx    10127 b- defN 80-Jan-01 00:00 pystarburst/_internal/utils.py
--rw-r--r--  2.0 unx    30492 b- defN 80-Jan-01 00:00 pystarburst/column.py
--rw-r--r--  2.0 unx      550 b- defN 80-Jan-01 00:00 pystarburst/context.py
--rw-r--r--  2.0 unx   104994 b- defN 80-Jan-01 00:00 pystarburst/dataframe.py
--rw-r--r--  2.0 unx    22528 b- defN 80-Jan-01 00:00 pystarburst/dataframe_na_functions.py
--rw-r--r--  2.0 unx     6676 b- defN 80-Jan-01 00:00 pystarburst/dataframe_stat_functions.py
--rw-r--r--  2.0 unx     5473 b- defN 80-Jan-01 00:00 pystarburst/dataframe_writer.py
--rw-r--r--  2.0 unx     2795 b- defN 80-Jan-01 00:00 pystarburst/exceptions.py
--rw-r--r--  2.0 unx   130960 b- defN 80-Jan-01 00:00 pystarburst/functions.py
--rw-r--r--  2.0 unx      989 b- defN 80-Jan-01 00:00 pystarburst/query_history.py
--rw-r--r--  2.0 unx    10967 b- defN 80-Jan-01 00:00 pystarburst/relational_grouped_dataframe.py
--rw-r--r--  2.0 unx     8845 b- defN 80-Jan-01 00:00 pystarburst/row.py
--rw-r--r--  2.0 unx    33911 b- defN 80-Jan-01 00:00 pystarburst/session.py
--rw-r--r--  2.0 unx    20382 b- defN 80-Jan-01 00:00 pystarburst/table.py
--rw-r--r--  2.0 unx     8082 b- defN 80-Jan-01 00:00 pystarburst/table_function.py
--rw-r--r--  2.0 unx    11933 b- defN 80-Jan-01 00:00 pystarburst/types.py
--rw-r--r--  2.0 unx     9653 b- defN 80-Jan-01 00:00 pystarburst/window.py
--rw-r--r--  2.0 unx    11351 b- defN 80-Jan-01 00:00 pystarburst-0.7.0.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     2836 b- defN 80-Jan-01 00:00 pystarburst-0.7.0.dist-info/METADATA
--rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 pystarburst-0.7.0.dist-info/WHEEL
-?rw-r--r--  2.0 unx     4600 b- defN 16-Jan-01 00:00 pystarburst-0.7.0.dist-info/RECORD
-48 files, 511887 bytes uncompressed, 123145 bytes compressed:  75.9%
+Zip file size: 133433 bytes, number of entries: 48
+-rw-r--r--  2.0 unx    12445 b- defN 80-Jan-01 00:00 pystarburst/__init__.py
+-rw-r--r--  2.0 unx       62 b- defN 80-Jan-01 00:00 pystarburst/_internal/__init__.py
+-rw-r--r--  2.0 unx       62 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/__init__.py
+-rw-r--r--  2.0 unx      588 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/analyzer.py
+-rw-r--r--  2.0 unx     2194 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/analyzer_utils.py
+-rw-r--r--  2.0 unx      344 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/base_model.py
+-rw-r--r--  2.0 unx     4372 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/dataframe_api_client.py
+-rw-r--r--  2.0 unx     1634 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/__init__.py
+-rw-r--r--  2.0 unx     1830 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/binary.py
+-rw-r--r--  2.0 unx     5829 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/general.py
+-rw-r--r--  2.0 unx      779 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/grouping_set.py
+-rw-r--r--  2.0 unx      676 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/sort.py
+-rw-r--r--  2.0 unx     1119 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/table.py
+-rw-r--r--  2.0 unx     1405 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/table_function.py
+-rw-r--r--  2.0 unx     1581 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/unary.py
+-rw-r--r--  2.0 unx     3217 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/expression/window.py
+-rw-r--r--  2.0 unx       62 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/__init__.py
+-rw-r--r--  2.0 unx     1295 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/__init__.py
+-rw-r--r--  2.0 unx     2777 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/binary.py
+-rw-r--r--  2.0 unx     1038 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/leaf.py
+-rw-r--r--  2.0 unx     1769 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/table.py
+-rw-r--r--  2.0 unx      584 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/table_function.py
+-rw-r--r--  2.0 unx     3324 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/logical_plan/unary.py
+-rw-r--r--  2.0 unx     1435 b- defN 80-Jan-01 00:00 pystarburst/_internal/analyzer/plan/trino_plan.py
+-rw-r--r--  2.0 unx     5642 b- defN 80-Jan-01 00:00 pystarburst/_internal/error_message.py
+-rw-r--r--  2.0 unx     6868 b- defN 80-Jan-01 00:00 pystarburst/_internal/server_connection.py
+-rw-r--r--  2.0 unx    12785 b- defN 80-Jan-01 00:00 pystarburst/_internal/type_utils.py
+-rw-r--r--  2.0 unx    10329 b- defN 80-Jan-01 00:00 pystarburst/_internal/utils.py
+-rw-r--r--  2.0 unx    30460 b- defN 80-Jan-01 00:00 pystarburst/column.py
+-rw-r--r--  2.0 unx      540 b- defN 80-Jan-01 00:00 pystarburst/context.py
+-rw-r--r--  2.0 unx   117082 b- defN 80-Jan-01 00:00 pystarburst/dataframe.py
+-rw-r--r--  2.0 unx    22426 b- defN 80-Jan-01 00:00 pystarburst/dataframe_na_functions.py
+-rw-r--r--  2.0 unx     6817 b- defN 80-Jan-01 00:00 pystarburst/dataframe_stat_functions.py
+-rw-r--r--  2.0 unx     5513 b- defN 80-Jan-01 00:00 pystarburst/dataframe_writer.py
+-rw-r--r--  2.0 unx     2785 b- defN 80-Jan-01 00:00 pystarburst/exceptions.py
+-rw-r--r--  2.0 unx   133349 b- defN 80-Jan-01 00:00 pystarburst/functions.py
+-rw-r--r--  2.0 unx      979 b- defN 80-Jan-01 00:00 pystarburst/query_history.py
+-rw-r--r--  2.0 unx    10955 b- defN 80-Jan-01 00:00 pystarburst/relational_grouped_dataframe.py
+-rw-r--r--  2.0 unx     8891 b- defN 80-Jan-01 00:00 pystarburst/row.py
+-rw-r--r--  2.0 unx    36224 b- defN 80-Jan-01 00:00 pystarburst/session.py
+-rw-r--r--  2.0 unx    20502 b- defN 80-Jan-01 00:00 pystarburst/table.py
+-rw-r--r--  2.0 unx     8068 b- defN 80-Jan-01 00:00 pystarburst/table_function.py
+-rw-r--r--  2.0 unx    11923 b- defN 80-Jan-01 00:00 pystarburst/types.py
+-rw-r--r--  2.0 unx     9900 b- defN 80-Jan-01 00:00 pystarburst/window.py
+-rw-r--r--  2.0 unx    11346 b- defN 80-Jan-01 00:00 pystarburst-0.8.0.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     2876 b- defN 80-Jan-01 00:00 pystarburst-0.8.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 pystarburst-0.8.0.dist-info/WHEEL
+?rw-r--r--  2.0 unx     4600 b- defN 16-Jan-01 00:00 pystarburst-0.8.0.dist-info/RECORD
+48 files, 531369 bytes uncompressed, 125925 bytes compressed:  76.3%
```

## zipnote {}

```diff
@@ -126,20 +126,20 @@
 
 Filename: pystarburst/types.py
 Comment: 
 
 Filename: pystarburst/window.py
 Comment: 
 
-Filename: pystarburst-0.7.0.dist-info/LICENSE.txt
+Filename: pystarburst-0.8.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: pystarburst-0.7.0.dist-info/METADATA
+Filename: pystarburst-0.8.0.dist-info/METADATA
 Comment: 
 
-Filename: pystarburst-0.7.0.dist-info/WHEEL
+Filename: pystarburst-0.8.0.dist-info/WHEEL
 Comment: 
 
-Filename: pystarburst-0.7.0.dist-info/RECORD
+Filename: pystarburst-0.8.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pystarburst/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 """
 Contains core classes of pystarburst.
 """
 
 __all__ = [
@@ -57,14 +57,15 @@
     Like,
     ListAgg,
     Literal,
     MultipleExpression,
     RegExpLike,
     ScalarSubquery,
     Star,
+    StructExpression,
     UnresolvedAttribute,
 )
 from pystarburst._internal.analyzer.expression.grouping_set import (
     Cube,
     GroupingSetsExpression,
     Rollup,
 )
@@ -104,14 +105,15 @@
     SpecifiedWindowFrame,
     UnboundedFollowing,
     UnboundedPreceding,
     UnspecifiedFrame,
     WindowExpression,
     WindowSpecDefinition,
 )
+from pystarburst._internal.analyzer.plan.logical_plan import StarburstDataframeVersion
 from pystarburst._internal.analyzer.plan.logical_plan.binary import (
     Except,
     Intersect,
     IntersectAll,
     Join,
     Union,
     UnionAll,
@@ -131,14 +133,15 @@
 )
 from pystarburst._internal.analyzer.plan.logical_plan.table_function import (
     TableFunctionRelation,
 )
 from pystarburst._internal.analyzer.plan.logical_plan.unary import (
     Aggregate,
     CreateView,
+    Explode,
     Filter,
     Limit,
     Pivot,
     Project,
     Sample,
     Sort,
     Stack,
@@ -289,20 +292,22 @@
     "Like": Like,
     "ListAgg": ListAgg,
     "Literal": Literal,
     "MultipleExpression": MultipleExpression,
     "RegExpLike": RegExpLike,
     "ScalarSubquery": ScalarSubquery,
     "Star": Star,
+    "StructExpression": StructExpression,
     "SubfieldInt": SubfieldInt,
     "SubfieldString": SubfieldString,
     "UnresolvedAttribute": UnresolvedAttribute,
 }
 
 _LOGICAL_PLAN_MAP = {
+    "StarburstDataframeVersion": StarburstDataframeVersion,
     # binary
     "Except": Except,
     "Intersect": Intersect,
     "IntersectAll": IntersectAll,
     "Union": Union,
     "UnionAll": UnionAll,
     "Join": Join,
@@ -318,14 +323,15 @@
     "TableMerge": TableMerge,
     "TableUpdate": TableUpdate,
     # table_function
     "TableFunctionRelation": TableFunctionRelation,
     # unary
     "Aggregate": Aggregate,
     "CreateView": CreateView,
+    "Explode": Explode,
     "Filter": Filter,
     "Limit": Limit,
     "Pivot": Pivot,
     "Project": Project,
     "Unpivot": Unpivot,
     "Sample": Sample,
     "Sort": Sort,
@@ -364,14 +370,15 @@
 LambdaParameter.update_forward_refs(**_EXPRESSION_MAP)
 Like.update_forward_refs(**_EXPRESSION_MAP)
 ListAgg.update_forward_refs(**_EXPRESSION_MAP)
 MultipleExpression.update_forward_refs(**_EXPRESSION_MAP)
 RegExpLike.update_forward_refs(**_EXPRESSION_MAP)
 ScalarSubquery.update_forward_refs(**_EXPRESSION_MAP)
 Star.update_forward_refs(**_EXPRESSION_MAP)
+StructExpression.update_forward_refs(**_EXPRESSION_MAP)
 SubfieldInt.update_forward_refs(**_EXPRESSION_MAP)
 SubfieldString.update_forward_refs(**_EXPRESSION_MAP)
 UnresolvedAttribute.update_forward_refs(**_EXPRESSION_MAP)
 Cube.update_forward_refs(**_EXPRESSION_MAP)
 Rollup.update_forward_refs(**_EXPRESSION_MAP)
 GroupingSetsExpression.update_forward_refs(**_EXPRESSION_MAP)
 SortOrder.update_forward_refs(**_EXPRESSION_MAP)
@@ -400,14 +407,15 @@
 
 # Trino plan
 TrinoPlan.update_forward_refs(**_EXPRESSION_MAP, **_LOGICAL_PLAN_MAP)
 
 # Logical plans
 Aggregate.update_forward_refs(**_EXPRESSION_MAP)
 Filter.update_forward_refs(**_EXPRESSION_MAP)
+Explode.update_forward_refs(**_EXPRESSION_MAP)
 Limit.update_forward_refs(**_EXPRESSION_MAP)
 Project.update_forward_refs(**_EXPRESSION_MAP)
 Join.update_forward_refs(**_EXPRESSION_MAP)
 UsingJoin.update_forward_refs(**_EXPRESSION_MAP)
 UpdateMergeExpression.update_forward_refs(**_EXPRESSION_MAP)
 CreateTable.update_forward_refs(**_EXPRESSION_MAP)
 TableMerge.update_forward_refs(**_EXPRESSION_MAP)
```

## pystarburst/_internal/__init__.py

```diff
@@ -1,3 +1,3 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
```

## pystarburst/_internal/analyzer/__init__.py

```diff
@@ -1,3 +1,3 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
```

## pystarburst/_internal/analyzer/analyzer.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 import pystarburst
 from pystarburst._internal.analyzer.dataframe_api_client import DataframeApiClient
 from pystarburst._internal.analyzer.plan.logical_plan import LogicalPlan
 from pystarburst._internal.analyzer.plan.trino_plan import TrinoPlan
```

## pystarburst/_internal/analyzer/analyzer_utils.py

```diff
@@ -1,17 +1,17 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 import re
 from typing import Optional, Union
 
 from pystarburst._internal.error_message import PyStarburstClientExceptionMessages
-from pystarburst._internal.utils import is_single_quoted
+from pystarburst._internal.utils import is_single_quoted, validate_object_name
 
 SINGLE_QUOTE = "'"
 DOUBLE_QUOTE = '"'
 EMPTY_STRING = ""
 
 
 def convert_value_to_sql_option(value: Optional[Union[str, bool, int, float]]) -> str:
@@ -39,14 +39,27 @@
 def quote_name(name: str) -> str:
     if ALREADY_QUOTED.match(name):
         return validate_quoted_name(name)
     else:
         return DOUBLE_QUOTE + escape_quotes(name.lower()) + DOUBLE_QUOTE
 
 
+def quote_table_qualified_identifier(name: str) -> str:
+    """
+    quotes table qualified identifiers, which could be passed in a forms:
+        - table or "table"
+        - schema.table or "schema"."table"
+        - catalog.schema.table or "catalog"."schema"."table"
+    """
+    qualified_table_name_list = validate_object_name(name)
+    quoted_qualified_table_name_list = [quote_name(i) for i in qualified_table_name_list]
+    quoted_qualified_table_name = ".".join(quoted_qualified_table_name_list)
+    return quoted_qualified_table_name
+
+
 def validate_quoted_name(name: str) -> str:
     if DOUBLE_QUOTE in name[1:-1].replace(DOUBLE_QUOTE + DOUBLE_QUOTE, EMPTY_STRING):
         raise PyStarburstClientExceptionMessages.PLAN_ANALYZER_INVALID_IDENTIFIER(name)
     else:
         return name
```

## pystarburst/_internal/analyzer/base_model.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from pydantic import BaseModel as PydanticBaseModel
 
 
 class BaseModel(PydanticBaseModel):
     class Config:
```

## pystarburst/_internal/analyzer/dataframe_api_client.py

```diff
@@ -1,25 +1,27 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
-
+import base64
 import copy
 import json
 from typing import Union
 
+import zstandard as zstd
 from pydantic import Field
 from trino.client import PROXIES, TrinoRequest, logger
 
 from pystarburst._internal.analyzer.base_model import BaseModel
 from pystarburst._internal.analyzer.plan.logical_plan import LogicalPlan
 from pystarburst._internal.analyzer.plan.trino_plan import TrinoPlan
 from pystarburst._internal.utils import PythonObjJSONEncoder
 from pystarburst.exceptions import (
     PyStarburstColumnException,
     PyStarburstGeneralException,
+    PyStarburstPlanException,
     PyStarburstSQLException,
 )
 
 
 class ErrorResponse(BaseModel):
     message: str
     error_code: str = Field(alias="errorCode")
@@ -47,19 +49,29 @@
             timeout=self._request_timeout,
             proxies=PROXIES,
         )
         return http_response
 
 
 class DataFrameTableFunction:
-    def __init__(self, cursor):
+    PREFIX = "$zstd:"
+
+    def __init__(self, cursor, session):
         self.cursor = cursor
+        self.session = session
 
     def execute(self, payload):
         payload_json = json.dumps(payload, cls=PythonObjJSONEncoder)
+        if (
+            not self.session._use_endpoint
+            and self.session._starburst_dataframe_version is not None
+            and len(payload_json) > 2 * 1024
+        ):
+            compressed = zstd.compress(payload_json.encode("utf-8"))
+            payload_json = DataFrameTableFunction.PREFIX + base64.b64encode(compressed).decode("utf-8")
         try:
             self.cursor.execute(f"SELECT trino_plan FROM TABLE(analyze_logical_plan(?))", [payload_json])
         except Exception as e:
             raise PyStarburstSQLException(f"Failed to analyze logical plan: {str(e.message)}") from e
         rows = self.cursor.fetchall()
         return rows[0][0]
 
@@ -79,15 +91,15 @@
                 conn._http_session,
                 conn.http_scheme,
                 conn.auth,
                 conn.max_attempts,
                 conn.request_timeout,
             )
         else:
-            trino_request = DataFrameTableFunction(cursor)
+            trino_request = DataFrameTableFunction(cursor, self.session)
         payload = self.serialize(logical_plan)
         if self.session._use_endpoint:
             response = trino_request.execute(payload).json()
         else:
             response = json.loads(trino_request.execute(payload))
         trino_plan = self.deserialize(response)
         # Keep the original source plan for supporting cloning dataframes
@@ -99,14 +111,16 @@
 
     def deserialize(self, json) -> TrinoPlan:
         response = Response.parse_obj(json).__root__
 
         if isinstance(response, ErrorResponse):
             message = response.message
             error_code = response.error_code
+            if error_code == "ANALYSIS_ERROR":
+                raise PyStarburstSQLException(message)
             if error_code == "SQL_ERROR":
                 raise PyStarburstSQLException(message)
             if error_code == "COLUMN_ERROR":
                 raise PyStarburstColumnException(message)
             raise PyStarburstGeneralException(message)
 
         assert isinstance(response, TrinoPlan)
```

## pystarburst/_internal/analyzer/expression/__init__.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Union
 
 from pystarburst._internal.analyzer.base_model import BaseModel
 
 ExpressionUnion = Union[
@@ -71,14 +71,15 @@
     "Like",
     "ListAgg",
     "Literal",
     "MultipleExpression",
     "RegExpLike",
     "ScalarSubquery",
     "Star",
+    "StructExpression",
     "SubfieldInt",
     "SubfieldString",
     "UnresolvedAttribute",
 ]
 
 
 class Expression(BaseModel):
```

## pystarburst/_internal/analyzer/expression/binary.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Annotated, Literal
 
 from pydantic import Field
 
 from pystarburst._internal.analyzer.expression import Expression, ExpressionUnion
```

## pystarburst/_internal/analyzer/expression/general.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 import math
 from typing import Annotated, Any, List
 from typing import Literal as TypingLiteral
 from typing import Optional
 
@@ -27,14 +27,19 @@
 
 
 class ArrayExpression(Expression):
     type: TypingLiteral["ArrayExpression"] = Field("ArrayExpression", alias="@type")
     elements: List[Annotated[ExpressionUnion, Field(discriminator="type")]]
 
 
+class StructExpression(Expression):
+    type: TypingLiteral["StructExpression"] = Field("StructExpression", alias="@type")
+    fields: List[Annotated[ExpressionUnion, Field(discriminator="type")]]
+
+
 class Attribute(Expression, NamedExpression):
     type: TypingLiteral["Attribute"] = Field("Attribute", alias="@type")
     id: str = Field(default_factory=random_string)
     name: str
     datatype: "DataTypeUnion" = Field(discriminator="type", alias="dataType")
     nullable: bool = Field(True)
```

## pystarburst/_internal/analyzer/expression/grouping_set.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Annotated, List, Literal
 
 from pydantic import Field
 
 from pystarburst._internal.analyzer.expression import Expression, ExpressionUnion
```

## pystarburst/_internal/analyzer/expression/sort.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from enum import Enum
 from typing import Literal, Optional
 
 from pydantic import Field
```

## pystarburst/_internal/analyzer/expression/table.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Annotated, List, Literal, Optional
 
 from pydantic import Field
 
 from pystarburst._internal.analyzer.expression import Expression, ExpressionUnion
```

## pystarburst/_internal/analyzer/expression/table_function.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Annotated, Dict, List, Literal, Optional
 
 from pydantic import Field
 
 from pystarburst._internal.analyzer.expression import Expression, ExpressionUnion
```

## pystarburst/_internal/analyzer/expression/unary.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Annotated, Literal
 
 from pydantic import Field
 
 from pystarburst._internal.analyzer.expression.general import (
```

## pystarburst/_internal/analyzer/expression/window.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Annotated, List, Literal, Optional, Union
 
 from pydantic import Field
 
 from pystarburst._internal.analyzer.base_model import BaseModel
```

## pystarburst/_internal/analyzer/plan/__init__.py

```diff
@@ -1,3 +1,3 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
```

## pystarburst/_internal/analyzer/plan/logical_plan/__init__.py

```diff
@@ -1,12 +1,12 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 from enum import Enum
-from typing import Optional, Union
+from typing import Literal, Optional, Union
 
 from pydantic.fields import Field
 
 from pystarburst._internal.analyzer.base_model import BaseModel
 from pystarburst._internal.utils import get_version
 
 
@@ -16,15 +16,20 @@
 
 
 class LogicalPlan(BaseModel):
     type_coercion_mode: str = Field(TypeCoercionMode.DEFAULT, alias="typeCoercionMode")
     pystarburst_version: Optional[str] = Field(get_version(), alias="pyStarburstVersion")
 
 
+class StarburstDataframeVersion(LogicalPlan):
+    type: Literal["StarburstDataframeVersion"] = Field("StarburstDataframeVersion", alias="@type")
+
+
 LogicalPlanUnion = Union[
+    "StarburstDataframeVersion",
     # binary
     "Except",
     "Intersect",
     "IntersectAll",
     "Union",
     "UnionAll",
     "Join",
@@ -40,14 +45,15 @@
     "TableMerge",
     "TableUpdate",
     # table_function
     "TableFunctionRelation",
     # unary
     "Aggregate",
     "CreateView",
+    "Explode",
     "Filter",
     "Limit",
     "Pivot",
     "Project",
     "Unpivot",
     "Sample",
     "Sort",
```

## pystarburst/_internal/analyzer/plan/logical_plan/binary.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from enum import Enum
 from typing import Annotated, List, Literal, Optional
 
 from pydantic import Field
```

## pystarburst/_internal/analyzer/plan/logical_plan/leaf.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import List
 from typing import Literal as TypingLiteral
 from typing import Optional
 
 from pydantic import Field
```

## pystarburst/_internal/analyzer/plan/logical_plan/table.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from enum import Enum
 from typing import Annotated, Dict, List, Literal, Optional
 
 from pydantic import Field
```

## pystarburst/_internal/analyzer/plan/logical_plan/table_function.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Literal, Union
 
 from pydantic import Field
 
 from pystarburst._internal.analyzer.expression.table_function import (
```

## pystarburst/_internal/analyzer/plan/logical_plan/unary.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Annotated, List, Literal
 
 from pydantic import Field
 
 from pystarburst._internal.analyzer.expression.general import ExpressionUnion
@@ -28,14 +28,22 @@
 
 
 class Filter(UnaryNode):
     type: Literal["Filter"] = Field("Filter", alias="@type")
     condition: ExpressionUnion = Field(discriminator="type")
 
 
+class Explode(UnaryNode):
+    type: Literal["Explode"] = Field("Explode", alias="@type")
+    explode_column: ExpressionUnion = Field(alias="explodeColumn", discriminator="type")
+    position_included: bool = Field(alias="positionIncluded")
+    inline: bool = Field()
+    outer: bool = Field()
+
+
 class Limit(UnaryNode):
     type: Literal["Limit"] = Field("Limit", alias="@type")
     limit_expr: ExpressionUnion = Field(alias="limitExpression", discriminator="type")
     offset_expr: ExpressionUnion = Field(alias="offsetExpression", discriminator="type")
 
 
 class Project(UnaryNode):
```

## pystarburst/_internal/analyzer/plan/trino_plan.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 import sys
 from typing import Dict, List, Optional
 
 import trino
 from pydantic import Field
@@ -15,14 +15,15 @@
 
 class TrinoPlan(BaseModel):
     queries: List[str]
     post_actions: Optional[List[str]] = Field(alias="postActions")
     source_plan: Optional[LogicalPlanUnion] = Field(alias="sourcePlan", discriminator="type", exclude=True)
     output: Optional[List["Attribute"]]
     alias_map: Dict[str, str] = Field(alias="aliasMap")
+    starburst_dataframe_version: Optional[str] = Field(alias="starburstDataFrameVersion")
 
     class Decorator:
         @staticmethod
         def wrap_exception(func):
             def wrap(*args, **kwargs):
                 try:
                     return func(*args, **kwargs)
```

## pystarburst/_internal/error_message.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 import trino.exceptions
 
 from pystarburst.exceptions import (
     PyStarburstColumnException,
     PyStarburstDataframeException,
```

## pystarburst/_internal/server_connection.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from logging import getLogger
 from typing import Any, Dict, Iterator, List, Optional, Set, Tuple, Union
 
 from trino.dbapi import Connection, Cursor, connect
```

## pystarburst/_internal/type_utils.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 # Code in this file may constitute partial or total reimplementation, or modification of
 # existing code originally distributed by the Apache Software Foundation as part of the
 # Apache Spark project, under the Apache License, Version 2.0.
 
 import ctypes
```

## pystarburst/_internal/utils.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 import array
 import binascii
 import datetime
 import decimal
 import importlib
@@ -41,15 +41,17 @@
 TRINO_CASE_INSENSITIVE_QUOTED_ID_PATTERN = r'("([a-z_][a-z0-9_\$]{0,255})")'
 TRINO_CASE_INSENSITIVE_UNQUOTED_SUFFIX_PATTERN = r"([a-zA-Z0-9_\$]{0,255})"
 
 # Valid name can be:
 #   identifier
 #   identifier.identifier
 #   identifier.identifier.identifier
-TRINO_OBJECT_RE_PATTERN = re.compile(f"^(?:(?:{TRINO_ID_PATTERN}\\.){{0,2}}){TRINO_ID_PATTERN}$")
+TRINO_OBJECT_RE_PATTERN = re.compile(
+    f"^(?:(?:{TRINO_ID_PATTERN}\\.){{0,1}})(?:(?:{TRINO_ID_PATTERN}\\.){{0,1}}){TRINO_ID_PATTERN}$"
+)
 
 TRINO_CATALOG_SCHEMA_PATTERN = re.compile(f"^(?:(?:{TRINO_ID_PATTERN}\\.){{0,1}}){TRINO_ID_PATTERN}$")
 
 TRINO_IDENTIFIER_PATTERN = re.compile(TRINO_ID_PATTERN)
 
 TRINO_ALIASED_PATTERN = re.compile(f".+\\s+(?:AS\\s+)?{TRINO_ID_PATTERN}")
 
@@ -76,15 +78,20 @@
 
 def validate_identifier_name(name: str):
     if not TRINO_IDENTIFIER_PATTERN.match(name):
         raise PyStarburstClientExceptionMessages.GENERAL_INVALID_OBJECT_NAME(name)
 
 
 def validate_object_name(name: str):
-    if not TRINO_OBJECT_RE_PATTERN.match(name):
+    matched = TRINO_OBJECT_RE_PATTERN.match(name)
+    if matched:
+        matched_groups = matched.groups()
+        matched_groups = [g for g in matched_groups if g is not None]
+        return matched_groups
+    else:
         raise PyStarburstClientExceptionMessages.GENERAL_INVALID_OBJECT_NAME(name)
 
 
 def validate_catalog_schema_name(name: str):
     if not TRINO_CATALOG_SCHEMA_PATTERN.match(name):
         raise PyStarburstClientExceptionMessages.GENERAL_INVALID_OBJECT_NAME(name)
```

## pystarburst/column.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Iterable, Optional, Union
 
 import pystarburst
 from pystarburst._internal.analyzer.analyzer_utils import quote_name
 from pystarburst._internal.analyzer.expression.binary import (
@@ -106,38 +106,42 @@
 
 
 class Column:
     """Represents a column or an expression in a :class:`DataFrame`.
 
     To access a Column object that refers a column in a :class:`DataFrame`, you can:
 
-        - Use the column name.
-        - Use the :func:`functions.col` function.
-        - Use the :func:`DataFrame.col` method.
-        - Use the index operator ``[]`` on a dataframe object with a column name.
-        - Use the dot operator ``.`` on a dataframe object with a column name.
-
-        >>> from pystarburst.functions import col
-        >>> df = session.create_dataframe([["John", 1], ["Mike", 11]], schema=["name", "age"])
-        >>> df.select("name").collect()
-        [Row(NAME='John'), Row(NAME='Mike')]
-        >>> df.select(col("name")).collect()
-        [Row(NAME='John'), Row(NAME='Mike')]
-        >>> df.select(df.col("name")).collect()
-        [Row(NAME='John'), Row(NAME='Mike')]
-        >>> df.select(df["name"]).collect()
-        [Row(NAME='John'), Row(NAME='Mike')]
-        >>> df.select(df.name).collect()
-        [Row(NAME='John'), Row(NAME='Mike')]
-
-        Trino object identifiers are case-insensitive.
-
-        The returned column names after a DataFrame is evaluated follow the case-sensitivity rules too.
-        The above ``df`` was created with column name "name" while the returned column name after ``collect()`` was called became "NAME".
-        It's because the column is regarded as ignore-case so the Trino cluster returns the upper case.
+    - Use the column name.
+    - Use the :func:`functions.col` function.
+    - Use the :func:`DataFrame.col` method.
+    - Use the index operator ``[]`` on a dataframe object with a column name.
+    - Use the dot operator ``.`` on a dataframe object with a column name.
+
+      >>> from pystarburst.functions import col
+      >>> df = session.create_dataframe([["John", 1], ["Mike", 11]], schema=["name", "age"])
+      >>> df.select("name").collect()
+      [Row(NAME='John'), Row(NAME='Mike')]
+      <BLANKLINE>
+      >>> df.select(col("name")).collect()
+      [Row(NAME='John'), Row(NAME='Mike')]
+      <BLANKLINE>
+      >>> df.select(df.col("name")).collect()
+      [Row(NAME='John'), Row(NAME='Mike')]
+      <BLANKLINE>
+      >>> df.select(df["name"]).collect()
+      [Row(NAME='John'), Row(NAME='Mike')]
+      <BLANKLINE>
+      >>> df.select(df.name).collect()
+      [Row(NAME='John'), Row(NAME='Mike')]
+
+      Trino object identifiers are case-insensitive.
+
+      The returned column names after a DataFrame is evaluated follow the case-sensitivity rules too.
+      The above ``df`` was created with column name "name" while the returned column name after ``collect()`` was called became "NAME".
+      It's because the column is regarded as ignore-case so the Trino cluster returns the upper case.
 
     To create a Column object that represents a constant value, use :func:`functions.lit`:
 
         >>> from pystarburst.functions import lit
         >>> df.select(col("name"), lit("const value").alias("literal_column")).collect()
         [Row(NAME='John', LITERAL_COLUMN='const value'), Row(NAME='Mike', LITERAL_COLUMN='const value')]
 
@@ -154,32 +158,32 @@
     ``*``, ``/``, ``%``                             Multiply, divide, remainder
     ``+``, ``-``                                    Plus, minus
     ``&``                                           And
     ``|``                                           Or
     ``==``, ``!=``, ``<``, ``<=``, ``>``, ``>=``    Equal to, not equal to, less than, less than or equal to, greater than, greater than or equal to
     ==============================================  ==============================================
 
-        The following examples demonstrate how to use Column objects in expressions:
+    The following examples demonstrate how to use Column objects in expressions:
 
-            >>> df = session.create_dataframe([[20, 5], [1, 2]], schema=["a", "b"])
-            >>> df.filter((col("a") == 20) | (col("b") <= 10)).collect()  # use parentheses before and after the | operator.
-            [Row(A=20, B=5), Row(A=1, B=2)]
-            >>> df.filter((df["a"] + df.b) < 10).collect()
-            [Row(A=1, B=2)]
-            >>> df.select((col("b") * 10).alias("c")).collect()
-            [Row(C=50), Row(C=20)]
-
-        When you use ``|``, ``&``, and ``~`` as logical operators on columns, you must always enclose column expressions
-        with parentheses as illustrated in the above example, because their order precedence is higher than ``==``, ``<``, etc.
-
-        Do not use ``and``, ``or``, and ``not`` logical operators on column objects, for instance, ``(df.col1 > 1) and (df.col2 > 2)`` is wrong.
-        The reason is Python doesn't have a magic method, or dunder method for them.
-        It will raise an error and tell you to use ``|``, ``&`` or ``~``, for which Python has magic methods.
-        A side effect is ``if column:`` will raise an error because it has a hidden call to ``bool(a_column)``, like using the ``and`` operator.
-        Use ``if a_column is None:`` instead.
+    >>> df = session.create_dataframe([[20, 5], [1, 2]], schema=["a", "b"])
+    >>> df.filter((col("a") == 20) | (col("b") <= 10)).collect()  # use parentheses before and after the | operator.
+    [Row(A=20, B=5), Row(A=1, B=2)]
+    >>> df.filter((df["a"] + df.b) < 10).collect()
+    [Row(A=1, B=2)]
+    >>> df.select((col("b") * 10).alias("c")).collect()
+    [Row(C=50), Row(C=20)]
+
+    When you use ``|``, ``&``, and ``~`` as logical operators on columns, you must always enclose column expressions
+    with parentheses as illustrated in the above example, because their order precedence is higher than ``==``, ``<``, etc.
+
+    Do not use ``and``, ``or``, and ``not`` logical operators on column objects, for instance, ``(df.col1 > 1) and (df.col2 > 2)`` is wrong.
+    The reason is Python doesn't have a magic method, or dunder method for them.
+    It will raise an error and tell you to use ``|``, ``&`` or ``~``, for which Python has magic methods.
+    A side effect is ``if column:`` will raise an error because it has a hidden call to ``bool(a_column)``, like using the ``and`` operator.
+    Use ``if a_column is None:`` instead.
 
     To access elements of a semi-structured Object and Array, use ``[]`` on a Column object:
 
         >>> from pystarburst.types import StringType, IntegerType
         >>> df_with_semi_data = session.create_dataframe([[{"k1": "v1", "k2": "v2"}, ["a0", 1, "a2"]]], schema=["object_column", "array_column"])
         >>> df_with_semi_data.select(df_with_semi_data["object_column"]["k1"].alias("k1_value"), df_with_semi_data["array_column"][0].alias("a0_value"), df_with_semi_data["array_column"][1].alias("a1_value")).collect()
         [Row(K1_VALUE='"v1"', A0_VALUE='"a0"', A1_VALUE='1')]
@@ -314,33 +318,33 @@
 
         For example, the following code returns a DataFrame that contains the rows where
         the column "a" contains the value 1, 2, or 3. This is equivalent to
         ``SELECT * FROM table WHERE a IN (1, 2, 3)``.
 
         :meth:`isin` is an alias for :meth:`in_`.
 
-        Examples::
+        Args:
+            vals: The values, or a :class:`DataFrame` instance to use to check for membership against this column.
+
+        Examples:
 
             >>> from pystarburst.functions import lit
             >>> df = session.create_dataframe([[1, "x"], [2, "y"] ,[4, "z"]], schema=["a", "b"])
             >>> # Basic example
             >>> df.filter(df["a"].in_(lit(1), lit(2), lit(3))).collect()
             [Row(A=1, B='x'), Row(A=2, B='y')]
-
+            <BLANKLINE>
             >>> # Check in membership for a DataFrame that has a single column
             >>> df_for_in = session.create_dataframe([[1], [2] ,[3]], schema=["col1"])
             >>> df.filter(df["a"].in_(df_for_in)).sort(df["a"].asc()).collect()
             [Row(A=1, B='x'), Row(A=2, B='y')]
-
+            <BLANKLINE>
             >>> # Use in with a select method call
             >>> df.select(df["a"].in_(lit(1), lit(2), lit(3)).alias("is_in_list")).collect()
             [Row(IS_IN_LIST=True), Row(IS_IN_LIST=True), Row(IS_IN_LIST=False)]
-
-        Args:
-            vals: The values, or a :class:`DataFrame` instance to use to check for membership against this column.
         """
         cols = parse_positional_args_to_list(*vals)
         cols = [_to_col_if_lit(col, "in_") for col in cols]
 
         column_count = len(self._expression.expressions) if isinstance(self._expression, MultipleExpression) else 1
 
         def value_mapper(value):
@@ -534,15 +538,15 @@
         """Returns true if this Column contains the specified regular expression.
 
         Args:
             pattern: A :class:`Column` or a ``str`` that indicates the pattern.
                 A ``str`` will be interpreted as a literal value instead of a column name.
 
         For details, see the Trino documentation on
-        `regular expressions <https://trino.io/docs/current/functions/regexp.html#regexp_like>`.
+        `regular expressions <https://trino.io/docs/current/functions/regexp.html#regexp_like>`_.
 
         :meth:`rlike` is an alias of :meth:`regexp`.
         :meth:`regexp_like` is an alias of :meth:`regexp`.
 
         """
         return Column(
             RegExpLike(
```

## pystarburst/context.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 """Context module for pystarburst."""
 import pystarburst
 
 
 def get_active_session() -> "pystarburst.Session":
```

## pystarburst/dataframe.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 import itertools
 import re
 from functools import cached_property
 from logging import getLogger
 from typing import (
@@ -43,14 +43,15 @@
 from pystarburst._internal.analyzer.plan.logical_plan.binary import (
     UnionAll,
     UsingJoin,
     create_join_type,
 )
 from pystarburst._internal.analyzer.plan.logical_plan.unary import (
     CreateView,
+    Explode,
     Filter,
     Limit,
     Project,
     Sample,
     Sort,
     Stack,
     Unpivot,
@@ -416,15 +417,17 @@
     def to_local_iterator(self, *, statement_properties: Optional[Dict[str, str]] = None) -> Iterator[Row]:
         """Executes the query representing this DataFrame and returns an iterator
         of :class:`Row` objects that you can use to retrieve the results.
 
         Unlike :meth:`collect`, this method does not load all data into memory
         at once.
 
-        Example::
+        :meth:`toLocalIterator` is an alias of :meth:`to_local_iterator`.
+
+        Examples:
 
             >>> df = session.table("prices")
             >>> for row in df.to_local_iterator():
             ...     print(row)
             Row(PRODUCT_ID='id1', AMOUNT=Decimal('10.00'))
             Row(PRODUCT_ID='id2', AMOUNT=Decimal('20.00'))
 
@@ -441,21 +444,22 @@
     def to_df(self, *names: Union[str, Iterable[str]]) -> "DataFrame":
         """
         Creates a new DataFrame containing columns with the specified names.
 
         The number of column names that you pass in must match the number of columns in the existing
         DataFrame.
 
-        Examples::
-
-            >>> df1 = session.range(1, 10, 2).to_df("col1")
-            >>> df2 = session.range(1, 10, 2).to_df(["col1"])
+        :meth:`toDF` is an alias of :meth:`to_df`.
 
         Args:
             names: list of new column names
+
+        Examples:
+            >>> df1 = session.range(1, 10, 2).to_df("col1")
+            >>> df2 = session.range(1, 10, 2).to_df(["col1"])
         """
         col_names = parse_positional_args_to_list(*names)
         if not all(isinstance(n, str) for n in col_names):
             raise TypeError("Invalid input type in to_df(), expected str or a list of strs.")
 
         if len(self._output) != len(col_names):
             raise ValueError(
@@ -470,17 +474,16 @@
             new_cols.append(Column(attr).alias(name))
         return self.select(new_cols)
 
     def to_pandas(self):
         """
         Returns a Pandas DataFrame using the results from the PyStarburst DataFrame.
 
-        Examples::
-
-        >>> df = session.create_dataframe([[1, "a", 1.0], [2, "b", 2.0]]).to_df("id", "value1", "value2").to_pandas()
+        Examples:
+            >>> df = session.create_dataframe([[1, "a", 1.0], [2, "b", 2.0]]).to_df("id", "value1", "value2").to_pandas()
         """
 
         if is_pandas_installed:
             import pandas as pd
 
         return pd.DataFrame(self.collect())
 
@@ -507,14 +510,15 @@
         >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()
         +-----+---+
         |float|int|
         +-----+---+
         |    1|  1|
         |    2|  2|
         +-----+---+
+        <BLANKLINE>
         >>> def add_n(input_df, n):
         ...     return input_df.select([(col(col_name) + n).alias(col_name)
         ...                             for col_name in input_df.columns])
         >>> df.transform(add_n, 1).transform(add_n, n=10).toDF("int", "float").show()
         +---+-----+
         |int|float|
         +---+-----+
@@ -579,32 +583,26 @@
     ) -> "DataFrame":
         """Returns a new DataFrame with the specified Column expressions as output
         (similar to SELECT in SQL). Only the Columns specified as arguments will be
         present in the resulting DataFrame.
 
         You can use any :class:`Column` expression or strings for named columns.
 
-        Example 1::
+        Args:
+            *cols: A :class:`Column`, :class:`str`, or a list of those.
+
+        Examples:
             >>> df = session.create_dataframe([[1, "some string value", 3, 4]], schema=["col1", "col2", "col3", "col4"])
             >>> df_selected = df.select(col("col1"), col("col2").substr(0, 10), df["col3"] + df["col4"])
-
-        Example 2::
-
+            <BLANKLINE>
             >>> df_selected = df.select("col1", "col2", "col3")
-
-        Example 3::
-
+            <BLANKLINE>
             >>> df_selected = df.select(["col1", "col2", "col3"])
-
-        Example 4::
-
+            <BLANKLINE>
             >>> df_selected = df.select(df["col1"], df.col2, df.col("col3"))
-
-        Args:
-            *cols: A :class:`Column`, :class:`str`, or a list of those.
         """
         exprs = parse_positional_args_to_list(*cols)
         if not exprs:
             raise ValueError("The input of select() cannot be empty")
 
         names = []
         table_func = None
@@ -628,15 +626,15 @@
         and :func:`functions.sql_expr`.
 
         :func:`selectExpr` is an alias of :func:`select_expr`.
 
         Args:
             exprs: The SQL expressions.
 
-        Examples::
+        Examples:
 
             >>> df = session.create_dataframe([-1, 2, 3], schema=["a"])  # with one pair of [], the dataframe has a single column and 3 rows.
             >>> df.select_expr("abs(a)", "a + 2", "cast(a as string)").show()
             --------------------------------------------
             |"ABS(A)"  |"A + 2"  |"CAST(A AS STRING)"  |
             --------------------------------------------
             |1         |1        |-1                   |
@@ -657,32 +655,30 @@
         """Returns a new DataFrame that excludes the columns with the specified names
         from the output.
 
         This is functionally equivalent to calling :func:`select()` and passing in all
         columns except the ones to exclude. This is a no-op if schema does not contain
         the given column name(s).
 
-        Example::
+        Args:
+            *cols: the columns to exclude, as :class:`str`, :class:`Column` or a list
+                of those.
+
+        Raises:
+            :class:`PyStarburstClientException`: if the resulting :class:`DataFrame`
+                contains no output columns.
 
+        Examples:
             >>> df = session.create_dataframe([[1, 2, 3]], schema=["a", "b", "c"])
             >>> df.drop("a", "b").show()
             -------
             |"C"  |
             -------
             |3    |
             -------
-            <BLANKLINE>
-
-        Args:
-            *cols: the columns to exclude, as :class:`str`, :class:`Column` or a list
-                of those.
-
-        Raises:
-            :class:`PyStarburstClientException`: if the resulting :class:`DataFrame`
-                contains no output columns.
         """
         # an empty list should be accept, as dropping nothing
         if not cols:
             raise ValueError("The input of drop() cannot be empty")
         exprs = parse_positional_args_to_list(*cols)
 
         names = []
@@ -704,29 +700,28 @@
         else:
             return self.select(list(keep_col_names))
 
     def filter(self, expr: ColumnOrSqlExpr) -> "DataFrame":
         """Filters rows based on the specified conditional expression (similar to WHERE
         in SQL).
 
-        Examples::
+        Args:
+            expr: a :class:`Column` expression or SQL text.
+
+        :meth:`where` is an alias of :meth:`filter`.
 
+        Examples:
             >>> df = session.create_dataframe([[1, 2], [3, 4]], schema=["A", "B"])
             >>> df_filtered = df.filter((col("A") > 1) & (col("B") < 100))  # Must use parenthesis before and after operator &.
 
             >>> # The following two result in the same SQL query:
             >>> df.filter(col("a") > 1).collect()
             [Row(A=3, B=4)]
             >>> df.filter("a > 1").collect()  # use SQL expression
             [Row(A=3, B=4)]
-
-        Args:
-            expr: a :class:`Column` expression or SQL text.
-
-        :meth:`where` is an alias of :meth:`filter`.
         """
         return self._with_plan(
             Filter(
                 type_coercion_mode=self._session._type_coercion_mode,
                 condition=_to_col_if_sql_expr(expr, "filter/where")._expression,
                 child=self._plan,
             )
@@ -735,57 +730,54 @@
     def sort(
         self,
         *cols: Union[ColumnOrName, Iterable[ColumnOrName]],
         ascending: Optional[Union[bool, int, List[Union[bool, int]]]] = None,
     ) -> "DataFrame":
         """Sorts a DataFrame by the specified expressions (similar to ORDER BY in SQL).
 
-        Examples::
+        :meth:`orderBy` and :meth:`order_by` are aliases of :meth:`sort`.
 
-            >>> from pystarburst.functions import col
+        Args:
+            *cols: A column name as :class:`str` or :class:`Column`, or a list of
+             columns to sort by.
+            ascending: A :class:`bool` or a list of :class:`bool` for sorting the
+             DataFrame, where ``True`` sorts a column in ascending order and ``False``
+             sorts a column in descending order . If you specify a list of multiple
+             sort orders, the length of the list must equal the number of columns.
 
+        Examples:
+            >>> from pystarburst.functions import col
             >>> df = session.create_dataframe([[1, 2], [3, 4], [1, 4]], schema=["A", "B"])
             >>> df.sort(col("A"), col("B").asc()).show()
             -------------
             |"A"  |"B"  |
             -------------
             |1    |2    |
             |1    |4    |
             |3    |4    |
             -------------
             <BLANKLINE>
-
             >>> df.sort(col("a"), ascending=False).show()
             -------------
             |"A"  |"B"  |
             -------------
             |3    |4    |
             |1    |2    |
             |1    |4    |
             -------------
             <BLANKLINE>
-
             >>> # The values from the list overwrite the column ordering.
             >>> df.sort(["a", col("b").desc()], ascending=[1, 1]).show()
             -------------
             |"A"  |"B"  |
             -------------
             |1    |2    |
             |1    |4    |
             |3    |4    |
             -------------
-            <BLANKLINE>
-
-        Args:
-            *cols: A column name as :class:`str` or :class:`Column`, or a list of
-             columns to sort by.
-            ascending: A :class:`bool` or a list of :class:`bool` for sorting the
-             DataFrame, where ``True`` sorts a column in ascending order and ``False``
-             sorts a column in descending order . If you specify a list of multiple
-             sort orders, the length of the list must equal the number of columns.
         """
         if not cols:
             raise ValueError("sort() needs at least one sort expression.")
         exprs = self._convert_cols_to_exprs("sort()", *cols)
         if not exprs:
             raise ValueError("sort() needs at least one sort expression.")
         orders = []
@@ -825,49 +817,44 @@
     ) -> "DataFrame":
         """Aggregate the data in the DataFrame. Use this method if you don't need to
         group the data (:func:`group_by`).
 
         Args:
             exprs: A variable length arguments list where every element is
 
-                - A Column object
-                - A tuple where the first element is a column object or a column name and the second element is the name of the aggregate function
-                - A list of the above
-
-                or a ``dict`` maps column names to aggregate function names.
-
-        Examples::
+                - a Column object
+                - a tuple where the first element is a column object or a column name and the second element is the name of the aggregate function
+                - a list of the above
+                - a ``dict`` maps column names to aggregate function names.
 
+        Examples:
             >>> from pystarburst.functions import col, stddev, stddev_pop
-
+            <BLANKLINE>
             >>> df = session.create_dataframe([[1, 2], [3, 4], [1, 4]], schema=["A", "B"])
             >>> df.agg(stddev(col("a"))).show()
             ----------------------
             |"STDDEV(A)"         |
             ----------------------
             |1.1547003940416753  |
             ----------------------
             <BLANKLINE>
-
             >>> df.agg(stddev(col("a")), stddev_pop(col("a"))).show()
             -------------------------------------------
             |"STDDEV(A)"         |"STDDEV_POP(A)"     |
             -------------------------------------------
             |1.1547003940416753  |0.9428091005076267  |
             -------------------------------------------
             <BLANKLINE>
-
             >>> df.agg(("a", "min"), ("b", "max")).show()
             -----------------------
             |"MIN(A)"  |"MAX(B)"  |
             -----------------------
             |1         |4         |
             -----------------------
             <BLANKLINE>
-
             >>> df.agg({"a": "count", "b": "sum"}).show()
             -------------------------
             |"COUNT(A)"  |"SUM(B)"  |
             -------------------------
             |3           |10        |
             -------------------------
             <BLANKLINE>
@@ -906,35 +893,41 @@
     ) -> "pystarburst.RelationalGroupedDataFrame":
         """Groups rows by the columns specified by expressions (similar to GROUP BY in
         SQL).
 
         This method returns a :class:`RelationalGroupedDataFrame` that you can use to
         perform aggregations on each group of data.
 
+        :meth:`groupBy` is an alias of :meth:`group_by`.
+
         Args:
             *cols: The columns to group by.
 
         Valid inputs are:
 
-            - Empty input
-            - One or multiple :class:`Column` object(s) or column name(s) (:class:`str`)
-            - A list of :class:`Column` objects or column names (:class:`str`)
+        - Empty input
+        - One or multiple :class:`Column` object(s) or column name(s) (:class:`str`)
+        - A list of :class:`Column` objects or column names (:class:`str`)
 
         Examples:
 
             >>> from pystarburst.functions import col, lit, sum as sum_, max as max_
             >>> df = session.create_dataframe([(1, 1),(1, 2),(2, 1),(2, 2),(3, 1),(3, 2)], schema=["a", "b"])
             >>> df.group_by().agg(sum_("b")).collect()
             [Row(SUM(B)=9)]
+            <BLANKLINE>
             >>> df.group_by("a").agg(sum_("b")).collect()
             [Row(A=1, SUM(B)=3), Row(A=2, SUM(B)=3), Row(A=3, SUM(B)=3)]
+            <BLANKLINE>
             >>> df.group_by(["a", lit("pystarburst")]).agg(sum_("b")).collect()
             [Row(A=1, LITERAL()='pystarburst', SUM(B)=3), Row(A=2, LITERAL()='snow', SUM(B)=3), Row(A=3, LITERAL()='snow', SUM(B)=3)]
+            <BLANKLINE>
             >>> df.group_by("a").agg((col("*"), "count"), max_("b")).collect()
             [Row(A=1, COUNT(LITERAL())=2, MAX(B)=2), Row(A=2, COUNT(LITERAL())=2, MAX(B)=2), Row(A=3, COUNT(LITERAL())=2, MAX(B)=2)]
+            <BLANKLINE>
             >>> df.group_by("a").function("avg")("b").collect()
             [Row(A=1, AVG(B)=Decimal('1.500000')), Row(A=2, AVG(B)=Decimal('1.500000')), Row(A=3, AVG(B)=Decimal('1.500000'))]
         """
         grouping_exprs = self._convert_cols_to_exprs("group_by()", *cols)
         return pystarburst.RelationalGroupedDataFrame(
             self,
             grouping_exprs,
@@ -955,31 +948,33 @@
         GROUP BY GROUPING SETS is an extension of the GROUP BY clause
         that allows computing multiple GROUP BY clauses in a single statement.
         The group set is a set of dimension columns.
 
         GROUP BY GROUPING SETS is equivalent to the UNION of two or
         more GROUP BY operations in the same result set.
 
+        :meth:`groupByGroupingSets` is an alias of :meth:`group_by_grouping_sets`.
 
-        Examples::
+        Args:
+            grouping_sets: The list of :class:`GroupingSets` to group by.
 
+        Examples:
             >>> from pystarburst import GroupingSets
             >>> df = session.create_dataframe([[1, 2, 10], [3, 4, 20], [1, 4, 30]], schema=["A", "B", "C"])
             >>> df.group_by_grouping_sets(GroupingSets([col("a")])).count().collect()
             [Row(A=1, COUNT=2), Row(A=3, COUNT=1)]
+            <BLANKLINE>
             >>> df.group_by_grouping_sets(GroupingSets(col("a"))).count().collect()
             [Row(A=1, COUNT=2), Row(A=3, COUNT=1)]
+            <BLANKLINE>
             >>> df.group_by_grouping_sets(GroupingSets([col("a")], [col("b")])).count().collect()
             [Row(A=1, B=None, COUNT=2), Row(A=3, B=None, COUNT=1), Row(A=None, B=2, COUNT=1), Row(A=None, B=4, COUNT=2)]
+            <BLANKLINE>
             >>> df.group_by_grouping_sets(GroupingSets([col("a"), col("b")], [col("c")])).count().collect()
             [Row(A=None, B=None, C=10, COUNT=1), Row(A=None, B=None, C=20, COUNT=1), Row(A=None, B=None, C=30, COUNT=1), Row(A=1, B=2, C=None, COUNT=1), Row(A=3, B=4, C=None, COUNT=1), Row(A=1, B=4, C=None, COUNT=1)]
-
-
-        Args:
-            grouping_sets: The list of :class:`GroupingSets` to group by.
         """
         return pystarburst.RelationalGroupedDataFrame(
             self,
             [gs._to_expression for gs in parse_positional_args_to_list(*grouping_sets)],
             pystarburst.relational_grouped_dataframe._GroupByType(),
         )
 
@@ -1045,16 +1040,19 @@
     ) -> "pystarburst.RelationalGroupedDataFrame":
         """Rotates this DataFrame by turning the unique values from one column in the input
         expression into multiple columns and aggregating results where required on any
         remaining column values.
 
         Only one aggregate is supported with pivot.
 
-        Example::
+        Args:
+            pivot_col: The column or name of the column to use.
+            values: A list of values in the column.
 
+        Examples:
             >>> create_result = session.sql('''create table monthly_sales(empid int, amount int, month varchar)
             ... as select * from values
             ... (1, 10000, 'JAN'),
             ... (1, 400, 'JAN'),
             ... (2, 4500, 'JAN'),
             ... (2, 35000, 'JAN'),
             ... (1, 5000, 'FEB'),
@@ -1064,19 +1062,14 @@
             >>> df.pivot("month", ['JAN', 'FEB']).sum("amount").show()
             -------------------------------
             |"EMPID"  |"'JAN'"  |"'FEB'"  |
             -------------------------------
             |1        |10400    |8000     |
             |2        |39500    |200      |
             -------------------------------
-            <BLANKLINE>
-
-        Args:
-            pivot_col: The column or name of the column to use.
-            values: A list of values in the column.
         """
         pc = self._convert_cols_to_exprs("pivot()", pivot_col)
         value_exprs = [v._expression if isinstance(v, Column) else Literal(value=v) for v in values]
         return pystarburst.RelationalGroupedDataFrame(
             self,
             [],
             pystarburst.relational_grouped_dataframe._PivotType(pc[0], value_exprs),
@@ -1088,21 +1081,23 @@
         unpivot_column_list: List[ColumnOrName],
         name_column: str,
         value_column: str,
     ) -> "DataFrame":
         """Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set.
         Note that UNPIVOT is not exactly the reverse of PIVOT as it cannot undo aggregations made by PIVOT.
 
+        :meth:`melt` is an alias of :meth:`unpivot`.
+
         Args:
             ids_column_list: The names of the columns in the source table or subequery that will be used as identifiers.
             unpivot_column_list: The names of the columns in the source table or subequery that will be narrowed into a single pivot column. The column names will populate ``name_column``, and the column values will populate ``value_column``.
             name_column: The name to assign to the generated column that will be populated with the names of the columns in the column list.
             value_column: The name to assign to the generated column that will be populated with the values from the columns in the column list.
 
-        Example::
+        Examples:
 
             >>> df = session.create_dataframe([
             ...     (1, 'electronics', 100, 200),
             ...     (2, 'clothes', 100, 300)
             ... ], schema=["empid", "dept", "jan", "feb"])
             >>> df = df.unpivot(["empid", "dept"], ["jan", "feb"], "month", "sales").sort("empid")
             >>> df.show()
@@ -1131,30 +1126,31 @@
 
     def stack(
         self,
         row_count: Column,
         *cols: ColumnOrName,
         ids_column_list: List[ColumnOrName] = [],
     ) -> "DataFrame":
-        """Separates col1, …, colk into n rows. Uses column names _1, _2, etc. by default unless specified otherwise.
+        """Separates col1, …, colk into n rows. Uses column names ``_1``, ``_2``, etc. by default unless specified otherwise.
 
         Args:
             row_count: number of rows to be separated
             cols: Input elements to be separated
             ids_column_list: (Keyword-only argument) The names of the columns in the source table or subequery that will be used as identifiers.
 
-        Example::
+        Examples:
             >>> df = session.createDataFrame([(1, 2, 3)], ["a", "b", "c"])
             >>> df.stack(lit(2), df.a, df.b, df.c).show()
             ---------------
             |"_1"  |"_2"  |
             ---------------
             |1     |2     |
             |3     |NULL  |
             ---------------
+            <BLANKLINE>
             >>> df.stack(lit(2), df.a, df.b, df.c, ids_column_list=["a", "b", "c"]).show()
             ---------------------------------
             |"a"  |"b"  |"c"  |"_4"  |"_5"  |
             ---------------------------------
             |1    |2    |3    |1     |2     |
             |1    |2    |3    |3     |NULL  |
             ---------------------------------
@@ -1168,25 +1164,246 @@
             row_count=row_count_expr,
             stack_column_list=stack_column_exprs,
             child=self._plan,
         )
 
         return self._with_plan(stack_plan)
 
+    def explode(
+        self,
+        explode_col: ColumnOrName,
+    ) -> "DataFrame":
+        """Adds new column(s) to DataFrame with expanded `ARRAY` or `MAP`, creating a new row for each element in the given array or map.
+        Uses the default column name `col` for elements in the array and `key` and `value` for elements in the map.
+
+        Args:
+            explode_col: target column to work on.
+
+        Examples:
+            >>> df = session.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={"a": "b"}),Row(a=2, intlist=[4,5,6], mapfield={"a": "b", "c": "d"})])
+            ------------------------------------------
+            |"a"  |"intlist"  |"mapfield"            |
+            ------------------------------------------
+            |1    |[1, 2, 3]  |{'a': 'b'}            |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |
+            ------------------------------------------
+            <BLANKLINE>
+            >>> df.explode(df.intlist)
+            --------------------------------------------------
+            |"a"  |"intlist"  |"mapfield"            |"col"  |
+            --------------------------------------------------
+            |1    |[1, 2, 3]  |{'a': 'b'}            |1      |
+            |1    |[1, 2, 3]  |{'a': 'b'}            |2      |
+            |1    |[1, 2, 3]  |{'a': 'b'}            |3      |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |4      |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |5      |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |6      |
+            --------------------------------------------------
+            <BLANKLINE>
+            >>> df.explode(df.mapfield)
+            ------------------------------------------------------------
+            |"a"  |"intlist"  |"mapfield"            |"key"  |"value"  |
+            ------------------------------------------------------------
+            |1    |[1, 2, 3]  |{'a': 'b'}            |a      |b        |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |a      |b        |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |c      |d        |
+            ------------------------------------------------------------
+        """
+        return self._explode(explode_col, False, False, False)
+
+    def explode_outer(
+        self,
+        explode_col: ColumnOrName,
+    ) -> "DataFrame":
+        """Adds new column(s) to DataFrame with expanded `ARRAY` or `MAP`, creating a new row for each element in the given array or map.
+        Unlike explode, if the array/map is null or empty then null is produced.
+        Uses the default column name `col` for elements in the array and `key` and `value` for elements in the map.
+
+        Args:
+            explode_col: target column to work on.
+
+        Examples:
+            >>> df = session.createDataFrame(
+            >>>     [(1, ["foo", "bar"], {"x": 1.0}), (2, [], {}), (3, None, None)],
+            >>>     ["id", "an_array", "a_map"])
+            --------------------------------------
+            |"id"  |"an_array"      |"a_map"     |
+            --------------------------------------
+            |1     |['foo', 'bar']  |{'x': 1.0}  |
+            |2     |[]              |{}          |
+            |3     |NULL            |NULL        |
+            --------------------------------------
+            <BLANKLINE>
+            >>> df.explode_outer(df.an_array).show()
+            ----------------------------------------------
+            |"id"  |"an_array"      |"a_map"     |"col"  |
+            ----------------------------------------------
+            |1     |['foo', 'bar']  |{'x': 1.0}  |foo    |
+            |1     |['foo', 'bar']  |{'x': 1.0}  |bar    |
+            |2     |[]              |{}          |NULL   |
+            |3     |NULL            |NULL        |NULL   |
+            ----------------------------------------------
+            <BLANKLINE>
+            >>> df.explode_outer(df.a_map).show()
+            --------------------------------------------------------
+            |"id"  |"an_array"      |"a_map"     |"key"  |"value"  |
+            --------------------------------------------------------
+            |1     |['foo', 'bar']  |{'x': 1.0}  |x      |1.0      |
+            |2     |[]              |{}          |NULL   |NULL     |
+            |3     |NULL            |NULL        |NULL   |NULL     |
+            --------------------------------------------------------
+        """
+        return self._explode(explode_col, False, False, True)
+
+    def posexplode(
+        self,
+        explode_col: ColumnOrName,
+    ) -> "DataFrame":
+        """Adds new columns to DataFrame with expanded `ARRAY` or `MAP`, creating a new row for each element with position in the given array or map. The position starts at 1.
+        Uses the default column name `pos` for position, `col` for elements in the array and `key` and `value` for elements in the map.
+
+        Args:
+            explode_col: target column to work on.
+
+        Examples:
+            >>> df = session.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={"a": "b"}),Row(a=2, intlist=[4,5,6], mapfield={"a": "b", "c": "d"})])
+            ------------------------------------------
+            |"a"  |"intlist"  |"mapfield"            |
+            ------------------------------------------
+            |1    |[1, 2, 3]  |{'a': 'b'}            |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |
+            ------------------------------------------
+            <BLANKLINE>
+            >>> df.posexplode(df.intlist)
+            ----------------------------------------------------------
+            |"a"  |"intlist"  |"mapfield"            |"pos"  |"col"  |
+            ----------------------------------------------------------
+            |1    |[1, 2, 3]  |{'a': 'b'}            |1      |1      |
+            |1    |[1, 2, 3]  |{'a': 'b'}            |2      |2      |
+            |1    |[1, 2, 3]  |{'a': 'b'}            |3      |3      |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |1      |4      |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |2      |5      |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |3      |6      |
+            ----------------------------------------------------------
+            <BLANKLINE>
+            >>> df.posexplode(df.mapfield)
+            --------------------------------------------------------------------
+            |"a"  |"intlist"  |"mapfield"            |"pos"  |"key"  |"value"  |
+            --------------------------------------------------------------------
+            |1    |[1, 2, 3]  |{'a': 'b'}            |1      |a      |b        |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |1      |a      |b        |
+            |2    |[4, 5, 6]  |{'a': 'b', 'c': 'd'}  |2      |c      |d        |
+            --------------------------------------------------------------------
+        """
+        return self._explode(explode_col, True, False, False)
+
+    def posexplode_outer(
+        self,
+        explode_col: ColumnOrName,
+    ) -> "DataFrame":
+        """Adds new columns to DataFrame with expanded `ARRAY` or `MAP`, creating a new row for each element with position in the given array or map. The position starts at 1.
+        Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.
+        Uses the default column name `pos` for position, `col` for elements in the array and `key` and `value` for elements in the map.
+
+        Args:
+            explode_col: target column to work on.
+
+        Examples:
+            >>> df = session.createDataFrame(
+            >>>     [(1, ["foo", "bar"], {"x": 1.0}), (2, [], {}), (3, None, None)],
+            >>>     ["id", "an_array", "a_map"])
+            --------------------------------------
+            |"id"  |"an_array"      |"a_map"     |
+            --------------------------------------
+            |1     |['foo', 'bar']  |{'x': 1.0}  |
+            |2     |[]              |{}          |
+            |3     |NULL            |NULL        |
+            --------------------------------------
+            <BLANKLINE>
+            >>> df.posexplode_outer(df.an_array).show()
+            ------------------------------------------------------
+            |"id"  |"an_array"      |"a_map"     |"pos"  |"col"  |
+            ------------------------------------------------------
+            |1     |['foo', 'bar']  |{'x': 1.0}  |1      |foo    |
+            |1     |['foo', 'bar']  |{'x': 1.0}  |2      |bar    |
+            |2     |[]              |{}          |NULL   |NULL   |
+            |3     |NULL            |NULL        |NULL   |NULL   |
+            ------------------------------------------------------
+            <BLANKLINE>
+            >>> df.posexplode_outer(df.a_map).show()
+            ----------------------------------------------------------------
+            |"id"  |"an_array"      |"a_map"     |"pos"  |"key"  |"value"  |
+            ----------------------------------------------------------------
+            |1     |['foo', 'bar']  |{'x': 1.0}  |1      |x      |1.0      |
+            |2     |[]              |{}          |NULL   |NULL   |NULL     |
+            |3     |NULL            |NULL        |NULL   |NULL   |NULL     |
+            ----------------------------------------------------------------
+        """
+        return self._explode(explode_col, True, False, True)
+
+    def inline(
+        self,
+        explode_col: ColumnOrName,
+    ) -> "DataFrame":
+        """Explodes an array of structs into a table.
+
+        Args:
+            explode_col: input column of values to explode.
+
+        Examples:
+        # TODO: add example after adding support for creating structs with `struct` function
+        """
+        return self._explode(explode_col, False, True, False)
+
+    def inline_outer(
+        self,
+        explode_col: ColumnOrName,
+    ) -> "DataFrame":
+        """Explodes an array of structs into a table.
+        Unlike inline, if the array is null or empty then null is produced for each nested column.
+
+        Args:
+            explode_col: input column of values to explode.
+
+        Examples:
+        # TODO: add example after adding support for creating structs with `struct` function
+        """
+        return self._explode(explode_col, False, True, True)
+
+    def _explode(
+        self,
+        explode_col: ColumnOrName,
+        position_included: bool,
+        inline: bool,
+        outer: bool,
+    ) -> "DataFrame":
+        explode_col_expr = self._convert_cols_to_exprs("explode()", explode_col).pop()
+
+        explode_plan = Explode(
+            type_coercion_mode=self._session._type_coercion_mode,
+            explode_column=explode_col_expr,
+            position_included=position_included,
+            inline=inline,
+            outer=outer,
+            child=self._plan,
+        )
+
+        return self._with_plan(explode_plan)
+
     def limit(self, n: int, offset: int = 0) -> "DataFrame":
         """Returns a new DataFrame that contains at most ``n`` rows from the current
         DataFrame, skipping ``offset`` rows from the beginning (similar to LIMIT and OFFSET in SQL).
 
         Note that this is a transformation method and not an action method.
 
         Args:
             n: Number of rows to return.
             offset: Number of rows to skip before the start of the result set. The default value is 0.
 
-        Example::
+        Examples:
 
             >>> df = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
             >>> df.limit(1).show()
             -------------
             |"A"  |"B"  |
             -------------
             |1    |2    |
@@ -1210,109 +1427,111 @@
         )
 
     def union(self, other: "DataFrame") -> "DataFrame":
         """Returns a new DataFrame that contains all the rows in the current DataFrame
         and another DataFrame (``other``), excluding any duplicate rows. Both input
         DataFrames must contain the same number of columns.
 
-        Example::
+        Args:
+            other: the other :class:`DataFrame` that contains the rows to include.
+
+        Examples:
             >>> df1 = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
             >>> df2 = session.create_dataframe([[0, 1], [3, 4]], schema=["c", "d"])
             >>> df1.union(df2).show()
             -------------
             |"A"  |"B"  |
             -------------
             |1    |2    |
             |3    |4    |
             |0    |1    |
             -------------
-            <BLANKLINE>
-
-        Args:
-            other: the other :class:`DataFrame` that contains the rows to include.
         """
         return self._with_plan(
             UnionPlan(type_coercion_mode=self._session._type_coercion_mode, left=self._plan, right=other._plan)
         )
 
     def union_all(self, other: "DataFrame") -> "DataFrame":
         """Returns a new DataFrame that contains all the rows in the current DataFrame
         and another DataFrame (``other``), including any duplicate rows. Both input
         DataFrames must contain the same number of columns.
 
-        Example::
+        :meth:`unionAll` is an alias of :meth:`union_all`.
 
+        Args:
+            other: the other :class:`DataFrame` that contains the rows to include.
+
+        Examples:
             >>> df1 = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
             >>> df2 = session.create_dataframe([[0, 1], [3, 4]], schema=["c", "d"])
             >>> df1.union_all(df2).show()
             -------------
             |"A"  |"B"  |
             -------------
             |1    |2    |
             |3    |4    |
             |0    |1    |
             |3    |4    |
             -------------
-            <BLANKLINE>
-
-        Args:
-            other: the other :class:`DataFrame` that contains the rows to include.
         """
         return self._with_plan(
             UnionAll(type_coercion_mode=self._session._type_coercion_mode, left=self._plan, right=other._plan)
         )
 
     def union_by_name(self, other: "DataFrame") -> "DataFrame":
         """Returns a new DataFrame that contains all the rows in the current DataFrame
         and another DataFrame (``other``), excluding any duplicate rows.
 
         This method matches the columns in the two DataFrames by their names, not by
         their positions. The columns in the other DataFrame are rearranged to match
         the order of columns in the current DataFrame.
 
-        Example::
+        :meth:`unionByName` is an alias of :meth:`union_by_name`.
+
+        Args:
+            other: the other :class:`DataFrame` that contains the rows to include.
+
+        Examples:
 
             >>> df1 = session.create_dataframe([[1, 2]], schema=["a", "b"])
             >>> df2 = session.create_dataframe([[2, 1]], schema=["b", "a"])
             >>> df1.union_by_name(df2).show()
             -------------
             |"A"  |"B"  |
             -------------
             |1    |2    |
             -------------
-            <BLANKLINE>
-
-        Args:
-            other: the other :class:`DataFrame` that contains the rows to include.
         """
         return self._union_by_name_internal(other, is_all=False)
 
     def union_all_by_name(self, other: "DataFrame") -> "DataFrame":
         """Returns a new DataFrame that contains all the rows in the current DataFrame
         and another DataFrame (``other``), including any duplicate rows.
 
         This method matches the columns in the two DataFrames by their names, not by
         their positions. The columns in the other DataFrame are rearranged to match
         the order of columns in the current DataFrame.
 
-        Example::
+        :meth:`unionAllByName` is an alias of :meth:`union_all_by_name`.
+
+        Args:
+            other: the other :class:`DataFrame` that contains the rows to include.
+
+        Examples:
 
             >>> df1 = session.create_dataframe([[1, 2]], schema=["a", "b"])
             >>> df2 = session.create_dataframe([[2, 1]], schema=["b", "a"])
             >>> df1.union_all_by_name(df2).show()
             -------------
             |"A"  |"B"  |
             -------------
             |1    |2    |
             |1    |2    |
             -------------
             <BLANKLINE>
-
-        Args:
-            other: the other :class:`DataFrame` that contains the rows to include.
         """
         return self._union_by_name_internal(other, is_all=True)
 
     def _union_by_name_internal(self, other: "DataFrame", is_all: bool = False) -> "DataFrame":
         left_output_attrs = self._output
         right_output_attrs = other._output
         right_output_attr_by_name = {rattr.name: rattr for rattr in right_output_attrs}
@@ -1341,81 +1560,77 @@
         return df
 
     def intersect(self, other: "DataFrame") -> "DataFrame":
         """Returns a new DataFrame that contains the intersection of rows from the
         current DataFrame and another DataFrame (``other``). Duplicate rows are
         eliminated.
 
-        Example::
+        Args:
+            other: the other :class:`DataFrame` that contains the rows to use for the
+                intersection.
 
+        Examples:
             >>> df1 = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
             >>> df2 = session.create_dataframe([[1, 2], [5, 6]], schema=["c", "d"])
             >>> df1.intersect(df2).show()
             -------------
             |"A"  |"B"  |
             -------------
             |1    |2    |
             -------------
-            <BLANKLINE>
-
-        Args:
-            other: the other :class:`DataFrame` that contains the rows to use for the
-                intersection.
         """
         return self._with_plan(
             Intersect(type_coercion_mode=self._session._type_coercion_mode, left=self._plan, right=other._plan)
         )
 
     def intersect_all(self, other: "DataFrame") -> "DataFrame":
         """Returns a new DataFrame that contains the intersection of rows from the
         current DataFrame and another DataFrame (``other``). Duplicate rows are
         persisted.
 
-        Example::
+        :meth:`intersectAll` is an alias of :meth:`intersect_all`.
+
+        Args:
+            other: the other :class:`DataFrame` that contains the rows to use for the
+                intersection.
 
+        Examples:
             >>> df1 = session.create_dataframe([("id1", 1), ("id1", 1), ("id", 1), ("id1", 3)]).to_df("id", "value")
             >>> df2 = session.create_dataframe([("id1", 1), ("id1", 1), ("id", 1), ("id1", 2)]).to_df("id", "value")
             >>> df1.intersect_all(df2).show()
             ------------------
             |"id"  |"value"  |
             ------------------
             |id1    |1       |
             |id1    |1       |
             |id     |1       |
             ------------------
-            <BLANKLINE>
-
-        Args:
-            other: the other :class:`DataFrame` that contains the rows to use for the
-                intersection.
         """
         return self._with_plan(
             IntersectAll(type_coercion_mode=self._session._type_coercion_mode, left=self._plan, right=other._plan)
         )
 
     def except_(self, other: "DataFrame") -> "DataFrame":
         """Returns a new DataFrame that contains all the rows from the current DataFrame
         except for the rows that also appear in the ``other`` DataFrame. Duplicate rows are eliminated.
 
-        Example::
+        :meth:`exceptAll`, :meth:`minus` and :meth:`subtract` are aliases of :meth:`except_`.
+
+        Args:
+            other: The :class:`DataFrame` that contains the rows to exclude.
 
+        Examples:
             >>> df1 = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
             >>> df2 = session.create_dataframe([[1, 2], [5, 6]], schema=["c", "d"])
             >>> df1.subtract(df2).show()
             -------------
             |"A"  |"B"  |
             -------------
             |3    |4    |
             -------------
-            <BLANKLINE>
-
-        :meth:`minus` and :meth:`subtract` are aliases of :meth:`except_`.
-
-        Args:
-            other: The :class:`DataFrame` that contains the rows to exclude.
         """
         return self._with_plan(Except(type_coercion_mode=self._session._type_coercion_mode, left=self._plan, right=other._plan))
 
     def join(
         self,
         right: "DataFrame",
         on: Optional[Union[ColumnOrName, Iterable[ColumnOrName]]] = None,
@@ -1451,15 +1666,15 @@
             lsuffix: Suffix to add to the overlapping columns of the left DataFrame.
             rsuffix: Suffix to add to the overlapping columns of the right DataFrame.
 
         Note:
             When both ``lsuffix`` and ``rsuffix`` are empty, the overlapping columns will have random column names in the resulting DataFrame.
             You can reference to these randomly named columns using :meth:`Column.alias` (See the first usage in Examples).
 
-        Examples::
+        Examples:
             >>> from pystarburst.functions import col
             >>> df1 = session.create_dataframe([[1, 2], [3, 4], [5, 6]], schema=["a", "b"])
             >>> df2 = session.create_dataframe([[1, 7], [3, 8]], schema=["a", "c"])
             >>> df1.join(df2, df1.a == df2.a).select(df1.a.alias("a_1"), df2.a.alias("a_2"), df1.b, df2.c).show()
             -----------------------------
             |"A_1"  |"A_2"  |"B"  |"C"  |
             -----------------------------
@@ -1554,20 +1769,24 @@
         using_columns = kwargs.get("using_columns") or on
         join_type = kwargs.get("join_type") or how
         if isinstance(right, DataFrame):
             if self is right or self._plan is right._plan:
                 raise PyStarburstClientExceptionMessages.DF_SELF_JOIN_NOT_SUPPORTED()
 
             # Parse using_columns arg
-            if column_to_bool(using_columns) is False:
-                return self.cross_join(right, lsuffix=lsuffix, rsuffix=rsuffix)
+            if isinstance(using_columns, bool):
+                raise TypeError(
+                    "'on' parameter does not accept a boolean value. For cross join, omit 'on', or use 'cross_join' method instead."
+                )
             elif isinstance(using_columns, str):
                 using_columns = [using_columns]
             elif isinstance(using_columns, Column):
                 using_columns = using_columns
+            elif using_columns is None:
+                return self.cross_join(right, lsuffix=lsuffix, rsuffix=rsuffix)
             elif (
                 isinstance(using_columns, Iterable)
                 and len(using_columns) > 0
                 and not all([isinstance(col, str) for col in using_columns])
             ):
                 bad_idx, bad_col = next((idx, col) for idx, col in enumerate(using_columns) if not isinstance(col, str))
                 raise TypeError(
@@ -1597,16 +1816,26 @@
         :class:`DataFrame` and another :class:`DataFrame` (``right``).
 
         If the current and ``right`` DataFrames have columns with the same name, and
         you need to refer to one of these columns in the returned DataFrame, use the
         :func:`col` function on the current or ``right`` DataFrame to disambiguate
         references to these columns.
 
-        Example::
+        :meth:`crossJoin` is an alias of :meth:`cross_join`.
+
+        Args:
+            right: the right :class:`DataFrame` to join.
+            lsuffix: Suffix to add to the overlapping columns of the left DataFrame.
+            rsuffix: Suffix to add to the overlapping columns of the right DataFrame.
+
+        Note:
+            If both ``lsuffix`` and ``rsuffix`` are empty, the overlapping columns will have random column names in the result DataFrame.
+            If either one is not empty, the overlapping columns won't have random names.
 
+        Examples:
             >>> df1 = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
             >>> df2 = session.create_dataframe([[5, 6], [7, 8]], schema=["c", "d"])
             >>> df1.cross_join(df2).sort("a", "b", "c", "d").show()
             -------------------------
             |"A"  |"B"  |"C"  |"D"  |
             -------------------------
             |1    |2    |5    |6    |
@@ -1622,24 +1851,14 @@
             |"A_L"  |"B_L"  |"A_R"  |"B_R"  |
             ---------------------------------
             |1      |2      |5      |6      |
             |1      |2      |7      |8      |
             |3      |4      |5      |6      |
             |3      |4      |7      |8      |
             ---------------------------------
-            <BLANKLINE>
-
-        Args:
-            right: the right :class:`DataFrame` to join.
-            lsuffix: Suffix to add to the overlapping columns of the left DataFrame.
-            rsuffix: Suffix to add to the overlapping columns of the right DataFrame.
-
-        Note:
-            If both ``lsuffix`` and ``rsuffix`` are empty, the overlapping columns will have random column names in the result DataFrame.
-            If either one is not empty, the overlapping columns won't have random names.
         """
         return self._join_dataframes_internal(
             right,
             JoinType.CROSS_JOIN,
             None,
             lsuffix=lsuffix,
             rsuffix=rsuffix,
@@ -1722,55 +1941,56 @@
         """
         Returns a DataFrame with an additional column with the specified name
         ``col_name``. The column is computed by using the specified expression ``col``.
 
         If a column with the same name already exists in the DataFrame, that column is
         replaced by the new column.
 
-        Example 1::
+        :meth:`withColumn` is an alias of :meth:`with_column`.
+
+        Args:
+            col_name: The name of the column to add or replace.
+            col: The :class:`Column` or :class:`table_function.TableFunctionCall` with single column output to add or replace.
+
+        Examples:
 
             >>> df = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
             >>> df.with_column("mean", (df["a"] + df["b"]) / 2).show()
             ------------------------
             |"A"  |"B"  |"MEAN"    |
             ------------------------
             |1    |2    |1.500000  |
             |3    |4    |3.500000  |
             ------------------------
-            <BLANKLINE>
-
-        Args:
-            col_name: The name of the column to add or replace.
-            col: The :class:`Column` or :class:`table_function.TableFunctionCall` with single column output to add or replace.
         """
         return self.with_columns([col_name], [col])
 
     def with_columns(self, col_names: List[str], values: List[Column]) -> "DataFrame":
         """Returns a DataFrame with additional columns with the specified names
         ``col_names``. The columns are computed by using the specified expressions
         ``values``.
 
         If columns with the same names already exist in the DataFrame, those columns
         are removed and appended at the end by new columns.
 
-        Example:
+        :meth:`withColumns` is an alias of :meth:`with_columns`.
+
+        Args:
+            col_names: A list of the names of the columns to add or replace.
+            values: A list of the :class:`Column` objects to add or replace.
 
+        Examples:
             >>> df = session.createDataFrame([(2, "Alice"), (5, "Bob")], schema=["age", "name"])
             >>> df.with_columns(['age2', 'age3'], [df.age + 2, df.age + 3]).show()
             ------------------------------------
             |"age"  |"name"  |"age2"  |"age3"  |
             ------------------------------------
             |2      |Alice   |4       |5       |
             |5      |Bob     |7       |8       |
             ------------------------------------
-            <BLANKLINE>
-
-        Args:
-            col_names: A list of the names of the columns to add or replace.
-            values: A list of the :class:`Column` objects to add or replace.
         """
         # Get a list of the new columns and their dedupped values
         qualified_names = [quote_name(n) for n in col_names]
         new_column_names = set(qualified_names)
 
         if len(col_names) != len(new_column_names):
             raise ValueError("The same column name is used multiple times in the col_names parameter.")
@@ -1795,15 +2015,15 @@
         return result[0][0]
 
     @property
     def write(self) -> DataFrameWriter:
         """Returns a new :class:`DataFrameWriter` object that you can use to write the data in the :class:`DataFrame` to
         a Trino cluster
 
-        Example::
+        Examples:
             >>> df = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
             >>> df.write.mode("overwrite").save_as_table("saved_table")
             >>> session.table("saved_table").show()
             -------------
             |"A"  |"B"  |
             -------------
             |1    |2    |
@@ -2026,30 +2246,29 @@
         """
         Computes basic statistics for numeric columns, which includes
         ``count``, ``mean``, ``stddev``, ``min``, and ``max``. If no columns
         are provided, this function computes statistics for all numerical or
         string columns. Non-numeric and non-string columns will be ignored
         when calling this method.
 
-        Example::
+        Args:
+            cols: The names of columns whose basic statistics are computed.
+
+        Examples:
             >>> df = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
             >>> desc_result = df.describe().sort("SUMMARY").show()
             -------------------------------------------------------
             |"SUMMARY"  |"A"                 |"B"                 |
             -------------------------------------------------------
             |count      |2.0                 |2.0                 |
             |max        |3.0                 |4.0                 |
             |mean       |2.0                 |3.0                 |
             |min        |1.0                 |2.0                 |
             |stddev     |1.4142135623730951  |1.4142135623730951  |
             -------------------------------------------------------
-            <BLANKLINE>
-
-        Args:
-            cols: The names of columns whose basic statistics are computed.
         """
         cols = parse_positional_args_to_list(*cols)
         df = self.select(cols) if len(cols) > 0 else self
 
         # ignore non-numeric and non-string columns
         numerical_string_col_type_dict = {
             field.name: field.datatype for field in df.schema.fields if isinstance(field.datatype, (StringType, _NumericType))
@@ -2171,32 +2390,30 @@
             res_df = res_df.union(agg_stat_df) if res_df else agg_stat_df
 
         return res_df
 
     def with_column_renamed(self, existing: ColumnOrName, new: str) -> "DataFrame":
         """Returns a DataFrame with the specified column ``existing`` renamed as ``new``.
 
-        Example::
+        :meth:`with_column_renamed` is an alias of :meth:`rename`.
 
+        Args:
+            existing: The old column instance or column name to be renamed.
+            new: The new column name.
+
+        Examples:
             >>> # This example renames the column `A` as `NEW_A` in the DataFrame.
             >>> df = session.sql("select 1 as A, 2 as B")
             >>> df_renamed = df.with_column_renamed(col("A"), "NEW_A")
             >>> df_renamed.show()
             -----------------
             |"NEW_A"  |"B"  |
             -----------------
             |1        |2    |
             -----------------
-            <BLANKLINE>
-
-        Args:
-            existing: The old column instance or column name to be renamed.
-            new: The new column name.
-
-        :meth:`with_column_renamed` is an alias of :meth:`rename`.
         """
         if isinstance(existing, str):
             old_name = quote_name(existing)
         elif isinstance(existing, Column):
             old_name = existing._expression
             if isinstance(existing._expression, UnresolvedAttribute):
                 old_name = existing._expression.name
@@ -2209,29 +2426,29 @@
             raise TypeError(f"{existing} must be a column name or Column object.")
 
         return self.with_columns_renamed({old_name: new})
 
     def with_columns_renamed(self, cols_map: dict) -> "DataFrame":
         """Returns a new DataFrame by renaming multiple columns.
 
-        Example::
+        :meth:`withColumnsRenamed` is an alias of :meth:`with_columns_renamed`.
 
+        Args:
+            cols_map: a dict of existing column names and corresponding desired column names.
+
+        Examples:
             >>> # This example renames the columns `A` as `NEW_A` and `B` as `NEW_B`
             >>> df = session.sql("select 1 as A, 2 as B")
             >>> df_renamed = df.with_columns_renamed({"A": "NEW_A", "B": "NEW_B"})
             >>> df_renamed.show()
             ---------------------
             |"NEW_A"  |"NEW_B"  |
             ---------------------
             |1        |2        |
             ---------------------
-            <BLANKLINE>
-
-        Args:
-            cols_map: a dict of existing column names and corresponding desired column names.
         """
         columns_renamed = {}
         for old_name, new_name in cols_map.items():
             old_quoted_name = quote_name(old_name)
             new_quoted_name = quote_name(new_name)
             to_be_renamed = [x for x in self._output if x.name.upper() == old_quoted_name.upper()]
             if not to_be_renamed:
@@ -2260,15 +2477,15 @@
         You can use :meth:`Table.drop_table` or the ``with`` statement to clean up the cached result when it's not needed.
         Refer to the example code below.
 
         Note:
             An error will be thrown if a cached result is cleaned up and it's used again,
             or any other DataFrames derived from the cached result are used again.
 
-        Examples::
+        Examples:
             >>> create_result = session.sql("create temp table RESULT (NUM int)").collect()
             >>> insert_result = session.sql("insert into RESULT values(1),(2)").collect()
 
             >>> df = session.table("RESULT")
             >>> df.collect()
             [Row(NUM=1), Row(NUM=2)]
 
@@ -2298,16 +2515,14 @@
             >>> # Clean up the cached result
             >>> df3.drop_table()
             >>> # use context manager to clean up the cached result after it's use.
             >>> with df2.cache_result() as df4:
             ...     df4.collect()
             [Row(NUM=1), Row(NUM=2)]
 
-        Args:
-
         Returns:
              A :class:`Table` object that holds the cached result in a temporary table.
              All operations on this new DataFrame have no effect on the original.
         """
         temp_table_name = random_name_for_temp_object(TempObjectType.TABLE)
         create_temp_table = self._session._plan_builder.create_temp_table(
             temp_table_name,
@@ -2326,28 +2541,28 @@
     def random_split(
         self,
         weights: List[float],
         seed: Optional[int] = None,
         *,
         statement_properties: Optional[Dict[str, str]] = None,
     ) -> List["DataFrame"]:
-        """
-        Randomly splits the current DataFrame into separate DataFrames,
+        """Randomly splits the current DataFrame into separate DataFrames,
         using the specified weights.
 
+        :meth:`randomSplit` is an alias of :meth:`random_split`.
+
         Args:
             weights: Weights to use for splitting the DataFrame. If the
                 weights don't add up to 1, the weights will be normalized.
                 Every number in ``weights`` has to be positive. If only one
                 weight is specified, the returned DataFrame list only includes
                 the current DataFrame.
             seed: The seed for sampling.
 
-        Example::
-
+        Examples:
             >>> df = session.range(10000)
             >>> weights = [0.1, 0.2, 0.3]
             >>> df_parts = df.random_split(weights)
             >>> len(df_parts) == len(weights)
             True
 
         Note:
@@ -2415,26 +2630,31 @@
         if not isinstance(alias, str):
             raise ValueError("alias should be a string")
         return self.select([col(c).alias(f"{alias}.{c}") for c in self.columns])
 
     def is_empty(self) -> bool:
         """Checks if the DataFrame is empty and returns a boolean value.
 
+        :meth:`isEmpty` is an alias of :meth:`is_empty`.
+
         Examples
         --------
         >>> from pystarburst.types import *
         >>> df_empty = session.createDataFrame([], schema=StructType([StructField('a', StringType(), True)]))
         >>> df_empty.isEmpty()
         True
+        <BLANKLINE>
         >>> df_non_empty = session.createDataFrame(["a"], schema=["a"])
         >>> df_non_empty.isEmpty()
         False
+        <BLANKLINE>
         >>> df_nulls = session.createDataFrame([(None, None)], schema=StructType([StructField("a", StringType(), True), StructField("b", IntegerType(), True)]))
         >>> df_nulls.isEmpty()
         False
+        <BLANKLINE>
         >>> df_no_rows = session.createDataFrame([], schema=StructType([StructField('id', IntegerType(), True), StructField('value', StringType(), True)]))
         >>> df_no_rows.isEmpty()
         True
         """
         return self.limit(1).count() == 0
 
     def to(self, schema: StructType) -> "DataFrame":
@@ -2447,18 +2667,20 @@
 
         Examples
         --------
         >>> from pystarburst.types import *
         >>> df = session.createDataFrame([("a", 1)], ["i", "j"])
         >>> df.schema
         StructType([StructField('i', StringType(), True), StructField('j', LongType(), True)])
+        <BLANKLINE>
         >>> schema = StructType([StructField("j", StringType()), StructField("i", StringType())])
         >>> df2 = df.to(schema)
         >>> df2.schema
-        >>> StructType([StructField('j', StringType(), True), StructField('i', StringType(), True)])
+        StructType([StructField('j', StringType(), True), StructField('i', StringType(), True)])
+        <BLANKLINE>
         >>> df2.show()
         +---+---+
         |  j|  i|
         +---+---+
         |  1|  a|
         +---+---+
         """
```

## pystarburst/dataframe_na_functions.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 import copy
 import math
 from datetime import datetime, time
 from logging import getLogger
 from numbers import Number
@@ -69,31 +69,28 @@
             how: An ``str`` with value either 'any' or 'all'. If 'any', drop a row if
                 it contains any nulls. If 'all', drop a row only if all its values are null.
                 The default value is 'any'. If ``thresh`` is provided, ``how`` will be ignored.
             thresh: The minimum number of non-null and non-NaN
                 values that should be in the specified columns in order for the
                 row to be included. It overwrites ``how``. In each case:
 
-                    * If ``thresh`` is not provided or ``None``, the length of ``subset``
-                      will be used when ``how`` is 'any' and 1 will be used when ``how``
-                      is 'all'.
-
-                    * If ``thresh`` is greater than the number of the specified columns,
-                      the method returns an empty DataFrame.
-
-                    * If ``thresh`` is less than 1, the method returns the original DataFrame.
+                * If ``thresh`` is not provided or ``None``, the length of ``subset``
+                  will be used when ``how`` is 'any' and 1 will be used when ``how``
+                  is 'all'.
+                * If ``thresh`` is greater than the number of the specified columns,
+                  the method returns an empty DataFrame.
+                * If ``thresh`` is less than 1, the method returns the original DataFrame.
 
             subset: A list of the names of columns to check for null and NaN values.
                 In each case:
 
-                    * If ``subset`` is not provided or ``None``, all columns will be included.
-
-                    * If ``subset`` is empty, the method returns the original DataFrame.
+                * If ``subset`` is not provided or ``None``, all columns will be included.
+                * If ``subset`` is empty, the method returns the original DataFrame.
 
-        Examples::
+        Examples:
 
             >>> df = session.create_dataframe([[1.0, 1], [float('nan'), 2], [None, 3], [4.0, None], [float('nan'), None]]).to_df("a", "b")
             >>> # drop a row if it contains any nulls, with checking all columns
             >>> df.na.drop().show()
             -------------
             |"A"  |"B"  |
             -------------
@@ -201,19 +198,18 @@
             value: A scalar value or a ``dict`` that associates the names of columns with the
                 values that should be used to replace null and NaN values in those
                 columns. If ``value`` is a ``dict``, ``subset`` is ignored. If ``value``
                 is an empty ``dict``, the method returns the original DataFrame.
             subset: A list of the names of columns to check for null and NaN values.
                 In each case:
 
-                    * If ``subset`` is not provided or ``None``, all columns will be included.
+                * If ``subset`` is not provided or ``None``, all columns will be included.
+                * If ``subset`` is empty, the method returns the original DataFrame.
 
-                    * If ``subset`` is empty, the method returns the original DataFrame.
-
-        Examples::
+        Examples:
 
             >>> df = session.create_dataframe([[1.0, 1], [float('nan'), 2], [None, 3], [4.0, None], [float('nan'), None]]).to_df("a", "b")
             >>> # fill null and NaN values in all columns
             >>> df.na.fill(3.14).show()
             ---------------
             |"A"   |"B"   |
             ---------------
@@ -250,19 +246,19 @@
             <BLANKLINE>
 
         Note:
             If the type of a given value in ``value`` doesn't match the
             column data type (e.g. a ``float`` for :class:`~pystarburst.types.StringType`
             column), this replacement will be skipped in this column. Especially,
 
-                * ``int`` can be filled in a column with
-                  :class:`~pystarburst.types.FloatType` or
-                  :class:`~pystarburst.types.DoubleType`, but ``float`` cannot
-                  filled in a column with :class:`~pystarburst.types.IntegerType`
-                  or :class:`~pystarburst.types.LongType`.
+            * ``int`` can be filled in a column with
+              :class:`~pystarburst.types.FloatType` or
+              :class:`~pystarburst.types.DoubleType`, but ``float`` cannot
+              filled in a column with :class:`~pystarburst.types.IntegerType`
+              or :class:`~pystarburst.types.LongType`.
 
         See Also:
             :func:`DataFrame.fillna`
         """
         # translate to
         # select col, iff(float_col = 'NaN' or float_col is null, replacement, float_col)
         # iff(non_float_col is null, replacement, non_float_col) from table where
@@ -346,15 +342,15 @@
                 ``to_replace``. If ``value`` is a scalar and ``to_replace`` is a list,
                 then ``value`` is used as a replacement for each item in ``to_replace``.
             subset: A list of the names of columns in which the values should be
                 replaced. If ``cols`` is not provided or ``None``, the replacement
                 will be applied to all columns. If ``cols`` is empty, the method
                 returns the original DataFrame.
 
-        Examples::
+        Examples:
 
             >>> df = session.create_dataframe([[1, 1.0, "1.0"], [2, 2.0, "2.0"]], schema=["a", "b", "c"])
             >>> # replace 1 with 3 in all columns
             >>> df.na.replace(1, 3).show()
             -------------------
             |"A"  |"B"  |"C"  |
             -------------------
@@ -402,21 +398,20 @@
             <BLANKLINE>
 
         Note:
             If the type of a given value in ``to_replace`` or ``value`` doesn't match the
             column data type (e.g. a ``float`` for :class:`~pystarburst.types.StringType`
             column), this replacement will be skipped in this column. Especially,
 
-                * ``int`` can replace or be replaced in a column with
-                  :class:`~pystarburst.types.FloatType` or
-                  :class:`~pystarburst.types.DoubleType`, but ``float`` cannot
-                  replace or be replaced in a column with :class:`~pystarburst.types.IntegerType`
-                  or :class:`~pystarburst.types.LongType`.
-
-                * ``None`` can replace or be replaced in a column with any data type.
+            * ``int`` can replace or be replaced in a column with
+              :class:`~pystarburst.types.FloatType` or
+              :class:`~pystarburst.types.DoubleType`, but ``float`` cannot
+              replace or be replaced in a column with :class:`~pystarburst.types.IntegerType`
+              or :class:`~pystarburst.types.LongType`.
+            * ``None`` can replace or be replaced in a column with any data type.
 
         See Also:
             :func:`DataFrame.replace`
         """
         if subset is None:
             subset = self._df.columns
         elif isinstance(subset, str):
```

## pystarburst/dataframe_stat_functions.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from functools import reduce
 from typing import Dict, Iterable, List, Optional, Union
 
 import pystarburst
 from pystarburst import Column
@@ -34,32 +34,33 @@
         percentile: Iterable[float],
         *,
         statement_properties: Optional[Dict[str, str]] = None,
     ) -> Union[List[float], List[List[float]]]:
         """For a specified numeric column and a list of desired quantiles, returns an approximate value for the column at each of the desired quantiles.
         This function uses the t-Digest algorithm.
 
-        Examples::
-
-            >>> df = session.create_dataframe([1, 2, 3, 4, 5, 6, 7, 8, 9, 0], schema=["a"])
-            >>> df.stat.approx_quantile("a", [0, 0.1, 0.4, 0.6, 1])
-            [-0.5, 0.5, 3.5, 5.5, 9.5]
-
-            >>> df2 = session.create_dataframe([[0.1, 0.5], [0.2, 0.6], [0.3, 0.7]], schema=["a", "b"])
-            >>> df2.stat.approx_quantile(["a", "b"], [0, 0.1, 0.6])
-            [[0.05, 0.15000000000000002, 0.25], [0.45, 0.55, 0.6499999999999999]]
+        :meth:`approxQuantile` is an alias of :meth:`approx_quantile`.
 
         Args:
             col: The name of the numeric column.
             percentile: A list of float values greater than or equal to 0.0 and less than 1.0.
 
         Returns:
              A list of approximate percentile values if ``col`` is a single column name, or a matrix
              with the dimensions ``(len(col) * len(percentile)`` containing the
              approximate percentile values if ``col`` is a list of column names.
+
+        Examples:
+            >>> df = session.create_dataframe([1, 2, 3, 4, 5, 6, 7, 8, 9, 0], schema=["a"])
+            >>> df.stat.approx_quantile("a", [0, 0.1, 0.4, 0.6, 1])
+            [-0.5, 0.5, 3.5, 5.5, 9.5]
+            <BLANKLINE>
+            >>> df2 = session.create_dataframe([[0.1, 0.5], [0.2, 0.6], [0.3, 0.7]], schema=["a", "b"])
+            >>> df2.stat.approx_quantile(["a", "b"], [0, 0.1, 0.6])
+            [[0.05, 0.15000000000000002, 0.25], [0.45, 0.55, 0.6499999999999999]]
         """
         temp_col_name = "t"
         if not percentile or not col:
             return []
         if isinstance(col, (Column, str)):
             df = self._df.select(approx_percentile_accumulate(col).as_(temp_col_name)).select(
                 [approx_percentile_estimate(temp_col_name, p) for p in percentile]
@@ -83,72 +84,72 @@
         col1: ColumnOrName,
         col2: ColumnOrName,
         *,
         statement_properties: Optional[Dict[str, str]] = None,
     ) -> Optional[float]:
         """Calculates the correlation coefficient for non-null pairs in two numeric columns.
 
-        Example::
-
-            >>> df = session.create_dataframe([[0.1, 0.5], [0.2, 0.6], [0.3, 0.7]], schema=["a", "b"])
-            >>> df.stat.corr("a", "b")
-            0.9999999999999991
-
         Args:
             col1: The name of the first numeric column to use.
             col2: The name of the second numeric column to use.
 
         Return:
             The correlation of the two numeric columns.
             If there is not enough data to generate the correlation, the method returns ``None``.
+
+        Examples:
+            >>> df = session.create_dataframe([[0.1, 0.5], [0.2, 0.6], [0.3, 0.7]], schema=["a", "b"])
+            >>> df.stat.corr("a", "b")
+            0.9999999999999991
         """
         df = self._df.select(corr_func(col1, col2))
         res = df._internal_collect(statement_properties=statement_properties)
         return res[0][0] if res[0] is not None else None
 
     def cov(
         self,
         col1: ColumnOrName,
         col2: ColumnOrName,
         *,
         statement_properties: Optional[Dict[str, str]] = None,
     ) -> Optional[float]:
         """Calculates the sample covariance for non-null pairs in two numeric columns.
 
-        Example::
-
-           >>> df = session.create_dataframe([[0.1, 0.5], [0.2, 0.6], [0.3, 0.7]], schema=["a", "b"])
-           >>> df.stat.cov("a", "b")
-           0.010000000000000037
-
         Args:
             col1: The name of the first numeric column to use.
             col2: The name of the second numeric column to use.
 
         Return:
             The sample covariance of the two numeric columns.
             If there is not enough data to generate the covariance, the method returns None.
+
+        Examples:
+
+           >>> df = session.create_dataframe([[0.1, 0.5], [0.2, 0.6], [0.3, 0.7]], schema=["a", "b"])
+           >>> df.stat.cov("a", "b")
+           0.010000000000000037
         """
         df = self._df.select(covar_samp(col1, col2))
         res = df._internal_collect(statement_properties=statement_properties)
         return res[0][0] if res[0] is not None else None
 
     def sample_by(self, col: ColumnOrName, fractions: Dict[LiteralType, float]) -> "pystarburst.DataFrame":
         """Returns a DataFrame containing a stratified sample without replacement, based on a ``dict`` that specifies the fraction for each stratum.
 
-        Example::
-
-            >>> df = session.create_dataframe([("Bob", 17), ("Alice", 10), ("Nico", 8), ("Bob", 12)], schema=["name", "age"])
-            >>> fractions = {"Bob": 0.5, "Nico": 1.0}
-            >>> sample_df = df.stat.sample_by("name", fractions)  # non-deterministic result
+        :meth:`sampleBy` is an alias of :meth:`sample_by`.
 
         Args:
             col: The name of the column that defines the strata.
             fractions: A ``dict`` that specifies the fraction to use for the sample for each stratum.
                 If a stratum is not specified in the ``dict``, the method uses 0 as the fraction.
+
+        Examples:
+            >>> df = session.create_dataframe([("Bob", 17), ("Alice", 10), ("Nico", 8), ("Bob", 12)], schema=["name", "age"])
+            >>> fractions = {"Bob": 0.5, "Nico": 1.0}
+            >>> sample_df = df.stat.sample_by("name", fractions)  # non-deterministic result
         """
         if not fractions:
             res_df = self._df.limit(0)
             return res_df
         col = _to_col_if_str(col, "sample_by")
         res_df = reduce(
             lambda x, y: x.union_all(y),
```

## pystarburst/dataframe_writer.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Dict, Iterable, Optional, Union
 
 import pystarburst  # for forward references of type hints
 from pystarburst import Literal
 from pystarburst._internal.analyzer.plan.logical_plan.table import CreateTable, SaveMode
@@ -16,35 +16,32 @@
 
     To use this object:
 
     1. Create an instance of a :class:`DataFrameWriter` by accessing the :attr:`DataFrame.write` property.
     2. (Optional) Specify the save mode by calling :meth:`mode`, which returns the same
        :class:`DataFrameWriter` that is configured to save data using the specified mode.
        The default mode is "errorifexists".
-    3. Call :meth:`save_as_table` or :meth:`copy_into_location` to save the data to the
+    3. Call :meth:`save_as_table` to save the data to the
        specified destination.
     """
 
     def __init__(self, dataframe: "pystarburst.dataframe.DataFrame") -> None:
         self._dataframe = dataframe
         self._save_mode = SaveMode.ERRORIFEXISTS
 
     def mode(self, save_mode: str) -> "DataFrameWriter":
         """Set the save mode of this :class:`DataFrameWriter`.
 
         Args:
-            save_mode: One of the following strings.
+            save_mode: one of the following strings:
 
-                "append": Append data of this DataFrame to existing data.
-
-                "overwrite": Overwrite existing data.
-
-                "errorifexists": Throw an exception if data already exists.
-
-                "ignore": Ignore this operation if data already exists.
+                - "append": Append data of this DataFrame to existing data.
+                - "overwrite": Overwrite existing data.
+                - "errorifexists": Throw an exception if data already exists.
+                - "ignore": Ignore this operation if data already exists.
 
                 Default value is "errorifexists".
 
         Returns:
             The :class:`DataFrameWriter` itself.
         """
         self._save_mode = str_to_enum(save_mode, SaveMode, "`save_mode`")
@@ -57,36 +54,35 @@
         mode: Optional[str] = None,
         column_order: str = "index",
         table_properties: Dict[str, Union[str, bool, int, float]] = None,
         statement_properties: Optional[Dict[str, str]] = None,
     ) -> None:
         """Writes the data to the specified table in a Trino cluster.
 
+        :meth:`saveAsTable` is an alias of :meth:`save_as_table`.
+
         Args:
             table_name: A string or list of strings that specify the table name or fully-qualified object identifier
                 (database name, schema name, and table name).
             mode: One of the following values. When it's ``None`` or not provided,
                 the save mode set by :meth:`mode` is used.
 
-                "append": Append data of this DataFrame to existing data.
-
-                "overwrite": Overwrite existing data.
-
-                "errorifexists": Throw an exception if data already exists.
-
-                "ignore": Ignore this operation if data already exists.
+                - "append": Append data of this DataFrame to existing data.
+                - "overwrite": Overwrite existing data.
+                - "errorifexists": Throw an exception if data already exists.
+                - "ignore": Ignore this operation if data already exists.
 
             column_order: When ``mode`` is "append", data will be inserted into the target table by matching column sequence or column name. Default is "index". When ``mode`` is not "append", the ``column_order`` makes no difference.
 
-                "index": Data will be inserted into the target table by column sequence.
-                "name": Data will be inserted into the target table by matching column names. If the target table has more columns than the source DataFrame, use this one.
+                - "index": Data will be inserted into the target table by column sequence.
+                - "name": Data will be inserted into the target table by matching column names. If the target table has more columns than the source DataFrame, use this one.
 
             table_properties: Any custom table properties used to create the table.
 
-        Examples::
+        Examples:
 
             >>> df = session.create_dataframe([[1,2],[3,4]], schema=["a", "b"])
             >>> df.write.mode("overwrite").save_as_table("my_table")
             >>> session.table("my_table").collect()
             [Row(A=1, B=2), Row(A=3, B=4)]
             >>> df.write.save_as_table("my_table", mode="append")
             >>> session.table("my_table").collect()
```

## pystarburst/exceptions.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 """This package contains all pystarburst client-side exceptions."""
 import logging
 from typing import Optional
 
 _logger = logging.getLogger(__name__)
```

## pystarburst/functions.py

```diff
@@ -1,20 +1,20 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 """
 Provides utility and SQL functions that generate :class:`~pystarburst.Column` expressions that you can pass to :class:`~pystarburst.DataFrame` transformation methods.
 
 These utility functions generate references to columns, literals, and SQL expressions (e.g. "c + 1").
 
-  - Use :func:`col()` to convert a column name to a :class:`Column` object. Refer to the API docs of :class:`Column` to know more ways of referencing a column.
-  - Use :func:`lit()` to convert a Python value to a :class:`Column` object that represents a constant value in Trino SQL.
-  - Use :func:`sql_expr()` to convert a Trino SQL expression to a :class:`Column`.
+- Use :func:`col()` to convert a column name to a :class:`Column` object. Refer to the API docs of :class:`Column` to know more ways of referencing a column.
+- Use :func:`lit()` to convert a Python value to a :class:`Column` object that represents a constant value in Trino SQL.
+- Use :func:`sql_expr()` to convert a Trino SQL expression to a :class:`Column`.
 
     >>> df = session.create_dataframe([[1, 'a', True, '2022-03-16'], [3, 'b', False, '2023-04-17']], schema=["a", "b", "c", "d"])
     >>> res1 = df.filter(col("a") == 1).collect()
     >>> res2 = df.filter(lit(1) == col("a")).collect()
     >>> res3 = df.filter(sql_expr("a = 1")).collect()
     >>> assert res1 == res2 == res3
     >>> res1
@@ -77,21 +77,22 @@
     ---------------------------
     |0.017453292519943295     |
     |0.05235987755982988      |
     ---------------------------
     <BLANKLINE>
 
 **How to find help on input parameters of the Python functions for SQL functions**
+
 The Python functions have the same name as the corresponding `SQL functions <https://trino.io/docs/current/functions.html>`_.
 
 By reading the API docs or the source code of a Python function defined in this module, you'll see the type hints of the input parameters and return type.
 The return type is always ``Column``. The input types tell you the acceptable values:
 
-  - ``ColumnOrName`` accepts a :class:`Column` object, or a column name in str. Most functions accept this type.
-    If you still want to pass a literal to it, use `lit(value)`, which returns a ``Column`` object that represents a literal value.
+- ``ColumnOrName`` accepts a :class:`Column` object, or a column name in str. Most functions accept this type.
+  If you still want to pass a literal to it, use `lit(value)`, which returns a ``Column`` object that represents a literal value.
 
     >>> df.select(avg("a")).show()
     ----------------
     |"AVG(""A"")"  |
     ----------------
     |2.000000      |
     ----------------
@@ -100,55 +101,55 @@
     ----------------
     |"AVG(""A"")"  |
     ----------------
     |2.000000      |
     ----------------
     <BLANKLINE>
 
-  - ``LiteralType`` accepts a value of type ``bool``, ``int``, ``float``, ``str``, ``bytearray``, ``decimal.Decimal``,
-    ``datetime.date``, ``datetime.datetime``, ``datetime.time``, or ``bytes``. An example is the third parameter of :func:`lead`.
+- ``LiteralType`` accepts a value of type ``bool``, ``int``, ``float``, ``str``, ``bytearray``, ``decimal.Decimal``,
+  ``datetime.date``, ``datetime.datetime``, ``datetime.time``, or ``bytes``. An example is the third parameter of :func:`lead`.
 
     >>> import datetime
     >>> from pystarburst.window import Window
     >>> df.select(col("d"), lead("d", 1, datetime.date(2024, 5, 18), False).over(Window.order_by("d")).alias("lead_day")).show()
     ---------------------------
     |"D"         |"LEAD_DAY"  |
     ---------------------------
     |2022-03-16  |2023-04-17  |
     |2023-04-17  |2024-05-18  |
     ---------------------------
     <BLANKLINE>
 
-  - ``ColumnOrLiteral`` accepts a ``Column`` object, or a value of ``LiteralType`` mentioned above.
-    The difference from ``ColumnOrLiteral`` is ``ColumnOrLiteral`` regards a str value as a SQL string value instead of
-    a column name. When a function is much more likely to accept a SQL constant value than a column expression, ``ColumnOrLiteral``
-    is used. Yet you can still pass in a ``Column`` object if you need to. An example is the second parameter of
-    :func:``when``.
+- ``ColumnOrLiteral`` accepts a ``Column`` object, or a value of ``LiteralType`` mentioned above.
+  The difference from ``ColumnOrLiteral`` is ``ColumnOrLiteral`` regards a str value as a SQL string value instead of
+  a column name. When a function is much more likely to accept a SQL constant value than a column expression, ``ColumnOrLiteral``
+  is used. Yet you can still pass in a ``Column`` object if you need to. An example is the second parameter of
+  :func:``when``.
 
     >>> df.select(when(df["a"] > 2, "Greater than 2").else_("Less than 2").alias("compare_with_2")).show()
     --------------------
     |"COMPARE_WITH_2"  |
     --------------------
     |Less than 2       |
     |Greater than 2    |
     --------------------
     <BLANKLINE>
 
-  - ``int``, ``bool``, ``str``, or another specific type accepts a value of that type. An example is :func:`to_decimal`.
+- ``int``, ``bool``, ``str``, or another specific type accepts a value of that type. An example is :func:`to_decimal`.
 
     >>> df.with_column("e", lit("1.2")).select(to_decimal("e", 5, 2)).show()
     -----------------------------
     |"TO_DECIMAL(""E"", 5, 2)"  |
     -----------------------------
     |1.20                       |
     |1.20                       |
     -----------------------------
     <BLANKLINE>
 
-  - ``ColumnOrSqlExpr`` accepts a ``Column`` object, or a SQL expression. For instance, the first parameter in :func:``when``.
+- ``ColumnOrSqlExpr`` accepts a ``Column`` object, or a SQL expression. For instance, the first parameter in :func:``when``.
 
     >>> df.select(when("a > 2", "Greater than 2").else_("Less than 2").alias("compare_with_2")).show()
     --------------------
     |"COMPARE_WITH_2"  |
     --------------------
     |Less than 2       |
     |Greater than 2    |
@@ -165,14 +166,15 @@
     FunctionExpression,
     LambdaFunctionExpression,
     LambdaParameter,
     ListAgg,
     Literal,
     MultipleExpression,
     Star,
+    StructExpression,
 )
 from pystarburst._internal.analyzer.expression.window import (
     FirstValue,
     Lag,
     LastValue,
     Lead,
     NthValue,
@@ -194,14 +196,15 @@
     _to_col_if_sql_expr,
     _to_col_if_str,
     _to_col_if_str_or_int,
 )
 from pystarburst.types import (
     BinaryType,
     DataType,
+    DateType,
     JsonType,
     StringType,
     TimestampNTZType,
 )
 
 
 def col(col_name: str) -> Column:
@@ -232,173 +235,173 @@
     return Column._expr(sql)
 
 
 def current_user() -> Column:
     """
     Returns the name of the user currently logged into the system.
 
-    Example:
+    Examples:
         >>> # Return result is tied to session, so we only test if the result exists
         >>> result = session.create_dataframe([1]).select(current_user()).collect()
         >>> assert result is not None
     """
     return builtin("current_user")()
 
 
 def current_catalog() -> Column:
     """Returns the name of the catalog in use for the current session.
 
-    Example:
+    Examples:
         >>> # Return result is tied to session, so we only test if the result exists
         >>> result = session.create_dataframe([1]).select(current_catalog()).collect()
         >>> assert result is not None
     """
     return builtin("current_catalog")()
 
 
 def current_groups() -> Column:
     """Returns the list of groups for the current user running the query.
 
-    Example:
+    Examples:
         >>> # Return result is tied to session, so we only test if the result exists
         >>> result = session.create_dataframe([1]).select(current_groups()).collect()
         >>> assert result is not None
     """
     return builtin("current_groups()")()
 
 
 def current_schema() -> Column:
     """Returns the name of the schema in use for the current session.
 
-    Example:
+    Examples:
         >>> # Return result is tied to session, so we only test if the result exists
         >>> result = session.create_dataframe([1]).select(current_schema()).collect()
         >>> assert result is not None
     """
     return builtin("current_schema")()
 
 
 def add_months(date_or_timestamp: ColumnOrName, number_of_months: Union[Column, int]) -> Column:
     """Adds or subtracts a specified number of months to a date or timestamp, preserving the end-of-month information.
 
-    Example:
+    Examples:
         >>> import datetime
         >>> df = session.create_dataframe([datetime.date(2022, 4, 6)], schema=["d"])
         >>> df.select(add_months("d", 4)).collect()[0][0]
         datetime.date(2022, 8, 6)
     """
     c = _to_col_if_str(date_or_timestamp, "add_months")
     return builtin("date_add")("month", number_of_months, c)
 
 
 def any_value(e: ColumnOrName) -> Column:
     """Returns a non-deterministic any value for the specified column.
     This is an aggregate and window function.
 
-    Example:
+    Examples:
         >>> df = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
         >>> result = df.select(any_value("a")).collect()
         >>> assert len(result) == 1  # non-deterministic value in result.
     """
     c = _to_col_if_str(e, "any_value")
     return call_builtin("any_value", c)
 
 
 def power(x: ColumnOrName, p: Union[Column, int]) -> Column:
     """Returns x raised to the power of p.
 
-    Example:
+    Examples:
         >>> df = session.create_dataframe([2], schema=["a"])
         >>> df.select(power("a", 2)).collect()[0][0]
         4
     """
     c = _to_col_if_str(x, "power")
     return call_builtin("power", c, p)
 
 
 def is_nan(x: ColumnOrName) -> Column:
     """Determine if x is not-a-number.
 
-    Example:
+    Examples:
         >>> df = session.create_dataframe([2], schema=["a"])
         >>> df.select(is_nan("a")).collect()[0][0]
         False
     """
     c = _to_col_if_str(x, "is_nan")
     return call_builtin("is_nan", c)
 
 
 def bitand(column1: ColumnOrName, column2: ColumnOrName) -> Column:
     """Returns the bitwise negation of a numeric expression.
 
-    Example:
+    Examples:
         >>> df = session.create_dataframe([19, 25], schema=["a", "b"])
         >>> df.select(bitand("a", "b")).collect()[0][0]
         17
     """
     c1 = _to_col_if_str(column1, "bitwise_and")
     c2 = _to_col_if_str(column2, "bitwise_and")
     return call_builtin("bitwise_and", c1, c2)
 
 
 def bitor(column1: ColumnOrName, column2: ColumnOrName) -> Column:
     """Returns the bitwise negation of a numeric expression.
 
-    Example:
+    Examples:
         >>> df = session.create_dataframe([19, 25], schema=["a", "b"])
         >>> df.select(bitor("a", "b")).collect()[0][0]
         27
     """
     c1 = _to_col_if_str(column1, "bitwise_or")
     c2 = _to_col_if_str(column2, "bitwise_or")
     return call_builtin("bitwise_or", c1, c2)
 
 
 def bitxor(column1: ColumnOrName, column2: ColumnOrName) -> Column:
     """Returns the bitwise XOR of x and y in 2’s complement representation,
 
-    Example:
+    Examples:
         >>> df = session.create_dataframe([19, 25], schema=["a", "b"])
         >>> df.select(bitxor("a", "b")).collect()[0][0]
         10
     """
     c1 = _to_col_if_str(column1, "bitwise_xor")
     c2 = _to_col_if_str(column2, "bitwise_xor")
     return call_builtin("bitwise_xor", c1, c2)
 
 
 def bitnot(e: ColumnOrName) -> Column:
     """Returns the bitwise negation of a numeric expression.
 
-    Example:
+    Examples:
         >>> df = session.create_dataframe([1], schema=["a"])
         >>> df.select(bitnot("a")).collect()[0][0]
         -2
     """
     c = _to_col_if_str(e, "bitwise_not")
     return call_builtin("bitwise_not", c)
 
 
 def bitshiftleft(to_shift_column: ColumnOrName, n: Union[Column, int]) -> Column:
     """Returns the left shifted value of value
 
-    Example:
+    Examples:
         >>> df = session.create_dataframe([2], schema=["a"])
         >>> df.select(bitshiftleft("a", 1)).collect()[0][0]
         4
     """
     c = _to_col_if_str(to_shift_column, "bitwise_left_shift")
     n = _to_col_if_str(n, "bitwise_left_shift")
     return call_builtin("bitwise_left_shift", c, n)
 
 
 def bitshiftright(to_shift_column: ColumnOrName, n: Union[Column, int]) -> Column:
     """Returns the right shifted value of value
 
-    Example:
+    Examples:
         >>> df = session.create_dataframe([2], schema=["a"])
         >>> df.select(bitshiftright("a", 1)).collect()[0][0]
         1
     """
     c = _to_col_if_str(to_shift_column, "bitwise_right_shift")
     n = _to_col_if_str(n, "bitwise_right_shift")
     return call_builtin("bitwise_right_shift", c, n)
@@ -410,39 +413,39 @@
     source_time: ColumnOrName,
     source_timezone: Optional[ColumnOrName] = None,
 ) -> Column:
     """Converts the given source_time to the target timezone.
 
     For timezone information, refer to the `Trino time conversion notes <https://trino.io/docs/current/functions/datetime.html#time-zone-conversion>`_
 
-        Args:
-            target_timezone: The time zone to which the input timestamp should be converted.=
-            source_time: The timestamp to convert. When it's a TIMESTAMP_LTZ, use ``None`` for ``source_timezone``.
-            source_timezone: The time zone for the ``source_time``. Required for timestamps with no time zone (i.e. TIMESTAMP_NTZ). Use ``None`` if the timestamps have a time zone (i.e. TIMESTAMP_LTZ). Default is ``None``.
-
-        Note:
-            The sequence of the 3 params is different from the SQL function, which two overloads:
-
-              - ``CONVERT_TIMEZONE( <source_tz> , <target_tz> , <source_timestamp_ntz> )``
-              - ``CONVERT_TIMEZONE( <target_tz> , <source_timestamp> )``
-
-            The first parameter ``source_tz`` is optional. But in Python an optional argument shouldn't be placed at the first.
-            So ``source_timezone`` is after ``source_time``.
-
-        Example:
-            >>> import datetime
-            >>> from dateutil import tz
-            >>> datetime_with_tz = datetime.datetime(2022, 4, 6, 9, 0, 0, tzinfo=tz.tzoffset("myzone", -3600*7))
-            >>> datetime_with_no_tz = datetime.datetime(2022, 4, 6, 9, 0, 0)
-            >>> df = session.create_dataframe([[datetime_with_tz, datetime_with_no_tz]], schema=["a", "b"])
-            >>> result = df.select(convert_timezone(lit("UTC"), col("a")), convert_timezone(lit("UTC"), col("b"), lit("Asia/Shanghai"))).collect()
-            >>> result[0][0]
-            datetime.datetime(2022, 4, 6, 16, 0, tzinfo=<UTC>)
-            >>> result[0][1]
-            datetime.datetime(2022, 4, 6, 1, 0)
+    Args:
+        target_timezone: The time zone to which the input timestamp should be converted.=
+        source_time: The timestamp to convert. When it's a TIMESTAMP_LTZ, use ``None`` for ``source_timezone``.
+        source_timezone: The time zone for the ``source_time``. Required for timestamps with no time zone (i.e. TIMESTAMP_NTZ). Use ``None`` if the timestamps have a time zone (i.e. TIMESTAMP_LTZ). Default is ``None``.
+
+    Note:
+        The sequence of the 3 params is different from the SQL function, which two overloads:
+
+        - ``CONVERT_TIMEZONE( <source_tz> , <target_tz> , <source_timestamp_ntz> )``
+        - ``CONVERT_TIMEZONE( <target_tz> , <source_timestamp> )``
+
+        The first parameter ``source_tz`` is optional. But in Python an optional argument shouldn't be placed at the first.
+        So ``source_timezone`` is after ``source_time``.
+
+    Examples:
+        >>> import datetime
+        >>> from dateutil import tz
+        >>> datetime_with_tz = datetime.datetime(2022, 4, 6, 9, 0, 0, tzinfo=tz.tzoffset("myzone", -3600*7))
+        >>> datetime_with_no_tz = datetime.datetime(2022, 4, 6, 9, 0, 0)
+        >>> df = session.create_dataframe([[datetime_with_tz, datetime_with_no_tz]], schema=["a", "b"])
+        >>> result = df.select(convert_timezone(lit("UTC"), col("a")), convert_timezone(lit("UTC"), col("b"), lit("Asia/Shanghai"))).collect()
+        >>> result[0][0]
+        datetime.datetime(2022, 4, 6, 16, 0, tzinfo=<UTC>)
+        >>> result[0][1]
+        datetime.datetime(2022, 4, 6, 1, 0)
     """
     source_tz = _to_col_if_str(source_timezone, "convert_timezone") if source_timezone is not None else None
     target_tz = _to_col_if_str(target_timezone, "convert_timezone")
     source_time_to_convert = _to_col_if_str(source_time, "convert_timezone")
 
     if source_timezone is None:
         return call_builtin("convert_timezone", target_tz, source_time_to_convert)
@@ -664,15 +667,15 @@
 
 def grouping(*cols: ColumnOrName) -> Column:
     """
     Describes which of a list of expressions are grouped in a row produced by a GROUP BY query.
 
     :func:`grouping_id` is an alias of :func:`grouping`.
 
-    Example::
+    Examples:
         >>> from pystarburst import GroupingSets
         >>> df = session.create_dataframe([[1, 2, 3], [4, 5, 6]],schema=["a", "b", "c"])
         >>> grouping_sets = GroupingSets([col("a")], [col("b")], [col("a"), col("b")])
         >>> df.group_by_grouping_sets(grouping_sets).agg([count("c"), grouping("a"), grouping("b"), grouping("a", "b")]).collect()
         [Row(A=1, B=2, COUNT(C)=1, GROUPING(A)=0, GROUPING(B)=0, GROUPING(A, B)=0), \
 Row(A=4, B=5, COUNT(C)=1, GROUPING(A)=0, GROUPING(B)=0, GROUPING(A, B)=0), \
 Row(A=1, B=None, COUNT(C)=1, GROUPING(A)=0, GROUPING(B)=1, GROUPING(A, B)=1), \
@@ -854,15 +857,17 @@
     """Returns values from the specified column rounded to the nearest equal or
     smaller integer."""
     c = _to_col_if_str(e, "floor")
     return builtin("floor")(c)
 
 
 def hypot(e: ColumnOrName, f: ColumnOrName) -> Column:
-    """Returns sqrt(a^2 + b^2)."""
+    """
+    Returns:
+        sqrt(a^2 + b^2)"""
     a = _to_col_if_str(e, "hypot")
     b = _to_col_if_str(f, "hypot")
     return builtin("sqrt")(a * a + b * b)
 
 
 def sin(e: ColumnOrName) -> Column:
     """Computes the sine of its argument; the argument should be expressed in radians."""
@@ -938,43 +943,42 @@
 
 def crc32(e: ColumnOrName) -> Column:
     """Computes the CRC-32 of binary."""
     c = _to_col_if_str(e, "crc32").cast(BinaryType())
     return builtin("crc32")(c)
 
 
-def xxhash64(e: ColumnOrName) -> Column:
+def xxhash64(col: ColumnOrName) -> Column:
     """Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm,
     and returns the result as a varbinary column.
 
-    Example::
+    Args:
+        col: The column or value to be hashed
 
+    Examples:
         >>> df = session.create_dataframe(['a'], schema=["a"])
         >>> df.select(xxhash64("a").alias("xxhash64")).collect()
         [Row(xxhash64=-3292477735350538661)]
-
-    Args:
-        e: The column or value to be hashed
     """
-    c = _to_col_if_str(e, "xxhash64").cast(BinaryType())
+    c = _to_col_if_str(col, "xxhash64").cast(BinaryType())
     return builtin("from_big_endian_64")(builtin("xxhash64")(c))
 
 
-def hash(e: ColumnOrName) -> Column:
+def hash(col: ColumnOrName) -> Column:
     """Computes the 128-bit MurmurHash3 hash of binaryhash
 
-    Example::
+    Args:
+        col: The column or value to be hashed
 
+    Examples:
         >>> df = session.create_dataframe(['a'], schema=["a"])
         >>> df.select(xxhash64("a").alias("hash")).collect()
         [Row(hash=-3292477735350538661)]
-    Args:
-        e: The column or value to be hashed
     """
-    c = _to_col_if_str(e, "hash").cast(BinaryType())
+    c = _to_col_if_str(col, "hash").cast(BinaryType())
     return builtin("murmur3")(c)
 
 
 def base64(e: ColumnOrName) -> Column:
     """Encodes binary into a base64 string representation."""
     c = _to_col_if_str(e, "base64").cast(BinaryType())
     return builtin("to_base64")(c)
@@ -1055,26 +1059,25 @@
     repeat_array = builtin("repeat")(c, lit(n))
     return builtin("array_join")(repeat_array, "")
 
 
 def reverse(col: ColumnOrName) -> Column:
     """Returns a reversed string or an array with reverse order of elements.
 
-    Example::
+    Examples:
 
         >>> df = session.create_dataframe([["Hello"], ["abc"]], schema=["col1"])
         >>> df.select(reverse(col("col1"))).show()
         -----------------------
         |"reverse(col1)"      |
         -----------------------
         |olleH                |
         |cba                  |
         -----------------------
         <BLANKLINE>
-
         >>> df = session.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], ['data'])
         >>> res = df.select(reverse(df.data).alias('r')).collect()
         [Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]
     """
     col = _to_col_if_str(col, "reverse")
     return builtin("reverse")(col)
 
@@ -1115,23 +1118,23 @@
 
 
 def instr(e: ColumnOrName, substring: ColumnOrLiteral) -> Column:
     """Locate the position of the first occurrence of substr column in the given string. Returns null if either of the arguments are null."""
     return strpos(e, substring, 1)
 
 
-def hex(e: ColumnOrName) -> Column:
+def hex(col: ColumnOrName) -> Column:
     """Computes the hex value of the given column."""
-    c = _to_col_if_str(e, "hex")
+    c = _to_col_if_str(col, "hex")
     return builtin("to_hex")(c.cast(BinaryType()))
 
 
-def unhex(e: ColumnOrName) -> Column:
+def unhex(col: ColumnOrName) -> Column:
     """Computes each pair of characters as a hexadecimal number and converts to the byte representation of number."""
-    c = _to_col_if_str(e, "unhex")
+    c = _to_col_if_str(col, "unhex")
     return builtin("from_hex")(c)
 
 
 def levenshtein_distance(l: ColumnOrName, r: ColumnOrLiteral) -> Column:
     """Computes the Levenshtein distance of the two given strings."""
     left = _to_col_if_str(l, "levenshtein")
     right = lit(r)
@@ -1309,23 +1312,23 @@
     rep = lit(replacement)
     return builtin(sql_func_name)(sub, pat, rep)
 
 
 def concat(*cols: ColumnOrName) -> Column:
     """Concatenates one or more strings, binary values, arrays. If any of the values is null, the result is also null.
 
-    Args::
+    Args:
         cols: A list of the columns to concatenate.
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame([('abcd','123')], ['s', 'd'])
         >>> df = df.select(concat(df.s, df.d).alias('s'))
         >>> df.collect()
         [Row(s='abcd123')]
-
+        <BLANKLINE>
         >>> df = session.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])
         >>> df = df.select(concat(df.a, df.b, df.c).alias("arr"))
         >>> df.collect()
         [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]
     """
     columns = [_to_col_if_str(c, "concat") for c in cols]
     return builtin("concat")(*columns)
@@ -1376,30 +1379,61 @@
     """Converts an input expression into the corresponding time."""
     if fmt is None:
         fmt = "hh24:mm:ss"
     c = _to_col_if_str(e, "to_time")
     return builtin("to_time")(c, fmt) if fmt is not None else builtin("to_time")(c)
 
 
-def to_timestamp(e: ColumnOrName, fmt: Optional["Column"] = None) -> Column:
-    """Converts an input expression into the corresponding timestamp."""
-    if fmt is None:
-        fmt = "yyyy-mm-dd hh24:mi:ss"
-    c = _to_col_if_str(e, "to_timestamp")
-    return iff(
-        starts_with(typeof(c), lit("varchar")), builtin("to_timestamp")(c.cast(StringType()), fmt), c.cast(TimestampNTZType())
-    )
+def to_timestamp(col: ColumnOrName, fmt: str = "yyyy-MM-dd HH:mm:ss") -> Column:
+    """Converts an input expression into a timestamp using the optionally specified format.
+    Use format compatible with JodaTime's `DateTimeFormat <https://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html>`_ pattern format.
 
+    Args:
+        col: column values to convert.
+        format: format to use to convert timestamp values. Default is ``yyyy-MM-dd HH:mm:ss``.
 
-def to_date(e: ColumnOrName, fmt: Optional["Column"] = None) -> Column:
-    """Converts an input expression into a date."""
-    if fmt is None:
-        fmt = "yyyy-mm-dd"
-    c = _to_col_if_str(e, "to_date")
-    return builtin("to_date")(c, fmt) if fmt is not None else builtin("to_date")(c)
+    Examples:
+        >>> df = session.createDataFrame([('1997-02-28 10:30:00',)], ['t'])
+        >>> df.select(to_timestamp(df.t).alias('dt')).collect()
+        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]
+        <BLANKLINE>
+        >>> df = session.createDataFrame([('1997-02-28 10:30:00',)], ['t'])
+        >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()
+        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]
+    """
+    return _to_date_or_timestamp(col, fmt, TimestampNTZType)
+
+
+def to_date(col: ColumnOrName, fmt: str = "yyyy-MM-dd") -> Column:
+    """Converts an input expression into a date using the optionally specified format.
+    Use format compatible with JodaTime's `DateTimeFormat <https://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html>`_ pattern format.
+
+    Args:
+        col: column values to convert.
+        format: format to use to convert date values. Default is ``yyyy-MM-dd``.
+
+    Examples:
+        >>> df = session.createDataFrame([('1997-02-28',)], ['t'])
+        >>> df.select(to_date(df.t).alias('date')).collect()
+        [Row(date=datetime.date(1997, 2, 28))]
+        <BLANKLINE>
+        >>> df = session.createDataFrame([('1997-02-28',)], ['t'])
+        >>> df.select(to_date(df.t, 'yyyy-MM-dd').alias('date')).collect()
+        [Row(date=datetime.date(1997, 2, 28))]
+    """
+    return _to_date_or_timestamp(col, fmt, DateType)
+
+
+def _to_date_or_timestamp(col: ColumnOrName, fmt: str, datetime_type: Union[DateType, TimestampNTZType]) -> Column:
+    c = _to_col_if_str(col, "_to_date_or_timestamp")
+    return iff(
+        starts_with(typeof(c), lit("varchar")),
+        (builtin("parse_datetime")(c.cast(StringType()), fmt)).cast(datetime_type()),
+        c.cast(datetime_type()),
+    )
 
 
 def current_timestamp(precision: Optional[int] = None) -> Column:
     """Returns the current timestamp for the system."""
     if precision is None:
         return sql_expr("current_timestamp")
     return builtin("current_timestamp")(precision)
@@ -1420,15 +1454,15 @@
     return builtin("current_timezone")()
 
 
 def hour(e: ColumnOrName) -> Column:
     """
     Extracts the hour from a date or timestamp.
 
-    Example::
+    Examples:
 
         >>> import datetime
         >>> df = session.create_dataframe([
         ...     datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f"),
         ...     datetime.datetime.strptime("2020-08-21 01:30:05.000", "%Y-%m-%d %H:%M:%S.%f")
         ... ], schema=["a"])
         >>> df.select(hour("a")).collect()
@@ -1438,15 +1472,15 @@
     return builtin("hour")(c)
 
 
 def minute(e: ColumnOrName) -> Column:
     """
     Extracts the minute from a date or timestamp.
 
-    Example::
+    Examples:
 
         >>> import datetime
         >>> df = session.create_dataframe([
         ...     datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f"),
         ...     datetime.datetime.strptime("2020-08-21 01:30:05.000", "%Y-%m-%d %H:%M:%S.%f")
         ... ], schema=["a"])
         >>> df.select(minute("a")).collect()
@@ -1456,15 +1490,15 @@
     return builtin("minute")(c)
 
 
 def second(e: ColumnOrName) -> Column:
     """
     Extracts the second from a date or timestamp.
 
-    Example::
+    Examples:
 
         >>> import datetime
         >>> df = session.create_dataframe([
         ...     datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f"),
         ...     datetime.datetime.strptime("2020-08-21 01:30:05.000", "%Y-%m-%d %H:%M:%S.%f")
         ... ], schema=["a"])
         >>> df.select(second("a")).collect()
@@ -1475,15 +1509,15 @@
 
 
 def month(e: ColumnOrName) -> Column:
     """
     Extracts the month from a date or timestamp.
 
 
-    Example::
+    Examples:
 
         >>> import datetime
         >>> df = session.create_dataframe([
         ...     datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f"),
         ...     datetime.datetime.strptime("2020-08-21 01:30:05.000", "%Y-%m-%d %H:%M:%S.%f")
         ... ], schema=["a"])
         >>> df.select(month("a")).collect()
@@ -1493,15 +1527,15 @@
     return builtin("month")(c)
 
 
 def quarter(e: ColumnOrName) -> Column:
     """
     Extracts the quarter from a date or timestamp.
 
-    Example::
+    Examples:
 
         >>> import datetime
         >>> df = session.create_dataframe([
         ...     datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f"),
         ...     datetime.datetime.strptime("2020-08-21 01:30:05.000", "%Y-%m-%d %H:%M:%S.%f")
         ... ], schema=["a"])
         >>> df.select(quarter("a")).collect()
@@ -1511,15 +1545,15 @@
     return builtin("quarter")(c)
 
 
 def year(e: ColumnOrName) -> Column:
     """
     Extracts the year from a date or timestamp.
 
-    Example::
+    Examples:
 
         >>> import datetime
         >>> df = session.create_dataframe([
         ...     datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f"),
         ...     datetime.datetime.strptime("2020-08-21 01:30:05.000", "%Y-%m-%d %H:%M:%S.%f")
         ... ], schema=["a"])
         >>> df.select(year("a")).collect()
@@ -1529,15 +1563,15 @@
     return builtin("year")(c)
 
 
 def day(e: ColumnOrName) -> Column:
     """
     Extracts the day from a date or timestamp.
 
-    Example::
+    Examples:
 
         >>> import datetime
         >>> df = session.create_dataframe([
         ...     datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f"),
         ...     datetime.datetime.strptime("2020-08-21 01:30:05.000", "%Y-%m-%d %H:%M:%S.%f")
         ... ], schema=["a"])
         >>> df.select(day("a")).collect()
@@ -1561,15 +1595,15 @@
 
     Args:
         col: The array column
 
     Returns:
         Returns a new ARRAY that contains only the distinct elements from the input ARRAY.
 
-    Example::
+    Examples:
 
         >>> from pystarburst.functions import array_construct,array_distinct,lit
         >>> df = session.createDataFrame([["1"]], ["A"])
         >>> df = df.withColumn("array", array_construct(lit(1), lit(1), lit(1), lit(2), lit(3), lit(2), lit(2)))
         >>> df.withColumn("array_d", array_distinct("ARRAY")).show()
         |"a"  |"array"                |"array_d"  |
         -------------------------------------------
@@ -1593,58 +1627,58 @@
     a2 = _to_col_if_str(array2, "array_intersect")
     return builtin("array_intersect")(a1, a2)
 
 
 def array_union(array1: ColumnOrName, array2: ColumnOrName) -> Column:
     """Returns an array of the elements in the union of array1 and array2, without duplicates.
 
-    Example::
+    Examples:
 
         >>> from pystarburst import Row
         >>> df = session.createDataFrame([Row(c1=["b", "a", "c"], c2=["c", "d", "a", "f"])])
         >>> df.select(array_union(df.c1, df.c2)).collect()
         [Row(array_union(c1, c2)=['b', 'a', 'c', 'd', 'f'])]
     """
     a1 = _to_col_if_str(array1, "array_union")
     a2 = _to_col_if_str(array2, "array_union")
     return builtin("array_union")(a1, a2)
 
 
 def array_except(array1: ColumnOrName, array2: ColumnOrName) -> Column:
     """Returns an array of elements in array1 but not in array2, without duplicates.
 
-    Example::
+    Examples:
 
         >>> from pystarburst import Row
         >>> df = session.createDataFrame([Row(c1=["b", "a", "c"], c2=["c", "d", "a", "f"])])
         >>> df.select(array_except(df.c1, df.c2)).collect()
         [Row(array_except(c1, c2)=['b'])]
     """
     a1 = _to_col_if_str(array1, "array_except")
     a2 = _to_col_if_str(array2, "array_except")
     return builtin("array_except")(a1, a2)
 
 
 def array_min(array: ColumnOrName) -> Column:
     """Returns the minimum value of input array. Null values are omitted.
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])
         >>> df.select(array_min(df.data).alias('min')).collect()
         [Row(min=1), Row(min=-1)]
     """
     a = _to_col_if_str(array, "array_min")
     a_wo_nulls = array_except(a, lit([None]))
     return builtin("array_min")(a_wo_nulls)
 
 
 def array_max(array: ColumnOrName) -> Column:
     """Returns the maximum value of input array. Null values are omitted.
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])
         >>> df.select(array_max(df.data).alias('max')).collect()
         [Row(max=3), Row(max=10)]
     """
     a = _to_col_if_str(array, "array_max")
     a_wo_nulls = array_except(a, lit([None]))
     return builtin("array_max")(a_wo_nulls)
@@ -1653,15 +1687,15 @@
 def flatten(array: ColumnOrName) -> Column:
     """Returns a single array from an array of arrays. If the array is nested more than
     two levels deep, then only a single level of nesting is removed.
 
     Args:
         array: the input array
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])
         >>> df.select(flatten(df.data)).show()
         ----------------------
         |"flatten(data)"     |
         ----------------------
         |[1, 2, 3, 4, 5, 6]  |
         |[4, 5]              |
@@ -1673,19 +1707,19 @@
 
 def array_sort(array: ColumnOrName, func: Callable = None) -> Column:
     """Sorts and returns the array based on the given comparator function.
     The comparator will take two nullable arguments representing two nullable elements of the array.
     It returns -1, 0, or 1 as the first nullable element is less than, equal to, or greater than the second nullable element.
     If the comparator function returns other values (including NULL), the query will fail and raise an error.
 
-    Example:
+    Examples:
         >>> df = session.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])
         >>> df.select(array_sort(df.data).alias('r')).collect()
         [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]
-
+        <BLANKLINE>
         >>> df = session.create_dataframe([([3, 2, 5, 1, 2],)], ["data"])
         >>> df.select(array_sort("data", lambda x, y: iff(x < y, 1, iff(x == y, 0, -1)))).collect()
         [Row([5, 3, 2, 2, 1])]
     """
     a = _to_col_if_str(array, "array_sort")
     if func:
         return builtin("array_sort")(a, _create_lambda(func))
@@ -1697,15 +1731,15 @@
     """Returns rows of array column in sorted order. Users can choose the sort order.
 
     Args:
         array: name of the column or column element which describes the column
         sort_ascending: Boolean that decides if array elements are sorted in ascending order.
             Defaults to True.
 
-    Examples::
+    Examples:
         >>> df = session.sql("select array[20, 0, null, 10] as a")
         >>> df.select(sort_array(df.a).as_("sorted_a")).show()
         ---------------------
         |"sorted_a"         |
         ---------------------
         |[0, 10, 20, None]  |
         ---------------------
@@ -1729,15 +1763,15 @@
 def shuffle(array: ColumnOrName) -> Column:
     """
     Generates a random permutation of the given array.
 
     Args:
         array: The column containing the source ARRAY.
 
-    Examples::
+    Examples:
         >>> df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], ['data'])
         >>> df.select(shuffle(df.data).alias('s')).collect()
         [Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]
     """
 
     a = _to_col_if_str(array, "shuffle")
     return builtin("shuffle")(a)
@@ -1748,15 +1782,15 @@
     If `step` is not set, incrementing by 1 if start is less than or equal to stop, otherwise -1.
 
     Args:
         start: the column that contains the integer to start with (inclusive).
         stop: the column that contains the integer to stop (inclusive).
         step: the column that contains the integer to increment.
 
-    Example::
+    Examples:
         >>> df1 = session.create_dataframe([(-2, 2)], ["a", "b"])
         >>> df1.select(sequence("a", "b").alias("result")).show()
         ---------------------
         |"result"           |
         ---------------------
         |[-2, -1, 0, 1, 2]  |
         ---------------------
@@ -1798,15 +1832,15 @@
 
 def all_match(array: ColumnOrName, func: Callable) -> Column:
     """Returns whether all elements of an array match the given predicate.
     Returns true if all the elements match the predicate (a special case is when the array is empty);
     false if one or more elements don’t match;
     NULL if the predicate function returns NULL for one or more elements and true for all other elements.
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame(
         ...     [(1, ["bar"]), (2, ["foo", "bar"]), (3, ["foobar", "foo"])],
         ...     ["key", "values"]
         ... )
         >>> df.select(forall("values", lambda x: x.rlike("foo")).alias("all_foo")).show()
         -------------
         |"all_foo"  |
@@ -1822,15 +1856,15 @@
 
 def any_match(array: ColumnOrName, func: Callable) -> Column:
     """Returns whether any elements of an array match the given predicate.
     Returns true if one or more elements match the predicate;
     false if none of the elements matches (a special case is when the array is empty);
     NULL if the predicate function returns NULL for one or more elements and false for all other elements.
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame([(1, [1, 2, 3, 4]), (2, [3, -1, 0])],["key", "values"])
         >>> df.select(exists("values", lambda x: x < 0).alias("any_negative")).show()
         ------------------
         |"any_negative"  |
         ------------------
         |False           |
         |True            |
@@ -1839,15 +1873,15 @@
     a = _to_col_if_str(array, "any_match")
     return builtin("any_match")(a, _create_lambda(func))
 
 
 def filter(array: ColumnOrName, func: Callable) -> Column:
     """Constructs an array from those elements of array for which func returns true
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame(
         ...     [(1, ["2018-09-20",  "2019-02-03", "2019-07-01", "2020-06-01"])],
         ...     ["key", "values"]
         ... )
         >>> def after_second_quarter(x):
         ...     return month(to_date(x)) > 6
         >>> df.select(
@@ -1866,15 +1900,15 @@
 def reduce(
     array: ColumnOrName, initialState: ColumnOrName, input_func: Callable, output_func: Optional[Callable] = None
 ) -> Column:
     """Returns a single value reduced from array. inputFunction will be invoked for each element in array in order.
     In addition to taking the element, inputFunction takes the current state, initially initialState, and returns the new state.
     outputFunction will be invoked to turn the final state into the result value. It may be the identity function (i -> i) (default if not specified).
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], ["id", "values"])
         >>> df.select(aggregate("values", lit(0.0), lambda acc, x: acc + x).alias("sum")).show()
         ---------
         |"sum"  |
         ---------
         |42.0   |
         ---------
@@ -1904,31 +1938,30 @@
 
 
 def datediff(part: str, col1: ColumnOrName, col2: ColumnOrName) -> Column:
     """Calculates the difference between two date, time, or timestamp columns based on the date or time part requested.
 
     `Supported date and time parts <https://trino.io/docs/current/functions/datetime.html?highlight=date_diff#date_diff>`_
 
-    Example::
+    Args:
+        part: The time part to use for calculating the difference
+        col1: The first timestamp column or minuend in the datediff
+        col2: The second timestamp column or the subtrahend in the datediff
+
+    Examples:
 
         >>> # year difference between two date columns
         >>> import datetime
         >>> date_df = session.create_dataframe([[datetime.date(2020, 1, 1), datetime.date(2021, 1, 1)]], schema=["date_col1", "date_col2"])
         >>> date_df.select(datediff("year", col("date_col1"), col("date_col2")).alias("year_diff")).show()
         ---------------
         |"YEAR_DIFF"  |
         ---------------
         |1            |
         ---------------
-        <BLANKLINE>
-
-    Args:
-        part: The time part to use for calculating the difference
-        col1: The first timestamp column or minuend in the datediff
-        col2: The second timestamp column or the subtrahend in the datediff
     """
     if not isinstance(part, str):
         raise ValueError("part must be a string")
     c1 = _to_col_if_str(col1, "date_diff")
     c2 = _to_col_if_str(col2, "date_diff")
     return builtin("date_diff")(part, c1, c2)
 
@@ -1943,85 +1976,82 @@
 
 
 def date_add(col: ColumnOrName, days: Union[ColumnOrName, int]) -> Column:
     """Adds the specified value in days from the specified date.
 
     `Supported date and time parts <https://trino.io/docs/current/functions/datetime.html#date_add>`_
 
-    Example::
+    Args:
+        col: The timestamp column
+        days: The number of days to add
+
+    Examples:
 
         >>> # add 24 days on dates
         >>> import datetime
         >>> date_df = session.create_dataframe([[datetime.date(2020, 10, 20)]], schema=["date_col"])
         >>> date_df.select(date_add(col("date_col"), lit(24)).alias("date")).show()
         ----------------
         |"DATE"        |
         ----------------
         |2020-11-13    |
         ----------------
-        <BLANKLINE>
-
-    Args:
-        col: The timestamp column
-        days: The number of days to add
     """
     c = _to_col_if_str(col, "date_add")
     d = _to_col_if_str_or_int(days, "date_add")
     return builtin("date_add")("day", d, c)
 
 
 def date_format(col: ColumnOrName, date_time_format: str) -> Column:
     """Format string that is compatible with JodaTime’s DateTimeFormat pattern format.
 
     `Supported date and time parts <https://trino.io/docs/current/functions/datetime.html#format_datetime>`_
 
-    Example::
+    Args:
+        col: The timestamp column
+        date_time_format: The format string
+
+    Examples:
 
         >>> # add one year on dates
         >>> import datetime
         >>> date_df = session.create_dataframe([[datetime.date(2020, 1, 1)]], schema=["date_col"])
         >>> date_df.select(date_format(col("date_col"), "YYYY/MM/dd").alias("date")).show()
         ----------------
         |"DATE"        |
         ----------------
         |"2021/01/01"  |
         ----------------
-        <BLANKLINE>
-
-    Args:
-        col: The timestamp column
-        date_time_format: The format string
     """
     if not isinstance(date_time_format, str):
         raise ValueError("part must be a string")
     c = _to_col_if_str(col, "format_datetime")
     return builtin("format_datetime")(c, date_time_format)
 
 
 def date_sub(col: ColumnOrName, days: Union[ColumnOrName, int]) -> Column:
     """Subtracts the specified value in days from the specified date.
 
     `Supported date and time parts <https://trino.io/docs/current/functions/datetime.html#date_add>`_
 
-    Example::
+    Args:
+        col: The timestamp column
+        days: The number of days to subtract
+
+    Examples:
 
         >>> # subtracts 24 days on dates
         >>> import datetime
         >>> date_df = session.create_dataframe([[datetime.date(2020, 10, 20)]], schema=["date_col"])
         >>> date_df.select(date_sub(col("date_col"), lit(24)).alias("date")).show()
         ----------------
         |"DATE"        |
         ----------------
         |2020-09-26    |
         ----------------
-        <BLANKLINE>
-
-    Args:
-        col: The timestamp column
-        days: The number of days to subtract
     """
     c = _to_col_if_str(col, "date_add")
     d = _to_col_if_str_or_int(days, "date_add")
     return builtin("date_add")("day", -d, c)
 
 
 def date_trunc(part: str, expr: ColumnOrName) -> Column:
@@ -2029,15 +2059,15 @@
     Truncates a DATE, TIME, or TIMESTAMP to the specified precision.
 
     Note that truncation is not the same as extraction. For example:
     - Truncating a timestamp down to the quarter returns the timestamp corresponding to midnight of the first day of the
     quarter for the input timestamp.
     - Extracting the quarter date part from a timestamp returns the quarter number of the year in the timestamp.
 
-    Example::
+    Examples:
         >>> import datetime
         >>> df = session.create_dataframe(
         ...     [[datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f")]],
         ...     schema=["a"],
         ... )
         >>> df.select(date_trunc("YEAR", "a"), date_trunc("MONTH", "a"), date_trunc("DAY", "a")).collect()
         [Row(DATE_TRUNC("YEAR", "A")=datetime.datetime(2020, 1, 1, 0, 0), DATE_TRUNC("MONTH", "A")=datetime.datetime(2020, 5, 1, 0, 0), DATE_TRUNC("DAY", "A")=datetime.datetime(2020, 5, 1, 0, 0))]
@@ -2050,15 +2080,15 @@
     return builtin("date_trunc")(part, expr_col)
 
 
 def dayofmonth(e: ColumnOrName) -> Column:
     """
     Extracts the corresponding day (number) of the month from a date or timestamp.
 
-    Example::
+    Examples:
         >>> import datetime
         >>> df = session.create_dataframe(
         ...     [[datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f")]],
         ...     schema=["a"],
         ... )
         >>> df.select(dayofmonth("a")).collect()
         [Row(day_of_month("A")=1)]
@@ -2067,15 +2097,15 @@
     return builtin("day_of_month")(c)
 
 
 def dayofweek(e: ColumnOrName) -> Column:
     """
     Extracts the corresponding day (number) of the week from a date or timestamp.
 
-    Example::
+    Examples:
         >>> import datetime
         >>> df = session.create_dataframe(
         ...     [[datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f")]],
         ...     schema=["a"],
         ... )
         >>> df.select(dayofweek("a")).collect()
         [Row(day_of_week("A")=5)]
@@ -2084,15 +2114,15 @@
     return builtin("day_of_week")(c)
 
 
 def dayofyear(e: ColumnOrName) -> Column:
     """
     Extracts the corresponding day (number) of the year from a date or timestamp.
 
-    Example::
+    Examples:
         >>> import datetime
         >>> df = session.create_dataframe(
         ...     [[datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f")]],
         ...     schema=["a"],
         ... )
         >>> df.select(dayofyear("a")).collect()
         [Row(day_of_year("A")=122)]
@@ -2103,40 +2133,39 @@
 
 def last_day(col: ColumnOrName) -> Column:
     """
     Returns the last day of the month which the given date belongs to.
 
     `Supported date and time parts <https://trino.io/docs/current/functions/datetime.html#last_day_of_month>`_
 
-    Example::
+    Args:
+        col: The timestamp column
+
+    Examples:
         >>> import datetime
         >>> df = session.create_dataframe(
         ...     [[datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f")]],
         ...     schema=["a"],
         ... )
         >>> date_df.select(last_day(col("a")).alias("date")).show()
         ----------------
         |"DATE"        |
         ----------------
         |2020-05-31    |
         ----------------
-        <BLANKLINE>
-
-    Args:
-        col: The timestamp column
     """
     c = _to_col_if_str(col, "last_day_of_month")
     return builtin("last_day_of_month")(c)
 
 
 def weekofyear(e: ColumnOrName) -> Column:
     """
     Extracts the corresponding week (number) of the year from a date or timestamp.
 
-    Example::
+    Examples:
 
         >>> import datetime
         >>> df = session.create_dataframe(
         ...     [[datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f")]],
         ...     schema=["a"],
         ... )
         >>> df.select(weekofyear("a")).collect()
@@ -2151,49 +2180,49 @@
     Truncates a DATE, TIME, or TIMESTAMP to the specified precision.
 
     Note that truncation is not the same as extraction. For example:
     - Truncating a timestamp down to the quarter returns the timestamp corresponding to midnight of the first day of the
     quarter for the input timestamp.
     - Extracting the quarter date part from a timestamp returns the quarter number of the year in the timestamp.
 
-    Example::
+    Args:
+        col: the date/time/timestamp column
+        trunc_format: the truncation format
+
+    Examples:
         >>> import datetime
         >>> df = session.create_dataframe(
         ...     [[datetime.datetime.strptime("2020-05-01 13:11:20.000", "%Y-%m-%d %H:%M:%S.%f")]],
         ...     schema=["a"],
         ... )
         >>> df.select(trunc("YEAR", "a"), trunc("MONTH", "a")).collect()
         [Row(DATE_TRUNC("YEAR", "A")=datetime.date(2020, 1, 1), DATE_TRUNC("MONTH", "A")=datetime.datetime(2020, 5, 1)]
-
-    Args:
-        col: the date/time/timestamp column
-        trunc_format: the truncation format
     """
     expr_col = _to_col_if_str(col, "trunc")
     expr_trunc_format = lit(trunc_format)
     return builtin("date_trunc")(expr_trunc_format, expr_col).cast("date")
 
 
 def make_date(col_year: ColumnOrName, col_month: ColumnOrName, col_day: ColumnOrName) -> Column:
     """
     Generate a date from a year, month and day columns.
 
-    Example::
+    Args:
+        col_year: The year column
+        col_month: The month column
+        col_day: The day column
+
+    Examples:
         >>> import datetime
         >>> df = session.create_dataframe(
         ...     [[2020, 1, 30]],
         ...     schema=["a", "b", "c"],
         ... )
         >>> df.select(make_date("a", "b", "c")).collect()
         [Row(MAKE_DATE("A", "B", "C")=datetime.date(2020, 1, 30)]
-
-    Args:
-        col_year: The year column
-        col_month: The month column
-        col_day: The day column
     """
     expr_col_year = _to_col_if_str(col_year, "make_date")
     expr_col_month = _to_col_if_str(col_month, "make_date")
     expr_col_day = _to_col_if_str(col_day, "make_date")
     return builtin("date_parse")(builtin("format")("%d/%d/%d", expr_col_year, expr_col_month, expr_col_day), "%Y/%m/%d").cast(
         "date"
     )
@@ -2202,59 +2231,58 @@
 def to_utc_timestamp(col: ColumnOrName, col_tz: ColumnOrLiteralStr) -> Column:
     """
     Takes a timestamp which is timezone-agnostic, and interprets it as a timestamp
     in the given timezone, and renders that timestamp as a timestamp in UTC.
 
     `Supported date and time parts <https://trino.io/docs/current/functions/datetime.html#with_timezone>`_
 
-    Example::
+    Args:
+        col: The timestamp column
+        col_tz: the timezone column
+
+    Examples:
         >>> df = session.create_dataframe(
         ...     [["1997-02-28 10:30:00", "Japan")]],
         ...     schema=["timestamp", "tz"],
         ... )
         >>> date_df.select(to_utc_timestamp(col("timestamp"), col("tz")).alias("datetime")).show()
         ------------------------
         |"DATETIME"            |
         ------------------------
         |1997-02-28 1:30:00 UTC|
         ------------------------
         <BLANKLINE>
-
-    Args:
-        col: The timestamp column
-        col_tz: the timezone column
     """
     expr_col = _to_col_if_str(col, "to_utc_timestamp")
     expr_col_tz = lit(col_tz)
     return builtin("at_timezone")(builtin("with_timezone")(expr_col.cast("timestampntz"), expr_col_tz), "UTC")
 
 
 def from_utc_timestamp(col: ColumnOrName, col_tz: ColumnOrLiteralStr) -> Column:
     """
     Takes a timestamp which is timezone-agnostic, and interprets it as a timestamp
     in UTC, and renders that timestamp as a timestamp in the given time zone.
 
     `Supported date and time parts <https://trino.io/docs/current/functions/datetime.html#with_timezone>`_
 
-    Example::
+    Args:
+        col: The timestamp column
+        col_tz: the timezone column
+
+    Examples:
         >>> df = session.create_dataframe(
         ...     [["1997-02-28 1:30:00", "Japan")]],
         ...     schema=["timestamp", "tz"],
         ... )
         >>> date_df.select(from_utc_timestamp(col("timestamp"), col("tz")).alias("datetime")).show()
         ---------------------------
         |"DATETIME"               |
         ---------------------------
         |1997-02-28 10:30:00 Japan|
         ---------------------------
-        <BLANKLINE>
-
-    Args:
-        col: The timestamp column
-        col_tz: the timezone column
     """
     expr_col = _to_col_if_str(col, "from_utc_timestamp")
     expr_col_tz = lit(col_tz)
     return builtin("at_timezone")(builtin("with_timezone")(expr_col.cast("timestampntz"), "UTC"), expr_col_tz)
 
 
 def typeof(col: ColumnOrName) -> Column:
@@ -2277,15 +2305,15 @@
     c = _to_col_if_str(e, "json_parse")
     return builtin("json_parse")(c)
 
 
 def json_array_length(col: ColumnOrName) -> Column:
     """Returns the array length of json (a string containing a JSON array):
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame([(None,), ('[1, 2, 3]',), ('[]',)], ['data'])
         >>> df.select(json_array_length(df.data)).show()
         -----------------------------
         |"json_array_length(data)"  |
         -----------------------------
         |NULL                       |
         |3                          |
@@ -2295,43 +2323,45 @@
     c = _to_col_if_str(col, "json_array_length")
     return builtin("json_array_length")(c)
 
 
 def to_json(col: ColumnOrName) -> Column:
     """Cast to a JSON string.
 
-    Example::
+    Examples:
         >>> data = [(1, {"name": "Alice"})]
         >>> df = session.createDataFrame(data, ["key", "value"])
         >>> df.select(to_json(df.value).alias("json")).collect()
         [Row(json='{"name":"Alice"}')]
+        <BLANKLINE>
         >>> data = [(1, [{"name": "Alice"}, {"name": "Bob"}])]
         >>> df = session.createDataFrame(data, ["key", "value"])
         >>> df.select(to_json(df.value).alias("json")).collect()
         [Row(json='[{"name":"Alice"},{"name":"Bob"}]')]
+        <BLANKLINE>
         >>> data = [(1, ["Alice", "Bob"])]
         >>> df = session.createDataFrame(data, ["key", "value"])
         >>> df.select(to_json(df.value).alias("json")).collect()
         [Row(json='["Alice","Bob"]')]
     """
     c = _to_col_if_str(col, "to_json")
     return cast(c, JsonType())
 
 
 def from_json(col: ColumnOrName, to: Union[str, DataType]) -> Column:
     """Casting to BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, REAL, DOUBLE or VARCHAR is supported.
     Casting to ARRAY and MAP is supported when the element type of the array is one of the supported types,
     or when the key type of the map is VARCHAR and value type of the map is one of the supported types.
 
-    Example::
+    Examples:
         >>> df = session.sql("SELECT JSON '{\"v1\":123,\"v2\":\"abc\",\"v3\":true}' as col1")
         >>> schema = StructType([StructField('v1', IntegerType(), nullable=True), StructField('v2', StringType(), nullable=True), StructField('v3', BooleanType(), nullable=True)])
         >>> df.select(from_json("col1", schema)).collect()
         [Row(col1=(v1: 123, v2: 'abc', v3: True))]
-
+        <BLANKLINE>
         >>> df = session.sql("select JSON '[1,null,456]' as col1")
         >>> df.select(from_json("col1", ArrayType(IntegerType()))).collect()
         [Row(col1=[1, None, 456])]
     """
     c = _to_col_if_str(col, "from_json")
     return cast(c, to)
 
@@ -2345,15 +2375,15 @@
         path: path to the json object to extract
         json_path_mode: The JSON path expression can be evaluated in two modes: strict and lax.
             In the strict mode, it is required that the input JSON data strictly
             fits the schema required by the path expression.
             In the lax mode, the input JSON data can diverge from the expected schema.
             Details and examples: https://trino.io/docs/current/functions/json.html#json-path-modes
 
-    Example::
+    Examples:
         >>> data = [("1", '''{"f1": "value1", "f2": "value2"}'''), ("2", '''{"f1": "value12"}''')]
         >>> df = session.createDataFrame(data, ["key", "jstring"])
         >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias("c0"), get_json_object(df.jstring, '$.f2').alias("c1")).collect()
         [Row(key='1', c0='"value1"', c1='"value2"'), Row(key='2', c0='"value12"', c1=None)]
     """
     if json_path_mode not in ("lax", "strict"):
         raise ValueError(f"Invalid json path mode '{json_path_mode}'. Expected 'lax' or 'strict'")
@@ -2369,15 +2399,15 @@
         fields: a field or fields to extract
         json_path_mode: The JSON path expression can be evaluated in two modes: strict and lax.
             In the strict mode, it is required that the input JSON data strictly
             fits the schema required by the path expression.
             In the lax mode, the input JSON data can diverge from the expected schema.
             Details and examples: https://trino.io/docs/current/functions/json.html#json-path-modes
 
-    Example::
+    Examples:
         >>> data = [("1", '''{"f1": "value1", "f2": "value2"}'''), ("2", '''{"f1": "value12"}''')]
         >>> df = session.createDataFrame(data, ["key", "jstring"])
         >>> json_tuple_list = json_tuple(df.jstring, 'f1', 'f2')
         >>> df.select(df.key, *json_tuple_list).collect()
         [Row('1', '"value1"', '"value2"'), Row('2', '"value12"', None)]
     """
     c = _to_col_if_str(col, "json_tuple")
@@ -2414,15 +2444,15 @@
 
     Args:
         array: Column containing the source ARRAY.
         element: Column containing the element to be prepended.
             The data type does need to match the data type of the
             existing elements in the ARRAY.
 
-    Example::
+    Examples:
         >>> from pystarburst import Row
         >>> df = session.create_dataframe([Row(a=[1, 2, 3])])
         >>> df.select(array_prepend("a", lit(4)).alias("result")).show()
         ----------------
         |"result"      |
         ----------------
         |[4, 1, 2, 3]  |
@@ -2437,15 +2467,15 @@
 def array_compact(array: ColumnOrName) -> Column:
     """Returns a compacted ARRAY with missing and null values removed,
     effectively converting sparse arrays into dense arrays.
 
     Args:
         array: Column containing the source ARRAY to be compacted
 
-    Example::
+    Examples:
         >>> from pystarburst import Row
         >>> df = session.create_dataframe([Row(a=[1, None, 3])])
         >>> df.select("a", array_compact("a").alias("compacted")).show()
         ------------------------------
         |"a"           |"compacted"  |
         ------------------------------
         |[1, None, 3]  |[1, 3]       |
@@ -2458,15 +2488,15 @@
 
 def array_construct(*args: ColumnOrLiteral) -> Column:
     """Creates a new array column.
 
     Args:
         args: Columns containing the values (or expressions that evaluate to values).
 
-    Example::
+    Examples:
         >>> df = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
         >>> df.select(array_construct("a", "b").alias("result")).show()
         ------------
         |"result"  |
         ------------
         |[1, 2]    |
         |[3, 4]    |
@@ -2500,15 +2530,15 @@
             effect as using array_prepend).
             A negative position is interpreted as an index from the back of the array (e.g.
             -1 results in insertion before the last element in the array).
         element: Column containing the element to be inserted. The new element is located at
             position pos. The relative order of the other elements from the source
             array is preserved.
 
-    Example::
+    Examples:
         >>> from pystarburst import Row
         >>> df = session.create_dataframe([Row([1, 2]), Row([1, 3])], schema=["a"])
         >>> df.select(array_insert("a", lit(1), lit(10)).alias("result")).show()
         --------------
         |"result"    |
         --------------
         |[10, 1, 2]  |
@@ -2531,29 +2561,30 @@
     a = _to_col_if_str(array, "array_remove")
     e = lit(element) if isinstance(element, (int, float)) else _to_col_if_str(element, "array_remove")
     return builtin("array_remove")(a, e)
 
 
 def element_at(col1: Union[ColumnOrName, int], col2: Union[ColumnOrName, int]) -> Column:
     """Returns element of array at given index or value for given key in map.
-    Notice that Trino ARRAY indexing starts from 1.
 
-    Example::
+    Note:
+        Trino ARRAY indexing starts from 1.
+
+    Examples:
 
         >>> from pystarburst.functions import lit
         >>> df = session.createDataFrame([({"a": 1.0, "b": 2.0}, [1, 2, 3],), ({}, [],)], ["map", "list"])
         >>> df.select(element_at(df.list, 1).as_("idx1")).sort(col("idx1")).show()
         ----------
         |"idx1"  |
         ----------
         |NULL    |
         |1       |
         ----------
         <BLANKLINE>
-
         >>> df.select(element_at(df.map, lit("a")).as_("get_a")).sort(col("get_a")).show()
         -----------
         |"get_a"  |
         -----------
         |NULL     |
         |1.0      |
         -----------
@@ -2562,15 +2593,15 @@
     c2 = _to_col_if_str_or_int(col2, "element_at")
     return builtin("element_at")(c1, c2)
 
 
 def get(col1: Union[ColumnOrName, int], col2: Union[ColumnOrName, int]) -> Column:
     """Returns element of array at given (0-based) index. If the index points outside of the array boundaries, then this function returns NULL.
 
-    Example::
+    Examples:
 
         >>> df = session.createDataFrame([({"a": 1.0, "b": 2.0}, [1, 2, 3],), ({}, [],)], ["map", "list"])
         >>> df.select(get(df.list, 1).as_("idx1")).sort(col("idx1")).show()
         ----------
         |"idx1"  |
         ----------
         |NULL    |
@@ -2581,24 +2612,26 @@
     c2 = _to_col_if_str_or_int(col2, "get")
 
     return iff(c2 >= 0, builtin("element_at")(c1, c2 + 1), None)
 
 
 def array_position(array: ColumnOrName, element: ColumnOrName) -> Column:
     """Returns the position of the first occurrence of the element in array (or 0 if not found).
-    Notice that Trino ARRAY indexing starts from 1"""
+
+    Note:
+        Trino ARRAY indexing starts from 1"""
     a = _to_col_if_str(array, "array_position")
     e = _to_col_if_str(element, "array_position")
     return builtin("array_position")(a, e)
 
 
 def size(array: ColumnOrName) -> Column:
     """Returns the cardinality (size) of the array or map.
 
-    Example::
+    Examples:
         >>> from pystarburst import Row
         >>> df = session.create_dataframe([Row(a=[1, 2, 3])])
         >>> df.select(size("a").alias("result")).show()
         ------------
         |"result"  |
         ------------
         |3         |
@@ -2607,15 +2640,18 @@
     """
     a = _to_col_if_str(array, "size")
     return builtin("cardinality")(a)
 
 
 def array_slice(array: ColumnOrName, start: Union[ColumnOrName, int], length: Union[ColumnOrName, int]) -> Column:
     """Subsets array starting from index `start` (or starting from the end if `start` is negative)
-    with a length of `length`. The position of the first element is 1"""
+    with a length of `length`.
+
+    Note:
+        The position of the first element is 1"""
     a = _to_col_if_str(array, "array_slice")
     s = _to_col_if_str_or_int(start, "array_slice")
     l = _to_col_if_str_or_int(length, "array_slice")
     return builtin("slice")(a, s, l)
 
 
 def array_join(array: ColumnOrName, delimiter: ColumnOrName, null_replacement: Optional[ColumnOrName] = None) -> Column:
@@ -2623,15 +2659,15 @@
 
     Args:
         array: Column containing the ARRAY of elements to convert to a string.
         delimiter: Column containing the string to put between each element (e.g. a space,
             comma, or other human-readable delimiter).
         null_replacement: Optional value to replace nulls.
 
-    Example::
+    Examples:
         >>> from pystarburst import Row
         >>> df = session.create_dataframe([Row(a=[1, 45, None])])
         >>> df.select(array_join("a", lit(",")).alias("result")).show()
         ------------
         |"result"  |
         ------------
         |1,45      |
@@ -2757,51 +2793,44 @@
     """Returns a conditional expression that you can pass to the filter or where methods to
     perform the equivalent of a WHERE ... IN query that matches rows containing a sequence of
     values.
 
     The expression evaluates to true if the values in a row matches the values in one of
     the specified sequences.
 
-    The following code returns a DataFrame that contains the rows in which
-    the columns `c1` and `c2` contain the values:
-    - `1` and `"a"`, or
-    - `2` and `"b"`
-    This is equivalent to ``SELECT * FROM table WHERE (c1, c2) IN ((1, 'a'), (2, 'b'))``.
-
-    Example::
+    Args:
+        cols: A list of the columns to compare for the IN operation.
+        vals: A list containing the values to compare for the IN operation.
 
+    Examples:
+        >>> # The following code returns a DataFrame that contains the rows in which
+        >>> # the columns `c1` and `c2` contain the values:
+        >>> # - `1` and `"a"`, or
+        >>> # - `2` and `"b"`
+        >>> # This is equivalent to ``SELECT * FROM table WHERE (c1, c2) IN ((1, 'a'), (2, 'b'))``.
         >>> df = session.create_dataframe([[1, "a"], [2, "b"], [3, "c"]], schema=["col1", "col2"])
         >>> df.filter(in_([col("col1"), col("col2")], [[1, "a"], [2, "b"]])).show()
         -------------------
         |"COL1"  |"COL2"  |
         -------------------
         |1       |a       |
         |2       |b       |
         -------------------
         <BLANKLINE>
-
-    The following code returns a DataFrame that contains the rows where
-    the values of the columns `c1` and `c2` in `df2` match the values of the columns
-    `a` and `b` in `df1`. This is equivalent to
-    ``SELECT * FROM table2 WHERE (c1, c2) IN (SELECT a, b FROM table1)``.
-
-    Example::
-
+        >>> # The following code returns a DataFrame that contains the rows where
+        >>> # the values of the columns `c1` and `c2` in `df2` match the values of the columns
+        >>> # `a` and `b` in `df1`. This is equivalent to
+        >>> # ``SELECT * FROM table2 WHERE (c1, c2) IN (SELECT a, b FROM table1)``.
         >>> df1 = session.sql("select 1, 'a'")
         >>> df.filter(in_([col("col1"), col("col2")], df1)).show()
         -------------------
         |"COL1"  |"COL2"  |
         -------------------
         |1       |a       |
         -------------------
-        <BLANKLINE>
-
-    Args::
-        cols: A list of the columns to compare for the IN operation.
-        vals: A list containing the values to compare for the IN operation.
     """
     vals = parse_positional_args_to_list(*vals)
     columns = [_to_col_if_str(c, "in_") for c in cols]
     return Column(MultipleExpression(expressions=[c._expression for c in columns])).in_(vals)
 
 
 def cume_dist() -> Column:
@@ -2917,45 +2946,68 @@
     Args:
         e: The desired number of buckets; must be a positive integer value.
     """
     c = _to_col_if_str_or_int(e, "ntile")
     return builtin("ntile")(c)
 
 
+def struct(*cols: ColumnOrLiteral) -> Column:
+    """Creates a new struct column.
+
+    Args:
+        cols: column names or Columns to contain in the output struct.
+
+    Examples:
+        >>> df = session.createDataFrame([("Alice", 2), ("Bob", 5)], ["name", "age"])
+        >>> df.select(struct('age', 'name').alias("struct")).collect()
+        [Row(struct=(age: 2, name: 'Alice')), Row(struct=(age: 5, name: 'Bob'))]
+        >>> df.select(struct([df.age, df.name]).alias("struct")).collect()
+        [Row(struct=(age: 2, name: 'Alice')), Row(struct=(age: 5, name: 'Bob'))]
+    """
+    if len(cols) == 1 and isinstance(cols[0], (list, set, tuple)):
+        cols = cols[0]
+    cols = [_to_col_if_str(col, "struct") for col in cols]
+    exprs = [Column._to_expr(col) for col in cols]
+    return Column(StructExpression(fields=exprs))
+
+
 def greatest(*columns: ColumnOrName) -> Column:
     """Returns the largest value from a list of expressions. If any of the argument values is NULL, the result is NULL. GREATEST supports all data types, including VARIANT."""
     c = [_to_col_if_str(ex, "greatest") for ex in columns]
     return builtin("greatest")(*c)
 
 
 def least(*columns: ColumnOrName) -> Column:
     """Returns the smallest value from a list of expressions. LEAST supports all data types, including VARIANT."""
     c = [_to_col_if_str(ex, "least") for ex in columns]
     return builtin("least")(*c)
 
 
 def listagg(
-    e: ColumnOrName, delimiter: str = "", *within_group: Union[ColumnOrName, Iterable[ColumnOrName]], is_distinct: bool = False
+    col: ColumnOrName,
+    delimiter: str = "",
+    *within_group: Union[ColumnOrName, Iterable[ColumnOrName]],
+    is_distinct: bool = False,
 ) -> Column:
     """
     Returns the concatenated input values, separated by `delimiter` string.
     See `LISTAGG <https://trino.io/docs/current/functions/aggregate.html#listagg>`_ for details.
 
     Args:
-        e: A :class:`Column` object or column name that determines the values
+        col: a :class:`Column` object or column name that determines the values
             to be put into the list.
-        delimiter: A string delimiter.
-        is_distinct: Whether the input expression is distinct.
+        delimiter: a string delimiter.
+        is_distinct: whether the input expression is distinct.
 
-    Examples::
+    Examples:
 
-        df.group_by(df.col1).agg(listagg(df.col2. ",", f.col2.asc()))
-        df.select(listagg(df["col2"], ",", f.col2.asc(), is_distinct=False)
+        >>> df.group_by(df.col1).agg(listagg(df.col2. ",", f.col2.asc()))
+        >>> df.select(listagg(df["col2"], ",", f.col2.asc(), is_distinct=False)
     """
-    c = _to_col_if_str(e, "listagg")
+    c = _to_col_if_str(col, "listagg")
     if not within_group:
         raise ValueError(f"within_group is missing")
     within_exprs = [_to_col_if_str(col, "within_group")._expression for col in parse_positional_args_to_list(*within_group)]
     return Column(ListAgg(col=c._expression, delimiter=delimiter, is_distinct=is_distinct, within_group=within_exprs))
 
 
 def when_matched(
@@ -2981,17 +3033,17 @@
 def fail(error_description: ColumnOrLiteralStr) -> Column:
     """
     Throws an exception with the provided error message.
 
     Args:
         error_description: A :class:`Column` object or column name that determines the error description
 
-    Examples::
+    Examples:
 
-        df.select(fail("unsupported operation"))
+        >>> df.select(fail("unsupported operation"))
     """
     c = lit(error_description)
     return builtin("fail")(c)
 
 
 def assert_true(col: ColumnOrName, error_description: ColumnOrLiteralStr = None) -> Column:
     """
@@ -3020,31 +3072,31 @@
     """Invokes a Trino table function, including system-defined table functions and user-defined table functions.
 
     It returns a :meth:`~pystarburst.table_function.TableFunctionCall` so you can specify the partition clause.
 
     Args:
         function_name: The name of the table function.
         args: The positional arguments of the table function.
-        **kwargs: The named arguments of the table function. Some table functions (e.g., ``flatten``) have named arguments instead of positional ones.
+        kwargs: The named arguments of the table function. Some table functions (e.g., ``flatten``) have named arguments instead of positional ones.
 
-    Example:
+    Examples:
             >>> from pystarburst.functions import lit
             >>> session.table_function(call_table_function("sequence", lit(0), lit(4)).over()).collect()
             [Row(sequential_number=0), Row(sequential_number=1), Row(sequential_number=2), Row(sequential_number=3), Row(sequential_number=4)]
     """
     return pystarburst.table_function.TableFunctionCall(function_name, *args, **kwargs)
 
 
 def table_function(function_name: str) -> Callable:
     """Create a function object to invoke a Trino table function.
 
     Args:
         function_name: The name of the table function.
 
-    Example:
+    Examples:
             >>> from pystarburst.functions import lit
             >>> sequence = table_function("sequence")
             >>> session.table_function(sequence(lit(0), lit(4)).over()).collect()
             [Row(sequential_number=0), Row(sequential_number=1), Row(sequential_number=2), Row(sequential_number=3), Row(sequential_number=4)]
     """
     return lambda *args, **kwargs: call_table_function(function_name, *args, **kwargs)
 
@@ -3056,15 +3108,15 @@
     Args:
         function_name: The name of built-in function in Trino
         args: Arguments can be in two types:
 
             - :class:`~pystarburst.Column`, or
             - Basic Python types, which are converted to pystarburst literals.
 
-    Example::
+    Examples:
         >>> df = session.create_dataframe([1, 2, 3, 4], schema=["a"])  # a single column with 4 rows
         >>> df.select(call_function("avg", col("a"))).show()
         ----------------
         |"avg(""a"")"  |
         ----------------
         |2.500000      |
         ----------------
@@ -3082,15 +3134,15 @@
 
     Args:
         function_name: The name of built-in function in Trino.
 
     Returns:
         A :class:`Callable` object for calling a Trino system-defined function.
 
-    Example::
+    Examples:
         >>> df = session.create_dataframe([1, 2, 3, 4], schema=["a"])  # a single column with 4 rows
         >>> df.select(call_function("avg", col("a"))).show()
         ----------------
         |"avg(""a"")"  |
         ----------------
         |2.500000      |
         ----------------
@@ -3171,248 +3223,236 @@
 
 
 def from_unixtime(col: ColumnOrName, date_time_format: str = "yyyy-MM-dd HH:mm:ss") -> Column:
     """Convert a Unix timestamp into a string with given pattern (‘yyyy-MM-dd HH:mm:ss’, by default)
 
     `Supported date and time parts <https://trino.io/docs/current/functions/datetime.html#from_unixtime>`_
 
-    Example::
+    Args:
+        col: The Unix timestamp column
+        date_time_format: The format string
+
+    Examples:
 
         >>> # Example run in the EST timezone
         >>> import datetime
         >>> date_df = session.create_dataframe([[1428476356]], schema=["unix_time"])
         >>> date_df.select(from_unixtime(col("unix_time"), "YYYY/MM/dd hh:mm:ss").alias("datetime")).show()
         ------------------------
         |"DATETIME"            |
         ------------------------
         |"2015/04/08 02:59:16" |
         ------------------------
-        <BLANKLINE>
-
-    Args:
-        col: The Unix timestamp column
-        date_time_format: The format string
     """
     if not isinstance(date_time_format, str):
         raise ValueError("date_time_format must be a string")
     c = _to_col_if_str(col, "from_unixtime")
     return builtin("format_datetime")(builtin("from_unixtime")(c), date_time_format)
 
 
 def unix_timestamp(col: ColumnOrName, date_time_format: str = "yyyy-MM-dd HH:mm:ss") -> Column:
     """Convert a time string with a given pattern (‘yyyy-MM-dd HH:mm:ss’, by default) to Unix timestamp (in seconds).
 
     `Supported date and time parts <https://trino.io/docs/current/functions/datetime.html#to_unixtime>`_
 
-    Example::
+    Args:
+        col: The timestamp column or addend in the date_format
+        date_time_format: The format string
+
+    Examples:
 
         >>> # Example run in the EST timezone
         >>> import datetime
         >>> date_df = session.create_dataframe([["04/08/2015 02:59:16"]], schema=["date_col"])
         >>> date_df.select(to_unixtime(col("date_col"), "MM/dd/yyyy hh:mm:ss").alias("unix_time")).show()
         ----------------
         |"UNIX_TIME"   |
         ----------------
         |1428476356    |
         ----------------
-        <BLANKLINE>
-
-    Args:
-        col: The timestamp column or addend in the date_format
-        date_time_format: The format string
     """
     if not isinstance(date_time_format, str):
         raise ValueError("part must be a string")
     c = _to_col_if_str(col, "parse_datetime")
     return builtin("to_unixtime")(builtin("parse_datetime")(c, date_time_format))
 
 
 def timestamp_seconds(col: ColumnOrName) -> Column:
     """Convert a Unix timestamp into a local datetime.
 
     `Supported date and time parts <https://trino.io/docs/current/functions/datetime.html#from_unixtime>`_
 
-    Example::
+    Args:
+        col: The Unix timestamp column
+
+    Examples:
 
         >>> # Example run in the EST timezone
         >>> import datetime
         >>> date_df = session.create_dataframe([[1428476356]], schema=["unix_time"])
         >>> date_df.select(timestamp_seconds(col("unix_time")).alias("datetime")).show()
         --------------------------
         |"DATETIME"              |
         --------------------------
         |2015-04-08 02:59:16.000 |
         --------------------------
-        <BLANKLINE>
-
-    Args:
-        col: The Unix timestamp column
     """
     c = _to_col_if_str(col, "from_unixtime")
     return builtin("from_unixtime")(c).cast("timestampntz")
 
 
 def create_map(*cols: ColumnOrName) -> Column:
     """Creates a new map out of a series of rows
 
-    Example:
+    Args:
+        cols: the column containing two-element rows, each one containing the key-value pair
+
+    Examples:
 
         >>> df = session.createDataFrame(
-        [
-            ("Alice", 2),
-            ("Bob", 5),
-            ("Charlie", 6),
-        ],
-        schema=["a", "b"])
+        ... [
+        ...     ("Alice", 2),
+        ...     ("Bob", 5),
+        ...     ("Charlie", 6),
+        ... ],
+        ... schema=["a", "b"])
         >>> df.select(create_map("a", "b")).show()
         ---------------------------------------------
         |"map_from_entries(array_agg(row (a, b)))"  |
         ---------------------------------------------
         |{'Bob': 5, 'Alice': 2, 'Charlie': 6}       |
         ---------------------------------------------
-
-    Args:
-        cols: the column containing two-element rows, each one containing the key-value pair
     """
     cs = [_to_col_if_str(c, "row") for c in cols]
     return builtin("map_from_entries")(builtin("array_agg")(builtin("row")(cs)))
 
 
 def map_from_arrays(col1: ColumnOrName, col2: ColumnOrName) -> Column:
     """Creates a new map from two arrays
 
-    Example:
+    Args:
+        col1: the array containing all the keys
+        col2: the array containing all the values
+
+    Examples:
 
         >>> df = session.createDataFrame([(["Alice", "Bob", "Charlie"], [2, 5, 8])], schema=["a", "b"])
         >>> df.select(map_from_arrays("a", "b")).show()
         ----------------------------------------
         |"map_from_entries(zip(a, b))"         |
         ----------------------------------------
         |{'Bob': 5, 'Alice': 2, 'Charlie': 8}  |
         ----------------------------------------
-
-    Args:
-        col1: the array containing all the keys
-        col2: the array containing all the values
     """
     c1 = _to_col_if_str(col1, "zip")
     c2 = _to_col_if_str(col2, "zip")
     return builtin("map_from_entries")(builtin("zip")(c1, c2))
 
 
 def map_keys(col: ColumnOrName) -> Column:
     """Returns an unordered array containing the keys of the map
 
-    Example:
+    Args:
+        col: the input map
+
+    Examples:
 
         >>> df = session.sql("SELECT MAP_FROM_ENTRIES(ARRAY[(1, 'a'), (2, 'b')]) as a")
         >>> df.select(map_keys('a')).show()
-
         -----------------
         |"map_keys(a)"  |
         -----------------
         |[1, 2]         |
         -----------------
-
-    Args:
-        col: the input map
     """
     c = _to_col_if_str(col, "map_keys")
     return builtin("map_keys")(c)
 
 
 def map_values(col: ColumnOrName) -> Column:
     """Returns an unordered array containing the values of the map
 
-    Example:
+    Args:
+        col: the input map
 
+    Examples:
         >>> df = session.sql("SELECT MAP_FROM_ENTRIES(ARRAY[(1, 'a'), (2, 'b')]) as a")
         >>> df.select(map_values('a')).show()
-
         -------------------
         |"map_values(a)"  |
         -------------------
         |['a', 'b']       |
         -------------------
-
-    Args:
-        col: the input map
     """
     c = _to_col_if_str(col, "map_values")
     return builtin("map_values")(c)
 
 
 def map_entries(col: ColumnOrName) -> Column:
     """Returns an unordered array of all entries in the given map
 
-    Example:
+    Args:
+        col: the input map
 
+    Examples:
         >>> df = session.sql("SELECT MAP_FROM_ENTRIES(ARRAY[(1, 'a'), (2, 'b')]) as a")
         >>> df.select(map_entries('a')).show()
-
         ------------------------
         |"map_entries(a)"      |
         ------------------------
         |[(1, 'a'), (2, 'b')]  |
         ------------------------
-
-    Args:
-        col: the input map
     """
     c = _to_col_if_str(col, "map_entries")
     return builtin("map_entries")(c)
 
 
 def map_from_entries(col: ColumnOrName) -> Column:
     """Converts an array of entries (key value struct types) to a map of values
 
-    Example:
+    Args:
+        col: the input map
 
+    Examples:
         >>> df = session.sql("SELECT ARRAY[(1, 'a'), (2, 'b')] as a")
         >>> df.select(map_from_entries('a')).show()
-
         -------------------------
         |"map_from_entries(a)"  |
         -------------------------
-        |{'1': 'a', '2': 'b'}   |
+        |{1: 'a', 2: 'b'}   |
         -------------------------
-
-    Args:
-        col: the input map
     """
     c = _to_col_if_str(col, "map_from_entries")
     return builtin("map_from_entries")(c)
 
 
 def map_concat(*cols: ColumnOrName) -> Column:
     """Returns the union of all the given maps
 
-    Example:
+    Args:
+        cols: the maps to concatenate
 
+    Examples:
         >>> df = session.sql("SELECT MAP_FROM_ENTRIES(ARRAY[(1, 'a'), (2, 'b')]) as a, MAP_FROM_ENTRIES(ARRAY[(3, 'c')]) as b")
         >>> df.select(map_concat('a', 'b')).show()
-
         ----------------------------------
         |"map_concat(a, b)"              |
         ----------------------------------
-        |{'1': 'a', '2': 'b', '3': 'c'}  |
+        |{1: 'a', 2: 'b', 3: 'c'}  |
         ----------------------------------
-
-    Args:
-        cols: the maps to concatenate
     """
     cs = [_to_col_if_str(c, "map_concat") for c in cols]
     return builtin("map_concat")(cs)
 
 
 def transform_keys(col: ColumnOrName, func: Callable) -> Column:
     """
     Returns a map that applies function to each entry of map and transforms the keys.
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame([(1, {"foo": -2.0, "bar": 2.0})], ["id", "data"])
         >>> row = df.select(transform_keys(
         ...     "data", lambda k, _: upper(k)).alias("data_upper")
         ... ).head()
         >>> sorted(row["data_upper"].items())
         [('BAR', 2.0), ('FOO', -2.0)]
     """
@@ -3420,15 +3460,15 @@
     return builtin("transform_keys")(c, _create_lambda(func))
 
 
 def transform_values(col: ColumnOrName, func: Callable) -> Column:
     """
     Returns a map that applies function to each entry of map and transforms the values.
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame([(1, {"IT": 10.0, "SALES": 2.0, "OPS": 24.0})], ["id", "data"])
         >>> row = df.select(transform_values(
         ...     "data", lambda k, v: v + 10.0
         ... ).alias("new_data")).head()
         >>> sorted(row["new_data"].items())
         [('IT', 20.0), ('OPS', 34.0), ('SALES', 12.0)]
     """
@@ -3436,15 +3476,15 @@
     return builtin("transform_values")(c, _create_lambda(func))
 
 
 def map_filter(col: ColumnOrName, func: Callable) -> Column:
     """
     Constructs a map from those entries of map for which function returns true.
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame([(1, {"foo": 42.0, "bar": 1.0, "baz": 32.0})], ["id", "data"])
         >>> row = df.select(map_filter(
         ...     "data", lambda _, v: v > 30.0).alias("data_filtered")
         ...                 ).head()
         >>> sorted(row["data_filtered"].items())
         [('baz', 32.0), ('foo', 42.0)]
     """
@@ -3453,15 +3493,15 @@
 
 
 def map_zip_with(col1: ColumnOrName, col2: ColumnOrName, func: Callable) -> Column:
     """
     Merges the two given maps into a single map by applying function to the pair of values with the same key.
     For keys only presented in one map, NULL will be passed as the value for the missing key.
 
-    Example::
+    Examples:
         >>> df = session.createDataFrame([
         ...     (1, {"IT": 24.0, "SALES": 12.00}, {"IT": 2.0, "SALES": 1.4})],
         ...     ["id", "base", "ratio"]
         ... )
         >>> row = df.select(map_zip_with(
         ...     "base", "ratio", lambda k, v1, v2: round(v1 * v2, 2)).alias("updated_data")
         ...                 ).head()
```

## pystarburst/query_history.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import List, NamedTuple
 
 import pystarburst
```

## pystarburst/relational_grouped_dataframe.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 import re
 from typing import Callable, Dict, List, Tuple, Union
 
 from pystarburst import functions
 from pystarburst._internal.analyzer.expression.general import (
@@ -155,19 +155,18 @@
 
     def agg(self, *exprs: Union[Column, Tuple[ColumnOrName, str], Dict[str, str]]) -> DataFrame:
         """Returns a :class:`DataFrame` with computed aggregates. See examples in :meth:`DataFrame.group_by`.
 
         Args:
             exprs: A variable length arguments list where every element is
 
-                - A Column object
-                - A tuple where the first element is a column object or a column name and the second element is the name of the aggregate function
-                - A list of the above
-
-                or a ``dict`` maps column names to aggregate function names.
+                - a Column object
+                - a tuple where the first element is a column object or a column name and the second element is the name of the aggregate function
+                - a list of the above
+                - a ``dict`` maps column names to aggregate function names.
 
         Note:
             The name of the aggregate function to compute must be a valid Trino `aggregate function
             <https://trino.io/docs/current/functions/aggregate.html>`_.
 
         See also:
             - :meth:`DataFrame.agg`
```

## pystarburst/row.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Any, Dict, Iterable, Union
 
 
 def _restore_row_from_pickle(values, named_values, fields):
     if named_values:
@@ -172,14 +172,16 @@
             _restore_row_from_pickle,
             (tuple(self), self._named_values, self._fields),
         )
 
     def as_dict(self, recursive: bool = False) -> Dict:
         """Convert to a dict if this row object has both keys and values.
 
+        :meth:`asDict` is an alias of :meth:`as_dict`.
+
         Args:
             recursive: Recursively convert child :class:`Row` objects to dicts. Default is False.
 
         >>> row = Row(name1=1, name2=2, name3=Row(childname=3))
         >>> row.as_dict()
         {'name1': 1, 'name2': 2, 'name3': Row(childname=3)}
         >>> row.as_dict(True)
```

## pystarburst/session.py

```diff
@@ -1,29 +1,37 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 import datetime
 import decimal
 import json
 import uuid
 from array import array
 from functools import reduce
+from json import JSONDecodeError
 from logging import getLogger
 from threading import RLock
 from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union
 
 from trino.dbapi import Connection
-from trino.exceptions import ProgrammingError
+from trino.exceptions import ProgrammingError, TrinoQueryError
 
 from pystarburst._internal.analyzer.analyzer import Analyzer
-from pystarburst._internal.analyzer.analyzer_utils import quote_name
+from pystarburst._internal.analyzer.analyzer_utils import (
+    convert_value_to_sql_option,
+    escape_quotes,
+    quote_name,
+)
 from pystarburst._internal.analyzer.expression.general import Attribute, Literal
-from pystarburst._internal.analyzer.plan.logical_plan import TypeCoercionMode
+from pystarburst._internal.analyzer.plan.logical_plan import (
+    StarburstDataframeVersion,
+    TypeCoercionMode,
+)
 from pystarburst._internal.analyzer.plan.logical_plan.leaf import (
     Query,
     Range,
     TrinoValues,
 )
 from pystarburst._internal.analyzer.plan.logical_plan.table import SaveMode
 from pystarburst._internal.analyzer.plan.logical_plan.table_function import (
@@ -191,26 +199,35 @@
         self,
         conn: ServerConnection,
         use_endpoint: Optional[bool] = False,
         type_coercion_mode: TypeCoercionMode = TypeCoercionMode.DEFAULT,
     ) -> None:
         self._session_id = uuid.uuid4()
         self._conn = conn
-        self._session_info = f"""
-"version" : {get_version()},
-"python.version" : {get_python_version()},
-"trino.version" : {get_connector_version()},
-"os.name" : {get_os_name()}
-"""
         self._last_action_id = 0
         self._last_canceled_id = 0
         self._use_endpoint = use_endpoint
         self._type_coercion_mode = type_coercion_mode
 
         self._analyzer = Analyzer(self)
+        self._starburst_dataframe_version = None
+        try:
+            self._starburst_dataframe_version = self._analyzer.resolve(StarburstDataframeVersion()).starburst_dataframe_version
+        except Exception as e:
+            if not (isinstance(e.__cause__, TrinoQueryError) and "Invalid JSON string" in e.__cause__.message) and not (
+                isinstance(e, JSONDecodeError) and "type id 'StarburstDataframeVersion'" in e.doc
+            ):
+                raise e
+        self._session_info = f"""
+"version" : {get_version()},
+"starburst-dataframe.version": {self._starburst_dataframe_version},
+"python.version" : {get_python_version()},
+"trino.version" : {get_connector_version()},
+"os.name" : {get_os_name()}
+"""
         _logger.info("pystarburst Session information: %s", self._session_info)
 
     def __enter__(self):
         return self
 
     def __exit__(self, exc_type, exc_val, exc_tb):
         self.close()
@@ -263,15 +280,15 @@
             name: A string or list of strings that specify the table name or
                 fully-qualified object identifier (database name, schema name, and table name).
 
             Note:
                 If your table name contains special characters, use double quotes to mark it like this, ``session.table('"my table"')``.
                 For fully qualified names, you need to use double quotes separately like this, ``session.table('"my db"."my schema"."my.table"')``.
 
-        Examples::
+        Examples:
 
             >>> df1 = session.create_dataframe([[1, 2], [3, 4]], schema=["a", "b"])
             >>> df1.write.save_as_table("my_table", mode="overwrite")
             >>> session.table("my_table").collect()
             [Row(A=1, B=2), Row(A=3, B=4)]
             >>> current_db = session.get_current_catalog()
             >>> current_schema = session.get_current_schema()
@@ -291,36 +308,36 @@
         *func_arguments: ColumnOrName,
         **func_named_arguments: ColumnOrName,
     ) -> DataFrame:
         """Creates a new DataFrame from the given Trino SQL table function.
 
         References: `Trino SQL functions <https://trino.io/docs/current/functions/table.html>`_.
 
+        Args:
+            func_name: The SQL function name.
+            func_arguments: The positional arguments for the SQL function.
+            func_named_arguments: The named arguments for the SQL function, if it accepts named arguments.
+
+        Returns:
+            A new :class:`DataFrame` with data from calling the table function.
+
         Example 1
             Query a table function by function name:
 
             >>> from pystarburst.functions import lit
             >>> session.table_function("sequence", lit(0), lit(4)).collect()
             [Row(sequential_number=0), Row(sequential_number=1), Row(sequential_number=2), Row(sequential_number=3), Row(sequential_number=4)]
 
         Example 2
             Define a table function variable and query it:
 
             >>> from pystarburst.functions import table_function, lit
             >>> sequence = table_function("sequence")
             >>> session.table_function(sequence(lit(0), lit(4))).collect()
             [Row(sequential_number=0), Row(sequential_number=1), Row(sequential_number=2), Row(sequential_number=3), Row(sequential_number=4)]
-
-        Args:
-            func_name: The SQL function name.
-            func_arguments: The positional arguments for the SQL function.
-            func_named_arguments: The named arguments for the SQL function, if it accepts named arguments.
-
-        Returns:
-            A new :class:`DataFrame` with data from calling the table function.
         """
         func_expr = _create_table_function_expression(func_name, *func_arguments, **func_named_arguments)
 
         d = DataFrame(
             self,
             self._analyzer.resolve(TableFunctionRelation(table_function=func_expr)),
         )
@@ -331,15 +348,15 @@
         Returns a new DataFrame representing the results of a SQL query.
         You can use this method to execute a SQL statement. Note that you still
         need to call :func:`DataFrame.collect` to execute this query in Trino.
 
         Args:
             query: The SQL statement to execute.
 
-        Example::
+        Examples:
 
             >>> # create a dataframe from a SQL query
             >>> df = session.sql("select 1/2")
             >>> # execute the query
             >>> df.collect()
             [Row(1/2=Decimal('0.500000'))]
         """
@@ -354,33 +371,35 @@
     def create_dataframe(
         self,
         data: Union[List, Tuple],
         schema: Optional[Union[StructType, List[str]]] = None,
     ) -> DataFrame:
         """Creates a new DataFrame containing the specified values from the local data.
 
+        :meth:`createDataFrame` is an alias of :meth:`create_dataframe`.
+
         Args:
             data: The local data for building a :class:`DataFrame`. ``data`` can only
                 be a :class:`list` or :class:`tuple`. Every element in
                 ``data`` will constitute a row in the DataFrame.
             schema: A :class:`~pystarburst.types.StructType` containing names and
                 data types of columns, or a list of column names, or ``None``.
                 When ``schema`` is a list of column names or ``None``, the schema of the
                 DataFrame will be inferred from the data across all rows. To improve
                 performance, provide a schema. This avoids the need to infer data types
                 with large data sets.
 
-        Examples::
+        Examples:
 
             >>> # create a dataframe with a schema
             >>> from pystarburst.types import IntegerType, StringType, StructField
             >>> schema = StructType([StructField("a", IntegerType()), StructField("b", StringType())])
             >>> session.create_dataframe([[1, "py"], [3, "trino"]], schema).collect()
             [Row(A=1, B='py'), Row(A=3, B='trino')]
-
+            <BLANKLINE>
             >>> # create a dataframe by inferring a schema from the data
             >>> from pystarburst import Row
             >>> # infer schema
             >>> session.create_dataframe([1, 2, 3, 4], schema=["a"]).collect()
             [Row(A=1), Row(A=2), Row(A=3), Row(A=4)]
             >>> session.create_dataframe([[1, 2, 3, 4]], schema=["a", "b", "c", "d"]).collect()
             [Row(A=1, B=2, C=3, D=4)]
@@ -518,15 +537,15 @@
 
         Args:
             start: The start of the range. If ``end`` is not specified,
                 ``start`` will be used as the value of ``end``.
             end: The end of the range.
             step: The step of the range.
 
-        Examples::
+        Examples:
 
             >>> session.range(10).collect()
             [Row(ID=0), Row(ID=1), Row(ID=2), Row(ID=3), Row(ID=4), Row(ID=5), Row(ID=6), Row(ID=7), Row(ID=8), Row(ID=9)]
             >>> session.range(1, 10).collect()
             [Row(ID=1), Row(ID=2), Row(ID=3), Row(ID=4), Row(ID=5), Row(ID=6), Row(ID=7), Row(ID=8), Row(ID=9)]
             >>> session.range(1, 10, 2).collect()
             [Row(ID=1), Row(ID=3), Row(ID=5), Row(ID=7), Row(ID=9)]
@@ -613,65 +632,62 @@
         >>> assert len(query_history.queries) == 1
         """
         query_listener = QueryHistory(self)
         self._conn.add_query_listener(query_listener)
         return query_listener
 
     def _table_exists(self, table_name: str):
-        validate_object_name(table_name)
-        # note: object name could have dots, e.g, a table could be created via: create table "abc.abc" (id int)
-        # currently validate_object_name does not allow it, but if in the future we want to support the case, we need to
-        # update the implementation accordingly in this method
-        qualified_table_name = table_name.split(".")
+        """Check if table exists. Accepts quoted or unquoted identifiers. Does not accept raw strings."""
+        qualified_table_name = validate_object_name(table_name)
         if len(qualified_table_name) == 1:
             # name in the form of "table"
-            tables = self._run_query(f"show tables like '{table_name}'")
+            table = convert_value_to_sql_option(table_name.removeprefix('"').removesuffix('"').replace('""', '"'))
+            tables = self._run_query(f"show tables like {table}")
         elif len(qualified_table_name) == 2:
             # name in the form of "schema.table" omitting database
             # schema: qualified_table_name[0]
             # table: qualified_table_name[1]
-            tables = self._run_query(f"show tables from {qualified_table_name[0]} like '{qualified_table_name[1]}'")
+            table = convert_value_to_sql_option(qualified_table_name[1].removeprefix('"').removesuffix('"').replace('""', '"'))
+            tables = self._run_query(f"show tables from {qualified_table_name[0]} like {table}")
         elif len(qualified_table_name) == 3:
             # name in the form of "catalog.schema.table"
             # database: qualified_table_name[0]
             # schema: qualified_table_name[1]
             # table: qualified_table_name[2]
-            tables = self._run_query(
-                f"show tables from {qualified_table_name[0]}.{qualified_table_name[1]} like '{qualified_table_name[2]}'"
-            )
-        else:
-            # we do not support len(qualified_table_name) > 3 for now
-            raise PyStarburstClientExceptionMessages.GENERAL_INVALID_OBJECT_NAME(table_name)
+            table = convert_value_to_sql_option(qualified_table_name[2].removeprefix('"').removesuffix('"').replace('""', '"'))
+            tables = self._run_query(f"show tables from {qualified_table_name[0]}.{qualified_table_name[1]} like {table}")
 
         return tables is not None and len(tables) > 0
 
     def _explain_query(self, query: str) -> Optional[str]:
         try:
             return self._run_query(f"explain analyze {query}")[0][0]
         # return None for queries which can't be explained
         except ProgrammingError:
             _logger.warning("query '%s' cannot be explained")
             return None
 
     def discover(self, uri, *, catalog_name=None, schema_name="", options=""):
-        """
-                Run Schema Discovery feature on specified location.
-                Args:
-                    uri: URI to scan
-                    catalog_name: catalog name to use. Must be specified here, or in connection parameters.
-                    schema_name: schema name to use - 'discovered' if not provided
-                    options: Discovery options
-        Invoke create_discovered_table method on result object to create discovered table.
-        Examples::
+        """Run Schema Discovery feature on specified location.
+
+        Args:
+            uri: URI to scan
+            catalog_name: catalog name to use. Must be specified here, or in connection parameters.
+            schema_name: schema name to use - 'discovered' if not provided
+            options: Discovery options
+        Invoke :meth:`register_discovered_table` method on result object to register discovered table.
+
+        Examples:
+            >>>
             # Run discovery:
             >>> schema_discovery_result = session.discover(uri, catalog_name='iceberg', schema_name='test_schema_1', options='discoveryMode=NORMAL')
-            # Create discovered table:
-            >>> schema_discovery_result.create_discovered_table(if_exists="OVERWRITE")
+            # Register discovered table:
+            >>> schema_discovery_result.register_discovered_table(if_exists="OVERWRITE")
             # Create Table (DataFrame) object from discovered table:
-            >>> df = session.table(schema_discovery_result.create_discovered_table(if_exists="IGNORE"))
+            >>> df = session.table(schema_discovery_result.register_discovered_table(if_exists="IGNORE"))
         """
 
         return SchemaDiscoveryResult(self, uri, catalog_name=catalog_name, schema_name=schema_name, options=options)
 
     createDataFrame = create_dataframe
 
 
@@ -685,104 +701,133 @@
         # Retrieve catalog name
         if catalog_name is None:
             catalog_name = self.session.get_current_catalog()
             if catalog_name is None:
                 raise PyStarburstSchemaDiscoveryException(
                     "Catalog not specified as an init argument, nor as connection property"
                 )
-        self.discovery_catalog = catalog_name.removeprefix('"').removesuffix('"')
+        self.discovery_catalog = quote_name(catalog_name)
 
-        self._run_schema_discovery(uri, catalog_name=catalog_name, schema_name=schema_name, options=options)
+        self._run_schema_discovery(uri, schema_name=schema_name, options=options)
 
-    def _run_schema_discovery(self, uri, *, catalog_name=None, schema_name="", options=""):
+    def _run_schema_discovery(self, uri, *, schema_name="", options=""):
         # Run schema discovery
         if schema_name:
-            schema_name = f" AND schema = '{schema_name}'"
+            schema_name = escape_quotes(schema_name.removeprefix('"').removesuffix('"'))
+            schema_name = f" AND schema = {convert_value_to_sql_option(schema_name)}"
         if options:
-            options = f" AND options = '{options}'"
+            options = f" AND options = {convert_value_to_sql_option(options)}"
         cursor = self.session._conn._cursor.execute(
-            f"SELECT * FROM {catalog_name}.schema_discovery.discovery WHERE uri = '{uri}'{schema_name}{options}"
+            f"SELECT * FROM {self.discovery_catalog}.schema_discovery.discovery WHERE uri = '{uri}'{schema_name}{options}"
         )
         schema_discovery_result = cursor.fetchall()
         column_names = [column.name for column in cursor.description]
         discovery = Row(*column_names)(*schema_discovery_result[0])
         discovery_actions = json.loads(discovery.json)
 
         # Retrieve schema name
         schemas_discovered = [action for action in discovery_actions if action["operationType"] == "CreateSchema"]
         self.discovery_schema = schemas_discovered[0]["schemaName"]
 
         # Retrieve table name
-        tables_discovered = [action for action in discovery_actions if action["operationType"] == "CreateTable"]
+        tables_discovered = [
+            action for action in discovery_actions if action["operationType"] in ["CreateTable", "RegisterTable"]
+        ]
         if len(tables_discovered) > 1:
             raise PyStarburstSchemaDiscoveryException(
                 "Found more than 1 table in specified location. Currently discovering only 1 table is supported. Please provide more precise location."
             )
         elif len(tables_discovered) == 0:
             raise PyStarburstSchemaDiscoveryException("No table found at the specified location")
-        self.discovery_table = tables_discovered[0]["table"]["tableName"]["tableName"]
+        if tables_discovered[0]["operationType"] == "CreateTable":
+            self.discovery_table = tables_discovered[0]["table"]["tableName"]["tableName"]
+        elif tables_discovered[0]["operationType"] == "RegisterTable":
+            self.discovery_table = tables_discovered[0]["tableName"]["tableName"]
 
         # Retrieve schema discovery sqls
         self.discovery_sqls = {}
         for query in discovery.sql.split(";"):
             query = query.removeprefix("\n").removeprefix("\n")
             if query.startswith("CREATE TABLE"):
                 # Add catalog and schema qualifier
                 query = query.replace(
                     f'"{self.discovery_table}"',
-                    f'"{self.discovery_catalog}"."{self.discovery_schema}"."{self.discovery_table}"',
+                    f'{self.discovery_catalog}."{self.discovery_schema}"."{self.discovery_table}"',
                 )
-                self.discovery_sqls["create_discovered_table"] = query
+                self.discovery_sqls["register_discovered_table"] = query
+                self.discovery_sqls[
+                    "unregister_discovered_table"
+                ] = f'DROP TABLE {self.discovery_catalog}."{self.discovery_schema}"."{self.discovery_table}"'
+            elif query.startswith("CALL system.register_table"):
+                # replace `""` with `"` and escape single quotes in register_table(schema_name) value
+                extracted_schema_name = query.split("register_table(schema_name => ")[1].split(", table_name => ")[0]
+                query = query.replace(
+                    extracted_schema_name,
+                    convert_value_to_sql_option(extracted_schema_name.replace('""', '"').removeprefix("'").removesuffix("'")),
+                )
+                # Add catalog qualifier
+                register_query = query.replace(
+                    "CALL system.register_table", f"CALL {self.discovery_catalog}.system.register_table"
+                )
+                self.discovery_sqls["register_discovered_table"] = register_query
+                unregister_query = (
+                    ",".join([s for s in register_query.split(",") if "table_location" not in s]).replace(
+                        "register_table", "unregister_table"
+                    )
+                    + ")"
+                )
+                self.discovery_sqls["unregister_discovered_table"] = unregister_query
             elif query.startswith("CREATE SCHEMA"):
                 # Add catalog qualifier
-                query = query.replace(f'"{self.discovery_schema}"', f'"{self.discovery_catalog}"."{self.discovery_schema}"')
+                query = query.replace(f'"{self.discovery_schema}"', f'{self.discovery_catalog}."{self.discovery_schema}"')
                 self.discovery_sqls["create_discovered_schema"] = query
-        self.discovery_sqls[
-            "drop_discovered_table"
-        ] = f'DROP TABLE "{self.discovery_catalog}"."{self.discovery_schema}"."{self.discovery_table}"'
 
-    def create_discovered_table(self, if_exists=SaveMode.ERRORIFEXISTS):
-        """
-        Create discovered table.
+    def register_discovered_table(self, if_exists=SaveMode.ERRORIFEXISTS):
+        """Register discovered table into the metastore.
+
         Args:
             if_exists: How to behave if the table already exists:
-                ERRORIFEXISTS: Raise an exception.
-                IGNORE: Preserve current table/schema, do nothing.
-                OVERWRITE: Drop current table and re-create it.
-        Examples::
+
+                - ERRORIFEXISTS: Raise an exception.
+                - IGNORE: Preserve current table/schema, do nothing.
+                - OVERWRITE: Unregister current table and re-register it.
+
+        Examples:
+            >>>
             # Run schema discovery
             >>> schema_discovery_result = session.discover(uri)
-            # Create discovered table:
-            >>> schema_discovery_result.create_discovered_table(if_exists="OVERWRITE")
+            # Register discovered table:
+            >>> schema_discovery_result.register_discovered_table(if_exists="OVERWRITE")
             # Create Table (DataFrame) object from discovered table:
-            >>> df = session.table(schema_discovery_result.create_discovered_table(if_exists="IGNORE"))
+            >>> df = session.table(schema_discovery_result.register_discovered_table(if_exists="IGNORE"))
         """
 
         # Create schema
         self.session._run_query(self.discovery_sqls["create_discovered_schema"])
 
-        # Create table
+        # Register table
         if_exists = str_to_enum(if_exists, SaveMode, "'mode'")
-        if not self.session._table_exists(f"{self.discovery_catalog}.{self.discovery_schema}.{self.discovery_table}"):
-            self.session._run_query(self.discovery_sqls["create_discovered_table"])
+        if not self.session._table_exists(f'{self.discovery_catalog}."{self.discovery_schema}"."{self.discovery_table}"'):
+            self.session._run_query(self.discovery_sqls["register_discovered_table"])
         else:
             if if_exists == SaveMode.IGNORE:
-                return f'"{self.discovery_catalog}"."{self.discovery_schema}"."{self.discovery_table}"'
+                return f'{self.discovery_catalog}."{self.discovery_schema}"."{self.discovery_table}"'
             elif if_exists == SaveMode.OVERWRITE:
-                self.session._run_query(self.discovery_sqls["drop_discovered_table"])
-                self.session._run_query(self.discovery_sqls["create_discovered_table"])
+                self.session._run_query(self.discovery_sqls["unregister_discovered_table"])
+                self.session._run_query(self.discovery_sqls["register_discovered_table"])
             elif if_exists == SaveMode.ERRORIFEXISTS:
                 raise PyStarburstSchemaDiscoveryException(f"Table already exists in '{self.discovery_schema}' schema")
 
-        return f'"{self.discovery_catalog}"."{self.discovery_schema}"."{self.discovery_table}"'
+        return f'{self.discovery_catalog}."{self.discovery_schema}"."{self.discovery_table}"'
 
-    def drop_discovered_table(self):
-        """
-        Drop discovered table.
-        Examples::
+    def unregister_discovered_table(self):
+        """Unregister discovered table from the metastore.
+
+        Examples:
+            >>>
             # Run schema discovery
             >>> schema_discovery_result = session.discover(uri)
-            # Drop discovered table:
-            >>> schema_discovery_result.drop_discovered_table()
+            # Unregister discovered table:
+            >>> schema_discovery_result.unregister_discovered_table()
         """
 
-        self.session._run_query(self.discovery_sqls["drop_discovered_table"])
+        self.session._run_query(self.discovery_sqls["unregister_discovered_table"])
```

## pystarburst/table.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 from typing import Dict, Iterable, NamedTuple, Optional, Union
 
 import pystarburst
 from pystarburst._internal.analyzer.expression.table import (
     Assignment,
@@ -67,15 +67,15 @@
 
         Args:
             assignments: A list of values or a ``dict`` that associates
                 the names of columns with the values that should be updated.
                 The value of ``assignments`` can either be a literal value or
                 a :class:`Column` object.
 
-        Example::
+        Examples:
 
             >>> # Adds a matched clause where a row in source is matched
             >>> # if its key is equal to the key of any row in target.
             >>> # For all such rows, update its value to the value of the
             >>> # corresponding row in source.
             >>> from pystarburst.functions import when_matched
             >>> target_df = session.create_dataframe([(10, "old"), (10, "too_old"), (11, "old")], schema=["key", "value"])
@@ -104,15 +104,15 @@
 
     def delete(self):
         """
         Defines a delete action for the matched clause and
         returns an updated :class:`WhenMatchedClause` with the new
         delete action added.
 
-        Example::
+        Examples:
 
             >>> # Adds a matched clause where a row in source is matched
             >>> # if its key is equal to the key of any row in target.
             >>> # For all such rows, delete them.
             >>> from pystarburst.functions import when_matched
             >>> target_df = session.create_dataframe([(10, "old"), (10, "too_old"), (11, "old")], schema=["key", "value"])
             >>> target_df.write.save_as_table("my_table", mode="overwrite")
@@ -160,30 +160,30 @@
 
         Args:
             assignments: A list of values or a ``dict`` that associates
                 the names of columns with the values that should be inserted.
                 The value of ``assignments`` can either be a literal value or
                 a :class:`Column` object.
 
-        Examples::
+        Examples:
 
             >>> # Adds a not-matched clause where a row in source is not matched
             >>> # if its key does not equal the key of any row in target.
             >>> # For all such rows, insert a row into target whose ley and value
             >>> # are assigned to the key and value of the not matched row.
             >>> from pystarburst.functions import when_not_matched
             >>> target_df = session.create_dataframe([(10, "old"), (10, "too_old"), (11, "old")], schema=["key", "value"])
             >>> target_df.write.save_as_table("my_table", mode="overwrite")
             >>> target = session.table("my_table")
             >>> source = session.create_dataframe([(12, "new")], schema=["key", "value"])
             >>> target.merge(source, target["key"] == source["key"], [when_not_matched().insert([source["key"], source["value"]])])
             MergeResult(rows_affected=1)
             >>> target.collect() # the rows are inserted
             [Row(KEY=12, VALUE='new'), Row(KEY=10, VALUE='old'), Row(KEY=10, VALUE='too_old'), Row(KEY=11, VALUE='old')]
-
+            <BLANKLINE>
             >>> # For all such rows, insert a row into target whose key is
             >>> # assigned to the key of the not matched row.
             >>> target_df.write.save_as_table("my_table", mode="overwrite")
             >>> target.merge(source, target["key"] == source["key"], [when_not_matched().insert({"key": source["key"]})])
             MergeResult(rows_affected=1)
             >>> target.collect() # the rows are inserted
             [Row(KEY=12, VALUE=None), Row(KEY=10, VALUE='old'), Row(KEY=10, VALUE='too_old'), Row(KEY=11, VALUE='old')]
@@ -256,16 +256,17 @@
 
         Sampling with a seed is not supported on views or subqueries. This method works on tables so it supports ``seed``.
         This is the main difference between :meth:`DataFrame.sample` and this method.
 
         Args:
             frac: The percentage of rows to be sampled.
             sampling_method: Specifies the sampling method to use:
-                - "BERNOULLI": Includes each row with a probability of p/100. Similar to flipping a weighted coin for each row.
-                - "SYSTEM": Includes each block of rows with a probability of p/100. Similar to flipping a weighted coin for each block of rows. This method does not support fixed-size sampling.
+
+                - BERNOULLI: Includes each row with a probability of p/100. Similar to flipping a weighted coin for each row.
+                - SYSTEM: Includes each block of rows with a probability of p/100. Similar to flipping a weighted coin for each block of rows. This method does not support fixed-size sampling.
                 Default is ``None``. Then the Trino cluster will use "BERNOULLI" by default.
 
         Note:
             - SYSTEM sampling is often faster than BERNOULLI sampling.
 
         """
         if sampling_method and sampling_method.upper() not in (
@@ -294,27 +295,27 @@
         Args:
             assignments: A ``dict`` that associates the names of columns with the
                 values that should be updated. The value of ``assignments`` can
                 either be a literal value or a :class:`Column` object.
             condition: An optional :class:`Column` object representing the
                 specified condition. It must be provided if ``source`` is provided.
 
-        Examples::
+        Examples:
 
             >>> target_df = session.create_dataframe([(1, 1),(1, 2),(2, 1),(2, 2),(3, 1),(3, 2)], schema=["a", "b"])
             >>> target_df.write.save_as_table("my_table", mode="overwrite")
             >>> t = session.table("my_table")
-
+            <BLANKLINE>
             >>> # update all rows in column "b" to 0 and all rows in column "a"
             >>> # to the summation of column "a" and column "b"
             >>> t.update({"b": 0, "a": t.a + t.b})
             UpdateResult(rows_updated=6, multi_joined_rows_updated=0)
             >>> t.collect()
             [Row(A=2, B=0), Row(A=3, B=0), Row(A=3, B=0), Row(A=4, B=0), Row(A=4, B=0), Row(A=5, B=0)]
-
+            <BLANKLINE>
             >>> # update all rows in column "b" to 0 where column "a" has value 1
             >>> target_df.write.save_as_table("my_table", mode="overwrite")
             >>> t.update({"b": 0}, t["a"] == 1)
             UpdateResult(rows_updated=2, multi_joined_rows_updated=0)
             >>> t.collect()
             [Row(A=1, B=0), Row(A=1, B=0), Row(A=2, B=1), Row(A=2, B=2), Row(A=3, B=1), Row(A=3, B=2)]
         """
@@ -342,26 +343,26 @@
         Deletes rows in a Table and returns a :class:`DeleteResult`,
         representing the number of rows deleted.
 
         Args:
             condition: An optional :class:`Column` object representing the
                 specified condition. It must be provided if ``source`` is provided.
 
-        Examples::
+        Examples:
 
             >>> target_df = session.create_dataframe([(1, 1),(1, 2),(2, 1),(2, 2),(3, 1),(3, 2)], schema=["a", "b"])
             >>> target_df.write.save_as_table("my_table", mode="overwrite")
             >>> t = session.table("my_table")
-
+            <BLANKLINE>
             >>> # delete all rows in a table
             >>> t.delete()
             DeleteResult(rows_deleted=6)
             >>> t.collect()
             []
-
+            <BLANKLINE>
             >>> # delete all rows where column "a" has value 1
             >>> target_df.write.save_as_table("my_table", mode="overwrite")
             >>> t.delete(t["a"] == 1)
             DeleteResult(rows_deleted=2)
             >>> t.collect()
             [Row(A=2, B=1), Row(A=2, B=2), Row(A=3, B=1), Row(A=3, B=2)]
         """
@@ -399,15 +400,15 @@
                 to join this :class:`Table` and ``source``.
             clauses: A list of matched or not-matched clauses specifying the actions
                 to perform when the values from this :class:`Table` and ``source``
                 match or not match on ``join_expr``. These actions can only be instances
                 of :class:`WhenMatchedClause` and :class:`WhenNotMatchedClause`, and will
                 be performed sequentially in this list.
 
-        Example::
+        Examples:
 
             >>> from pystarburst.functions import when_matched, when_not_matched
             >>> target_df = session.create_dataframe([(10, "old"), (10, "too_old"), (11, "old")], schema=["key", "value"])
             >>> target_df.write.save_as_table("my_table", mode="overwrite")
             >>> target = session.table("my_table")
             >>> source = session.create_dataframe([(10, "new"), (12, "new"), (13, "old")], schema=["key", "value"])
             >>> target.merge(source, target["key"] == source["key"],
@@ -439,13 +440,13 @@
             )
         )
         new_df._internal_collect(statement_properties=statement_properties)
         rowcount = new_df._session._conn._cursor.rowcount
         return _get_merge_result(rowcount)
 
     def drop_table(self) -> None:
-        """Drops the table from the Trino cluster.
+        """Drops the table from the Trino cluster, if exists.
 
         Note that subsequent operations such as :meth:`DataFrame.select`, :meth:`DataFrame.collect` on this ``Table`` instance and the derived DataFrame will raise errors because the underlying
         table in the Trino cluster no longer exists.
         """
-        self._session.sql(f"drop table {self.table_name}")._internal_collect()
+        self._session.sql(f"drop table if exists {self.table_name}")._internal_collect()
```

## pystarburst/table_function.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 """Contains table function related classes."""
 from typing import Dict, Iterable, List, Optional, Tuple, Union
 
 from pystarburst._internal.analyzer.analyzer_utils import quote_name
 from pystarburst._internal.analyzer.expression.sort import SortDirection, SortOrder
@@ -60,16 +60,16 @@
         order_by: Optional[Union[ColumnOrName, Iterable[ColumnOrName]]] = None,
     ) -> "TableFunctionCall":
         """Specify the partitioning plan for this table function call when you lateral join this table function.
 
         When a query does a lateral join on a table function, the query feeds data to the table function row by row.
         Before rows are passed to table functions, the rows can be grouped into partitions. Partitioning has two main benefits:
 
-          - Partitioning allows Trino to divide up the workload to improve parallelization and thus performance.
-          - Partitioning allows Trino to process all rows with a common characteristic as a group. You can return results that are based on all rows in the group, not just on individual rows.
+        - Partitioning allows Trino to divide up the workload to improve parallelization and thus performance.
+        - Partitioning allows Trino to process all rows with a common characteristic as a group. You can return results that are based on all rows in the group, not just on individual rows.
 
         Refer to `table functions and partitions <https://trino.io/docs/current/functions/table.html>`__ for more information.
 
         Args:
             partition_by: Specify the partitioning column(s). It tells the table function to partition by these columns.
             order_by: Specify the ``order by`` column(s). It tells the table function to process input rows with this order within a partition.
```

## pystarburst/types.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 """This package contains all pystarburst logical types."""
 import re
 from typing import List, Literal, Optional, Union
 
 from pydantic import Field
```

## pystarburst/window.py

```diff
@@ -1,10 +1,10 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
-# Copyright (c) 2022-2023 Starburst Data, Inc. All rights reserved.
+# Copyright (c) Starburst Data, Inc. All rights reserved.
 #
 
 """Window frames in pystarburst."""
 import sys
 from typing import Iterable, List, Optional, Tuple, Union
 
 import pystarburst
@@ -42,15 +42,15 @@
         boundary_end = Literal(value=end)
 
     return boundary_start, boundary_end
 
 
 class Window:
     """
-    Examples::
+    Examples:
 
         >>> from pystarburst.functions import col, avg
         >>> window1 = Window.partition_by("value").order_by("key").rows_between(Window.CURRENT_ROW, 2)
         >>> window2 = Window.order_by(col("key").desc()).range_between(Window.UNBOUNDED_PRECEDING, Window.UNBOUNDED_FOLLOWING)
         >>> df = session.create_dataframe([(1, "1"), (2, "2"), (1, "3"), (2, "4")], schema=["key", "value"])
         >>> df.select(avg("value").over(window1).as_("window1"), avg("value").over(window2).as_("window2")).collect()
         [Row(WINDOW1=3.0, WINDOW2=2.5), Row(WINDOW1=2.0, WINDOW2=2.5), Row(WINDOW1=1.0, WINDOW2=2.5), Row(WINDOW1=4.0, WINDOW2=2.5)]
@@ -169,14 +169,16 @@
             ColumnOrName,
             Iterable[ColumnOrName],
         ],
     ) -> "WindowSpec":
         """
         Returns a new :class:`WindowSpec` object with the new partition by clause.
 
+        :meth:`partitionBy` is an alias of :meth:`partition_by`.
+
         See Also:
             - :func:`Window.partition_by`
         """
         exprs = parse_positional_args_to_list(*cols)
         partition_spec = [
             e._expression if isinstance(e, pystarburst.column.Column) else pystarburst.column.Column(e)._expression
             for e in exprs
@@ -190,14 +192,16 @@
             ColumnOrName,
             Iterable[ColumnOrName],
         ],
     ) -> "WindowSpec":
         """
         Returns a new :class:`WindowSpec` object with the new order by clause.
 
+        :meth:`orderBy` is an alias of :meth:`order_by`.
+
         See Also:
             - :func:`Window.order_by`
         """
         exprs = parse_positional_args_to_list(*cols)
         order_spec = []
         for e in exprs:
             if isinstance(e, str):
@@ -210,28 +214,32 @@
 
         return WindowSpec(self.partition_spec, order_spec if len(order_spec) > 0 else None, self.frame)
 
     def rows_between(self, start: int, end: int) -> "WindowSpec":
         """
         Returns a new :class:`WindowSpec` object with the new row frame clause.
 
+        :meth:`rowsBetween` is an alias of :meth:`rows_between`.
+
         See Also:
             - :func:`Window.rows_between`
         """
         boundary_start, boundary_end = _convert_boundary_to_expr(start, end)
         return WindowSpec(
             self.partition_spec,
             self.order_spec,
             SpecifiedWindowFrame(frame_type=RowFrame(), lower=boundary_start, upper=boundary_end),
         )
 
     def range_between(self, start: int, end: int) -> "WindowSpec":
         """
         Returns a new :class:`WindowSpec` object with the new range frame clause.
 
+        :meth:`rangeBetween` is an alias of :meth:`range_between`.
+
         See Also:
             - :func:`Window.range_between`
         """
         boundary_start, boundary_end = _convert_boundary_to_expr(start, end)
         return WindowSpec(
             self.partition_spec,
             self.order_spec,
```

## Comparing `pystarburst-0.7.0.dist-info/LICENSE.txt` & `pystarburst-0.8.0.dist-info/LICENSE.txt`

 * *Files 0% similar despite different names*

```diff
@@ -183,15 +183,15 @@
       replaced with your own identifying information. (Don't include
       the brackets!)  The text should be enclosed in the appropriate
       comment syntax for the file format. We also recommend that a
       file or class name and description of purpose be included on the
       same "printed page" as the copyright notice for easier
       identification within third-party archives.
 
-   Copyright 2022 Starburst Data, Inc.
+   Copyright Starburst Data, Inc.
 
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
 
        http://www.apache.org/licenses/LICENSE-2.0
```

## Comparing `pystarburst-0.7.0.dist-info/METADATA` & `pystarburst-0.8.0.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,28 +1,29 @@
 Metadata-Version: 2.1
 Name: pystarburst
-Version: 0.7.0
+Version: 0.8.0
 Summary: PyStarburst DataFrame API allows you to query and transform data in Starburst products in a data pipeline without having to download the data locally.
 Home-page: https://starburst.io
 License: Apache-2.0
 Author: Starburst Data
 Author-email: info@starburstdata.com
 Requires-Python: >=3.9,<4.0
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: Programming Language :: Python :: 3.12
 Provides-Extra: pandas
-Requires-Dist: pandas (>=1.5.3,<2.0.0) ; extra == "pandas"
+Requires-Dist: pandas (>=2.2,<3.0) ; extra == "pandas"
 Requires-Dist: pydantic (>=1.10.14,<2.0.0)
 Requires-Dist: python-dateutil (>=2.8.2,<3.0.0)
-Requires-Dist: trino (>=0.327.0,<0.328.0)
+Requires-Dist: trino (>=0.328.0,<0.329.0)
 Requires-Dist: urllib3 (>=2.2.0,<3.0.0)
+Requires-Dist: zstandard (>=0.22.0,<0.23.0)
 Project-URL: Repository, https://github.com/starburstdata/pystarburst-examples
 Description-Content-Type: text/markdown
 
 # PyStarburst DataFrame API
 
 PyStarburst DataFrame API allows you to query and transform data in Starburst products in a data pipeline without having to download the data locally.
```

## Comparing `pystarburst-0.7.0.dist-info/RECORD` & `pystarburst-0.8.0.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,48 +1,48 @@
-pystarburst/__init__.py,sha256=EKET3-HIAj9NSH5J5XJPZbqAD6O5PQAtaCw0JOQpDLU,12104
-pystarburst/_internal/__init__.py,sha256=TTH4GZY1cRR-tFXtl9cwkVgo-7z0nVkX18BlOo_idxU,72
-pystarburst/_internal/analyzer/__init__.py,sha256=TTH4GZY1cRR-tFXtl9cwkVgo-7z0nVkX18BlOo_idxU,72
-pystarburst/_internal/analyzer/analyzer.py,sha256=fZYMDmwcrfFeFCHgw-8UuSgfnPvVjCPaCo7KZkzJJy0,598
-pystarburst/_internal/analyzer/analyzer_utils.py,sha256=iDNsgMUKv8m0wCn7P3AqXFuFJZUT8s6mrxBNlx0Pzqw,1638
-pystarburst/_internal/analyzer/base_model.py,sha256=NY6334NdKcm0zne5ebZG-sZatEodYNUix3uNcOkyZSg,354
-pystarburst/_internal/analyzer/dataframe_api_client.py,sha256=lUkhqZL7AckntDOLS6S1K1eDPULVxMnDBteAwP9s7mM,3780
-pystarburst/_internal/analyzer/expression/__init__.py,sha256=OZAfMELpijzNqAxMvMMA7HssJLMHxhHSs7UYxhyzzRA,1620
-pystarburst/_internal/analyzer/expression/binary.py,sha256=RQE_odvEPQO2HC2IKr2H6Kj4D0ZQb-Ljp3vYTLQ1f1Y,1840
-pystarburst/_internal/analyzer/expression/general.py,sha256=7ANnx7IDP3se1-gQrv5KY-s3nVfeNQuk2sRNjYjxBN8,5640
-pystarburst/_internal/analyzer/expression/grouping_set.py,sha256=NHEwBxPN0BU0N_A_zjkpVlrdvdIm0Fbt6Yf_s8REC9U,789
-pystarburst/_internal/analyzer/expression/sort.py,sha256=PX3Qwa2EvQWKpF5WVNW-ja7BkzdCjtvAy5i3U0fOoqU,686
-pystarburst/_internal/analyzer/expression/table.py,sha256=KjlRKGDcH9g-5IusBUr81dAa-M1eV4tghJaA2OPRk74,1129
-pystarburst/_internal/analyzer/expression/table_function.py,sha256=7CTt7hsPpkfkF3ULTftZZVUyNipI-jO6k4uFlBdIRyU,1415
-pystarburst/_internal/analyzer/expression/unary.py,sha256=eiF-iH454Whn0H3KpEyAu6vVEVKV5wNgHojsgYyuLwE,1591
-pystarburst/_internal/analyzer/expression/window.py,sha256=ifOQKBu36AdEzJAVNP0dm6LOhnskvkHnrq5u99PYBZ8,3227
-pystarburst/_internal/analyzer/plan/__init__.py,sha256=TTH4GZY1cRR-tFXtl9cwkVgo-7z0nVkX18BlOo_idxU,72
-pystarburst/_internal/analyzer/plan/logical_plan/__init__.py,sha256=OtN-wFHGkKRWvIPelZGjvNIJaV_ztjfg9Lv9H-A7zVk,1101
-pystarburst/_internal/analyzer/plan/logical_plan/binary.py,sha256=369mr5Qm3a2fwrp84VxCPoFvBzSzjGB6tblfpkefVzE,2787
-pystarburst/_internal/analyzer/plan/logical_plan/leaf.py,sha256=euI3nvd2Ee7TOg72VU3ls5DYwbPpfRdBCxSpABqWAeI,1048
-pystarburst/_internal/analyzer/plan/logical_plan/table.py,sha256=JWawqauWFoyKm1lhhS7FAVT8uSjvZ5GuMcsjxDF696I,1779
-pystarburst/_internal/analyzer/plan/logical_plan/table_function.py,sha256=qyOiH8EuTI2pT3OL1Nqk2bP8rd0oCX0FIdN_omr28JY,594
-pystarburst/_internal/analyzer/plan/logical_plan/unary.py,sha256=ItzXBjRTg3rVrv43ZVp34W1Md1_53TetzUaG3Bsicpg,3039
-pystarburst/_internal/analyzer/plan/trino_plan.py,sha256=-3iXJ8IpZi2kKGa_NLyPHkA3G7M7vtz_QQP8piexRCs,1355
-pystarburst/_internal/error_message.py,sha256=MdPBWF_Bi7fbm_oTxLQBbtxwoD3lGQKXFaRkR6uvMFw,5652
-pystarburst/_internal/server_connection.py,sha256=cpzeDwvEUxj2Yg-FvzThQppsTm1SEyIjXKBItIYVJRE,6878
-pystarburst/_internal/type_utils.py,sha256=CkdNkJindPWoDRfORA3pWM84y4UQDEEu0eeZKhaZ81g,12795
-pystarburst/_internal/utils.py,sha256=k1tvnz8Q_smTT5j-cLUbbh4Hh1j0HYbfTJQYW7g78sw,10127
-pystarburst/column.py,sha256=Q4BbZxcUzlayveI6rDInEjXcPR6vOZOFq3uXgwtbPdg,30492
-pystarburst/context.py,sha256=1YG_q6gfjn41hhqb5HynOz4d3FMAA3jRonDfPG3U7Lg,550
-pystarburst/dataframe.py,sha256=MDZTQMICZDpsZ9K4jgePL60_0umGSyvkhU-QLP0wzTQ,104994
-pystarburst/dataframe_na_functions.py,sha256=zqF6qfu0Ma6eRVkVV7OHtDu4DOTWH91JfPra0gzIIeg,22528
-pystarburst/dataframe_stat_functions.py,sha256=5WdbMJUUkDX2DZMLGiSPJISrmy7LmxPDIccTfEzi42w,6676
-pystarburst/dataframe_writer.py,sha256=q_9UM2vcETwgYwFaRebiS59CWXkL-hlVX79A4PgLOU4,5473
-pystarburst/exceptions.py,sha256=mziJeouu6d82Zxqr_6-BcceMJaaNecFAJu2PFhfjtjk,2795
-pystarburst/functions.py,sha256=fAGFqKZExvdCRpKLFI2WjEcDMwlOYop74YwnV8kK_io,130960
-pystarburst/query_history.py,sha256=BaX1IHDx0vCtbsHCox92gKSTbOWS-317f4lZHtqbYFY,989
-pystarburst/relational_grouped_dataframe.py,sha256=pYlKfY9Jto7kro_c44b-G9YwgZL2mPsJKB07NmywHRA,10967
-pystarburst/row.py,sha256=LeuY-9gIeaHYCTdm4-KVMqeNYa0W7clGk-98ys6_U04,8845
-pystarburst/session.py,sha256=_V9zgzKtR6gOOATNaDm_BpiTQfojbV70IdpzAuCqeJU,33911
-pystarburst/table.py,sha256=V61b4iJs1t6zZucj4HsuyRh373PRYS_3hLK__GlmFJY,20382
-pystarburst/table_function.py,sha256=aKJmw-ngUkReMV74JmzwUvOsbDO5ogrv0koxBBOGQO0,8082
-pystarburst/types.py,sha256=zcrA9wCrS1TnHjsgndbB6SmJ11Tvcwd52Jj23gBE2EM,11933
-pystarburst/window.py,sha256=-yVIQi6EaAcM3AH-2PS1SNY4dN6VsjXrR7rKxciuMuA,9653
-pystarburst-0.7.0.dist-info/LICENSE.txt,sha256=l-azkNWvH44oaqZiy0Va0OyLD66lGJqo9iv__mQEzuM,11351
-pystarburst-0.7.0.dist-info/METADATA,sha256=ONX_tg3Z_QRQrgUf_TpqPvVQFeKihKTsD9-IIsRVHQ8,2836
-pystarburst-0.7.0.dist-info/WHEEL,sha256=FMvqSimYX_P7y0a7UY-_Mc83r5zkBZsCYPm7Lr0Bsq4,88
-pystarburst-0.7.0.dist-info/RECORD,,
+pystarburst/__init__.py,sha256=Xxv_gMOellwC0fmTlABerP7UzbJpWIMtUIvQRwZLnII,12445
+pystarburst/_internal/__init__.py,sha256=eI7Jwdf0jvRLa7Zsdg1d4MxxwlTW1ccEgU0Ny6D2Af8,62
+pystarburst/_internal/analyzer/__init__.py,sha256=eI7Jwdf0jvRLa7Zsdg1d4MxxwlTW1ccEgU0Ny6D2Af8,62
+pystarburst/_internal/analyzer/analyzer.py,sha256=7p1UsZ8TliQ9PVB58V0Qk3G7Hp3OiwTnlnkFDiZhOtQ,588
+pystarburst/_internal/analyzer/analyzer_utils.py,sha256=E5RLjwsKb99I-pJbGCrfevuIg1O7MpcHLpj1hEwUYP4,2194
+pystarburst/_internal/analyzer/base_model.py,sha256=mKw9-VD3mUzTysCKl5yfb873dhmKjbJmG-YiXnfU5kw,344
+pystarburst/_internal/analyzer/dataframe_api_client.py,sha256=xUbGB4Rpyio_HiVbBnHmR0LOb71wrO_kYth67PRERB0,4372
+pystarburst/_internal/analyzer/expression/__init__.py,sha256=D3MX2W7qFitDiXfR3e_7dm70zPEE7kNxifZUkpUWKuc,1634
+pystarburst/_internal/analyzer/expression/binary.py,sha256=whRYFZcPUyLO-KS2Kr0hDFusnzQtjwXUQh4GKbtUodQ,1830
+pystarburst/_internal/analyzer/expression/general.py,sha256=t4QVftvWV-fa6R8kxwHKz5qT-AbZ84lJG5lX2o4ilzg,5829
+pystarburst/_internal/analyzer/expression/grouping_set.py,sha256=ljzZhpctMHhllTvoyuUElC71KCzul95xa4dsoEPPD3o,779
+pystarburst/_internal/analyzer/expression/sort.py,sha256=vBPq9uykbmd29nSz_weyAnsH50QkLO-HdZ87NzptleM,676
+pystarburst/_internal/analyzer/expression/table.py,sha256=pmnPfySeRPpieCD3uN-H79-vaRwXYYevaGz1hByZxac,1119
+pystarburst/_internal/analyzer/expression/table_function.py,sha256=CYJk0L7WA8Py3j72tvQzBgGvDhXNgBk4C2UNphtLweI,1405
+pystarburst/_internal/analyzer/expression/unary.py,sha256=BeTWcFp97PGkeE6vMBnOwSbc_CUb4KHRX1LGHNxcdpg,1581
+pystarburst/_internal/analyzer/expression/window.py,sha256=jlSWTVbhk5PhLGslpYtiDV3krsvY9fg7WbYzmsePlso,3217
+pystarburst/_internal/analyzer/plan/__init__.py,sha256=eI7Jwdf0jvRLa7Zsdg1d4MxxwlTW1ccEgU0Ny6D2Af8,62
+pystarburst/_internal/analyzer/plan/logical_plan/__init__.py,sha256=KCBeGO2Fe_g4tH22NI5pe6pzAjFqi5uVdfoqrLXNwCg,1295
+pystarburst/_internal/analyzer/plan/logical_plan/binary.py,sha256=S0goKF3RcoFrlikZR5zIlflXzxsQ1Bi3nqzDewSe26I,2777
+pystarburst/_internal/analyzer/plan/logical_plan/leaf.py,sha256=NnIHBzAFAZOfKFc0y0xfeIMGAJXtQWDKpZOE_wvXocg,1038
+pystarburst/_internal/analyzer/plan/logical_plan/table.py,sha256=4vCD91yILXbd9MGLlTP3FNDin10PT1ytfn1SQl3kBJo,1769
+pystarburst/_internal/analyzer/plan/logical_plan/table_function.py,sha256=oZdi9h7P71rzIOwVLO55UNRaIXiKXH4le-EmGml8PPE,584
+pystarburst/_internal/analyzer/plan/logical_plan/unary.py,sha256=FYDBdO0phGnyb0bTcAdSCDGYHGHWi8FtU_4SdTFa1q0,3324
+pystarburst/_internal/analyzer/plan/trino_plan.py,sha256=0oc5hBsghBVH_OqXPCkKiEvV6vjH61GPxzlRbgjvEHQ,1435
+pystarburst/_internal/error_message.py,sha256=oCwM9TciEFcHKzQTZej0D2oFGs3fnqCw5M3Si_Lidys,5642
+pystarburst/_internal/server_connection.py,sha256=-qSfoZgEDk43K54R8XwcecBfd7_nnCOSGy2KKoUi9QM,6868
+pystarburst/_internal/type_utils.py,sha256=DxrNg-773siPljaZXXpNMXmdEyyZP1IoNsnkKZ8jR8k,12785
+pystarburst/_internal/utils.py,sha256=toxGWyfzaE0gIKU2KQfP8XGCQzfKJufhxIFZyTRUvcs,10329
+pystarburst/column.py,sha256=UlC8cA9YLhuQLwrL-B2KV191lBm0hIIzzsy4ZntzqEU,30460
+pystarburst/context.py,sha256=UJOcLU2ryMt3dRTLbq4EdA5GfzTWStg5-MIufx2McHQ,540
+pystarburst/dataframe.py,sha256=TGN8u3GzMPdqLUuNytE2ok9rVlej2c88oEKE5Qe-qGo,117082
+pystarburst/dataframe_na_functions.py,sha256=WxwNKKakgkApwGCs7VTWS88T38H9mhp8HtRU5lXxeNE,22426
+pystarburst/dataframe_stat_functions.py,sha256=P0oXRs7TdUYushTrPUpmMPjVZLJf6W7yyXL-A9JXLbA,6817
+pystarburst/dataframe_writer.py,sha256=R9nFUaUxelE3ZNORQr40kbJimSKFuthR7icI7lmp3ec,5513
+pystarburst/exceptions.py,sha256=csETDmatcJysDo_YkbLu4GSnfaQa9kRJhn_5hHYoh1o,2785
+pystarburst/functions.py,sha256=RRWNtzo1LcJ-4GDc6b-MT7akSuoTsH9uNhkHrvx_tRc,133349
+pystarburst/query_history.py,sha256=YCktkNOii3Z0mOVHdAX-tq6sSFY_Crrp4yORsL7yQkY,979
+pystarburst/relational_grouped_dataframe.py,sha256=gRNuwx3fTPNe0pJbJA0gMF7Q922RGzt4E9vCyTyFKZ4,10955
+pystarburst/row.py,sha256=2B1I0yGmVA6giuckRUL2EFfcQwlprT-x8hPFSvx5ENE,8891
+pystarburst/session.py,sha256=nkZ9Ogd_GCpUxI5c5PhtuWuO3wfvE-HH-hi89yXjbcU,36224
+pystarburst/table.py,sha256=LkKWkgY0hIJyrkTrI6ra7aeRbfmWsT2SVLTBXxZlgOk,20502
+pystarburst/table_function.py,sha256=AIVteA5rQdXVvXJK_IphFUPr5UwGwDauKO6JhwDuN9c,8068
+pystarburst/types.py,sha256=oY8gn16tMNII3sfDVXPUkSq_lTw8yRsbSVXdBM3X3Jk,11923
+pystarburst/window.py,sha256=MYud5ny8LP4oBE_Y0QtnAVfVR1PDi_s_Z9Iu9_coZl4,9900
+pystarburst-0.8.0.dist-info/LICENSE.txt,sha256=b4C_P1Oe9Ao_R6ijox1Jw4vqE89bH1ZJmRCe4GNKOB8,11346
+pystarburst-0.8.0.dist-info/METADATA,sha256=IGLxKcSmgRFXxH5327Aw47XnvVyJFnwBBgWXoHo6TuY,2876
+pystarburst-0.8.0.dist-info/WHEEL,sha256=sP946D7jFCHeNz5Iq4fL4Lu-PrWrFsgfLXbbkciIZwg,88
+pystarburst-0.8.0.dist-info/RECORD,,
```

