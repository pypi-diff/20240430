# Comparing `tmp/spinedb_api-0.30.5.tar.gz` & `tmp/spinedb_api-0.31.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "spinedb_api-0.30.5.tar", last modified: Tue Apr 23 14:05:35 2024, max compression
+gzip compressed data, was "spinedb_api-0.31.0.tar", last modified: Tue Apr 30 07:31:12 2024, max compression
```

## Comparing `spinedb_api-0.30.5.tar` & `spinedb_api-0.31.0.tar`

### file list

```diff
@@ -1,184 +1,195 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.575116 spinedb_api-0.30.5/
--rw-r--r--   0 runner    (1001) docker     (127)      161 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/.gitattributes
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.547115 spinedb_api-0.30.5/.github/
--rw-r--r--   0 runner    (1001) docker     (127)      296 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/.github/pull_request_template.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.547115 spinedb_api-0.30.5/.github/workflows/
--rw-r--r--   0 runner    (1001) docker     (127)     1262 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/.github/workflows/run_unit_tests.yml
--rw-r--r--   0 runner    (1001) docker     (127)      278 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/.gitignore
--rw-r--r--   0 runner    (1001) docker     (127)      567 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/.readthedocs.yml
--rw-r--r--   0 runner    (1001) docker     (127)    32493 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/COPYING
--rw-r--r--   0 runner    (1001) docker     (127)     7651 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/COPYING.LESSER
--rw-r--r--   0 runner    (1001) docker     (127)       52 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (127)     3811 2024-04-23 14:05:35.575116 spinedb_api-0.30.5/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)     2714 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.547115 spinedb_api-0.30.5/bin/
--rw-r--r--   0 runner    (1001) docker     (127)       76 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/bin/build_doc.bat
--rw-r--r--   0 runner    (1001) docker     (127)      346 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/bin/build_doc.py
--rw-r--r--   0 runner    (1001) docker     (127)     1265 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/bin/update_copyrights.py
--rw-r--r--   0 runner    (1001) docker     (127)     3248 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/deploy-key.enc
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.551115 spinedb_api-0.30.5/docs/
--rw-r--r--   0 runner    (1001) docker     (127)      584 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/docs/Makefile
--rw-r--r--   0 runner    (1001) docker     (127)      756 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/docs/make.bat
--rw-r--r--   0 runner    (1001) docker     (127)      423 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/docs/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.551115 spinedb_api-0.30.5/docs/source/
--rw-r--r--   0 runner    (1001) docker     (127)     6693 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/docs/source/conf.py
--rw-r--r--   0 runner    (1001) docker     (127)      981 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/docs/source/front_matter.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.551115 spinedb_api-0.30.5/docs/source/img/
--rw-r--r--   0 runner    (1001) docker     (127)     5205 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/docs/source/img/spinetoolbox_on_wht.svg
--rw-r--r--   0 runner    (1001) docker     (127)      669 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/docs/source/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2961 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/docs/source/metadata_description.rst
--rw-r--r--   0 runner    (1001) docker     (127)    10457 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/docs/source/parameter_value_format.rst
--rw-r--r--   0 runner    (1001) docker     (127)      912 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/docs/source/results_metadata_description.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2250 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/docs/source/tutorial.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.551115 spinedb_api-0.30.5/fig/
--rw-r--r--   0 runner    (1001) docker     (127)    55634 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/fig/eu-emblem-low-res.jpg
--rw-r--r--   0 runner    (1001) docker     (127)    16474 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/pylintrc
--rw-r--r--   0 runner    (1001) docker     (127)     1704 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (127)        5 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-04-23 14:05:35.575116 spinedb_api-0.30.5/setup.cfg
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.555115 spinedb_api-0.30.5/spinedb_api/
--rw-r--r--   0 runner    (1001) docker     (127)     4307 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.559116 spinedb_api-0.30.5/spinedb_api/alembic/
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/README
--rw-r--r--   0 runner    (1001) docker     (127)     2015 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/env.py
--rw-r--r--   0 runner    (1001) docker     (127)      494 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/script.py.mako
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.559116 spinedb_api-0.30.5/spinedb_api/alembic/versions/
--rw-r--r--   0 runner    (1001) docker     (127)     1816 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/070a0eb89e88_drop_category_tables.py
--rw-r--r--   0 runner    (1001) docker     (127)     4013 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/0c7d199ae915_add_list_value_table.py
--rw-r--r--   0 runner    (1001) docker     (127)     2371 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/1892adebc00f_create_metadata_tables.py
--rw-r--r--   0 runner    (1001) docker     (127)     2929 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/1e4997105288_separate_type_from_value.py
--rw-r--r--   0 runner    (1001) docker     (127)     5987 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/39e860a11b05_add_alternatives_and_scenarios.py
--rw-r--r--   0 runner    (1001) docker     (127)     2685 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/51fd7b69acf7_add_parameter_tag_and_parameter_value_list.py
--rw-r--r--   0 runner    (1001) docker     (127)     1280 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/738d494a08ac_fix_foreign_key_constraints_in_object_.py
--rw-r--r--   0 runner    (1001) docker     (127)     1308 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/7d0b467f2f4e_fix_foreign_key_constraints_in_entity_.py
--rw-r--r--   0 runner    (1001) docker     (127)     2461 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/8c19c53d5701_rename_parameter_to_parameter_definition.py
--rw-r--r--   0 runner    (1001) docker     (127)     2542 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/989fccf80441_replace_values_with_reference_to_list_.py
--rw-r--r--   0 runner    (1001) docker     (127)     1240 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/9da58d2def22_create_entity_group_table.py
--rw-r--r--   0 runner    (1001) docker     (127)    19946 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/bba1e2ef5153_move_to_entity_based_design.py
--rw-r--r--   0 runner    (1001) docker     (127)     4341 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/bf255c179bce_get_rid_of_unused_fields_in_parameter_.py
--rw-r--r--   0 runner    (1001) docker     (127)     3618 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/defbda3bf2b5_add_tool_feature_tables.py
--rw-r--r--   0 runner    (1001) docker     (127)     6031 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/fbb540efbf15_add_support_for_mysql.py
--rw-r--r--   0 runner    (1001) docker     (127)     1230 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic/versions/fd542cebf699_drop_on_update_clauses_from_object_and_.py
--rw-r--r--   0 runner    (1001) docker     (127)     1697 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/alembic.ini
--rw-r--r--   0 runner    (1001) docker     (127)    29319 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/check_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)    25153 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/db_cache.py
--rw-r--r--   0 runner    (1001) docker     (127)     2368 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/db_mapping.py
--rw-r--r--   0 runner    (1001) docker     (127)    25298 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/db_mapping_add_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)    91853 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/db_mapping_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    41812 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/db_mapping_check_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     3853 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/db_mapping_commit_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)    15289 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/db_mapping_query_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)    16048 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/db_mapping_remove_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)    16363 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/db_mapping_update_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     8840 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/diff_db_mapping.py
--rw-r--r--   0 runner    (1001) docker     (127)     6398 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/diff_db_mapping_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     5036 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/diff_db_mapping_commit_mixin.py
--rw-r--r--   0 runner    (1001) docker     (127)     3492 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/exception.py
--rw-r--r--   0 runner    (1001) docker     (127)    13191 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/export_functions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.563115 spinedb_api-0.30.5/spinedb_api/export_mapping/
--rw-r--r--   0 runner    (1001) docker     (127)     1649 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/export_mapping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    62024 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/export_mapping/export_mapping.py
--rw-r--r--   0 runner    (1001) docker     (127)     4246 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/export_mapping/generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4039 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/export_mapping/group_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)    10321 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/export_mapping/pivot.py
--rw-r--r--   0 runner    (1001) docker     (127)    30010 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/export_mapping/settings.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.563115 spinedb_api-0.30.5/spinedb_api/filters/
--rw-r--r--   0 runner    (1001) docker     (127)      979 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/filters/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6647 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/filters/alternative_filter.py
--rw-r--r--   0 runner    (1001) docker     (127)     6217 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/filters/execution_filter.py
--rw-r--r--   0 runner    (1001) docker     (127)     9696 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/filters/renamer.py
--rw-r--r--   0 runner    (1001) docker     (127)    10075 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/filters/scenario_filter.py
--rw-r--r--   0 runner    (1001) docker     (127)    10062 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/filters/tool_filter.py
--rw-r--r--   0 runner    (1001) docker     (127)    10710 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/filters/tools.py
--rw-r--r--   0 runner    (1001) docker     (127)    12624 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/filters/value_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)     7238 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/graph_layout_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    37410 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)    85683 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/import_functions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.563115 spinedb_api-0.30.5/spinedb_api/import_mapping/
--rw-r--r--   0 runner    (1001) docker     (127)      979 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/import_mapping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15373 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/import_mapping/generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    43513 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/import_mapping/import_mapping.py
--rw-r--r--   0 runner    (1001) docker     (127)    17679 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/import_mapping/import_mapping_compat.py
--rw-r--r--   0 runner    (1001) docker     (127)     4103 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/import_mapping/type_conversion.py
--rw-r--r--   0 runner    (1001) docker     (127)     8855 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/mapping.py
--rw-r--r--   0 runner    (1001) docker     (127)    55459 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/parameter_value.py
--rw-r--r--   0 runner    (1001) docker     (127)     8370 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/perfect_split.py
--rw-r--r--   0 runner    (1001) docker     (127)     3277 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/purge.py
--rw-r--r--   0 runner    (1001) docker     (127)     4636 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/server_client_helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)     4089 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_db_client.py
--rw-r--r--   0 runner    (1001) docker     (127)    21936 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_db_server.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.563115 spinedb_api-0.30.5/spinedb_api/spine_io/
--rw-r--r--   0 runner    (1001) docker     (127)     1042 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.567115 spinedb_api-0.30.5/spinedb_api/spine_io/exporters/
--rw-r--r--   0 runner    (1001) docker     (127)     1052 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/exporters/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2655 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/exporters/csv_writer.py
--rw-r--r--   0 runner    (1001) docker     (127)    12860 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/exporters/excel.py
--rw-r--r--   0 runner    (1001) docker     (127)     4453 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/exporters/excel_writer.py
--rw-r--r--   0 runner    (1001) docker     (127)     5687 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/exporters/gdx_writer.py
--rw-r--r--   0 runner    (1001) docker     (127)     6188 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/exporters/sql_writer.py
--rw-r--r--   0 runner    (1001) docker     (127)     5205 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/exporters/writer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2624 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/gdx_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.567115 spinedb_api-0.30.5/spinedb_api/spine_io/importers/
--rw-r--r--   0 runner    (1001) docker     (127)     1010 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/importers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7377 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/importers/csv_reader.py
--rw-r--r--   0 runner    (1001) docker     (127)     4077 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/importers/datapackage_reader.py
--rw-r--r--   0 runner    (1001) docker     (127)    10593 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/importers/excel_reader.py
--rw-r--r--   0 runner    (1001) docker     (127)     4808 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/importers/gdx_connector.py
--rw-r--r--   0 runner    (1001) docker     (127)     3654 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/importers/json_reader.py
--rw-r--r--   0 runner    (1001) docker     (127)     6536 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/importers/reader.py
--rw-r--r--   0 runner    (1001) docker     (127)     3520 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/spinedb_api/spine_io/importers/sqlalchemy_connector.py
--rw-r--r--   0 runner    (1001) docker     (127)      413 2024-04-23 14:05:35.000000 spinedb_api-0.30.5/spinedb_api/version.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.575116 spinedb_api-0.30.5/spinedb_api.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)     3811 2024-04-23 14:05:35.000000 spinedb_api-0.30.5/spinedb_api.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)     5976 2024-04-23 14:05:35.000000 spinedb_api-0.30.5/spinedb_api.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-23 14:05:35.000000 spinedb_api-0.30.5/spinedb_api.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-23 14:05:35.000000 spinedb_api-0.30.5/spinedb_api.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (127)      229 2024-04-23 14:05:35.000000 spinedb_api-0.30.5/spinedb_api.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)       17 2024-04-23 14:05:35.000000 spinedb_api-0.30.5/spinedb_api.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.571116 spinedb_api-0.30.5/tests/
--rw-r--r--   0 runner    (1001) docker     (127)     1032 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.571116 spinedb_api-0.30.5/tests/export_mapping/
--rw-r--r--   0 runner    (1001) docker     (127)      979 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/export_mapping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    80599 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/export_mapping/test_export_mapping.py
--rw-r--r--   0 runner    (1001) docker     (127)     8695 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/export_mapping/test_pivot.py
--rw-r--r--   0 runner    (1001) docker     (127)    14923 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/export_mapping/test_settings.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.571116 spinedb_api-0.30.5/tests/filters/
--rw-r--r--   0 runner    (1001) docker     (127)      979 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/filters/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8745 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/filters/test_alternative_filter.py
--rw-r--r--   0 runner    (1001) docker     (127)    12146 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/filters/test_renamer.py
--rw-r--r--   0 runner    (1001) docker     (127)    22023 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/filters/test_scenario_filter.py
--rw-r--r--   0 runner    (1001) docker     (127)    10229 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/filters/test_tool_filter.py
--rw-r--r--   0 runner    (1001) docker     (127)    13710 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/filters/test_tools.py
--rw-r--r--   0 runner    (1001) docker     (127)    10429 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/filters/test_value_transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.571116 spinedb_api-0.30.5/tests/import_mapping/
--rw-r--r--   0 runner    (1001) docker     (127)      979 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/import_mapping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    24167 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/import_mapping/test_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    89876 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/import_mapping/test_import_mapping.py
--rw-r--r--   0 runner    (1001) docker     (127)     4632 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/import_mapping/test_type_conversion.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.571116 spinedb_api-0.30.5/tests/spine_io/
--rw-r--r--   0 runner    (1001) docker     (127)     1048 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.575116 spinedb_api-0.30.5/tests/spine_io/exporters/
--rw-r--r--   0 runner    (1001) docker     (127)     1058 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/exporters/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4584 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/exporters/test_csv_writer.py
--rw-r--r--   0 runner    (1001) docker     (127)     6507 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/exporters/test_excel_writer.py
--rw-r--r--   0 runner    (1001) docker     (127)    15865 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/exporters/test_gdx_writer.py
--rw-r--r--   0 runner    (1001) docker     (127)    12196 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/exporters/test_sql_writer.py
--rw-r--r--   0 runner    (1001) docker     (127)     3367 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/exporters/test_writer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 14:05:35.575116 spinedb_api-0.30.5/tests/spine_io/importers/
--rw-r--r--   0 runner    (1001) docker     (127)     1058 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/importers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3606 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/importers/test_CSVConnector.py
--rw-r--r--   0 runner    (1001) docker     (127)    12476 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/importers/test_GdxConnector.py
--rw-r--r--   0 runner    (1001) docker     (127)     3415 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/importers/test_datapackage_reader.py
--rw-r--r--   0 runner    (1001) docker     (127)     1385 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/importers/test_excel_reader.py
--rw-r--r--   0 runner    (1001) docker     (127)     1986 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/importers/test_json_reader.py
--rw-r--r--   0 runner    (1001) docker     (127)     1407 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/importers/test_sqlalchemy_connector.py
--rw-r--r--   0 runner    (1001) docker     (127)     6253 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/spine_io/test_excel_integration.py
--rw-r--r--   0 runner    (1001) docker     (127)    51540 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/test_DatabaseMapping.py
--rw-r--r--   0 runner    (1001) docker     (127)    73511 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/test_DiffDatabaseMapping.py
--rw-r--r--   0 runner    (1001) docker     (127)     4195 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/test_check_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     9000 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/test_export_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     1844 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/test_helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)    74718 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/test_import_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     2515 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/test_mapping.py
--rw-r--r--   0 runner    (1001) docker     (127)     8438 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/test_migration.py
--rw-r--r--   0 runner    (1001) docker     (127)    45877 2024-04-23 14:05:28.000000 spinedb_api-0.30.5/tests/test_parameter_value.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.765573 spinedb_api-0.31.0/
+-rw-r--r--   0 runner    (1001) docker     (127)      161 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/.gitattributes
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.737573 spinedb_api-0.31.0/.github/
+-rw-r--r--   0 runner    (1001) docker     (127)      296 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/.github/pull_request_template.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.737573 spinedb_api-0.31.0/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (127)     3648 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/.github/workflows/run_unit_tests.yml
+-rw-r--r--   0 runner    (1001) docker     (127)      331 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)      674 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/.readthedocs.yml
+-rw-r--r--   0 runner    (1001) docker     (127)     1247 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/CHANGELOG.md
+-rw-r--r--   0 runner    (1001) docker     (127)    32493 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/COPYING
+-rw-r--r--   0 runner    (1001) docker     (127)     7651 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/COPYING.LESSER
+-rw-r--r--   0 runner    (1001) docker     (127)       52 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (127)     3843 2024-04-30 07:31:12.765573 spinedb_api-0.31.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     2714 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.737573 spinedb_api-0.31.0/benchmarks/
+-rw-r--r--   0 runner    (1001) docker     (127)      933 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/benchmarks/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/benchmarks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1537 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/benchmarks/datetime_from_database.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1122 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/benchmarks/map_from_database.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2029 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/benchmarks/mapped_item_getitem.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1837 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/benchmarks/update_default_value_to_different_value.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1533 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/benchmarks/update_default_value_to_same_value.py
+-rw-r--r--   0 runner    (1001) docker     (127)      835 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/benchmarks/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.737573 spinedb_api-0.31.0/bin/
+-rw-r--r--   0 runner    (1001) docker     (127)       76 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/bin/build_doc.bat
+-rw-r--r--   0 runner    (1001) docker     (127)      346 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/bin/build_doc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3248 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/deploy-key.enc
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.737573 spinedb_api-0.31.0/docs/
+-rw-r--r--   0 runner    (1001) docker     (127)      584 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/docs/Makefile
+-rw-r--r--   0 runner    (1001) docker     (127)      756 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/docs/make.bat
+-rw-r--r--   0 runner    (1001) docker     (127)      423 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/docs/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.741573 spinedb_api-0.31.0/docs/source/
+-rw-r--r--   0 runner    (1001) docker     (127)     9650 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/docs/source/conf.py
+-rw-r--r--   0 runner    (1001) docker     (127)      985 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/docs/source/front_matter.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.741573 spinedb_api-0.31.0/docs/source/img/
+-rw-r--r--   0 runner    (1001) docker     (127)     5205 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/docs/source/img/spinetoolbox_on_wht.svg
+-rw-r--r--   0 runner    (1001) docker     (127)      646 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/docs/source/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     1678 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/docs/source/metadata.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    12306 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/docs/source/parameter_value_format.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     7264 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/docs/source/tutorial.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.741573 spinedb_api-0.31.0/fig/
+-rw-r--r--   0 runner    (1001) docker     (127)    55634 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/fig/eu-emblem-low-res.jpg
+-rw-r--r--   0 runner    (1001) docker     (127)    16474 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/pylintrc
+-rw-r--r--   0 runner    (1001) docker     (127)     1674 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (127)        5 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-04-30 07:31:12.765573 spinedb_api-0.31.0/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.745573 spinedb_api-0.31.0/spinedb_api/
+-rw-r--r--   0 runner    (1001) docker     (127)     3689 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.745573 spinedb_api-0.31.0/spinedb_api/alembic/
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/README
+-rw-r--r--   0 runner    (1001) docker     (127)     2015 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/env.py
+-rw-r--r--   0 runner    (1001) docker     (127)      494 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/script.py.mako
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.749573 spinedb_api-0.31.0/spinedb_api/alembic/versions/
+-rw-r--r--   0 runner    (1001) docker     (127)     1816 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/070a0eb89e88_drop_category_tables.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4013 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/0c7d199ae915_add_list_value_table.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2371 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/1892adebc00f_create_metadata_tables.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2929 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/1e4997105288_separate_type_from_value.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5987 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/39e860a11b05_add_alternatives_and_scenarios.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2685 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/51fd7b69acf7_add_parameter_tag_and_parameter_value_list.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1265 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/5385f063bef2_create_superclass_subclass_table.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5425 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/6b7c994c1c61_drop_object_and_relationship_tables.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1280 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/738d494a08ac_fix_foreign_key_constraints_in_object_.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1308 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/7d0b467f2f4e_fix_foreign_key_constraints_in_entity_.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1139 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/8b0eff478bcb_add_active_by_default_to_entity_class.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2461 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/8c19c53d5701_rename_parameter_to_parameter_definition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4761 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/989fccf80441_replace_values_with_reference_to_list_.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1240 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/9da58d2def22_create_entity_group_table.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19946 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/bba1e2ef5153_move_to_entity_based_design.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4341 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/bf255c179bce_get_rid_of_unused_fields_in_parameter_.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1847 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/ce9faa82ed59_create_entity_alternative_table.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3618 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/defbda3bf2b5_add_tool_feature_tables.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6031 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/fbb540efbf15_add_support_for_mysql.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1230 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic/versions/fd542cebf699_drop_on_update_clauses_from_object_and_.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1697 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/alembic.ini
+-rw-r--r--   0 runner    (1001) docker     (127)    11352 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/compatibility.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45892 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/db_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    48172 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/db_mapping_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7691 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/db_mapping_commit_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    63433 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/db_mapping_query_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2685 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/exception.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8281 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/export_functions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.749573 spinedb_api-0.31.0/spinedb_api/export_mapping/
+-rw-r--r--   0 runner    (1001) docker     (127)     1496 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/export_mapping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    53281 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/export_mapping/export_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4282 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/export_mapping/generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4083 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/export_mapping/group_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10409 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/export_mapping/pivot.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22552 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/export_mapping/settings.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.749573 spinedb_api-0.31.0/spinedb_api/filters/
+-rw-r--r--   0 runner    (1001) docker     (127)     1023 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/filters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6615 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/filters/alternative_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6104 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/filters/execution_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9600 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/filters/renamer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15637 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/filters/scenario_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10263 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/filters/tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11835 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/filters/value_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8940 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/graph_layout_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33569 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28826 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/import_functions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.753573 spinedb_api-0.31.0/spinedb_api/import_mapping/
+-rw-r--r--   0 runner    (1001) docker     (127)     1102 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/import_mapping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15737 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/import_mapping/generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    39773 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/import_mapping/import_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14574 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/import_mapping/import_mapping_compat.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4162 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/import_mapping/type_conversion.py
+-rw-r--r--   0 runner    (1001) docker     (127)    34047 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/mapped_items.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8841 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    60563 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/parameter_value.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8457 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/perfect_split.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3574 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/purge.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5355 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/query.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4591 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/server_client_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5171 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_db_client.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28277 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_db_server.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.753573 spinedb_api-0.31.0/spinedb_api/spine_io/
+-rw-r--r--   0 runner    (1001) docker     (127)     1086 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.753573 spinedb_api-0.31.0/spinedb_api/spine_io/exporters/
+-rw-r--r--   0 runner    (1001) docker     (127)     1096 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/exporters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2699 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/exporters/csv_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12696 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/exporters/excel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4497 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/exporters/excel_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5731 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/exporters/gdx_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6232 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/exporters/sql_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5246 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/exporters/writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2668 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/gdx_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.753573 spinedb_api-0.31.0/spinedb_api/spine_io/importers/
+-rw-r--r--   0 runner    (1001) docker     (127)     1054 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/importers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7421 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/importers/csv_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4374 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/importers/datapackage_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10523 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/importers/excel_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4852 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/importers/gdx_connector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3698 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/importers/json_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6698 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/importers/reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3564 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/spine_io/importers/sqlalchemy_connector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2917 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/spinedb_api/temp_id.py
+-rw-r--r--   0 runner    (1001) docker     (127)      413 2024-04-30 07:31:12.000000 spinedb_api-0.31.0/spinedb_api/version.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.765573 spinedb_api-0.31.0/spinedb_api.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)     3843 2024-04-30 07:31:12.000000 spinedb_api-0.31.0/spinedb_api.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     6390 2024-04-30 07:31:12.000000 spinedb_api-0.31.0/spinedb_api.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-30 07:31:12.000000 spinedb_api-0.31.0/spinedb_api.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-30 07:31:12.000000 spinedb_api-0.31.0/spinedb_api.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (127)      236 2024-04-30 07:31:12.000000 spinedb_api-0.31.0/spinedb_api.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       28 2024-04-30 07:31:12.000000 spinedb_api-0.31.0/spinedb_api.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.757573 spinedb_api-0.31.0/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)     1076 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5225 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/custom_db_mapping.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.757573 spinedb_api-0.31.0/tests/export_mapping/
+-rw-r--r--   0 runner    (1001) docker     (127)     1023 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/export_mapping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    77801 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/export_mapping/test_export_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8739 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/export_mapping/test_pivot.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14520 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/export_mapping/test_settings.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.761573 spinedb_api-0.31.0/tests/filters/
+-rw-r--r--   0 runner    (1001) docker     (127)     1023 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/filters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8620 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/filters/test_alternative_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2111 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/filters/test_execution_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12059 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/filters/test_renamer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27281 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/filters/test_scenario_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10018 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/filters/test_tool_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13678 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/filters/test_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10462 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/filters/test_value_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.761573 spinedb_api-0.31.0/tests/import_mapping/
+-rw-r--r--   0 runner    (1001) docker     (127)     1023 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/import_mapping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24295 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/import_mapping/test_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    87939 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/import_mapping/test_import_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4676 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/import_mapping/test_type_conversion.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.761573 spinedb_api-0.31.0/tests/spine_io/
+-rw-r--r--   0 runner    (1001) docker     (127)     1092 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.761573 spinedb_api-0.31.0/tests/spine_io/exporters/
+-rw-r--r--   0 runner    (1001) docker     (127)     1102 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/exporters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4564 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/exporters/test_csv_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6457 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/exporters/test_excel_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15994 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/exporters/test_gdx_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12163 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/exporters/test_sql_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3400 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/exporters/test_writer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 07:31:12.765573 spinedb_api-0.31.0/tests/spine_io/importers/
+-rw-r--r--   0 runner    (1001) docker     (127)     1102 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/importers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3650 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/importers/test_CSVConnector.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12520 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/importers/test_GdxConnector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4565 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/importers/test_datapackage_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1429 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/importers/test_excel_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2030 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/importers/test_json_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2075 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/importers/test_reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1451 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/importers/test_sqlalchemy_connector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6242 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/spine_io/test_excel_integration.py
+-rw-r--r--   0 runner    (1001) docker     (127)   192142 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/test_DatabaseMapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4684 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/test_check_integrity.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3202 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/test_db_mapping_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4462 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/test_db_server.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8217 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/test_export_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4079 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/test_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    77621 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/test_import_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2559 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/test_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8600 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/test_migration.py
+-rw-r--r--   0 runner    (1001) docker     (127)    48629 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/test_parameter_value.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5815 2024-04-30 07:31:06.000000 spinedb_api-0.31.0/tests/test_purge.py
```

### Comparing `spinedb_api-0.30.5/.readthedocs.yml` & `spinedb_api-0.31.0/.readthedocs.yml`

 * *Files 20% similar despite different names*

```diff
@@ -1,24 +1,28 @@
 # .readthedocs.yml
 # Read the Docs configuration file
 # See https://docs.readthedocs.io/en/stable/config-file/v2.html for details
 
 # Required
 version: 2
 
+# Set the version of Python and other tools you might need
+build:
+  os: ubuntu-22.04
+  tools:
+    python: "3.9"
+    
 # Build documentation in the docs/ directory with Sphinx
 sphinx:
   builder: html
   configuration: docs/source/conf.py
 
 # Optionally build your docs in additional formats such as PDF and ePub
 formats:
   - htmlzip
   - pdf
 
 # Optionally set the version of Python and requirements required to build your docs
 python:
-  version: 3.8
   install:
-    - method: pip
-      path: .
+    - requirements: requirements.txt
     - requirements: docs/requirements.txt
```

### Comparing `spinedb_api-0.30.5/COPYING` & `spinedb_api-0.31.0/COPYING`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/COPYING.LESSER` & `spinedb_api-0.31.0/COPYING.LESSER`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/PKG-INFO` & `spinedb_api-0.31.0/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 Metadata-Version: 2.1
 Name: spinedb_api
-Version: 0.30.5
+Version: 0.31.0
 Summary: An API to talk to Spine databases.
 Author-email: Spine Project consortium <spine_info@vtt.fi>
 License: LGPL-3.0-or-later
 Project-URL: Repository, https://github.com/spine-tools/Spine-Database-API
 Keywords: energy system modelling,workflow,optimisation,database
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)
 Classifier: Operating System :: OS Independent
-Requires-Python: <3.12,>=3.8.1
+Requires-Python: >=3.8.1
 Description-Content-Type: text/markdown
 License-File: COPYING
 License-File: COPYING.LESSER
 Requires-Dist: sqlalchemy<1.4,>=1.3
 Requires-Dist: alembic>=1.7
 Requires-Dist: faker>=8.1.2
 Requires-Dist: datapackage>=1.15.2
@@ -24,14 +24,15 @@
 Requires-Dist: gdx2py>=2.1.1
 Requires-Dist: ijson>=3.1.4
 Requires-Dist: chardet>=4.0.0
 Requires-Dist: pymysql>=1.0.2
 Requires-Dist: psycopg2
 Provides-Extra: dev
 Requires-Dist: coverage[toml]; extra == "dev"
+Requires-Dist: pyperf; extra == "dev"
 
 # Spine Database API
 
 [![Documentation Status](https://readthedocs.org/projects/spine-database-api/badge/?version=latest)](https://spine-database-api.readthedocs.io/en/latest/?badge=latest)
 [![Unit tests](https://github.com/spine-tools/Spine-Database-API/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/Spine-Database-API/actions?query=workflow%3A"Unit+tests")
 [![codecov](https://codecov.io/gh/spine-tools/Spine-Database-API/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/Spine-Database-API)
 [![PyPI version](https://badge.fury.io/py/spinedb-api.svg)](https://badge.fury.io/py/spinedb-api)
```

#### html2text {}

```diff
@@ -1,27 +1,27 @@
-Metadata-Version: 2.1 Name: spinedb_api Version: 0.30.5 Summary: An API to talk
+Metadata-Version: 2.1 Name: spinedb_api Version: 0.31.0 Summary: An API to talk
 to Spine databases. Author-email: Spine Project consortium
 vtt.fi> License: LGPL-3.0-or-later Project-URL: Repository, https://github.com/
 spine-tools/Spine-Database-API Keywords: energy system
 modelling,workflow,optimisation,database Classifier: Programming Language ::
 Python :: 3 Classifier: License :: OSI Approved :: GNU Lesser General Public
 License v3 (LGPLv3) Classifier: Operating System :: OS Independent Requires-
-Python: <3.12,>=3.8.1 Description-Content-Type: text/markdown License-File:
-COPYING License-File: COPYING.LESSER Requires-Dist: sqlalchemy<1.4,>=1.3
-Requires-Dist: alembic>=1.7 Requires-Dist: faker>=8.1.2 Requires-Dist:
-datapackage>=1.15.2 Requires-Dist: python-dateutil>=2.8.1 Requires-Dist:
-numpy>=1.20.2 Requires-Dist: scipy>=1.7.1 Requires-Dist:
-openpyxl!=3.1.1,>=3.0.7 Requires-Dist: gdx2py>=2.1.1 Requires-Dist:
-ijson>=3.1.4 Requires-Dist: chardet>=4.0.0 Requires-Dist: pymysql>=1.0.2
-Requires-Dist: psycopg2 Provides-Extra: dev Requires-Dist: coverage[toml];
-extra == "dev" # Spine Database API [![Documentation Status](https://
-readthedocs.org/projects/spine-database-api/badge/?version=latest)](https://
-spine-database-api.readthedocs.io/en/latest/?badge=latest) [![Unit tests]
-(https://github.com/spine-tools/Spine-Database-API/workflows/Unit%20tests/
-badge.svg)](https://github.com/spine-tools/Spine-Database-API/
+Python: >=3.8.1 Description-Content-Type: text/markdown License-File: COPYING
+License-File: COPYING.LESSER Requires-Dist: sqlalchemy<1.4,>=1.3 Requires-Dist:
+alembic>=1.7 Requires-Dist: faker>=8.1.2 Requires-Dist: datapackage>=1.15.2
+Requires-Dist: python-dateutil>=2.8.1 Requires-Dist: numpy>=1.20.2 Requires-
+Dist: scipy>=1.7.1 Requires-Dist: openpyxl!=3.1.1,>=3.0.7 Requires-Dist:
+gdx2py>=2.1.1 Requires-Dist: ijson>=3.1.4 Requires-Dist: chardet>=4.0.0
+Requires-Dist: pymysql>=1.0.2 Requires-Dist: psycopg2 Provides-Extra: dev
+Requires-Dist: coverage[toml]; extra == "dev" Requires-Dist: pyperf; extra ==
+"dev" # Spine Database API [![Documentation Status](https://readthedocs.org/
+projects/spine-database-api/badge/?version=latest)](https://spine-database-
+api.readthedocs.io/en/latest/?badge=latest) [![Unit tests](https://github.com/
+spine-tools/Spine-Database-API/workflows/Unit%20tests/badge.svg)](https://
+github.com/spine-tools/Spine-Database-API/
 actions?query=workflow%3A"Unit+tests") [![codecov](https://codecov.io/gh/spine-
 tools/Spine-Database-API/branch/master/graph/badge.svg)](https://codecov.io/gh/
 spine-tools/Spine-Database-API) [![PyPI version](https://badge.fury.io/py/
 spinedb-api.svg)](https://badge.fury.io/py/spinedb-api) A Python package to
 access and manipulate Spine databases in a customary, unified way. ## License
 Spine Database API is released under the GNU Lesser General Public License
 (LGPL) license. All accompanying documentation and manual are released under
```

### Comparing `spinedb_api-0.30.5/README.md` & `spinedb_api-0.31.0/README.md`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/deploy-key.enc` & `spinedb_api-0.31.0/deploy-key.enc`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/docs/Makefile` & `spinedb_api-0.31.0/docs/Makefile`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/docs/make.bat` & `spinedb_api-0.31.0/docs/make.bat`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/docs/source/front_matter.rst` & `spinedb_api-0.31.0/docs/source/front_matter.rst`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-..  spinedb_api tutorial
+..  spinedb_api front matter
     Created: 18.6.2018
 
 .. _SQLAlchemy: http://www.sqlalchemy.org/
 
 
 ***************
 Front matter
```

### Comparing `spinedb_api-0.30.5/docs/source/img/spinetoolbox_on_wht.svg` & `spinedb_api-0.31.0/docs/source/img/spinetoolbox_on_wht.svg`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/docs/source/index.rst` & `spinedb_api-0.31.0/docs/source/index.rst`

 * *Files 12% similar despite different names*

```diff
@@ -13,16 +13,16 @@
 .. toctree::
    :maxdepth: 1
    :caption: Contents:
 
    front_matter
    tutorial
    parameter_value_format
-   metadata_description
-   results_metadata_description
+   metadata
+   db_mapping_schema
    autoapi/index
 
 Indices and tables
 ==================
 
 * :ref:`genindex`
 * :ref:`modindex`
```

### Comparing `spinedb_api-0.30.5/docs/source/parameter_value_format.rst` & `spinedb_api-0.31.0/docs/source/parameter_value_format.rst`

 * *Files 18% similar despite different names*

```diff
@@ -1,11 +1,21 @@
+.. _parameter_value_format:
+
+
 **********************
 Parameter value format
 **********************
 
+.. note::
+
+   Client code should almost never convert parameter values to JSON and back manually.
+   For most cases, JSON should be considered an implementation detail.
+   Clients should rather use :func:`.to_database` and :func:`.from_database` which shield
+   from abrupt changes in the database representation.
+
 Parameter values are specified using JSON in the ``value`` field of the ``parameter_value`` table.
 This document describes the JSON specification for parameter values of special type
 (namely, date-time, duration, time-pattern, time-series, array, and map.)
 
 A value of special type is a JSON object with two mandatory properties, ``type`` and ``data``:
 
 - ``type`` indicates the value *type* and must be a JSON string
@@ -97,14 +107,17 @@
   where ``s1``, ``s2``, ..., are intervals as described above.
 - A union of ranges.
   The format is ``r1,r2,...``,
   where ``r1``, ``r2``, ..., are either intervals or intersections of intervals as described above.
 
 The ``data`` property must be a JSON object mapping time periods to values.
 
+A time-pattern may have an additional property, ``index_name``.
+``index_name`` must be a JSON string. If not specified, a default name 'p' will be used.
+
 Example
 ~~~~~~~
 
 The following corresponds to a parameter which takes the value ``300`` in months 1 to 4 *and* 9 to 12,
 and the value ``221.5`` in months 5 to 8.
 
 .. code-block:: json
@@ -129,29 +142,31 @@
 
 - A JSON object mapping time-stamps to values.
 - A two-column JSON array listing tuples of the form [time-stamp, value].
 - A (one-column) JSON array of values.
   In this case it is assumed that the time-series begins at the first hour of *any* year,
   has a resolution of one hour, and repeats cyclically until the *end* of time.
 
-In case of time-series, the specification may have one additional property, ``index``.
+In case of time-series, the specification may have two additional properties, ``index`` and ``index_name``.
 ``index`` must be a JSON object with the following properties, all of them optional:
 
 - ``start``: the *first* time-stamp, used in case ``data`` is a one-column array (ignored otherwise).
   It must be a JSON string in the `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ format.
   The default is ``0001-01-01T00:00:00``.
 - ``resolution``: the 'time between stamps', used in case ``data`` is a one-column array (ignored otherwise).
   Accepted values are the same as for the ``data`` property of [duration](#duration) values.
   The default is ``1 hour``.
   If ``resolution`` is itself an array, then it is either trunk or repeated so as to fit ``data``.
 - ``ignore_year``: a JSON boolean to indicate whether or not the time-series should apply to *any* year.
   The default is ``false``, unless ``data`` is a one-column array and ``start`` is not given.
 - ``repeat``: a JSON boolean whether or not the time-series should repeat cyclically until the *end* of time.
   The default is ``false``, unless ``data`` is a one-column array and ``start`` is not given.
 
+``index_name`` must be a JSON string. If not specified, a default name 't' will be used.
+
 Examples
 ~~~~~~~~
 
 Dictionary:
 
 .. code-block:: json
 
@@ -198,31 +213,50 @@
        "start": "2019-01-01T00:00",
        "resolution": "30 minutes",
        "ignore_year": false,
        "repeat": true
      }
    }
 
+Two-column array with named indices:
+
+.. code-block:: json
+
+   {
+
+     "type": "time_series",
+     "data": [
+       ["2019-01-01T00:00", 1],
+       ["2019-01-01T00:30", 2],
+       ["2019-01-01T02:00", 8]
+     ],
+     "index_name": "Time stamps"
+   }
+
 Array
 -----
 
 If the ``type`` property is ``array``, then the ``data`` property specifies a one dimensional array.
 This is a list of values with zero based indexing.
 All values are of the same type which is specified by an optional ``value_type`` property.
 If specified, ``value_type`` must be one of the following: ``float``, ``str``, ``duration``, or ``date_time``.
 If omitted, ``value_type`` defaults to ``float``
 
-The ``data`` property must be a JSON list. The elements depent on ``value_type``:
+The ``data`` property must be a JSON list. The elements depend on ``value_type``:
 
 - If ``value_type`` is ``float`` then all elements in ``data`` must be JSON numbers.
 - If ``value_type`` is ``str`` then all elements in ``data`` must be JSON strings.
 - If ``value_type`` is ``duration`` then all elements in ``data`` must be single extensions of time.
 - If ``value_type`` is ``date_time`` then all elements in ``data`` must be JSON strings
   in the `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_ format.
 
+An array may have an additional property, ``index_name``.
+``index_name`` must be a JSON string. If not specified, a default name 'i' will be used.
+
+
 Examples
 ~~~~~~~~
 
 An array of numbers:
 
 .. code-block:: json
 
@@ -237,14 +271,25 @@
 
    {
      "type": "array",
      "value_type": "duration",
      "data": ["3 months", "2Y", "4 minutes"]
    }
 
+An array of strings with index name:
+
+.. code-block:: json
+
+   {
+     "type": "array",
+     "data": ["one", "two"],
+     "index_name": "step"
+   }
+
+
 Map
 ---
 
 If the ``type`` property is ``map``, then the ``data`` property specifies indexed array data.
 An additional ``index_type`` specifies the type of the index and must be one of the following:
 ``float``, ``str``, ``duration``, or ``date_time``.
 
@@ -265,14 +310,17 @@
   * a datetime, e.g. ``{"type": "date_time", "data": "2020-01-01T12:00"``}
   * a map, e.g. ``{"type": "map", "index_type": "str", "data":{"a": 2, "b": 3}}``
   * any of the following: time-series, array, time-pattern
 
 Optionally, the ``data`` property can be a two-column JSON array
 where the first element is the key and the second the value.
 
+A map may have an additional property, ``index_name``.
+``index_name`` must be a JSON string. If not specified, a default name 'x' will be used.
+
 Examples
 ~~~~~~~~
 
 Dictionary:
 
 .. code-block:: json
 
@@ -317,17 +365,33 @@
 ================ ================ =================== =====
 
 .. code-block:: json
 
    {
      "type": "map",
      "index_type": "date_time",
+     "index_name": "Forecast time",
      "data": [
        ["2020-04-17T08:00",
-        {"type": "map", "index_type": "date_time", "data": [
-          ["2020-04-17T08:00", {"type": "map", "index_type": "float", "data": [[0, 23.0], [1, 5.5]]}],
-          ["2020-04-17T09:00", {"type": "map", "index_type": "float", "data": [[0, 24.0], [1, 6.6]]}],
-          ["2020-04-17T10:00", {"type": "map", "index_type": "float", "data": [[0, 25.0], [1, 7.7]]}]
+        {"type": "map", "index_type": "date_time", "index_name": "Target time", "data": [
+          [
+            "2020-04-17T08:00", {"type": "map",
+                                 "index_type": "float",
+                                 "index_name": "Stochastic scenario",
+                                 "data": [[0, 23.0], [1, 5.5]]}
+          ],
+          [
+            "2020-04-17T09:00", {"type": "map",
+                                 "index_type": "float",
+                                 "index_name": "Stochastic scenario",
+                                 "data": [[0, 24.0], [1, 6.6]]}
+          ],
+          [
+            "2020-04-17T10:00", {"type": "map",
+                                 "index_type": "float",
+                                 "index_name": "Stochastic scenario",
+                                 "data": [[0, 25.0], [1, 7.7]]}
+          ]
         ]}
        ]
      ]
    }
```

### Comparing `spinedb_api-0.30.5/fig/eu-emblem-low-res.jpg` & `spinedb_api-0.31.0/fig/eu-emblem-low-res.jpg`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/pylintrc` & `spinedb_api-0.31.0/pylintrc`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/pyproject.toml` & `spinedb_api-0.31.0/pyproject.toml`

 * *Files 6% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 keywords = ["energy system modelling", "workflow", "optimisation", "database"]
 readme = {file = "README.md", content-type = "text/markdown"}
 classifiers = [
     "Programming Language :: Python :: 3",
     "License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)",
     "Operating System :: OS Independent",
 ]
-requires-python = ">=3.8.1, <3.12"
+requires-python = ">=3.8.1"
 dependencies = [
     # v1.4 does not pass tests
     "sqlalchemy >=1.3, <1.4",
     "alembic >=1.7",
     "faker >=8.1.2",
     "datapackage >=1.15.2",
     "python-dateutil >=2.8.1",
@@ -30,15 +30,15 @@
     "psycopg2",
 ]
 
 [project.urls]
 Repository = "https://github.com/spine-tools/Spine-Database-API"
 
 [project.optional-dependencies]
-dev = ["coverage[toml]"]
+dev = ["coverage[toml]", "pyperf"]
 
 [build-system]
 requires = ["setuptools>=64", "setuptools_scm[toml]>=6.2", "wheel", "build"]
 build-backend = "setuptools.build_meta"
 
 [tool.setuptools_scm]
 write_to = "spinedb_api/version.py"
@@ -61,9 +61,8 @@
 branch = true
 
 [tool.coverage.report]
 ignore_errors = true
 
 [tool.black]
 line-length = 120
-skip-string-normalization = true
 exclude = '\.git'
```

### Comparing `spinedb_api-0.30.5/spinedb_api/__init__.py` & `spinedb_api-0.31.0/spinedb_api/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,97 +1,78 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
+"""
+A package to interact with Spine DBs.
+"""
+
 from .db_mapping import DatabaseMapping
-from .diff_db_mapping import DiffDatabaseMapping
 from .exception import (
     SpineDBAPIError,
     SpineIntegrityError,
     SpineDBVersionError,
-    SpineTableNotFoundError,
-    RecordNotFoundError,
-    ParameterValueError,
     ParameterValueFormatError,
     InvalidMapping,
 )
 from .helpers import (
     naming_convention,
     create_spine_metadata,
     SUPPORTED_DIALECTS,
     create_new_spine_database,
     copy_database,
-    is_unlocked,
-    is_head,
     is_empty,
     forward_sweep,
     Asterisk,
 )
-from .check_functions import (
-    check_alternative,
-    check_scenario,
-    check_scenario_alternative,
-    check_object_class,
-    check_object,
-    check_wide_relationship_class,
-    check_wide_relationship,
-    check_parameter_definition,
-    check_parameter_value,
-    check_parameter_value_list,
-)
 from .import_functions import (
     import_alternatives,
     import_data,
+    import_entity_classes,
+    import_entities,
+    import_entity_alternatives,
+    import_parameter_definitions,
+    import_parameter_values,
     import_object_classes,
     import_objects,
     import_object_parameters,
     import_object_parameter_values,
     import_parameter_value_lists,
     import_relationship_classes,
     import_relationship_parameter_values,
     import_relationship_parameters,
     import_relationships,
     import_scenarios,
     import_scenario_alternatives,
-    import_tools,
-    import_features,
-    import_tool_features,
-    import_tool_feature_methods,
     import_metadata,
     import_object_metadata,
     import_relationship_metadata,
     import_object_parameter_value_metadata,
     import_relationship_parameter_value_metadata,
     get_data_for_import,
 )
 from .export_functions import (
-    export_alternatives,
     export_data,
-    export_object_classes,
-    export_object_groups,
-    export_object_parameters,
-    export_object_parameter_values,
-    export_objects,
-    export_relationship_classes,
-    export_relationship_parameter_values,
-    export_relationship_parameters,
-    export_relationships,
-    export_scenario_alternatives,
+    export_entity_classes,
+    export_entity_groups,
+    export_entities,
+    export_parameter_value_lists,
+    export_parameter_definitions,
+    export_parameter_values,
     export_scenarios,
-    export_tools,
-    export_features,
-    export_tool_features,
-    export_tool_feature_methods,
+    export_alternatives,
+    export_scenario_alternatives,
 )
 from .import_mapping.import_mapping_compat import import_mapping_from_dict
 from .import_mapping.generator import get_mapped_data
 from .parameter_value import (
     convert_containers_to_maps,
     convert_leaf_maps_to_specialized_containers,
     convert_map_to_dict,
@@ -99,26 +80,24 @@
     duration_to_relativedelta,
     relativedelta_to_duration,
     from_database,
     to_database,
     Array,
     DateTime,
     Duration,
-    IndexedNumberArray,
     IndexedValue,
     Map,
     TimePattern,
     TimeSeries,
     TimeSeriesFixedResolution,
     TimeSeriesVariableResolution,
     ListValueRef,
 )
 from .filters.alternative_filter import apply_alternative_filter_to_parameter_value_sq
 from .filters.scenario_filter import apply_scenario_filter_to_subqueries
-from .filters.tool_filter import apply_tool_filter_to_entity_sq
 from .filters.execution_filter import apply_execution_filter
 from .filters.renamer import apply_renaming_to_parameter_definition_sq, apply_renaming_to_entity_class_sq
 from .filters.tools import (
     append_filter_config,
     apply_filter_stack,
     clear_filter_configs,
     config_to_shorthand,
```

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/env.py` & `spinedb_api-0.31.0/spinedb_api/alembic/env.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 # This line sets up loggers basically.
 fileConfig(config.config_file_name)
 
 # add your model's MetaData object here
 # for 'autogenerate' support
 import sys
 
-sys.path = ['', '..'] + sys.path[1:]
+sys.path = ["", ".."] + sys.path[1:]
 from spinedb_api.helpers import create_spine_metadata
 
 target_metadata = create_spine_metadata()
 
 # other values from the config, defined by the needs of env.py,
 # can be acquired:
 # my_important_option = config.get_main_option("my_important_option")
```

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/070a0eb89e88_drop_category_tables.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/070a0eb89e88_drop_category_tables.py`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/0c7d199ae915_add_list_value_table.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/0c7d199ae915_add_list_value_table.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,16 +8,16 @@
 from alembic import op
 import sqlalchemy as sa
 from sqlalchemy.ext.automap import automap_base
 from sqlalchemy.orm import sessionmaker
 from spinedb_api.helpers import LONGTEXT_LENGTH
 
 # revision identifiers, used by Alembic.
-revision = '0c7d199ae915'
-down_revision = '7d0b467f2f4e'
+revision = "0c7d199ae915"
+down_revision = "7d0b467f2f4e"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
     # Rescue current data
     conn = op.get_bind()
@@ -32,49 +32,49 @@
     m = sa.MetaData(op.get_bind())
     m.reflect()
     # Change schema
     if "next_id" in m.tables:
         with op.batch_alter_table("next_id") as batch_op:
             batch_op.add_column(sa.Column("list_value_id", sa.Integer, server_default=sa.null()))
     op.create_table(
-        'list_value',
-        sa.Column('id', sa.Integer, primary_key=True),
-        sa.Column('parameter_value_list_id', sa.Integer, sa.ForeignKey("parameter_value_list.id"), nullable=False),
-        sa.Column('index', sa.Integer, nullable=False),
-        sa.Column('type', sa.String(255)),
-        sa.Column('value', sa.LargeBinary(LONGTEXT_LENGTH), server_default=sa.null()),
-        sa.Column('commit_id', sa.Integer, sa.ForeignKey("commit.id")),
-        sa.UniqueConstraint('parameter_value_list_id', 'index'),
+        "list_value",
+        sa.Column("id", sa.Integer, primary_key=True),
+        sa.Column("parameter_value_list_id", sa.Integer, sa.ForeignKey("parameter_value_list.id"), nullable=False),
+        sa.Column("index", sa.Integer, nullable=False),
+        sa.Column("type", sa.String(255)),
+        sa.Column("value", sa.LargeBinary(LONGTEXT_LENGTH), server_default=sa.null()),
+        sa.Column("commit_id", sa.Integer, sa.ForeignKey("commit.id")),
+        sa.UniqueConstraint("parameter_value_list_id", "index"),
     )
     # NOTE: At some point, by mistake, we modified ``helpers.create_new_spine_database`` by specifying a name for the fk
     # that refers parameter_value_list in tool_feature_method. But since this was just a mistake, we didn't provide a
     # migration script that would have made the change consistent. As result, databases created using this version of
     # the function (or latter) have that given name for the constraint, whereas databases created using a prior version
     # that followed the migration path have the automatic name from the naming convention. So we don't know for sure how
     # the constraint is named and we need to find out as below.
     fk_name = next(
         x["name"]
         for x in sa.inspect(conn).get_foreign_keys("tool_feature_method")
         if x["referred_table"] == "parameter_value_list"
-        and x["referred_columns"] == ['id', 'value_index']
-        and x["constrained_columns"] == ['parameter_value_list_id', 'method_index']
+        and x["referred_columns"] == ["id", "value_index"]
+        and x["constrained_columns"] == ["parameter_value_list_id", "method_index"]
     )
     with op.batch_alter_table("tool_feature_method") as batch_op:
-        batch_op.drop_constraint(fk_name, type_='foreignkey')
+        batch_op.drop_constraint(fk_name, type_="foreignkey")
     with op.batch_alter_table("parameter_value_list") as batch_op:
-        batch_op.drop_column('value_index')
-        batch_op.drop_column('value')
+        batch_op.drop_column("value_index")
+        batch_op.drop_column("value")
     with op.batch_alter_table("tool_feature_method") as batch_op:
         batch_op.create_foreign_key(
             None,
-            'list_value',
-            ['parameter_value_list_id', 'method_index'],
-            ['parameter_value_list_id', 'index'],
-            onupdate='CASCADE',
-            ondelete='CASCADE',
+            "list_value",
+            ["parameter_value_list_id", "method_index"],
+            ["parameter_value_list_id", "index"],
+            onupdate="CASCADE",
+            ondelete="CASCADE",
         )
     # Add rescued data
     pvl_items = list({x.id: {"id": x.id, "name": x.name, "commit_id": x.commit_id} for x in pvl}.values())
     lv_items = [{"parameter_value_list_id": x.id, "index": x.value_index, "value": x.value, "type": None} for x in pvl]
     tfm_items = [
         {c: getattr(x, c) for c in ("id", "tool_feature_id", "parameter_value_list_id", "method_index", "commit_id")}
         for x in tfm
```

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/1892adebc00f_create_metadata_tables.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/1892adebc00f_create_metadata_tables.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,16 +6,16 @@
 
 """
 from alembic import op
 import sqlalchemy as sa
 
 
 # revision identifiers, used by Alembic.
-revision = '1892adebc00f'
-down_revision = 'defbda3bf2b5'
+revision = "1892adebc00f"
+down_revision = "defbda3bf2b5"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
     m = sa.MetaData(op.get_bind())
     m.reflect()
```

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/1e4997105288_separate_type_from_value.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/1e4997105288_separate_type_from_value.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,16 +10,16 @@
 from alembic import op
 import sqlalchemy as sa
 from sqlalchemy.ext.automap import automap_base
 from sqlalchemy.orm import sessionmaker
 
 
 # revision identifiers, used by Alembic.
-revision = '1e4997105288'
-down_revision = 'fbb540efbf15'
+revision = "1e4997105288"
+down_revision = "fbb540efbf15"
 branch_labels = None
 depends_on = None
 
 LONGTEXT_LENGTH = 2 ** 32 - 1
 
 
 def upgrade():
@@ -30,22 +30,22 @@
     Base.prepare(conn, reflect=True)
     # Get items to update
     pd_items = _get_items(session, Base, "parameter_definition")
     pv_items = _get_items(session, Base, "parameter_value")
     pvl_items = _get_pvl_items(session, Base)
     # Alter tables
     with op.batch_alter_table("parameter_definition") as batch_op:
-        batch_op.drop_column('data_type')
+        batch_op.drop_column("data_type")
         batch_op.drop_column("default_value")
         batch_op.add_column(sa.Column("default_value", sa.LargeBinary(LONGTEXT_LENGTH), server_default=sa.null()))
-        batch_op.add_column(sa.Column('default_type', sa.String(length=255), nullable=True))
+        batch_op.add_column(sa.Column("default_type", sa.String(length=255), nullable=True))
     with op.batch_alter_table("parameter_value") as batch_op:
         batch_op.drop_column("value")
         batch_op.add_column(sa.Column("value", sa.LargeBinary(LONGTEXT_LENGTH), server_default=sa.null()))
-        batch_op.add_column(sa.Column('type', sa.String(length=255), nullable=True))
+        batch_op.add_column(sa.Column("type", sa.String(length=255), nullable=True))
     with op.batch_alter_table("parameter_value_list") as batch_op:
         batch_op.drop_column("value")
         batch_op.add_column(sa.Column("value", sa.LargeBinary(LONGTEXT_LENGTH), server_default=sa.null()))
     # Do update items
     Base = automap_base()
     Base.prepare(conn, reflect=True)
     session.bulk_update_mappings(Base.classes.parameter_definition, pd_items)
```

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/39e860a11b05_add_alternatives_and_scenarios.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/39e860a11b05_add_alternatives_and_scenarios.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,16 +9,16 @@
 from alembic import op
 import sqlalchemy as sa
 from sqlalchemy.ext.automap import automap_base
 from sqlalchemy.orm import sessionmaker
 
 
 # revision identifiers, used by Alembic.
-revision = '39e860a11b05'
-down_revision = '9da58d2def22'
+revision = "39e860a11b05"
+down_revision = "9da58d2def22"
 branch_labels = None
 depends_on = None
 
 
 def create_new_tables():
     op.create_table(
         "alternative",
@@ -81,15 +81,15 @@
         batch_op.add_column(sa.Column("alternative_id", sa.Integer, nullable=True))
         batch_op.create_unique_constraint(None, ["parameter_definition_id", "entity_id", "alternative_id"])
         if "uq_parameter_value_parameter_definition_identity_id" in parameter_value_uq_names:
             batch_op.drop_constraint("uq_parameter_value_parameter_definition_identity_id")
 
     op.execute("UPDATE parameter_value SET alternative_id = 1")
     with op.batch_alter_table("parameter_value") as batch_op:
-        batch_op.alter_column('alternative_id', nullable=False)
+        batch_op.alter_column("alternative_id", nullable=False)
         batch_op.create_foreign_key(
             None, "alternative", ("alternative_id",), ("id",), onupdate="CASCADE", ondelete="CASCADE"
         )
 
     m = sa.MetaData(op.get_bind())
     m.reflect()
     if "next_id" in m.tables:
@@ -111,17 +111,17 @@
                 scenario_alternative_id = 1
             """,
             user=user,
             date=date,
         )
 
     with op.batch_alter_table("entity_type") as batch_op:
-        batch_op.alter_column('commit_id', nullable=False)
+        batch_op.alter_column("commit_id", nullable=False)
     with op.batch_alter_table("entity_class_type") as batch_op:
-        batch_op.alter_column('commit_id', nullable=False)
+        batch_op.alter_column("commit_id", nullable=False)
 
 
 def upgrade():
     create_new_tables()
     Session = sessionmaker(bind=op.get_bind())
     session = Session()
     Base = automap_base()
@@ -142,10 +142,10 @@
     m.reflect()
     if "next_id" in m.tables:
         with op.batch_alter_table("next_id") as batch_op:
             batch_op.drop_column("alternative_id")
             batch_op.drop_column("scenario_id")
             batch_op.drop_column("scenario_alternative_id")
     with op.batch_alter_table("entity_type") as batch_op:
-        batch_op.alter_column('commit_id', nullable=True)
+        batch_op.alter_column("commit_id", nullable=True)
     with op.batch_alter_table("entity_class_type") as batch_op:
-        batch_op.alter_column('commit_id', nullable=True)
+        batch_op.alter_column("commit_id", nullable=True)
```

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/51fd7b69acf7_add_parameter_tag_and_parameter_value_list.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/51fd7b69acf7_add_parameter_tag_and_parameter_value_list.py`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/738d494a08ac_fix_foreign_key_constraints_in_object_.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/738d494a08ac_fix_foreign_key_constraints_in_object_.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,38 +6,38 @@
 
 """
 from alembic import op
 from spinedb_api.helpers import naming_convention
 
 
 # revision identifiers, used by Alembic.
-revision = '738d494a08ac'
-down_revision = '1e4997105288'
+revision = "738d494a08ac"
+down_revision = "1e4997105288"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
     with op.batch_alter_table("object", naming_convention=naming_convention) as batch_op:
-        batch_op.drop_constraint('fk_object_entity_id_entity', type_='foreignkey')
+        batch_op.drop_constraint("fk_object_entity_id_entity", type_="foreignkey")
         batch_op.create_foreign_key(
-            op.f('fk_object_entity_id_entity'),
-            'entity',
-            ['entity_id', 'type_id'],
-            ['id', 'type_id'],
+            op.f("fk_object_entity_id_entity"),
+            "entity",
+            ["entity_id", "type_id"],
+            ["id", "type_id"],
             onupdate="CASCADE",
             ondelete="CASCADE",
         )
     with op.batch_alter_table("relationship", naming_convention=naming_convention) as batch_op:
-        batch_op.drop_constraint('fk_relationship_entity_id_entity', type_='foreignkey')
+        batch_op.drop_constraint("fk_relationship_entity_id_entity", type_="foreignkey")
         batch_op.create_foreign_key(
-            op.f('fk_relationship_entity_id_entity'),
-            'entity',
-            ['entity_id', 'type_id'],
-            ['id', 'type_id'],
+            op.f("fk_relationship_entity_id_entity"),
+            "entity",
+            ["entity_id", "type_id"],
+            ["id", "type_id"],
             onupdate="CASCADE",
             ondelete="CASCADE",
         )
 
 
 def downgrade():
     pass
```

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/7d0b467f2f4e_fix_foreign_key_constraints_in_entity_.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/7d0b467f2f4e_fix_foreign_key_constraints_in_entity_.py`

 * *Files 18% similar despite different names*

```diff
@@ -6,36 +6,36 @@
 
 """
 from alembic import op
 from spinedb_api.helpers import naming_convention
 
 
 # revision identifiers, used by Alembic.
-revision = '7d0b467f2f4e'
-down_revision = 'fd542cebf699'
+revision = "7d0b467f2f4e"
+down_revision = "fd542cebf699"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
     with op.batch_alter_table("object_class", naming_convention=naming_convention) as batch_op:
-        batch_op.drop_constraint('fk_object_class_entity_class_id_entity_class', type_='foreignkey')
+        batch_op.drop_constraint("fk_object_class_entity_class_id_entity_class", type_="foreignkey")
         batch_op.create_foreign_key(
-            op.f('fk_object_class_entity_class_id_entity_class'),
-            'entity_class',
-            ['entity_class_id', 'type_id'],
-            ['id', 'type_id'],
+            op.f("fk_object_class_entity_class_id_entity_class"),
+            "entity_class",
+            ["entity_class_id", "type_id"],
+            ["id", "type_id"],
             ondelete="CASCADE",
         )
     with op.batch_alter_table("relationship_class", naming_convention=naming_convention) as batch_op:
-        batch_op.drop_constraint('fk_relationship_class_entity_class_id_entity_class', type_='foreignkey')
+        batch_op.drop_constraint("fk_relationship_class_entity_class_id_entity_class", type_="foreignkey")
         batch_op.create_foreign_key(
-            op.f('fk_relationship_class_entity_class_id_entity_class'),
-            'entity_class',
-            ['entity_class_id', 'type_id'],
-            ['id', 'type_id'],
+            op.f("fk_relationship_class_entity_class_id_entity_class"),
+            "entity_class",
+            ["entity_class_id", "type_id"],
+            ["id", "type_id"],
             ondelete="CASCADE",
         )
 
 
 def downgrade():
     pass
```

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/8c19c53d5701_rename_parameter_to_parameter_definition.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/8c19c53d5701_rename_parameter_to_parameter_definition.py`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/9da58d2def22_create_entity_group_table.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/9da58d2def22_create_entity_group_table.py`

 * *Files 22% similar despite different names*

```diff
@@ -6,16 +6,16 @@
 
 """
 from alembic import op
 import sqlalchemy as sa
 
 
 # revision identifiers, used by Alembic.
-revision = '9da58d2def22'
-down_revision = '070a0eb89e88'
+revision = "9da58d2def22"
+down_revision = "070a0eb89e88"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
     m = sa.MetaData(op.get_bind())
     m.reflect()
```

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/bba1e2ef5153_move_to_entity_based_design.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/bba1e2ef5153_move_to_entity_based_design.py`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/bf255c179bce_get_rid_of_unused_fields_in_parameter_.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/bf255c179bce_get_rid_of_unused_fields_in_parameter_.py`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/defbda3bf2b5_add_tool_feature_tables.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/defbda3bf2b5_add_tool_feature_tables.py`

 * *Files 1% similar despite different names*

```diff
@@ -6,16 +6,16 @@
 
 """
 from alembic import op
 import sqlalchemy as sa
 
 
 # revision identifiers, used by Alembic.
-revision = 'defbda3bf2b5'
-down_revision = '39e860a11b05'
+revision = "defbda3bf2b5"
+down_revision = "39e860a11b05"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
     m = sa.MetaData(op.get_bind())
     m.reflect()
```

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic/versions/fd542cebf699_drop_on_update_clauses_from_object_and_.py` & `spinedb_api-0.31.0/spinedb_api/alembic/versions/fd542cebf699_drop_on_update_clauses_from_object_and_.py`

 * *Files 17% similar despite different names*

```diff
@@ -6,36 +6,36 @@
 
 """
 from alembic import op
 from spinedb_api.helpers import naming_convention
 
 
 # revision identifiers, used by Alembic.
-revision = 'fd542cebf699'
-down_revision = '738d494a08ac'
+revision = "fd542cebf699"
+down_revision = "738d494a08ac"
 branch_labels = None
 depends_on = None
 
 
 def upgrade():
     with op.batch_alter_table("object", naming_convention=naming_convention) as batch_op:
-        batch_op.drop_constraint('fk_object_entity_id_entity', type_='foreignkey')
+        batch_op.drop_constraint("fk_object_entity_id_entity", type_="foreignkey")
         batch_op.create_foreign_key(
-            op.f('fk_object_entity_id_entity'),
-            'entity',
-            ['entity_id', 'type_id'],
-            ['id', 'type_id'],
+            op.f("fk_object_entity_id_entity"),
+            "entity",
+            ["entity_id", "type_id"],
+            ["id", "type_id"],
             ondelete="CASCADE",
         )
     with op.batch_alter_table("relationship", naming_convention=naming_convention) as batch_op:
-        batch_op.drop_constraint('fk_relationship_entity_id_entity', type_='foreignkey')
+        batch_op.drop_constraint("fk_relationship_entity_id_entity", type_="foreignkey")
         batch_op.create_foreign_key(
-            op.f('fk_relationship_entity_id_entity'),
-            'entity',
-            ['entity_id', 'type_id'],
-            ['id', 'type_id'],
+            op.f("fk_relationship_entity_id_entity"),
+            "entity",
+            ["entity_id", "type_id"],
+            ["id", "type_id"],
             ondelete="CASCADE",
         )
 
 
 def downgrade():
     pass
```

### Comparing `spinedb_api-0.30.5/spinedb_api/alembic.ini` & `spinedb_api-0.31.0/spinedb_api/alembic.ini`

 * *Files identical despite different names*

### Comparing `spinedb_api-0.30.5/spinedb_api/check_functions.py` & `spinedb_api-0.31.0/spinedb_api/import_functions.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,643 +1,678 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
-# Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
-# General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
-# option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
+# Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
+# Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
+# any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
-"""Functions for checking whether inserting data into a Spine database leads
-to the violation of integrity constraints.
-
 """
+Functions for importing data into a Spine database in a standard format.
+This functionaly is equivalent to the one provided by :meth:`.DatabaseMapping.add_update_item`,
+but the syntax is a little more compact.
+"""
+from collections import defaultdict
 
-from .parameter_value import dump_db_value, from_database, ParameterValueFormatError
-from .exception import SpineIntegrityError
+from .parameter_value import to_database, fix_conflict
+from .helpers import _parse_metadata
 
-# NOTE: We parse each parameter value or default value before accepting it. Is it too much?
 
+def import_data(db_map, unparse_value=to_database, on_conflict="merge", **kwargs):
+    """Imports data into a Spine database using a standard format.
 
-def check_alternative(item, current_items):
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError("Missing alternative name.")
-    if name in current_items:
-        raise SpineIntegrityError(f"There can't be more than one alternative called '{name}'.", id=current_items[name])
-
-
-def check_scenario(item, current_items):
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError("Missing scenario name.")
-    if name in current_items:
-        raise SpineIntegrityError(f"There can't be more than one scenario called '{name}'.", id=current_items[name])
-
-
-def check_scenario_alternative(item, ids_by_alt_id, ids_by_rank, scenario_names, alternative_names):
-    """
-    Checks if given scenario alternative violates a database's integrity.
-
-    Args:
-        item (dict: a scenario alternative item for checking; must contain the following fields:
-            - "scenario_id": scenario's id
-            - "alternative_id": alternative's id
-            - "rank": alternative's rank within the scenario
-        ids_by_alt_id (dict): a mapping from (scenario id, alternative id) tuples to scenario_alternative ids
-            already in the database
-        ids_by_rank (dict): a mapping from (scenario id, rank) tuples to scenario_alternative ranks already in the database
-        scenario_names (Iterable): the names of existing scenarios in the database keyed by id
-        alternative_names (Iterable): the names of existing alternatives in the database keyed by id
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        scen_id = item["scenario_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing scenario identifier.")
-    try:
-        alt_id = item["alternative_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing alternative identifier.")
-    try:
-        rank = item["rank"]
-    except KeyError:
-        raise SpineIntegrityError("Missing scenario alternative rank.")
-    scen_name = scenario_names.get(scen_id)
-    if scen_name is None:
-        raise SpineIntegrityError(f"Scenario with id {scen_id} does not have a name.")
-    alt_name = alternative_names.get(alt_id)
-    if alt_name is None:
-        raise SpineIntegrityError(f"Alternative with id {alt_id} does not have a name.")
-    dup_id = ids_by_alt_id.get((scen_id, alt_id))
-    if dup_id is not None:
-        raise SpineIntegrityError(f"Alternative {alt_name} already exists in scenario {scen_name}.", id=dup_id)
-    dup_id = ids_by_rank.get((scen_id, rank))
-    if dup_id is not None:
-        raise SpineIntegrityError(
-            f"Rank {rank} already exists in scenario {scen_name}. Cannot give the same rank for "
-            f"alternative {alt_name}.",
-            id=dup_id,
-        )
+    Example::
+
+            entity_classes = [
+                ('example_class', ()), ('other_class', ()), ('multi_d_class', ('example_class', 'other_class'))
+            ]
+            alternatives = [('example_alternative', 'An example')]
+            scenarios = [('example_scenario', 'An example')]
+            scenario_alternatives = [
+                ('example_scenario', 'example_alternative'), ('example_scenario', 'Base', 'example_alternative')
+            ]
+            parameter_value_lists = [("example_list", "value1"), ("example_list", "value2")]
+            parameter_definitions = [('example_class', 'example_parameter'), ('multi_d_class', 'other_parameter')]
+            entities = [
+                ('example_class', 'example_entity'),
+                ('example_class', 'example_group'),
+                ('example_class', 'example_member'),
+                ('other_class', 'other_entity'),
+                ('multi_d_class', ('example_entity', 'other_entity')),
+            ]
+            entity_groups = [
+                ('example_class', 'example_group', 'example_member'),
+                ('example_class', 'example_group', 'example_entity'),
+            ]
+            parameter_values = [
+                ('example_object_class', 'example_entity', 'example_parameter', 3.14),
+                ('multi_d_class', ('example_entity', 'other_entity'), 'rel_parameter', 2.718),
+            ]
+            entity_alternatives = [
+                ('example_class', 'example_entity', "example_alternative", True),
+                ('example_class', 'example_entity', "example_alternative", False),
+            ]
+            import_data(
+                db_map,
+                entity_classes=entity_classes,
+                alternatives=alternatives,
+                scenarios=scenarios,
+                scenario_alternatives=scenario_alternatives,
+                parameter_value_lists=parameter_value_lists,
+                parameter_definitions=parameter_definitions,
+                entities=entities,
+                entity_groups=entity_groups,
+                parameter_values=parameter_values,
+                entity_alternatives=entity_alternatives,
+            )
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        on_conflict (str): Conflict resolution strategy for :func:`parameter_value.fix_conflict`
+        entity_classes (list(tuple(str,tuple,str,int)): tuples of
+            (name, dimension name tuple, description, display icon integer)
+        parameter_definitions (list(tuple(str,str,str,str)):
+            tuples of (class name, parameter name, default value, parameter value list name, description)
+        entities: (list(tuple(str,str or tuple(str)): tuples of (class name, entity name or element name list)
+        entity_alternatives: (list(tuple(str,str or tuple(str),str,bool): tuples of
+            (class name, entity name or element name list, alternative name, activity)
+        entity_groups (list(tuple(str,str,str))): tuples of (class name, group entity name, member entity name)
+        parameter_values (list(tuple(str,str or tuple(str),str,str|numeric,str]):
+            tuples of (class name, entity name or element name list, parameter name, value, alternative name)
+        alternatives (list(str,str)): tuples of (name, description)
+        scenarios (list(str,str)): tuples of (name, description)
+        scenario_alternatives (list(str,str,str)): tuples of
+            (scenario name, alternative name, preceeding alternative name)
+        parameter_value_lists (list(str,str|numeric)): tuples of (list name, value)
+
+    Returns:
+        int: number of items imported
+        list: errors
+    """
+    all_errors = []
+    num_imports = 0
+    for item_type, items in get_data_for_import(db_map, unparse_value=unparse_value, on_conflict=on_conflict, **kwargs):
+        if isinstance(items, tuple):
+            items, input_errors = items
+            all_errors.extend(input_errors)
+        added, updated, errors = db_map.add_update_items(item_type, *items, strict=False)
+        num_imports += len(added + updated)
+        all_errors.extend(errors)
+    return num_imports, all_errors
+
+
+def get_data_for_import(
+    db_map,
+    unparse_value=to_database,
+    on_conflict="merge",
+    entity_classes=(),
+    entities=(),
+    entity_groups=(),
+    entity_alternatives=(),  # TODO
+    parameter_definitions=(),
+    parameter_values=(),
+    parameter_value_lists=(),
+    alternatives=(),
+    scenarios=(),
+    scenario_alternatives=(),
+    metadata=(),
+    entity_metadata=(),
+    parameter_value_metadata=(),
+    superclass_subclasses=(),
+    # legacy
+    object_classes=(),
+    relationship_classes=(),
+    object_parameters=(),
+    relationship_parameters=(),
+    objects=(),
+    relationships=(),
+    object_groups=(),
+    object_parameter_values=(),
+    relationship_parameter_values=(),
+    object_metadata=(),
+    relationship_metadata=(),
+    object_parameter_value_metadata=(),
+    relationship_parameter_value_metadata=(),
+    # removed
+    tools=(),
+    features=(),
+    tool_features=(),
+    tool_feature_methods=(),
+):
+    """Yields data to import into a Spine DB.
 
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        on_conflict (str): Conflict resolution strategy for :func:`~spinedb_api.parameter_value.fix_conflict`
+        entity_classes (list(tuple(str,tuple,str,int)): tuples of
+            (name, dimension name tuple, description, display icon integer)
+        parameter_definitions (list(tuple(str,str,str,str)):
+            tuples of (class name, parameter name, default value, parameter value list name)
+        entities: (list(tuple(str,str or tuple(str)): tuples of (class name, entity name or element name list)
+        entity_alternatives: (list(tuple(str,str or tuple(str),str,bool): tuples of
+            (class name, entity name or element name list, alternative name, activity)
+        entity_groups (list(tuple(str,str,str))): tuples of (class name, group entity name, member entity name)
+        parameter_values (list(tuple(str,str or tuple(str),str,str|numeric,str]):
+            tuples of (class name, entity name or element name list, parameter name, value, alternative name)
+        alternatives (list(str,str)): tuples of (name, description)
+        scenarios (list(str,str)): tuples of (name, description)
+        scenario_alternatives (list(str,str,str)): tuples of
+            (scenario name, alternative name, preceeding alternative name)
+        parameter_value_lists (list(str,str|numeric)): tuples of (list name, value)
 
-def check_object_class(item, current_items, object_class_type):
-    """Check whether the insertion of an object class item
-    results in the violation of an integrity constraint.
-
-    Args:
-        item (dict): An object class item to be checked.
-        current_items (dict): A dictionary mapping names to ids of object classes already in the database.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the object class name. Probably a bug, please report."
+    Yields:
+        str: item type
+        tuple(list,list,list): tuple of (items to add, items to update, errors)
+    """
+    # NOTE: The order is important, because of references. E.g., we want to import alternatives before parameter_values
+    if alternatives:
+        yield ("alternative", _get_alternatives_for_import(db_map, alternatives))
+    if scenarios:
+        yield ("scenario", _get_scenarios_for_import(db_map, scenarios))
+    if scenario_alternatives:
+        yield ("scenario_alternative", _get_scenario_alternatives_for_import(db_map, scenario_alternatives))
+    if entity_classes:
+        for bucket in _get_entity_classes_for_import(db_map, entity_classes):
+            yield ("entity_class", bucket)
+    if object_classes:  # Legacy
+        yield from get_data_for_import(db_map, entity_classes=_object_classes_to_entity_classes(object_classes))
+    if relationship_classes:  # Legacy
+        yield from get_data_for_import(db_map, entity_classes=relationship_classes)
+    if superclass_subclasses:
+        yield ("superclass_subclass", _get_superclass_subclasses_for_import(db_map, superclass_subclasses))
+    if entities:
+        for bucket in _get_entities_for_import(db_map, entities):
+            yield ("entity", bucket)
+    if objects:  # Legacy
+        yield from get_data_for_import(db_map, entities=objects)
+    if relationships:  # Legacy
+        yield from get_data_for_import(db_map, entities=relationships)
+    if entity_alternatives:
+        yield ("entity_alternative", _get_entity_alternatives_for_import(db_map, entity_alternatives))
+    if entity_groups:
+        yield ("entity_group", _get_entity_groups_for_import(db_map, entity_groups))
+    if object_groups:  # Legacy
+        yield from get_data_for_import(db_map, entity_groups=object_groups)
+    if parameter_value_lists:
+        yield ("parameter_value_list", _get_parameter_value_lists_for_import(db_map, parameter_value_lists))
+        yield ("list_value", _get_list_values_for_import(db_map, parameter_value_lists, unparse_value))
+    if parameter_definitions:
+        yield (
+            "parameter_definition",
+            _get_parameter_definitions_for_import(db_map, parameter_definitions, unparse_value),
+        )
+    if object_parameters:  # Legacy
+        yield from get_data_for_import(db_map, unparse_value=unparse_value, parameter_definitions=object_parameters)
+    if relationship_parameters:  # Legacy
+        yield from get_data_for_import(
+            db_map, unparse_value=unparse_value, parameter_definitions=relationship_parameters
+        )
+    if parameter_values:
+        yield (
+            "parameter_value",
+            _get_parameter_values_for_import(db_map, parameter_values, unparse_value, on_conflict),
         )
-    if not name:
-        raise SpineIntegrityError("Object class name is an empty string and therefore not valid")
-    if "type_id" in item and item["type_id"] != object_class_type:
-        raise SpineIntegrityError(
-            f"Object class '{name}' does not have a type_id of an object class.", id=current_items[name]
+    if object_parameter_values:  # Legacy
+        yield from get_data_for_import(
+            db_map, unparse_value=unparse_value, on_conflict=on_conflict, parameter_values=object_parameter_values
         )
-    if name in current_items:
-        raise SpineIntegrityError(f"There can't be more than one object class called '{name}'.", id=current_items[name])
+    if relationship_parameter_values:  # Legacy
+        yield from get_data_for_import(
+            db_map, unparse_value=unparse_value, on_conflict=on_conflict, parameter_values=relationship_parameter_values
+        )
+    if metadata:
+        yield ("metadata", _get_metadata_for_import(db_map, metadata))
+    if entity_metadata:
+        yield ("metadata", _get_metadata_for_import(db_map, (ent_metadata[2] for ent_metadata in entity_metadata)))
+        yield ("entity_metadata", _get_entity_metadata_for_import(db_map, entity_metadata))
+    if parameter_value_metadata:
+        yield (
+            "metadata",
+            _get_metadata_for_import(db_map, (pval_metadata[3] for pval_metadata in parameter_value_metadata)),
+        )
+        yield ("parameter_value_metadata", _get_parameter_value_metadata_for_import(db_map, parameter_value_metadata))
+    if object_metadata:  # Legacy
+        yield from get_data_for_import(db_map, entity_metadata=object_metadata)
+    if relationship_metadata:  # Legacy
+        yield from get_data_for_import(db_map, entity_metadata=relationship_metadata)
+    if object_parameter_value_metadata:  # Legacy
+        yield from get_data_for_import(db_map, parameter_value_metadata=object_parameter_value_metadata)
+    if relationship_parameter_value_metadata:  # Legacy
+        yield from get_data_for_import(db_map, parameter_value_metadata=relationship_parameter_value_metadata)
 
 
-def check_object(item, current_items, object_class_ids, object_entity_type):
-    """Check whether the insertion of an object item
-    results in the violation of an integrity constraint.
+def import_superclass_subclasses(db_map, data):
+    """Imports superclass_subclasses into a Spine database using a standard format.
 
     Args:
-        item (dict): An object item to be checked.
-        current_items (dict): A dictionary mapping tuples (class_id, name) to ids of objects already in the database.
-        object_class_ids (list): A list of object class ids in the database.
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data (list(tuple(str,tuple,str,int)): tuples of (superclass name, subclass name)
 
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    Returns:
+        int: number of items imported
+        list: errors
     """
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the object name. Probably a bug, please report."
-        )
-    if not name:
-        raise SpineIntegrityError("Object name is an empty string and therefore not valid")
-    if "type_id" in item and item["type_id"] != object_entity_type:
-        raise SpineIntegrityError(f"Object '{name}' does not have entity type of and object", id=current_items[name])
-    try:
-        class_id = item["class_id"]
-    except KeyError:
-        raise SpineIntegrityError(f"Object '{name}' does not have an object class id.")
-    if class_id not in object_class_ids:
-        raise SpineIntegrityError(f"Object class id for object '{name}' not found.")
-    if (class_id, name) in current_items:
-        raise SpineIntegrityError(
-            f"There's already an object called '{name}' in the same object class.", id=current_items[class_id, name]
-        )
+    return import_data(db_map, superclass_subclasses=data)
 
 
-def check_wide_relationship_class(wide_item, current_items, object_class_ids, relationship_class_type):
-    """Check whether the insertion of a relationship class item
-    results in the violation of an integrity constraint.
-
-    Args:
-        wide_item (dict): A wide relationship class item to be checked.
-        current_items (dict): A dictionary mapping names to ids of relationship classes already in the database.
-        object_class_ids (list): A list of object class ids in the database.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        name = wide_item["name"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the relationship class name. "
-            "Probably a bug, please report."
-        )
-    if not name:
-        raise SpineIntegrityError(f"Name '{name}' is not valid")
-    try:
-        given_object_class_id_list = wide_item["object_class_id_list"]
-    except KeyError:
-        raise SpineIntegrityError(
-            f"Python KeyError: There is no dictionary keys for the object class ids of relationship class '{name}'. "
-            "Probably a bug, please report."
-        )
-    if not given_object_class_id_list:
-        raise SpineIntegrityError(f"At least one object class is needed for the relationship class '{name}'.")
-    if not all(id_ in object_class_ids for id_ in given_object_class_id_list):
-        raise SpineIntegrityError(
-            f"At least one of the object class ids of the relationship class '{name}' is not in the database."
-        )
-    if "type_id" in wide_item and wide_item["type_id"] != relationship_class_type:
-        raise SpineIntegrityError(f"Relationship class '{name}' must have correct type_id .", id=current_items[name])
-    if name in current_items:
-        raise SpineIntegrityError(
-            f"There can't be more than one relationship class with the name '{name}'.", id=current_items[name]
-        )
+def import_entity_classes(db_map, data):
+    """Imports entity classes into a Spine database using a standard format.
 
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data (list(tuple(str,tuple,str,int,bool)): tuples of
+            (name, dimension name tuple, description, display icon integer, active by default flag)
+
+    Returns:
+        int: number of items imported
+        list: errors
+    """
+    return import_data(db_map, entity_classes=data)
 
-def check_wide_relationship(
-    wide_item, current_items_by_name, current_items_by_obj_lst, relationship_classes, objects, relationship_entity_type
-):
-    """Check whether the insertion of a relationship item
-    results in the violation of an integrity constraint.
+
+def import_entities(db_map, data):
+    """Imports entities into a Spine database using a standard format.
 
     Args:
-        wide_item (dict): A wide relationship item to be checked.
-        current_items_by_name (dict): A dictionary mapping tuples (class_id, name) to ids of
-            relationships already in the database.
-        current_items_by_obj_lst (dict): A dictionary mapping tuples (class_id, object_name_list) to ids
-            of relationships already in the database.
-        relationship_classes (dict): A dictionary of wide relationship class items in the database keyed by id.
-        objects (dict): A dictionary of object items in the database keyed by id.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-
-    try:
-        name = wide_item["name"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the relationship name. Probably a bug, please report."
-        )
-    if not name:
-        raise SpineIntegrityError("Relationship name is an empty string, which is not valid")
-    try:
-        class_id = wide_item["class_id"]
-    except KeyError:
-        raise SpineIntegrityError(
-            f"Python KeyError: There is no dictionary key for the relationship class id of relationship '{name}'. "
-            "Probably a bug, please report"
-        )
-    if "type_id" in wide_item and wide_item["type_id"] != relationship_entity_type:
-        raise SpineIntegrityError(
-            f"Relationship '{name}' does not have entity type of a relationship.",
-            id=current_items_by_name[class_id, name],
-        )
-    if (class_id, name) in current_items_by_name:
-        raise SpineIntegrityError(
-            f"There's already a relationship called '{name}' in the same class.",
-            id=current_items_by_name[class_id, name],
-        )
-    try:
-        object_class_id_list = relationship_classes[class_id]["object_class_id_list"]
-    except KeyError:
-        raise SpineIntegrityError(f"There is no object class id list for relationship '{name}'")
-    try:
-        object_id_list = tuple(wide_item["object_id_list"])
-    except KeyError:
-        raise SpineIntegrityError(f"There is no object id list for relationship '{name}'")
-    try:
-        given_object_class_id_list = tuple(objects[id]["class_id"] for id in object_id_list)
-    except KeyError:
-        raise SpineIntegrityError(f"Some of the objects in relationship '{name}' are invalid.")
-    if given_object_class_id_list != object_class_id_list:
-        object_name_list = [objects[id]["name"] for id in object_id_list]
-        relationship_class_name = relationship_classes[class_id]["name"]
-        raise SpineIntegrityError(
-            f"Incorrect objects '{object_name_list}' for relationship class '{relationship_class_name}'."
-        )
-    if (class_id, object_id_list) in current_items_by_obj_lst:
-        object_name_list = [objects[id]["name"] for id in object_id_list]
-        relationship_class_name = relationship_classes[class_id]["name"]
-        raise SpineIntegrityError(
-            "There's already a relationship between objects {} in class {}.".format(
-                object_name_list, relationship_class_name
-            ),
-            id=current_items_by_obj_lst[class_id, object_id_list],
-        )
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data: (list(tuple(str,str or tuple(str)): tuples of (class name, entity name or element name list)
 
+    Returns:
+        int: number of items imported
+        list: errors
+    """
+    return import_data(db_map, entities=data)
 
-def check_entity_group(item, current_items, entities):
-    """Check whether the insertion of an entity group item
-    results in the violation of an integrity constraint.
-
-    Args:
-        item (dict): An entity group item to be checked.
-        current_items (dict): A dictionary mapping tuples (entity_id, member_id) to ids of entity groups
-            already in the database.
-        entities (dict): A dictionary mapping entity class ids, to entity ids, to entity items already in the db
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        entity_id = item["entity_id"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the entity id of entity group. "
-            "Probably a bug, please report."
-        )
-    try:
-        member_id = item["member_id"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the member id of an entity group. "
-            "Probably a bug, please report."
-        )
-    try:
-        entity_class_id = item["entity_class_id"]
-    except KeyError:
-        raise SpineIntegrityError(
-            "Python KeyError: There is no dictionary key for the entity class id of entity group. "
-            "Probably a bug, please report."
-        )
-    ents = entities.get(entity_class_id)
-    if ents is None:
-        raise SpineIntegrityError("Entity class not found for entity group.")
-    entity = ents.get(entity_id)
-    if not entity:
-        raise SpineIntegrityError("No entity id for the entity group.")
-    member = ents.get(member_id)
-    if not member:
-        raise SpineIntegrityError("Entity group has no members.")
-    if (entity_id, member_id) in current_items:
-        raise SpineIntegrityError(
-            "{0} is already a member in {1}.".format(member["name"], entity["name"]),
-            id=current_items[entity_id, member_id],
-        )
 
+def import_entity_alternatives(db_map, data):
+    """Imports entity alternatives into a Spine database using a standard format.
 
-def check_parameter_definition(item, current_items, entity_class_ids, parameter_value_lists, list_values):
-    """Check whether the insertion of a parameter definition item
-    results in the violation of an integrity constraint.
-
-    Args:
-        item (dict): A parameter definition item to be checked.
-        current_items (dict): A dictionary mapping tuples (entity_class_id, name) to ids of parameter definitions
-            already in the database.
-        entity_class_ids (Iterable): A set of entity class ids in the database.
-        parameter_value_lists (dict): A dictionary of value-lists in the database keyed by id.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    name = item.get("name")
-    if not name:
-        raise SpineIntegrityError("No name provided for a parameter definition.")
-    entity_class_id = item.get("entity_class_id")
-    if not entity_class_id:
-        raise SpineIntegrityError(f"Missing entity class id for parameter definition '{name}'.")
-    if entity_class_id not in entity_class_ids:
-        raise SpineIntegrityError(
-            f"Entity class id for parameter definition '{name}' not found in the entity class "
-            "ids of the current database."
-        )
-    if (entity_class_id, name) in current_items:
-        raise SpineIntegrityError(
-            "There's already a parameter called {0} in entity class with id {1}.".format(name, entity_class_id),
-            id=current_items[entity_class_id, name],
-        )
-    replace_default_values_with_list_references(item, parameter_value_lists, list_values)
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data: (list(tuple(str,str or tuple(str),str,bool): tuples of
+            (class name, entity name or element name list, alternative name, activity)
+
+    Returns:
+        int: number of items imported
+        list: errors
+    """
+    return import_data(db_map, entity_alternatives=data)
 
 
-def check_parameter_value(
-    item, current_items, parameter_definitions, entities, parameter_value_lists, list_values, alternatives
-):
-    """Check whether the insertion of a parameter value item results in the violation of an integrity constraint.
+def import_entity_groups(db_map, data):
+    """Imports entity groups into a Spine database using a standard format.
 
     Args:
-        item (dict): A parameter value item to be checked.
-        current_items (dict): A dictionary mapping tuples (entity_id, parameter_definition_id)
-            to ids of parameter values already in the database.
-        parameter_definitions (dict): A dictionary of parameter definition items in the database keyed by id.
-        entities (dict): A dictionary of entity items already in the database keyed by id.
-        parameter_value_lists (dict): A dictionary of value-lists in the database keyed by id.
-        list_values (dict): A dictionary of list-values in the database keyed by id.
-        alternatives (set)
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    try:
-        parameter_definition_id = item["parameter_definition_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter identifier.")
-    try:
-        parameter_definition = parameter_definitions[parameter_definition_id]
-    except KeyError:
-        raise SpineIntegrityError("Parameter not found.")
-    alt_id = item.get("alternative_id")
-    if alt_id not in alternatives:
-        raise SpineIntegrityError("Alternative not found.")
-    entity_id = item.get("entity_id")
-    if not entity_id:
-        raise SpineIntegrityError("Missing object or relationship identifier.")
-    try:
-        entity_class_id = entities[entity_id]["class_id"]
-    except KeyError:
-        raise SpineIntegrityError("Entity not found")
-    if entity_class_id != parameter_definition["entity_class_id"]:
-        entity_name = entities[entity_id]["name"]
-        parameter_name = parameter_definition["name"]
-        raise SpineIntegrityError("Incorrect entity '{}' for parameter '{}'.".format(entity_name, parameter_name))
-    if (entity_id, parameter_definition_id, alt_id) in current_items:
-        entity_name = entities[entity_id]["name"]
-        parameter_name = parameter_definition["name"]
-        raise SpineIntegrityError(
-            "The value of parameter '{}' for entity '{}' is already specified.".format(parameter_name, entity_name),
-            id=current_items[entity_id, parameter_definition_id, alt_id],
-        )
-    replace_parameter_values_with_list_references(item, parameter_definitions, parameter_value_lists, list_values)
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data (list(tuple(str,str,str))): tuples of (class name, group entity name, member entity name)
+
+    Returns:
+        int: number of items imported
+        list: errors
+    """
+    return import_data(db_map, entity_groups=data)
 
 
-def replace_default_values_with_list_references(item, parameter_value_lists, list_values):
-    parameter_value_list_id = item.get("parameter_value_list_id")
-    return _replace_values_with_list_references(
-        "parameter_definition", item, parameter_value_list_id, parameter_value_lists, list_values
-    )
+def import_parameter_definitions(db_map, data, unparse_value=to_database):
+    """Imports parameter definitions into a Spine database using a standard format.
 
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data (list(tuple(str,str,str,str)):
+            tuples of (class name, parameter name, default value, parameter value list name)
 
-def replace_parameter_values_with_list_references(item, parameter_definitions, parameter_value_lists, list_values):
-    parameter_definition_id = item["parameter_definition_id"]
-    parameter_definition = parameter_definitions[parameter_definition_id]
-    parameter_value_list_id = parameter_definition["parameter_value_list_id"]
-    return _replace_values_with_list_references(
-        "parameter_value", item, parameter_value_list_id, parameter_value_lists, list_values
-    )
+    Returns:
+        int: number of items imported
+        list: errors
+    """
+    return import_data(db_map, parameter_definitions=data, unparse_value=unparse_value)
 
 
-def _replace_values_with_list_references(item_type, item, parameter_value_list_id, parameter_value_lists, list_values):
-    if parameter_value_list_id is None:
-        return False
-    if parameter_value_list_id not in parameter_value_lists:
-        raise SpineIntegrityError("Parameter value list not found.")
-    value_id_list = parameter_value_lists[parameter_value_list_id]
-    if value_id_list is None:
-        raise SpineIntegrityError("Parameter value list is empty!")
-    value_key, type_key = {
-        "parameter_value": ("value", "type"),
-        "parameter_definition": ("default_value", "default_type"),
-    }[item_type]
-    value = dict.get(item, value_key)
-    value_type = dict.get(item, type_key)
-    try:
-        parsed_value = from_database(value, value_type)
-    except ParameterValueFormatError as err:
-        raise SpineIntegrityError(f"Invalid {value_key} '{value}': {err}") from None
-    if parsed_value is None:
-        return False
-    list_value_id = next((id_ for id_ in value_id_list if list_values.get(id_) == parsed_value), None)
-    if list_value_id is None:
-        valid_values = ", ".join(f"{dump_db_value(list_values.get(id_))[0].decode('utf8')!r}" for id_ in value_id_list)
-        raise SpineIntegrityError(
-            f"Invalid {value_key} '{parsed_value}' - it should be one from the parameter value list: {valid_values}."
-        )
-    item[value_key] = str(list_value_id).encode("UTF8")
-    item[type_key] = "list_value_ref"
-    item["list_value_id"] = list_value_id
-    return True
+def import_parameter_values(db_map, data, unparse_value=to_database, on_conflict="merge"):
+    """Imports parameter values into a Spine database using a standard format.
 
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data (list(tuple(str,str or tuple(str),str,str|numeric,str]):
+            tuples of (class name, entity name or element name list, parameter name, value, alternative name)
+        on_conflict (str): Conflict resolution strategy for :func:`~spinedb_api.parameter_value.fix_conflict`
 
-def check_parameter_value_list(item, current_items):
-    """Check whether the insertion of a parameter value-list item results in the violation of an integrity constraint.
+    Returns:
+        int: number of items imported
+        list: errors
+    """
+    return import_data(db_map, parameter_values=data, unparse_value=unparse_value, on_conflict=on_conflict)
+
+
+def import_alternatives(db_map, data):
+    """Imports alternatives into a Spine database using a standard format.
 
     Args:
-        item (dict): A parameter value-list item to be checked.
-        current_items (dict): A dictionary mapping names to ids of parameter value-lists already in the database.
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data (list(str,str)): tuples of (name, description)
 
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
+    Returns:
+        int: number of items imported
+        list: errors
     """
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter value list name.")
-    if name in current_items:
-        raise SpineIntegrityError(
-            "There can't be more than one parameter value_list called '{}'.".format(name), id=current_items[name]
-        )
+    return import_data(db_map, alternatives=data)
 
 
-def check_list_value(item, list_names_by_id, list_value_ids_by_index, list_value_ids_by_value):
-    """Check whether the insertion of a list value item results in the violation of an integrity constraint.
+def import_scenarios(db_map, data):
+    """Imports scenarios into a Spine database using a standard format.
 
     Args:
-        item (dict): A list value item to be checked.
-        list_names_by_id (dict): Mapping parameter value list ids to names.
-        list_value_ids_by_index (dict): Mapping tuples (list id, index) to ids of existing list values.
-        list_value_ids_by_value (dict): Mapping tuples (list id, type, value) to ids of existing list values.
-
-    Raises:
-        SpineIntegrityError: if the insertion of the item violates an integrity constraint.
-    """
-    keys = {"parameter_value_list_id", "index", "value", "type"}
-    missing_keys = keys - item.keys()
-    if missing_keys:
-        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
-    list_id = item["parameter_value_list_id"]
-    list_name = list_names_by_id.get(list_id)
-    if list_name is None:
-        raise SpineIntegrityError("Unknown parameter value list identifier.")
-    index = item["index"]
-    type_ = item["type"]
-    value = item["value"]
-    dup_id = list_value_ids_by_index.get((list_id, index))
-    if dup_id is not None:
-        raise SpineIntegrityError(f"'{list_name}' already has the index '{index}'.", id=dup_id)
-    dup_id = list_value_ids_by_value.get((list_id, type_, value))
-    if dup_id is not None:
-        raise SpineIntegrityError(f"'{list_name}' already has the value '{from_database(value, type_)}'.", id=dup_id)
-
-
-def check_tool(item, current_items):
-    try:
-        name = item["name"]
-    except KeyError:
-        raise SpineIntegrityError("Missing tool name.")
-    if name in current_items:
-        raise SpineIntegrityError(f"There can't be more than one tool called '{name}'.", id=current_items[name])
-
-
-def check_feature(item, current_items, parameter_definitions):
-    try:
-        parameter_definition_id = item["parameter_definition_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter identifier.")
-    try:
-        parameter_value_list_id = item["parameter_value_list_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter value list identifier.")
-    try:
-        parameter_definition = parameter_definitions[parameter_definition_id]
-    except KeyError:
-        raise SpineIntegrityError("Parameter not found.")
-    if parameter_value_list_id is None:
-        raise SpineIntegrityError(f"Parameter '{parameter_definition['name']}' doesn't have a value list.")
-    if parameter_value_list_id != parameter_definition["parameter_value_list_id"]:
-        raise SpineIntegrityError("Parameter definition and value list don't match.")
-    if parameter_definition_id in current_items:
-        raise SpineIntegrityError(
-            f"There's already a feature defined for parameter '{parameter_definition['name']}'.",
-            id=current_items[parameter_definition_id],
-        )
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data (list(str, bool, str)): tuples of (name, <unused_bool>, description)
+
+    Returns:
+        int: number of items imported
+        list: errors
+    """
+    return import_data(db_map, scenarios=data)
+
+
+def import_scenario_alternatives(db_map, data):
+    """Imports scenario alternatives into a Spine database using a standard format.
 
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data (list(str,str,str)): tuples of (scenario name, alternative name, preceeding alternative name)
+
+    Returns:
+        int: number of items imported
+        list: errors
+    """
+    return import_data(db_map, scenario_alternatives=data)
+
+
+def import_parameter_value_lists(db_map, data, unparse_value=to_database):
+    """Imports parameter value lists into a Spine database using a standard format.
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data (list(str,str|numeric)): tuples of (list name, value)
 
-def check_tool_feature(item, current_items, tools, features):
-    try:
-        tool_id = item["tool_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing tool identifier.")
-    try:
-        feature_id = item["feature_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing feature identifier.")
-    try:
-        parameter_value_list_id = item["parameter_value_list_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter value list identifier.")
-    try:
-        tool = tools[tool_id]
-    except KeyError:
-        raise SpineIntegrityError("Tool not found.")
-    try:
-        feature = features[feature_id]
-    except KeyError:
-        raise SpineIntegrityError("Feature not found.")
-    dup_id = current_items.get((tool_id, feature_id))
-    if dup_id is not None:
-        raise SpineIntegrityError(f"Tool '{tool['name']}' already has feature '{feature['name']}'.", id=dup_id)
-    if parameter_value_list_id != feature["parameter_value_list_id"]:
-        raise SpineIntegrityError("Feature and parameter value list don't match.")
-
-
-def check_tool_feature_method(item, current_items, tool_features, parameter_value_lists):
-    try:
-        tool_feature_id = item["tool_feature_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing tool feature identifier.")
-    try:
-        parameter_value_list_id = item["parameter_value_list_id"]
-    except KeyError:
-        raise SpineIntegrityError("Missing parameter value list identifier.")
-    try:
-        method_index = item["method_index"]
-    except KeyError:
-        raise SpineIntegrityError("Missing method index.")
-    try:
-        tool_feature = tool_features[tool_feature_id]
-    except KeyError:
-        raise SpineIntegrityError("Tool feature not found.")
-    try:
-        parameter_value_list = parameter_value_lists[parameter_value_list_id]
-    except KeyError:
-        raise SpineIntegrityError("Parameter value list not found.")
-    dup_id = current_items.get((tool_feature_id, method_index))
-    if dup_id is not None:
-        raise SpineIntegrityError("Tool feature already has the given method.", id=dup_id)
-    if parameter_value_list_id != tool_feature["parameter_value_list_id"]:
-        raise SpineIntegrityError("Feature and parameter value list don't match.")
-    if method_index not in parameter_value_list["value_index_list"]:
-        raise SpineIntegrityError("Invalid method for tool feature.")
-
-
-def check_metadata(item, metadata):
-    """Check whether the entity metadata item violates an integrity constraint.
-
-    Args:
-        item (dict): An entity metadata item to be checked.
-        metadata (dict): Mapping from metadata name and value to metadata id.
-
-    Raises:
-        SpineIntegrityError: if the item violates an integrity constraint.
-    """
-    keys = {"name", "value"}
-    missing_keys = keys - item.keys()
-    if missing_keys:
-        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
-
-
-def check_entity_metadata(item, entities, metadata):
-    """Check whether the entity metadata item violates an integrity constraint.
-
-    Args:
-        item (dict): An entity metadata item to be checked.
-        entities (set of int): Available entity ids.
-        metadata (set of int): Available metadata ids.
-
-    Raises:
-        SpineIntegrityError: if the item violates an integrity constraint.
-    """
-    keys = {"entity_id", "metadata_id"}
-    missing_keys = keys - item.keys()
-    if missing_keys:
-        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
-    if item["entity_id"] not in entities:
-        raise SpineIntegrityError("Unknown entity identifier.")
-    if item["metadata_id"] not in metadata:
-        raise SpineIntegrityError("Unknown metadata identifier.")
-
-
-def check_parameter_value_metadata(item, values, metadata):
-    """Check whether the parameter value metadata item violates an integrity constraint.
-
-    Args:
-        item (dict): An entity metadata item to be checked.
-        values (set of int): Available parameter value ids.
-        metadata (set of int): Available metadata ids.
-
-    Raises:
-        SpineIntegrityError: if the item violates an integrity constraint.
-    """
-    keys = {"parameter_value_id", "metadata_id"}
-    missing_keys = keys - item.keys()
-    if missing_keys:
-        raise SpineIntegrityError(f"Missing keys: {', '.join(missing_keys)}.")
-    if item["parameter_value_id"] not in values:
-        raise SpineIntegrityError("Unknown parameter value identifier.")
-    if item["metadata_id"] not in metadata:
-        raise SpineIntegrityError("Unknown metadata identifier.")
+    Returns:
+        int: number of items imported
+        list: errors
+    """
+    return import_data(db_map, parameter_value_lists=data, unparse_value=unparse_value)
+
+
+def import_metadata(db_map, data):
+    """Imports metadata into a Spine database using a standard format.
+
+    Args:
+        db_map (spinedb_api.DiffDatabaseMapping): database mapping
+        data (list(tuple(str,str))): tuples of (entry name, value)
+
+    Returns:
+        int: number of items imported
+        list: errors
+    """
+    return import_data(db_map, metadata=data)
+
+
+def import_object_classes(db_map, data):
+    return import_data(db_map, object_classes=data)
+
+
+def import_relationship_classes(db_map, data):
+    return import_data(db_map, relationship_classes=data)
+
+
+def import_objects(db_map, data):
+    return import_data(db_map, objects=data)
+
+
+def import_object_groups(db_map, data):
+    return import_data(db_map, object_groups=data)
+
+
+def import_relationships(db_map, data):
+    return import_data(db_map, relationships=data)
+
+
+def import_object_parameters(db_map, data, unparse_value=to_database):
+    return import_data(db_map, object_parameters=data, unparse_value=unparse_value)
+
+
+def import_relationship_parameters(db_map, data, unparse_value=to_database):
+    return import_data(db_map, relationship_parameters=data, unparse_value=unparse_value)
+
+
+def import_object_parameter_values(db_map, data, unparse_value=to_database, on_conflict="merge"):
+    return import_data(db_map, object_parameter_values=data, unparse_value=unparse_value, on_conflict=on_conflict)
+
+
+def import_relationship_parameter_values(db_map, data, unparse_value=to_database, on_conflict="merge"):
+    return import_data(db_map, relationship_parameter_values=data, unparse_value=unparse_value, on_conflict=on_conflict)
+
+
+def import_object_metadata(db_map, data):
+    return import_data(db_map, object_metadata=data)
+
+
+def import_relationship_metadata(db_map, data):
+    return import_data(db_map, relationship_metadata=data)
+
+
+def import_object_parameter_value_metadata(db_map, data):
+    return import_data(db_map, object_parameter_value_metadata=data)
+
+
+def import_relationship_parameter_value_metadata(db_map, data):
+    return import_data(db_map, relationship_parameter_value_metadata=data)
+
+
+def _get_entity_classes_for_import(db_map, data):
+    dim_name_list_by_name = {}
+    items = []
+    key = ("name", "dimension_name_list", "description", "display_icon", "active_by_default")
+    for x in data:
+        if isinstance(x, str):
+            x = x, ()
+        name, *optionals = x
+        dim_name_list = optionals.pop(0) if optionals else ()
+        item = dict(zip(key, (name, dim_name_list, *optionals)))
+        items.append(item)
+        dim_name_list_by_name[name] = dim_name_list
+
+    def _ref_count(name):
+        dim_name_list = dim_name_list_by_name.get(name, ())
+        return len(dim_name_list) + sum((_ref_count(dim_name) for dim_name in dim_name_list), start=0)
+
+    items_by_ref_count = {}
+    for item in items:
+        items_by_ref_count.setdefault(_ref_count(item["name"]), []).append(item)
+    return (items_by_ref_count[ref_count] for ref_count in sorted(items_by_ref_count))
+
+
+def _get_superclass_subclasses_for_import(db_map, data):
+    key = ("superclass_name", "subclass_name")
+    return (dict(zip(key, x)) for x in data)
+
+
+def _get_entities_for_import(db_map, data):
+    items_by_el_count = {}
+    key = ("entity_class_name", "entity_byname", "description")
+    for class_name, name_or_el_name_list, *optionals in data:
+        if isinstance(name_or_el_name_list, (list, tuple)):
+            el_count = len(name_or_el_name_list)
+            byname = name_or_el_name_list
+        else:
+            el_count = 0
+            byname = (name_or_el_name_list,)
+        item = dict(zip(key, (class_name, byname, *optionals)))
+        items_by_el_count.setdefault(el_count, []).append(item)
+    return (items_by_el_count[el_count] for el_count in sorted(items_by_el_count))
+
+
+def _get_entity_alternatives_for_import(db_map, data):
+    for class_name, entity_name_or_element_name_list, alternative, active in data:
+        is_zero_dim = isinstance(entity_name_or_element_name_list, str)
+        entity_byname = (entity_name_or_element_name_list,) if is_zero_dim else entity_name_or_element_name_list
+        key = ("entity_class_name", "entity_byname", "alternative_name", "active")
+        yield dict(zip(key, (class_name, entity_byname, alternative, active)))
+
+
+def _get_entity_groups_for_import(db_map, data):
+    key = ("entity_class_name", "group_name", "member_name")
+    return (dict(zip(key, x)) for x in data)
+
+
+def _get_parameter_definitions_for_import(db_map, data, unparse_value):
+    key = ("entity_class_name", "name", "default_value", "default_type", "parameter_value_list_name", "description")
+    for class_name, parameter_name, *optionals in data:
+        if not optionals:
+            yield dict(zip(key, (class_name, parameter_name)))
+            continue
+        value = optionals.pop(0)
+        value, type_ = unparse_value(value)
+        yield dict(zip(key, (class_name, parameter_name, value, type_, *optionals)))
+
+
+def _get_parameter_values_for_import(db_map, data, unparse_value, on_conflict):
+    seen = set()
+    errors = []
+    items = []
+    key = ("entity_class_name", "entity_byname", "parameter_definition_name", "alternative_name", "value", "type")
+    for class_name, entity_byname, parameter_name, value, *optionals in data:
+        if isinstance(entity_byname, str):
+            entity_byname = (entity_byname,)
+        else:
+            entity_byname = tuple(entity_byname)
+        alternative_name = optionals[0] if optionals else db_map.get_import_alternative_name()
+        unique_values = (class_name, entity_byname, parameter_name, alternative_name)
+        if unique_values in seen:
+            dupe = dict(zip(key, unique_values))
+            errors.append(
+                f"attempting to import more than one parameter_value with {dupe} - only first will be considered"
+            )
+            continue
+        seen.add(unique_values)
+        value, type_ = unparse_value(value)
+        item = dict(zip(key, unique_values + (None, None)))
+        pv = db_map.mapped_table("parameter_value").find_item(item)
+        if pv:
+            value, type_ = fix_conflict((value, type_), (pv["value"], pv["type"]), on_conflict)
+        item.update({"value": value, "type": type_})
+        items.append(item)
+    return items, errors
+
+
+def _get_alternatives_for_import(db_map, data):
+    key = ("name", "description")
+    return ({"name": x} if isinstance(x, str) else dict(zip(key, x)) for x in data)
+
+
+def _get_scenarios_for_import(db_map, data):
+    key = ("name", "active", "description")
+    return ({"name": x} if isinstance(x, str) else dict(zip(key, x)) for x in data)
+
+
+def _get_scenario_alternatives_for_import(db_map, data):
+    # FIXME: maybe when updating, we only want to match by (scen_name, alt_name) and not by (scen_name, rank)
+    alt_name_list_by_scen_name = {}
+    succ_by_pred_by_scen_name = defaultdict(dict)
+    for scen_name, predecessor, *optionals in data:
+        successor = optionals[0] if optionals else None
+        succ_by_pred_by_scen_name[scen_name][predecessor] = successor
+    for scen_name, succ_by_pred in succ_by_pred_by_scen_name.items():
+        scen = db_map.mapped_table("scenario").find_item({"name": scen_name})
+        alternative_name_list = alt_name_list_by_scen_name[scen_name] = scen.get("alternative_name_list", [])
+        alternative_name_list.append(None)  # So alternatives where successor is None find their place at the tail
+        while succ_by_pred:
+            some_added = False
+            for pred, succ in list(succ_by_pred.items()):
+                if succ in alternative_name_list:
+                    if pred in alternative_name_list:
+                        alternative_name_list.remove(pred)
+                    i = alternative_name_list.index(succ)
+                    alternative_name_list.insert(i, pred)
+                    del succ_by_pred[pred]
+                    some_added = True
+            if not some_added:
+                break
+        alternative_name_list.pop(-1)  # Remove the None
+    items = (
+        {"scenario_name": scen_name, "alternative_name": alt_name, "rank": k + 1}
+        for scen_name, alternative_name_list in alt_name_list_by_scen_name.items()
+        for k, alt_name in enumerate(alternative_name_list)
+    )
+    errors = (
+        f"can't insert alternative '{pred}' before '{succ}' because the latter is not in scenario '{scen_name}'"
+        for scen, succ_by_pred in succ_by_pred_by_scen_name.items()
+        for pred, succ in succ_by_pred.items()
+    )
+    return items, errors
+
+
+def _get_parameter_value_lists_for_import(db_map, data):
+    return ({"name": x} for x in {x[0]: None for x in data})
+
+
+def _get_list_values_for_import(db_map, data, unparse_value):
+    index_by_list_name = {}
+    for list_name, value in data:
+        value, type_ = unparse_value(value)
+        index = index_by_list_name.get(list_name)
+        if index is None:
+            current_list = db_map.mapped_table("parameter_value_list").find_item({"name": list_name})
+            list_value_idx_by_val_typ = {
+                (x["value"], x["type"]): x["index"]
+                for x in db_map.mapped_table("list_value").valid_values()
+                if x["parameter_value_list_id"] == current_list["id"]
+            }
+            if (value, type_) in list_value_idx_by_val_typ:
+                continue
+            index = max((idx for idx in list_value_idx_by_val_typ.values()), default=-1)
+        index += 1
+        index_by_list_name[list_name] = index
+        yield {"parameter_value_list_name": list_name, "value": value, "type": type_, "index": index}
+
+
+def _get_metadata_for_import(db_map, data):
+    for metadata in data:
+        for name, value in _parse_metadata(metadata):
+            yield {"name": name, "value": value}
+
+
+def _get_entity_metadata_for_import(db_map, data):
+    key = ("entity_class_name", "entity_byname", "metadata_name", "metadata_value")
+    for class_name, entity_byname, metadata in data:
+        if isinstance(entity_byname, str):
+            entity_byname = (entity_byname,)
+        for name, value in _parse_metadata(metadata):
+            yield dict(zip(key, (class_name, entity_byname, name, value)))
+
+
+def _get_parameter_value_metadata_for_import(db_map, data):
+    key = (
+        "entity_class_name",
+        "entity_byname",
+        "parameter_definition_name",
+        "metadata_name",
+        "metadata_value",
+        "alternative_name",
+    )
+    for class_name, entity_byname, parameter_name, metadata, *optionals in data:
+        if isinstance(entity_byname, str):
+            entity_byname = (entity_byname,)
+        alternative_name = optionals[0] if optionals else db_map.get_import_alternative_name()
+        for name, value in _parse_metadata(metadata):
+            yield dict(zip(key, (class_name, entity_byname, parameter_name, name, value, alternative_name)))
+
+
+# Legacy
+def _object_classes_to_entity_classes(data):
+    for x in data:
+        if isinstance(x, str):
+            yield x, ()
+        else:
+            name, *optionals = x
+            yield name, (), *optionals
```

### Comparing `spinedb_api-0.30.5/spinedb_api/db_mapping.py` & `spinedb_api-0.31.0/spinedb_api/exception.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,48 +1,82 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
 """
-Provides :class:`.DatabaseMapping`.
-
+Spine DB API exceptions.
 """
 
-from .db_mapping_query_mixin import DatabaseMappingQueryMixin
-from .db_mapping_base import DatabaseMappingBase
-from .db_mapping_add_mixin import DatabaseMappingAddMixin
-from .db_mapping_check_mixin import DatabaseMappingCheckMixin
-from .db_mapping_update_mixin import DatabaseMappingUpdateMixin
-from .db_mapping_remove_mixin import DatabaseMappingRemoveMixin
-from .db_mapping_commit_mixin import DatabaseMappingCommitMixin
-from .filters.tools import apply_filter_stack, load_filters
-
-
-class DatabaseMapping(
-    DatabaseMappingQueryMixin,
-    DatabaseMappingCheckMixin,
-    DatabaseMappingAddMixin,
-    DatabaseMappingUpdateMixin,
-    DatabaseMappingRemoveMixin,
-    DatabaseMappingCommitMixin,
-    DatabaseMappingBase,
-):
-    """A basic read-write database mapping.
-
-    :param str db_url: A database URL in RFC-1738 format pointing to the database to be mapped.
-    :param str username: A user name. If ``None``, it gets replaced by the string ``"anon"``.
-    :param bool upgrade: Whether or not the db at the given URL should be upgraded to the most recent version.
+
+class SpineDBAPIError(Exception):
+    """Basic exception for errors raised by the API."""
+
+    def __init__(self, msg=None):
+        super().__init__(msg)
+        self.msg = msg
+
+    def __str__(self):
+        return self.msg
+
+
+class SpineIntegrityError(SpineDBAPIError):
+    """Database integrity error while inserting/updating records.
+
+    Attributes:
+        msg (str): the message to be displayed
+        id (int): the id the instance that caused a unique violation
     """
 
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._init_type_attributes()
-        if self._filter_configs is not None:
-            stack = load_filters(self._filter_configs)
-            apply_filter_stack(self, stack)
+    def __init__(self, msg=None, id=None):
+        super().__init__(msg)
+        self.id = id
+
+
+class SpineDBVersionError(SpineDBAPIError):
+    """Database version error."""
+
+    def __init__(self, url=None, current=None, expected=None, upgrade_available=True):
+        super().__init__(msg="The database at '{}' is not the expected version.".format(url))
+        self.url = url
+        self.current = current
+        self.expected = expected
+        self.upgrade_available = upgrade_available
+
+
+class ParameterValueFormatError(SpineDBAPIError):
+    """
+    Failure in encoding/decoding a parameter value.
+
+    Attributes:
+        msg (str): an error message
+    """
+
+    def __init__(self, msg):
+        super().__init__(msg)
+
+
+class InvalidMapping(SpineDBAPIError):
+    """
+    Failure in import/export mapping.
+    """
+
+    def __init__(self, msg):
+        super().__init__(msg)
+
+
+class InvalidMappingComponent(InvalidMapping):
+    def __init__(self, msg, rank=None, key=None):
+        super().__init__(msg)
+        self.rank = rank
+        self.key = key
+
+
+class ConnectorError(SpineDBAPIError):
+    """Failure in import/export connector."""
```

### Comparing `spinedb_api-0.30.5/spinedb_api/db_mapping_check_mixin.py` & `spinedb_api-0.31.0/spinedb_api/db_mapping_base.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,896 +1,1325 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
-"""Provides :class:`.DatabaseMappingCheckMixin`.
+from multiprocessing import RLock
+from enum import Enum, unique, auto
+from difflib import SequenceMatcher
+from .temp_id import TempId, resolve
+from .exception import SpineDBAPIError
+from .helpers import Asterisk
+
+# TODO: Implement MappedItem.pop() to do lookup?
+
+
+@unique
+class Status(Enum):
+    """Mapped item status."""
+
+    committed = auto()
+    to_add = auto()
+    to_update = auto()
+    to_remove = auto()
+    added_and_removed = auto()
+    compromised = auto()
+
+
+class DatabaseMappingBase:
+    """An in-memory mapping of a DB, mapping item types (table names), to numeric ids, to items.
+
+    This class is not meant to be used directly. Instead, you should subclass it to fit your particular DB schema.
+
+    When subclassing, you need to implement :meth:`item_types`, :meth:`item_factory`, :meth:`_make_sq`,
+    and :meth:`_query_commit_count`.
+    """
+
+    def __init__(self):
+        self.closed = False
+        self._mapped_tables = {}
+        self._fetched = {}
+        self._locks = {}
+        self._commit_count = None
+        item_types = self.item_types()
+        self._sorted_item_types = []
+        while item_types:
+            item_type = item_types.pop(0)
+            if self.item_factory(item_type).ref_types() & set(item_types):
+                item_types.append(item_type)
+            else:
+                self._sorted_item_types.append(item_type)
 
-"""
-# TODO: Review docstrings, they are almost good
+    @staticmethod
+    def item_types():
+        """Returns a list of public item types from the DB mapping schema (equivalent to the table names).
 
-from contextlib import contextmanager
-from itertools import chain
-from .exception import SpineIntegrityError
-from .check_functions import (
-    check_alternative,
-    check_scenario,
-    check_scenario_alternative,
-    check_object_class,
-    check_object,
-    check_wide_relationship_class,
-    check_wide_relationship,
-    check_entity_group,
-    check_parameter_definition,
-    check_parameter_value,
-    check_parameter_value_list,
-    check_list_value,
-    check_feature,
-    check_tool,
-    check_tool_feature,
-    check_tool_feature_method,
-    check_entity_metadata,
-    check_metadata,
-    check_parameter_value_metadata,
-)
-from .parameter_value import from_database
-
-
-# NOTE: To check for an update we remove the current instance from our lookup dictionary,
-# check for an insert of the updated instance,
-# and finally reinsert the instance to the dictionary
-class DatabaseMappingCheckMixin:
-    """Provides methods to check whether insert and update operations violate Spine db integrity constraints."""
+        :meta private:
 
-    def check_items(self, tablename, *items, for_update=False, strict=False, cache=None):
-        return {
-            "alternative": self.check_alternatives,
-            "scenario": self.check_scenarios,
-            "scenario_alternative": self.check_scenario_alternatives,
-            "object": self.check_objects,
-            "object_class": self.check_object_classes,
-            "relationship_class": self.check_wide_relationship_classes,
-            "relationship": self.check_wide_relationships,
-            "entity_group": self.check_entity_groups,
-            "parameter_definition": self.check_parameter_definitions,
-            "parameter_value": self.check_parameter_values,
-            "parameter_value_list": self.check_parameter_value_lists,
-            "list_value": self.check_list_values,
-            "feature": self.check_features,
-            "tool": self.check_tools,
-            "tool_feature": self.check_tool_features,
-            "tool_feature_method": self.check_tool_feature_methods,
-            "metadata": self.check_metadata,
-            "entity_metadata": self.check_entity_metadata,
-            "parameter_value_metadata": self.check_parameter_value_metadata,
-        }[tablename](*items, for_update=for_update, strict=strict, cache=cache)
-
-    def check_features(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether features passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"feature"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        feature_ids = {x.parameter_definition_id: x.id for x in cache.get("feature", {}).values()}
-        parameter_definitions = {
-            x.id: {
-                "name": x.parameter_name,
-                "entity_class_id": x.entity_class_id,
-                "parameter_value_list_id": x.value_list_id,
-            }
-            for x in cache.get("parameter_definition", {}).values()
-        }
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "feature", item, {("parameter_definition_id",): feature_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_feature(item, feature_ids, parameter_definitions)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_tools(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether tools passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"tool"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        tool_ids = {x.name: x.id for x in cache.get("tool", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "tool", item, {("name",): tool_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_tool(item, tool_ids)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_tool_features(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether tool features passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"tool_feature"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        tool_feature_ids = {(x.tool_id, x.feature_id): x.id for x in cache.get("tool_feature", {}).values()}
-        tools = {x.id: x._asdict() for x in cache.get("tool", {}).values()}
-        features = {
-            x.id: {
-                "name": x.entity_class_name + "/" + x.parameter_definition_name,
-                "parameter_value_list_id": x.parameter_value_list_id,
-            }
-            for x in cache.get("feature", {}).values()
-        }
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "tool_feature",
-                    item,
-                    {("tool_id", "feature_id"): tool_feature_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_tool_feature(item, tool_feature_ids, tools, features)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_tool_feature_methods(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether tool feature methods passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"tool_feature_method"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        tool_feature_method_ids = {
-            (x.tool_feature_id, x.method_index): x.id for x in cache.get("tool_feature_method", {}).values()
-        }
-        tool_features = {x.id: x._asdict() for x in cache.get("tool_feature", {}).values()}
-        parameter_value_lists = {
-            x.id: {"name": x.name, "value_index_list": x.value_index_list}
-            for x in cache.get("parameter_value_list", {}).values()
-        }
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "tool_feature_method",
-                    item,
-                    {("tool_feature_id", "method_index"): tool_feature_method_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_tool_feature_method(item, tool_feature_method_ids, tool_features, parameter_value_lists)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_alternatives(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether alternatives passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"alternative"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        alternative_ids = {x.name: x.id for x in cache.get("alternative", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "alternative", item, {("name",): alternative_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_alternative(item, alternative_ids)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_scenarios(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether scenarios passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"scenario"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        scenario_ids = {x.name: x.id for x in cache.get("scenario", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "scenario", item, {("name",): scenario_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_scenario(item, scenario_ids)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_scenario_alternatives(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether scenario alternatives passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"scenario_alternative"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        ids_by_alt_id = {}
-        ids_by_rank = {}
-        for item in cache.get("scenario_alternative", {}).values():
-            ids_by_alt_id[item.scenario_id, item.alternative_id] = item.id
-            ids_by_rank[item.scenario_id, item.rank] = item.id
-        scenario_names = {s.id: s.name for s in cache.get("scenario", {}).values()}
-        alternative_names = {s.id: s.name for s in cache.get("alternative", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "scenario_alternative",
-                    item,
-                    {("scenario_id", "alternative_id"): ids_by_alt_id, ("scenario_id", "rank"): ids_by_rank},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_scenario_alternative(item, ids_by_alt_id, ids_by_rank, scenario_names, alternative_names)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_object_classes(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether object classes passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"object_class"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        object_class_ids = {x.name: x.id for x in cache.get("object_class", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "object_class", item, {("name",): object_class_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_object_class(item, object_class_ids, self.object_class_type)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_objects(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether objects passed as argument respect integrity constraints.
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"object"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        object_ids = {(x.class_id, x.name): x.id for x in cache.get("object", {}).values()}
-        object_class_ids = [x.id for x in cache.get("object_class", {}).values()]
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "object", item, {("class_id", "name"): object_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_object(item, object_ids, object_class_ids, self.object_entity_type)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_wide_relationship_classes(self, *wide_items, for_update=False, strict=False, cache=None):
-        """Check whether relationship classes passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"relationship_class"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_wide_items = list()
-        relationship_class_ids = {x.name: x.id for x in cache.get("relationship_class", {}).values()}
-        object_class_ids = [x.id for x in cache.get("object_class", {}).values()]
-        for wide_item in wide_items:
-            try:
-                with self._manage_stocks(
-                    "relationship_class",
-                    wide_item,
-                    {("name",): relationship_class_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as wide_item:
-                    check_wide_relationship_class(
-                        wide_item, relationship_class_ids, object_class_ids, self.relationship_class_type
-                    )
-                    checked_wide_items.append(wide_item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_wide_items, intgr_error_log
-
-    def check_wide_relationships(self, *wide_items, for_update=False, strict=False, cache=None):
-        """Check whether relationships passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"relationship"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_wide_items = list()
-        relationship_ids_by_name = {(x.class_id, x.name): x.id for x in cache.get("relationship", {}).values()}
-        relationship_ids_by_obj_lst = {
-            (x.class_id, x.object_id_list): x.id for x in cache.get("relationship", {}).values()
-        }
-        relationship_classes = {
-            x.id: {"object_class_id_list": x.object_class_id_list, "name": x.name}
-            for x in cache.get("relationship_class", {}).values()
-        }
-        objects = {x.id: {"class_id": x.class_id, "name": x.name} for x in cache.get("object", {}).values()}
-        for wide_item in wide_items:
-            try:
-                with self._manage_stocks(
-                    "relationship",
-                    wide_item,
-                    {
-                        ("class_id", "name"): relationship_ids_by_name,
-                        ("class_id", "object_id_list"): relationship_ids_by_obj_lst,
-                    },
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as wide_item:
-                    check_wide_relationship(
-                        wide_item,
-                        relationship_ids_by_name,
-                        relationship_ids_by_obj_lst,
-                        relationship_classes,
-                        objects,
-                        self.relationship_entity_type,
-                    )
-                    checked_wide_items.append(wide_item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_wide_items, intgr_error_log
-
-    def check_entity_groups(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether entity groups passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"entity_group"}, include_ancestors=True)
-        intgr_error_log = list()
-        checked_items = list()
-        current_ids = {(x.group_id, x.member_id): x.id for x in cache.get("entity_group", {}).values()}
-        entities = {}
-        for entity in chain(cache.get("object", {}).values(), cache.get("relationship", {}).values()):
-            entities.setdefault(entity.class_id, dict())[entity.id] = entity._asdict()
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "entity_group", item, {("entity_id", "member_id"): current_ids}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_entity_group(item, current_ids, entities)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_parameter_definitions(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether parameter definitions passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns:
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"parameter_definition", "parameter_value"}, include_ancestors=True)
-        parameter_definition_ids_with_values = {
-            value.parameter_id for value in cache.get("parameter_value", {}).values()
-        }
-        intgr_error_log = []
-        checked_items = list()
-        parameter_definition_ids = {
-            (x.entity_class_id, x.parameter_name): x.id for x in cache.get("parameter_definition", {}).values()
-        }
-        object_class_ids = {x.id for x in cache.get("object_class", {}).values()}
-        relationship_class_ids = {x.id for x in cache.get("relationship_class", {}).values()}
-        entity_class_ids = object_class_ids | relationship_class_ids
-        parameter_value_lists = {x.id: x.value_id_list for x in cache.get("parameter_value_list", {}).values()}
-        list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
-        for item in items:
-            object_class_id = item.get("object_class_id")
-            relationship_class_id = item.get("relationship_class_id")
-            if object_class_id and relationship_class_id:
-                e = SpineIntegrityError("Can't associate a parameter to both an object and a relationship class.")
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
+        Returns:
+            list(str)
+        """
+        raise NotImplementedError()
+
+    @staticmethod
+    def all_item_types():
+        """Returns a list of all item types from the DB mapping schema (equivalent to the table names).
+
+        :meta private:
+
+        Returns:
+            list(str)
+        """
+        raise NotImplementedError()
+
+    @staticmethod
+    def item_factory(item_type):
+        """Returns a subclass of :class:`.MappedItemBase` to make items of given type.
+
+        :meta private:
+
+        Args:
+            item_type (str)
+
+        Returns:
+            function
+        """
+        raise NotImplementedError()
+
+    def _make_query(self, item_type, **kwargs):
+        """Returns a :class:`~spinedb_api.query.Query` object to fetch items of given type.
+
+        Args:
+            item_type (str)
+            **kwargs: query filters
+
+        Returns:
+            :class:`~spinedb_api.query.Query` or None if the mapping is closed.
+        """
+        if self.closed:
+            return None
+        sq = self._make_sq(item_type)
+        qry = self.query(sq)
+        for key, value in kwargs.items():
+            if isinstance(value, tuple):
                 continue
-            if object_class_id:
-                class_ids = object_class_ids
-            elif relationship_class_id:
-                class_ids = relationship_class_ids
-            else:
-                class_ids = entity_class_ids
-            entity_class_id = object_class_id or relationship_class_id
-            if entity_class_id is not None:
-                item["entity_class_id"] = entity_class_id
-            try:
-                if (
-                    for_update
-                    and item["id"] in parameter_definition_ids_with_values
-                    and item["parameter_value_list_id"] != cache["parameter_definition"][item["id"]].value_list_id
-                ):
-                    raise SpineIntegrityError(
-                        f"Can't change value list on parameter {item['name']} because it has parameter values."
-                    )
-                with self._manage_stocks(
-                    "parameter_definition",
-                    item,
-                    {("entity_class_id", "name"): parameter_definition_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as full_item:
-                    check_parameter_definition(
-                        full_item, parameter_definition_ids, class_ids, parameter_value_lists, list_values
+            value = resolve(value)
+            if hasattr(sq.c, key):
+                qry = qry.filter(getattr(sq.c, key) == value)
+            elif key in self.item_factory(item_type)._external_fields:
+                src_key, key = self.item_factory(item_type)._external_fields[key]
+                ref_type, ref_key = self.item_factory(item_type)._references[src_key]
+                ref_sq = self._make_sq(ref_type)
+                try:
+                    qry = qry.filter(
+                        getattr(sq.c, src_key) == getattr(ref_sq.c, ref_key), getattr(ref_sq.c, key) == value
                     )
-                    checked_items.append(full_item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_parameter_values(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether parameter values passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"parameter_value"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        parameter_value_ids = {
-            (x.entity_id, x.parameter_id, x.alternative_id): x.id for x in cache.get("parameter_value", {}).values()
-        }
-        parameter_definitions = {
-            x.id: {
-                "name": x.parameter_name,
-                "entity_class_id": x.entity_class_id,
-                "parameter_value_list_id": x.value_list_id,
-            }
-            for x in cache.get("parameter_definition", {}).values()
-        }
-        entities = {
-            x.id: {"class_id": x.class_id, "name": x.name}
-            for x in chain(cache.get("object", {}).values(), cache.get("relationship", {}).values())
-        }
-        parameter_value_lists = {x.id: x.value_id_list for x in cache.get("parameter_value_list", {}).values()}
-        list_values = {x.id: from_database(x.value, x.type) for x in cache.get("list_value", {}).values()}
-        alternatives = set(a.id for a in cache.get("alternative", {}).values())
-        for item in items:
-            entity_id = item.get("object_id") or item.get("relationship_id")
-            if entity_id is not None:
-                item["entity_id"] = entity_id
-            try:
-                with self._manage_stocks(
-                    "parameter_value",
-                    item,
-                    {("entity_id", "parameter_definition_id", "alternative_id"): parameter_value_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_parameter_value(
-                        item,
-                        parameter_value_ids,
-                        parameter_definitions,
-                        entities,
-                        parameter_value_lists,
-                        list_values,
-                        alternatives,
-                    )
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_parameter_value_lists(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether parameter value-lists passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"parameter_value_list"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        parameter_value_list_ids = {x.name: x.id for x in cache.get("parameter_value_list", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "parameter_value_list",
-                    item,
-                    {("name",): parameter_value_list_ids},
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_parameter_value_list(item, parameter_value_list_ids)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_list_values(self, *items, for_update=False, strict=False, cache=None):
-        """Check whether list values passed as argument respect integrity constraints.
-
-        Args:
-            items (Iterable): One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"list_value"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        list_value_ids_by_index = {
-            (x.parameter_value_list_id, x.index): x.id for x in cache.get("list_value", {}).values()
-        }
-        list_value_ids_by_value = {
-            (x.parameter_value_list_id, x.type, x.value): x.id for x in cache.get("list_value", {}).values()
+                except AttributeError:
+                    pass
+        return qry
+
+    def _make_sq(self, item_type):
+        """Returns a :class:`~sqlalchemy.sql.expression.Alias` object representing a subquery
+        to collect items of given type.
+
+        Args:
+            item_type (str)
+
+        Returns:
+            :class:`~sqlalchemy.sql.expression.Alias`
+        """
+        raise NotImplementedError()
+
+    def _query_commit_count(self):
+        """Returns the number of rows in the commit table in the DB.
+
+        Returns:
+            int
+        """
+        raise NotImplementedError()
+
+    def make_item(self, item_type, **item):
+        factory = self.item_factory(item_type)
+        return factory(self, item_type, **item)
+
+    def dirty_ids(self, item_type):
+        return {
+            item["id"]
+            for item in self.mapped_table(item_type).valid_values()
+            if item.status in (Status.to_add, Status.to_update)
         }
-        list_names_by_id = {x.id: x.name for x in cache.get("parameter_value_list", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "list_value",
-                    item,
-                    {
-                        ("parameter_value_list_id", "index"): list_value_ids_by_index,
-                        ("parameter_value_list_id", "type", "value"): list_value_ids_by_value,
-                    },
-                    for_update,
-                    cache,
-                    intgr_error_log,
-                ) as item:
-                    check_list_value(item, list_names_by_id, list_value_ids_by_index, list_value_ids_by_value)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_metadata(self, *items, for_update=False, strict=False, cache=None):
-        """Checks whether metadata respects integrity constraints.
-
-        Args:
-            *items: One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-            cache (dict, optional): Database cache
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"metadata"})
-        intgr_error_log = []
-        checked_items = list()
-        metadata = {(x.name, x.value): x.id for x in cache.get("metadata", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "metadata", item, {("name", "value"): metadata}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_metadata(item, metadata)
-                    if (item["name"], item["value"]) not in metadata:
-                        checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_entity_metadata(self, *items, for_update=False, strict=False, cache=None):
-        """Checks whether entity metadata respects integrity constraints.
-
-        Args:
-            *items: One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-            cache (dict, optional): Database cache
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"entity_metadata"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        entities = {x.id for x in cache.get("object", {}).values()}
-        entities |= {x.id for x in cache.get("relationship", {}).values()}
-        metadata = {x.id for x in cache.get("metadata", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks("entity_metadata", item, {}, for_update, cache, intgr_error_log) as item:
-                    check_entity_metadata(item, entities, metadata)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
-
-    def check_parameter_value_metadata(self, *items, for_update=False, strict=False, cache=None):
-        """Checks whether parameter value metadata respects integrity constraints.
-
-        Args:
-            *items: One or more Python :class:`dict` objects representing the items to be checked.
-            strict (bool): Whether or not the method should raise :exc:`~.exception.SpineIntegrityError`
-                if one of the items violates an integrity constraint.
-            cache (dict, optional): Database cache
-
-        Returns
-            list: items that passed the check.
-            list: :exc:`~.exception.SpineIntegrityError` instances corresponding to found violations.
-        """
-        if cache is None:
-            cache = self.make_cache({"parameter_value_metadata"}, include_ancestors=True)
-        intgr_error_log = []
-        checked_items = list()
-        values = {x.id for x in cache.get("parameter_value", {}).values()}
-        metadata = {x.id for x in cache.get("metadata", {}).values()}
-        for item in items:
-            try:
-                with self._manage_stocks(
-                    "parameter_value_metadata", item, {}, for_update, cache, intgr_error_log
-                ) as item:
-                    check_parameter_value_metadata(item, values, metadata)
-                    checked_items.append(item)
-            except SpineIntegrityError as e:
-                if strict:
-                    raise e
-                intgr_error_log.append(e)
-        return checked_items, intgr_error_log
 
-    @contextmanager
-    def _manage_stocks(self, item_type, item, existing_ids_by_pk, for_update, cache, intgr_error_log):
+    def _dirty_items(self):
+        """Returns a list of tuples of the form (item_type, (to_add, to_update, to_remove)) corresponding to
+        items that have been modified but not yet committed.
+
+        Returns:
+            list
+        """
+        real_commit_count = self._query_commit_count()
+        dirty_items = []
+        purged_item_types = {x for x in self.item_types() if self.mapped_table(x).purged}
+        self._add_descendants(purged_item_types)
+        for item_type in self._sorted_item_types:
+            self.do_fetch_all(item_type, commit_count=real_commit_count)  # To fix conflicts in add_item_from_db
+            mapped_table = self.mapped_table(item_type)
+            to_add = []
+            to_update = []
+            to_remove = []
+            for item in mapped_table.valid_values():
+                if item.status == Status.to_add:
+                    to_add.append(item)
+                elif item.status == Status.to_update:
+                    to_update.append(item)
+            if item_type in purged_item_types:
+                to_remove.append(mapped_table.wildcard_item)
+                to_remove.extend(mapped_table.values())
+            else:
+                for item in mapped_table.values():
+                    item.validate()
+                    if item.status == Status.to_remove:
+                        to_remove.append(item)
+            if to_add or to_update or to_remove:
+                dirty_items.append((item_type, (to_add, to_update, to_remove)))
+        return dirty_items
+
+    def _rollback(self):
+        """Discards uncommitted changes.
+
+        Namely, removes all the added items, resets all the updated items, and restores all the removed items.
+
+        Returns:
+            bool: False if there is no uncommitted items, True if successful.
+        """
+        dirty_items = self._dirty_items()
+        if not dirty_items:
+            return False
+        to_add_by_type = []
+        to_update_by_type = []
+        to_remove_by_type = []
+        for item_type, (to_add, to_update, to_remove) in reversed(dirty_items):
+            to_add_by_type.append((item_type, to_add))
+            to_update_by_type.append((item_type, to_update))
+            to_remove_by_type.append((item_type, to_remove))
+        for item_type, to_remove in to_remove_by_type:
+            mapped_table = self.mapped_table(item_type)
+            for item in to_remove:
+                mapped_table.restore_item(item["id"])
+        for item_type, to_update in to_update_by_type:
+            mapped_table = self.mapped_table(item_type)
+            for item in to_update:
+                mapped_table.update_item(item.backup)
+        for item_type, to_add in to_add_by_type:
+            mapped_table = self.mapped_table(item_type)
+            for item in to_add:
+                if mapped_table.remove_item(item) is not None:
+                    item.invalidate_id()
+        return True
+
+    def _refresh(self):
+        """Clears fetch progress, so the DB is queried again."""
+        if self._commit_count == self._query_commit_count():
+            return
+        self._fetched.clear()
+        for item_type in self.item_types():
+            mapped_table = self.mapped_table(item_type)
+            for item in mapped_table.values():
+                item.handle_refresh()
+
+    def _check_item_type(self, item_type):
+        if item_type not in self.all_item_types():
+            candidate = max(self.all_item_types(), key=lambda x: SequenceMatcher(None, item_type, x).ratio())
+            raise SpineDBAPIError(f"Invalid item type '{item_type}' - maybe you meant '{candidate}'?")
+
+    def mapped_table(self, item_type):
+        if item_type not in self._mapped_tables:
+            self._check_item_type(item_type)
+            self._mapped_tables[item_type] = _MappedTable(self, item_type)
+        return self._mapped_tables[item_type]
+
+    def reset(self, *item_types):
+        """Resets the mapping for given item types as if nothing was fetched from the DB or modified in the mapping.
+        Any modifications in the mapping that aren't committed to the DB are lost after this.
+        """
+        item_types = set(self.item_types()) if not item_types else set(item_types) & set(self.item_types())
+        self._add_descendants(item_types)
+        for item_type in item_types:
+            self._mapped_tables.pop(item_type, None)
+            self._fetched.pop(item_type, None)
+
+    def reset_purging(self):
+        """Resets purging status for all item types.
+
+        Fetching items of an item type that has been purged will automatically mark those items removed.
+        Resetting the purge status lets fetched items to be added unmodified.
+        """
+        for mapped_table in self._mapped_tables.values():
+            mapped_table.wildcard_item.status = Status.committed
+
+    def _add_descendants(self, item_types):
+        while True:
+            changed = False
+            for item_type in set(self.item_types()) - item_types:
+                if self.item_factory(item_type).ref_types() & item_types:
+                    item_types.add(item_type)
+                    changed = True
+            if not changed:
+                break
+
+    def get_mapped_item(self, item_type, id_, fetch=True):
+        mapped_table = self.mapped_table(item_type)
+        return mapped_table.find_item_by_id(id_, fetch=fetch) or {}
+
+    def _get_next_chunk(self, item_type, offset, limit, **kwargs):
+        """Gets chunk of items from the DB.
+
+        Returns:
+            list(dict): list of dictionary items.
+        """
+        qry = self._make_query(item_type, **kwargs)
+        if not qry:
+            return []
+        if not limit:
+            return [dict(x) for x in qry]
+        return [dict(x) for x in qry.limit(limit).offset(offset)]
+
+    def do_fetch_more(self, item_type, offset=0, limit=None, **kwargs):
+        """Fetches items from the DB and adds them to the mapping.
+
+        Args:
+            item_type (str)
+
+        Returns:
+            list(MappedItem): items fetched from the DB.
+        """
+        chunk = self._get_next_chunk(item_type, offset, limit, **kwargs)
+        if not chunk:
+            return []
+        real_commit_count = self._query_commit_count()
+        is_db_dirty = self._get_commit_count() != real_commit_count
+        if is_db_dirty:
+            # We need to fetch the most recent references because their ids might have changed in the DB
+            for ref_type in self.item_factory(item_type).ref_types():
+                if ref_type != item_type:
+                    self.do_fetch_all(ref_type, commit_count=real_commit_count)
+        mapped_table = self.mapped_table(item_type)
+        items = []
+        new_items = []
+        # Add items first
+        for x in chunk:
+            item, new = mapped_table.add_item_from_db(x, not is_db_dirty)
+            if new:
+                new_items.append(item)
+            else:
+                item.handle_refetch()
+            items.append(item)
+        # Once all items are added, add the unique key values
+        # Otherwise items that refer to other items that come later in the query will be seen as corrupted
+        for item in new_items:
+            mapped_table.add_unique(item)
+        return items
+
+    def _get_commit_count(self):
+        """Returns current commit count.
+
+        Returns:
+            int
+        """
+        if self._commit_count is None:
+            self._commit_count = self._query_commit_count()
+        return self._commit_count
+
+    def do_fetch_all(self, item_type, commit_count=None):
+        """Fetches all items of given type, but only once for each commit_count.
+        In other words, the second time this method is called with the same commit_count, it does nothing.
+        If not specified, commit_count defaults to the result of self._get_commit_count().
+
+        Args:
+            item_type (str)
+            commit_count (int,optional)
+        """
+        if commit_count is None:
+            commit_count = self._get_commit_count()
+        with self._locks.setdefault(item_type, RLock()):
+            if self._fetched.get(item_type, -1) < commit_count:
+                self._fetched[item_type] = commit_count
+                self.do_fetch_more(item_type, offset=0, limit=None)
+
+
+class _MappedTable(dict):
+    def __init__(self, db_map, item_type, *args, **kwargs):
+        """
+        Args:
+            db_map (DatabaseMappingBase): the DB mapping where this mapped table belongs.
+            item_type (str): the item type, equal to a table name
+        """
+        super().__init__(*args, **kwargs)
+        self._db_map = db_map
+        self._item_type = item_type
+        self._ids_by_unique_key_value = {}
+        self._temp_id_lookup = {}
+        self.wildcard_item = MappedItemBase(self._db_map, self._item_type, id=Asterisk)
+
+    @property
+    def purged(self):
+        return self.wildcard_item.status == Status.to_remove
+
+    @purged.setter
+    def purged(self, purged):
+        self.wildcard_item.status = Status.to_remove if purged else Status.committed
+
+    def get(self, id_, default=None):
+        id_ = self._temp_id_lookup.get(id_, id_)
+        return super().get(id_, default)
+
+    def _new_id(self):
+        return TempId.new_unique(self._item_type, self._temp_id_lookup)
+
+    def _unique_key_value_to_id(self, key, value, fetch=True):
+        """Returns the id that has the given value for the given unique key, or None.
+
+        Args:
+            key (tuple)
+            value (tuple)
+            fetch (bool): whether to fetch the DB until found.
+
+        Returns:
+            int or None
+        """
+        value = tuple(tuple(x) if isinstance(x, list) else x for x in value)
+        ids = self._ids_by_unique_key_value.get(key, {}).get(value, [])
+        if not ids and fetch:
+            self._db_map.do_fetch_all(self._item_type)
+            ids = self._ids_by_unique_key_value.get(key, {}).get(value, [])
+        return None if not ids else ids[-1]
+
+    def _unique_key_value_to_item(self, key, value, fetch=True, valid_only=True):
+        id_ = self._unique_key_value_to_id(key, value, fetch=fetch)
+        mapped_item = self.get(id_)
+        if mapped_item is None:
+            return None
+        if valid_only and not mapped_item.is_valid():
+            return None
+        return mapped_item
+
+    def valid_values(self):
+        return (x for x in self.values() if x.is_valid())
+
+    def _make_item(self, item):
+        """Returns a mapped item.
+
+        Args:
+            item (dict): the 'db item' to use as base
+
+        Returns:
+            MappedItem
+        """
+        return self._db_map.make_item(self._item_type, **item)
+
+    def find_item(self, item, skip_keys=(), fetch=True):
+        """Returns a MappedItemBase that matches the given dictionary-item.
+
+        Args:
+            item (dict)
+
+        Returns:
+            MappedItemBase or None
+        """
+        id_ = item.get("id")
+        if id_ is not None:
+            return self.find_item_by_id(id_, fetch=fetch)
+        return self._find_item_by_unique_key(item, skip_keys=skip_keys, fetch=fetch)
+
+    def find_item_by_id(self, id_, fetch=True):
+        current_item = self.get(id_, {})
+        if not current_item and fetch:
+            self._db_map.do_fetch_all(self._item_type)
+            current_item = self.get(id_, {})
+        return current_item
+
+    def _find_item_by_unique_key(self, item, skip_keys=(), fetch=True, valid_only=True):
+        for key, value in self._db_map.item_factory(self._item_type).unique_values_for_item(item, skip_keys=skip_keys):
+            current_item = self._unique_key_value_to_item(key, value, fetch=fetch, valid_only=valid_only)
+            if current_item:
+                return current_item
+        # Maybe item is missing some key stuff, so try with a resolved and polished MappedItem too...
+        mapped_item = self._make_item(item)
+        error = mapped_item.resolve_internal_fields(skip_keys=item.keys())
+        if error:
+            return {}
+        error = mapped_item.polish()
+        if error:
+            return {}
+        for key, value in mapped_item.unique_key_values(skip_keys=skip_keys):
+            current_item = self._unique_key_value_to_item(key, value, fetch=fetch, valid_only=valid_only)
+            if current_item:
+                return current_item
+        return {}
+
+    def checked_item_and_error(self, item, for_update=False):
         if for_update:
-            try:
-                id_ = item["id"]
-            except KeyError:
-                raise SpineIntegrityError(f"Missing {item_type} identifier.") from None
-            try:
-                full_item = cache.get(item_type, {})[id_]
-            except KeyError:
-                raise SpineIntegrityError(f"{item_type} not found.") from None
+            current_item = self.find_item(item)
+            if not current_item:
+                return None, f"no {self._item_type} matching {item} to update"
+            full_item, merge_error = current_item.merge(item)
+            if full_item is None:
+                return None, merge_error
         else:
-            id_ = None
-            full_item = cache.make_item(item_type, item)
+            current_item = None
+            full_item, merge_error = item, None
+        candidate_item = self._make_item(full_item)
+        error = self._prepare_item(candidate_item, current_item, item)
+        if error:
+            return None, error
+        valid_types = (type(None),) if for_update else ()
+        self.check_fields(candidate_item._asdict(), valid_types=valid_types)
+        return candidate_item, merge_error
+
+    def _prepare_item(self, candidate_item, current_item, original_item):
+        """Prepares item for insertion or update, returns any errors.
+
+        Args:
+            candidate_item (MappedItem)
+            current_item (MappedItem)
+            original_item (dict)
+
+        Returns:
+            str or None: errors if any.
+        """
+        error = candidate_item.resolve_internal_fields(skip_keys=original_item.keys())
+        if error:
+            return error
+        error = candidate_item.check_mutability()
+        if error:
+            return error
+        error = candidate_item.polish()
+        if error:
+            return error
+        first_invalid_key = candidate_item.first_invalid_key()
+        if first_invalid_key:
+            return f"invalid {first_invalid_key} for {self._item_type}"
         try:
-            existing_ids_by_key = {
-                _get_key(full_item, pk): existing_ids for pk, existing_ids in existing_ids_by_pk.items()
-            }
+            for key, value in candidate_item.unique_key_values():
+                empty = {k for k, v in zip(key, value) if v == ""}
+                if empty:
+                    return f"invalid empty keys {empty} for {self._item_type}"
+                unique_item = self._unique_key_value_to_item(key, value)
+                if unique_item not in (None, current_item) and unique_item.is_valid():
+                    return f"there's already a {self._item_type} with {dict(zip(key, value))}"
         except KeyError as e:
-            raise SpineIntegrityError(f"Missing key field {e} for {item_type}.") from None
-        if for_update:
-            try:
-                # Remove from existing
-                for key, existing_ids in existing_ids_by_key.items():
-                    del existing_ids[key]
-            except KeyError:
-                raise SpineIntegrityError(f"{item_type} not found.") from None
-            intgr_error_log += _fix_immutable_fields(item_type, full_item, item)
-            full_item.update(item)
+            return f"missing {e} for {self._item_type}"
+
+    def item_to_remove_and_error(self, id_):
+        if id_ is Asterisk:
+            return self.wildcard_item, None
+        current_item = self.find_item({"id": id_})
+        if not current_item:
+            return None, None
+        return current_item, current_item.check_mutability()
+
+    def add_unique(self, item):
+        id_ = item["id"]
+        for key, value in item.unique_key_values():
+            self._ids_by_unique_key_value.setdefault(key, {}).setdefault(value, []).append(id_)
+
+    def remove_unique(self, item):
+        id_ = item["id"]
+        for key, value in item.unique_key_values():
+            ids = self._ids_by_unique_key_value.get(key, {}).get(value, [])
+            if id_ in ids:
+                ids.remove(id_)
+
+    def _make_and_add_item(self, item):
+        if not isinstance(item, MappedItemBase):
+            item = self._make_item(item)
+            item.polish()
+        db_id = item.pop("id", None) if item.has_valid_id else None
+        item["id"] = new_id = self._new_id()
+        if db_id is not None:
+            new_id.resolve(db_id)
+        self[new_id] = item
+        return item
+
+    def add_item_from_db(self, item, is_db_clean):
+        """Adds an item fetched from the DB.
+
+        Args:
+            item (dict): item from the DB.
+            is_db_clean (Bool)
+
+        Returns:
+            tuple(MappedItem,bool): A mapped item and whether it needs to be added to the unique key values dict.
+        """
+        mapped_item = self._find_item_by_unique_key(item, fetch=False, valid_only=False)
+        if mapped_item and (is_db_clean or self._same_item(mapped_item, item)):
+            mapped_item.force_id(item["id"])
+            return mapped_item, False
+        mapped_item = self.get(item["id"])
+        if mapped_item and (is_db_clean or self._same_item(mapped_item.db_equivalent(), item)):
+            return mapped_item, False
+        conflicting_item = self.get(item["id"])
+        if conflicting_item is not None:
+            conflicting_item.handle_id_steal()
+        mapped_item = self._make_and_add_item(item)
+        if self.purged:
+            # Lazy purge: instead of fetching all at purge time, we purge stuff as it comes.
+            mapped_item.cascade_remove()
+        return mapped_item, True
+
+    def _same_item(self, mapped_item, db_item):
+        """Whether the two given items have the same unique keys.
+
+        Args:
+            mapped_item (MappedItemBase): an item in the in-memory mapping
+            db_item (dict): an item just fetched from the DB
+        """
+        db_item = self._db_map.make_item(self._item_type, **db_item)
+        db_item.polish()
+        return dict(mapped_item.unique_key_values()) == dict(db_item.unique_key_values())
+
+    def check_fields(self, item, valid_types=()):
+        factory = self._db_map.item_factory(self._item_type)
+
+        def _error(key, value, valid_types):
+            if key in set(factory._internal_fields) | set(factory._external_fields) | factory._private_fields | {
+                "id",
+                "commit_id",
+            }:
+                # The user seems to know what they're doing
+                return
+            f_dict = factory.fields.get(key)
+            if f_dict is None:
+                valid_args = ", ".join(factory.fields)
+                return f"invalid keyword argument '{key}' for '{self._item_type}' - valid arguments are {valid_args}."
+            valid_types = valid_types + (f_dict["type"],)
+            if f_dict.get("optional", False):
+                valid_types = valid_types + (type(None),)
+            if not isinstance(value, valid_types):
+                return (
+                    f"invalid type for '{key}' of '{self._item_type}' - "
+                    f"got {type(value).__name__}, expected {f_dict['type'].__name__}."
+                )
+
+        errors = list(filter(lambda x: x is not None, (_error(key, value, valid_types) for key, value in item.items())))
+        if errors:
+            raise SpineDBAPIError("\n".join(errors))
+
+    def add_item(self, item):
+        item = self._make_and_add_item(item)
+        self.add_unique(item)
+        item.status = Status.to_add
+        return item
+
+    def update_item(self, item):
+        current_item = self.find_item(item)
+        current_item.cascade_remove_unique()
+        current_item.update(item)
+        current_item.cascade_add_unique()
+        current_item.cascade_update()
+        return current_item
+
+    def remove_item(self, item):
+        if not item:
+            return None
+        if item is self.wildcard_item:
+            self.purged = True
+            for current_item in self.valid_values():
+                current_item.cascade_remove()
+            return self.wildcard_item
+        item.cascade_remove()
+        return item
+
+    def restore_item(self, id_):
+        if id_ is Asterisk:
+            self.purged = False
+            for current_item in self.values():
+                current_item.cascade_restore()
+            return self.wildcard_item
+        current_item = self.find_item({"id": id_})
+        if current_item:
+            current_item.cascade_restore()
+        return current_item
+
+
+class MappedItemBase(dict):
+    """A dictionary that represents a db item."""
+
+    fields = {}
+    """A dictionary mapping fields to a another dict mapping "type" to a Python type,
+    "value" to a description of the value for the key, and "optional" to a bool."""
+    _defaults = {}
+    """A dictionary mapping fields to their default values"""
+    _unique_keys = ()
+    """A tuple where each element is itself a tuple of fields corresponding to a unique key"""
+    _references = {}
+    """A dictionary mapping source fields, to a tuple of reference item type and reference field.
+    Used to access external fields.
+    """
+    _external_fields = {}
+    """A dictionary mapping fields that are not in the original dictionary, to a tuple of source field
+    and target field.
+    When accessing fields in _external_fields, we first find the reference pointed at by the source field,
+    and then return the target field of that reference.
+    """
+    _alt_references = {}
+    """A dictionary mapping source fields, to a tuple of reference item type and reference fields.
+    Used only to resolve internal fields at item creation.
+    """
+    _internal_fields = {}
+    """A dictionary mapping fields that are not in the original dictionary, to a tuple of source field
+    and target field.
+    When resolving fields in _internal_fields, we first find the alt_reference pointed at by the source field,
+    and then use the target field of that reference.
+    """
+    _private_fields = set()
+    """A set with fields that should be ignored in validations."""
+    is_protected = False
+
+    def __init__(self, db_map, item_type, **kwargs):
+        """
+        Args:
+            db_map (DatabaseMappingBase): the DB where this item belongs.
+        """
+        super().__init__(**kwargs)
+        self._db_map = db_map
+        self._item_type = item_type
+        self._referrers = {}
+        self._weak_referrers = {}
+        self.restore_callbacks = set()
+        self.update_callbacks = set()
+        self.remove_callbacks = set()
+        self._has_valid_id = True
+        self._removed = False
+        self._valid = None
+        self._status = Status.committed
+        self._removal_source = None
+        self._status_when_removed = None
+        self._status_when_committed = None
+        self._backup = None
+        self.public_item = PublicItem(self._db_map, self)
+
+    def handle_refetch(self):
+        """Called when an equivalent item is fetched from the DB.
+
+        1. If this item is compromised, then mark it as committed.
+        2. If this item is committed, then assume the one from the DB is newer and reset the state.
+           Otherwise assume *this* is newer and do nothing.
+        """
+        if self.status == Status.compromised:
+            self.status = Status.committed
+        if self.is_committed():
+            self._removed = False
+            self._valid = None
+
+    def handle_refresh(self):
+        """Called when the mapping is refreshed.
+
+        If this item is committed, then set it as compromised.
+        """
+        if self.status == Status.committed:
+            self.status = Status.compromised
+
+    @classmethod
+    def ref_types(cls):
+        """Returns a set of item types that this class refers.
+
+        Returns:
+            set(str)
+        """
+        return set(ref_type for ref_type, _ref_key in cls._references.values())
+
+    @property
+    def status(self):
+        """Returns the status of this item.
+
+        Returns:
+            Status
+        """
+        return self._status
+
+    @status.setter
+    def status(self, status):
+        """Sets the status of this item.
+
+        Args:
+            status (Status)
+        """
+        self._status = status
+
+    @property
+    def backup(self):
+        """Returns the committed version of this item.
+
+        Returns:
+            dict or None
+        """
+        return self._backup
+
+    @property
+    def removed(self):
+        """Returns whether or not this item has been removed.
+
+        Returns:
+            bool
+        """
+        return self._removed
+
+    @property
+    def item_type(self):
+        """Returns this item's type
+
+        Returns:
+            str
+        """
+        return self._item_type
+
+    @property
+    def key(self):
+        """Returns a tuple (item_type, id) for convenience, or None if this item doesn't yet have an id.
+        TODO: When does the latter happen?
+
+        Returns:
+            tuple(str,int) or None
+        """
+        id_ = dict.get(self, "id")
+        if id_ is None:
+            return None
+        return (self._item_type, id_)
+
+    @property
+    def has_valid_id(self):
+        return self._has_valid_id
+
+    def invalidate_id(self):
+        """Sets id as invalid."""
+        self._has_valid_id = False
+
+    def _extended(self):
+        """Returns a dict from this item's original fields plus all the references resolved statically.
+
+        Returns:
+            dict
+        """
+        d = self._asdict()
+        d.update({key: self[key] for key in self._external_fields})
+        return d
+
+    def _asdict(self):
+        """Returns a dict from this item's original fields.
+
+        Returns:
+            dict
+        """
+        return dict(self)
+
+    def resolve(self):
+        return {k: resolve(v) for k, v in self._asdict().items()}
+
+    def merge(self, other):
+        """Merges this item with another and returns the merged item together with any errors.
+        Used for updating items.
+
+        Args:
+            other (dict): the item to merge into this.
+
+        Returns:
+            dict: merged item.
+            str: error description if any.
+        """
+        if not self._something_to_update(other):
+            # Nothing to update, that's fine
+            return None, ""
+        merged = {**self._extended(), **other}
+        if not isinstance(merged["id"], int):
+            merged["id"] = self["id"]
+        return merged, ""
+
+    def _something_to_update(self, other):
+        def _convert(x):
+            if isinstance(x, list):
+                x = tuple(x)
+            return resolve(x)
+
+        return not all(
+            _convert(self.get(key)) == _convert(value)
+            for key, value in other.items()
+            if value is not None
+            or self.fields.get(key, {}).get("optional", False)  # Ignore mandatory fields that are None
+        )
+
+    def db_equivalent(self):
+        """The equivalent of this item in the DB.
+
+        Returns:
+            MappedItemBase
+        """
+        if self.status == Status.to_update:
+            db_item = self._db_map.make_item(self._item_type, **self.backup)
+            db_item.polish()
+            return db_item
+        return self
+
+    def first_invalid_key(self):
+        """Goes through the ``_references`` class attribute and returns the key of the first reference
+        that cannot be resolved.
+
+        Returns:
+            str or None: unresolved reference's key if any.
+        """
+        return next((src_key for src_key, ref in self._resolve_refs() if not ref), None)
+
+    def _resolve_refs(self):
+        """Goes through the ``_references`` class attribute and tries to resolve them.
+        If successful, replace source fields referring to db-ids with the reference's TempId.
+
+        Yields:
+            tuple(str,MappedItem or None): the source field and resolved ref.
+        """
+        for src_key, (ref_type, ref_key) in self._references.items():
+            ref = self._get_full_ref(src_key, ref_type, ref_key)
+            if isinstance(ref, tuple):
+                for r in ref:
+                    yield src_key, r
+            else:
+                yield src_key, ref
+
+    def _get_full_ref(self, src_key, ref_type, ref_key, strong=True):
         try:
-            yield full_item
-            # Check is performed at this point
-        except SpineIntegrityError:  # pylint: disable=try-except-raise
-            # Check didn't pass, so reraise
-            raise
+            src_val = self[src_key]
+        except KeyError:
+            return {}
+        if isinstance(src_val, tuple):
+            ref = tuple(self._get_ref(ref_type, {ref_key: x}, strong=strong) for x in src_val)
+            if all(ref) and ref_key == "id":
+                self[src_key] = tuple(r["id"] for r in ref)
+            return ref
+        ref = self._get_ref(ref_type, {ref_key: src_val}, strong=strong)
+        if ref and ref_key == "id":
+            self[src_key] = ref["id"]
+        return ref
+
+    @classmethod
+    def unique_values_for_item(cls, item, skip_keys=()):
+        for key in cls._unique_keys:
+            if key not in skip_keys:
+                value = tuple(item.get(k) for k in key)
+                if None not in value:
+                    yield key, value
+
+    def unique_key_values(self, skip_keys=()):
+        """Yields tuples of unique keys and their values.
+
+        Args:
+            skip_keys: Don't yield these keys
+
+        Yields:
+            tuple(tuple,tuple): the first element is the unique key, the second is the values.
+        """
+        yield from self.unique_values_for_item(self, skip_keys=skip_keys)
+
+    def resolve_internal_fields(self, skip_keys=()):
+        """Goes through the ``_internal_fields`` class attribute and updates this item
+        by resolving those references.
+        Returns any error.
+
+        Args:
+            skip_keys (tuple): don't resolve references for these keys.
+
+        Returns:
+            str or None: error description if any.
+        """
+        for key in self._internal_fields:
+            if key in skip_keys:
+                continue
+            error = self._do_resolve_internal_field(key)
+            if error:
+                return error
+
+    def _do_resolve_internal_field(self, key):
+        src_key, target_key = self._internal_fields[key]
+        src_val = tuple(dict.pop(self, k, None) or self.get(k) for k in src_key)
+        if None in src_val:
+            return
+        ref_type, ref_key = self._alt_references[src_key]
+        mapped_table = self._db_map.mapped_table(ref_type)
+        if all(isinstance(v, (tuple, list)) for v in src_val):
+            refs = []
+            for v in zip(*src_val):
+                ref = mapped_table.find_item(dict(zip(ref_key, v)))
+                if not ref:
+                    return f"can't find {ref_type} with {dict(zip(ref_key, v))}"
+                refs.append(ref)
+            self[key] = tuple(ref[target_key] for ref in refs)
         else:
-            # Check passed, so add to existing
-            for key, existing_ids in existing_ids_by_key.items():
-                existing_ids[key] = id_
-            if for_update:
-                cache.get(item_type, {})[id_] = full_item
-
-
-def _get_key_values(item, pk):
-    for field in pk:
-        value = item[field]
-        if isinstance(value, list):
-            value = tuple(value)
-        yield value
-
-
-def _get_key(item, pk):
-    key = tuple(_get_key_values(item, pk))
-    if len(key) > 1:
-        return key
-    return key[0]
-
-
-def _fix_immutable_fields(item_type, current_item, item):
-    immutable_fields = {
-        "object": ("class_id",),
-        "relationship_class": ("object_class_id_list",),
-        "relationship": ("class_id",),
-        "parameter_definition": ("entity_class_id", "object_class_id", "relationship_class_id"),
-        "parameter_value": ("entity_class_id", "object_class_id", "relationship_class_id"),
-    }.get(item_type, ())
-    fixed = []
-    for field in immutable_fields:
-        if current_item.get(field) is None:
-            continue
-        if field in item and item[field] != current_item[field]:
-            fixed.append(field)
-        item[field] = current_item[field]
-    if fixed:
-        fixed = ', '.join([f"'{field}'" for field in fixed])
-        return [SpineIntegrityError(f"Can't update fixed fields {fixed}")]
-    return []
+            ref = mapped_table.find_item(dict(zip(ref_key, src_val)))
+            if not ref:
+                return f"can't find {ref_type} with {dict(zip(ref_key, src_val))}"
+            self[key] = ref[target_key]
+
+    def polish(self):
+        """Polishes this item once all it's references have been resolved. Returns any error.
+
+        The base implementation sets defaults but subclasses can do more work if needed.
+
+        Returns:
+            str or None: error description if any.
+        """
+        for key, default_value in self._defaults.items():
+            self.setdefault(key, default_value)
+        return ""
+
+    def check_mutability(self):
+        """Called before adding, updating, or removing this item. Returns any errors that prevent that.
+
+        Returns:
+            str or None: error description if any.
+        """
+        return ""
+
+    def _get_ref(self, ref_type, key_val, strong=True):
+        """Collects a reference from the in-memory mapping.
+        Adds this item to the reference's list of referrers if strong is True;
+        or weak referrers if strong is False.
+
+        Args:
+            ref_type (str): The reference's type
+            key_val (dict): The reference's key and value to match
+            strong (bool): True if the reference corresponds to a foreign key, False otherwise
+
+        Returns:
+            MappedItemBase or dict
+        """
+        mapped_table = self._db_map.mapped_table(ref_type)
+        ref = mapped_table.find_item(key_val, fetch=True)
+        if not ref:
+            return {}
+        if strong:
+            ref.add_referrer(self)
+        else:
+            ref.add_weak_referrer(self)
+            if ref.removed:
+                return {}
+        return ref
+
+    def _invalidate_ref(self, ref_type, key_val):
+        """Invalidates a reference previously collected from the in-memory mapping.
+
+        Args:
+            ref_type (str): The reference's type
+            key_val (dict): The reference's key and value to match
+        """
+        mapped_table = self._db_map.mapped_table(ref_type)
+        ref = mapped_table.find_item(key_val)
+        ref.remove_referrer(self)
+
+    def is_valid(self):
+        """Checks if this item has all its references.
+        Removes the item from the in-memory mapping if not valid by calling ``cascade_remove``.
+
+        Returns:
+            bool
+        """
+        if self.status == Status.compromised:
+            return False
+        self.validate()
+        return self._valid
+
+    def validate(self):
+        """Resolves all references and checks if the item is valid.
+        The item is valid if it's not removed, has all of its references, and none of them is removed."""
+        if self._valid is not None:
+            return
+        refs = [ref for _, ref in self._resolve_refs()]
+        self._valid = not self._removed and all(ref and not ref.removed for ref in refs)
+        if not self._valid:
+            self.cascade_remove()
+
+    def add_referrer(self, referrer):
+        """Adds a strong referrer to this item. Strong referrers are removed, updated and restored
+        in cascade with this item.
+
+        Args:
+            referrer (MappedItemBase)
+        """
+        key = referrer.key
+        if key is None:
+            return
+        self._referrers[key] = self._weak_referrers.pop(key, referrer)
+
+    def remove_referrer(self, referrer):
+        """Removes a strong referrer.
+
+        Args:
+            referrer (MappedItemBase)
+        """
+        key = referrer.key
+        if key is not None:
+            self._referrers.pop(key, None)
+
+    def add_weak_referrer(self, referrer):
+        """Adds a weak referrer to this item.
+        Weak referrers' update callbacks are called whenever this item changes.
+
+        Args:
+            referrer (MappedItemBase)
+        """
+        key = referrer.key
+        if key is None:
+            return
+        if key not in self._referrers:
+            self._weak_referrers[key] = referrer
+
+    def _update_weak_referrers(self):
+        for weak_referrer in self._weak_referrers.values():
+            weak_referrer.call_update_callbacks()
+
+    def cascade_restore(self, source=None):
+        """Restores this item (if removed) and all its referrers in cascade.
+        Also, updates items' status and calls their restore callbacks.
+        """
+        if not self._removed:
+            return
+        if source is not self._removal_source:
+            return
+        if self.status in (Status.added_and_removed, Status.to_remove):
+            self._status = self._status_when_removed
+        elif self.status == Status.committed:
+            self._status = Status.to_add
+        else:
+            raise RuntimeError("invalid status for item being restored")
+        self._removed = False
+        self._valid = None
+        # First restore this, then referrers
+        obsolete = set()
+        for callback in list(self.restore_callbacks):
+            if not callback(self):
+                obsolete.add(callback)
+        self.restore_callbacks -= obsolete
+        for referrer in self._referrers.values():
+            referrer.cascade_restore(source=self)
+        self._update_weak_referrers()
+
+    def cascade_remove(self, source=None):
+        """Removes this item and all its referrers in cascade.
+        Also, updates items' status and calls their remove callbacks.
+        """
+        if self._removed:
+            return
+        self._status_when_removed = self._status
+        if self._status == Status.to_add:
+            self._status = Status.added_and_removed
+        elif self._status in (Status.committed, Status.to_update):
+            self._status = Status.to_remove
+        else:
+            raise RuntimeError("invalid status for item being removed")
+        self._removal_source = source
+        self._removed = True
+        self._valid = None
+        # First remove referrers, then this
+        for referrer in self._referrers.values():
+            referrer.cascade_remove(source=self)
+        self._update_weak_referrers()
+        obsolete = set()
+        for callback in list(self.remove_callbacks):
+            if not callback(self):
+                obsolete.add(callback)
+        self.remove_callbacks -= obsolete
+
+    def cascade_update(self):
+        """Updates this item and all its referrers in cascade.
+        Also, calls items' update callbacks.
+        """
+        if self._removed:
+            return
+        self.call_update_callbacks()
+        for referrer in self._referrers.values():
+            referrer.cascade_update()
+        self._update_weak_referrers()
+
+    def call_update_callbacks(self):
+        obsolete = set()
+        for callback in list(self.update_callbacks):
+            if not callback(self):
+                obsolete.add(callback)
+        self.update_callbacks -= obsolete
+
+    def cascade_add_unique(self):
+        """Adds item and all its referrers unique keys and ids in cascade."""
+        mapped_table = self._db_map.mapped_table(self._item_type)
+        mapped_table.add_unique(self)
+        for referrer in self._referrers.values():
+            referrer.cascade_add_unique()
+
+    def cascade_remove_unique(self):
+        """Removes item and all its referrers unique keys and ids in cascade."""
+        mapped_table = self._db_map.mapped_table(self._item_type)
+        mapped_table.remove_unique(self)
+        for referrer in self._referrers.values():
+            referrer.cascade_remove_unique()
+
+    def is_committed(self):
+        """Returns whether or not this item is committed to the DB.
+
+        Returns:
+            bool
+        """
+        return self._status == Status.committed
+
+    def commit(self, commit_id):
+        """Sets this item as committed with the given commit id."""
+        self._status_when_committed = self._status
+        self._status = Status.committed
+        if commit_id:
+            self["commit_id"] = commit_id
+
+    def __repr__(self):
+        """Overridden to return a more verbose representation."""
+        return f"{self._item_type}{self._extended()}"
+
+    def __getattr__(self, name):
+        """Overridden to return the dictionary key named after the attribute, or None if it doesn't exist."""
+        # FIXME: We should try and get rid of this one
+        return self.get(name)
+
+    def __getitem__(self, key):
+        """Overridden to return references."""
+        source_and_target_key = self._external_fields.get(key)
+        if source_and_target_key:
+            source_key, target_key = source_and_target_key
+            ref_type, ref_key = self._references[source_key]
+            ref = self._get_full_ref(source_key, ref_type, ref_key)
+            if isinstance(ref, tuple):
+                return tuple(r.get(target_key) for r in ref)
+            return ref.get(target_key)
+        return super().__getitem__(key)
+
+    def __setitem__(self, key, value):
+        """Sets id valid if key is 'id'."""
+        if key == "id":
+            self._has_valid_id = True
+        super().__setitem__(key, value)
+
+    def get(self, key, default=None):
+        """Overridden to return references."""
+        try:
+            return self[key]
+        except KeyError:
+            return default
+
+    def update(self, other):
+        """Overridden to update the item status and also to invalidate references that become obsolete."""
+        if self._status == Status.committed:
+            self._status = Status.to_update
+            self._backup = self._asdict()
+        elif self._status in (Status.to_remove, Status.added_and_removed):
+            raise RuntimeError("invalid status of item being updated")
+        for src_key, (ref_type, ref_key) in self._references.items():
+            src_val = self[src_key]
+            if src_key in other and other[src_key] != src_val:
+                # Invalidate references
+                if isinstance(src_val, tuple):
+                    for x in src_val:
+                        self._invalidate_ref(ref_type, {ref_key: x})
+                else:
+                    self._invalidate_ref(ref_type, {ref_key: src_val})
+        id_ = self["id"]
+        super().update(other)
+        self["id"] = id_
+        if self._asdict() == self._backup:
+            self._status = Status.committed
+
+    def force_id(self, id_):
+        """Makes sure this item's has the given id_, corresponding to the new id of the item
+        in the DB after some external changes.
+
+        Args:
+            id_ (int): The most recent id_ of the item as fetched from the DB.
+        """
+        mapped_id = self["id"]
+        if mapped_id == id_:
+            return
+        # Resolve the TempId to the new db id (and commit the item if pending)
+        mapped_id.resolve(id_)
+        if self.status == Status.to_add:
+            self.status = Status.committed
+
+    def handle_id_steal(self):
+        """Called when a new item is fetched from the DB with this item's id."""
+        self["id"].unresolve()
+        # TODO: Test if the below works...
+        if self.is_committed():
+            self._status = self._status_when_committed
+        if self._status == Status.to_update:
+            self._status = Status.to_add
+        elif self._status == Status.to_remove:
+            self._status = Status.committed
+            self._status_when_removed = Status.to_add
+
+
+class PublicItem:
+    def __init__(self, db_map, mapped_item):
+        self._db_map = db_map
+        self._mapped_item = mapped_item
+
+    @property
+    def item_type(self):
+        return self._mapped_item.item_type
+
+    def __getitem__(self, key):
+        return self._mapped_item[key]
+
+    def __eq__(self, other):
+        if isinstance(other, dict):
+            return self._mapped_item == other
+        return super().__eq__(other)
+
+    def __repr__(self):
+        return repr(self._mapped_item)
+
+    def __str__(self):
+        return str(self._mapped_item)
+
+    def get(self, key, default=None):
+        return self._mapped_item.get(key, default)
+
+    def validate(self):
+        self._mapped_item.validate()
+
+    def is_valid(self):
+        return self._mapped_item.is_valid()
+
+    def is_committed(self):
+        return self._mapped_item.is_committed()
+
+    def _asdict(self):
+        return self._mapped_item._asdict()
+
+    def _extended(self):
+        return self._mapped_item._extended()
+
+    def update(self, **kwargs):
+        self._db_map.update_item(self.item_type, id=self["id"], **kwargs)
+
+    def remove(self):
+        return self._db_map.remove_item(self.item_type, self["id"])
+
+    def restore(self):
+        return self._db_map.restore_item(self.item_type, self["id"])
+
+    def add_update_callback(self, callback):
+        self._mapped_item.update_callbacks.add(callback)
+
+    def add_remove_callback(self, callback):
+        self._mapped_item.remove_callbacks.add(callback)
+
+    def add_restore_callback(self, callback):
+        self._mapped_item.restore_callbacks.add(callback)
+
+    def resolve(self):
+        return self._mapped_item.resolve()
```

### Comparing `spinedb_api-0.30.5/spinedb_api/diff_db_mapping.py` & `spinedb_api-0.31.0/spinedb_api/db_mapping_commit_mixin.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,178 +1,163 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
-"""
-Provides :class:`DiffDatabaseMapping`.
-
-"""
-
+from sqlalchemy import and_, or_
 from sqlalchemy.sql.expression import bindparam
 from sqlalchemy.exc import DBAPIError
-from .db_mapping_query_mixin import DatabaseMappingQueryMixin
-from .db_mapping_check_mixin import DatabaseMappingCheckMixin
-from .db_mapping_add_mixin import DatabaseMappingAddMixin
-from .db_mapping_update_mixin import DatabaseMappingUpdateMixin
-from .db_mapping_remove_mixin import DatabaseMappingRemoveMixin
-from .diff_db_mapping_commit_mixin import DiffDatabaseMappingCommitMixin
-from .diff_db_mapping_base import DiffDatabaseMappingBase
-from .filters.tools import apply_filter_stack, load_filters
 from .exception import SpineDBAPIError
+from .temp_id import TempId, resolve
+from .helpers import group_consecutive, Asterisk
 
 
-class DiffDatabaseMapping(
-    DatabaseMappingQueryMixin,
-    DatabaseMappingCheckMixin,
-    DatabaseMappingAddMixin,
-    DatabaseMappingUpdateMixin,
-    DatabaseMappingRemoveMixin,
-    DiffDatabaseMappingCommitMixin,
-    DiffDatabaseMappingBase,
-):
-    """A read-write database mapping.
-
-    Provides methods to *stage* any number of changes (namely, ``INSERT``, ``UPDATE`` and ``REMOVE`` operations)
-    over a Spine database, as well as to commit or rollback the batch of changes.
-
-    For convenience, querying this mapping return results *as if* all the staged changes were already committed.
-
-    :param str db_url: A database URL in RFC-1738 format pointing to the database to be mapped.
-    :param str username: A user name. If ``None``, it gets replaced by the string ``"anon"``.
-    :param bool upgrade: Whether or not the db at the given URL should be upgraded to the most recent version.
-    """
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        self._init_type_attributes()
-        if self._filter_configs is not None:
-            stack = load_filters(self._filter_configs)
-            apply_filter_stack(self, stack)
-
-    def _add_items(self, tablename, *items):
-        self._add_commit_id_and_ids(tablename, *items)
-        ids = {x["id"] for x in items}
-        for tablename_ in self._do_add_items(tablename, *items):
-            self.added_item_id[tablename_].update(ids)
-            self._clear_subqueries(tablename_)
-        return ids
-
-    def _readd_items(self, tablename, *items):
-        ids = set(x["id"] for x in items)
-        for tablename_ in self._do_add_items(tablename, *items):
-            self.added_item_id[tablename_].update(ids)
-            self._clear_subqueries(tablename_)
-
-    def _get_table_for_insert(self, tablename):
-        return self._diff_table(tablename)
-
-    def _get_items_for_update_and_insert(self, tablename, checked_items):
-        """Return lists of items for update and insert.
-        Items in the diff table should be updated, whereas items in the original table
-        should be marked as dirty and inserted into the corresponding diff table."""
-        items_for_update = list()
-        items_for_insert = list()
-        dirty_ids = set()
-        updated_ids = set()
-        id_field = self.table_ids.get(tablename, "id")
-        for item in checked_items:
-            id_ = item[id_field]
-            updated_ids.add(id_)
-            if id_ in self.added_item_id[tablename] | self.updated_item_id[tablename]:
-                items_for_update.append(item)
-            else:
-                items_for_insert.append(item)
-                dirty_ids.add(id_)
-        return items_for_update, items_for_insert, dirty_ids, updated_ids
-
-    def _do_update_items(self, tablename, *items):
-        items_for_update, items_for_insert, dirty_ids, updated_ids = self._get_items_for_update_and_insert(
-            tablename, items
-        )
-        if self.committing:
-            try:
-                self._update_and_insert_items(tablename, items_for_update, items_for_insert)
-                self.updated_item_id[tablename].update(dirty_ids)
-                self._mark_as_dirty(tablename, dirty_ids)
-            except DBAPIError as e:
-                msg = f"DBAPIError while updating {tablename} items: {e.orig.args}"
-                raise SpineDBAPIError(msg)
-        return updated_ids
-
-    def _update_and_insert_items(self, tablename, items_for_update, items_for_insert):
-        diff_table = self._diff_table(tablename)
-        if items_for_update:
-            upd = diff_table.update()
-            for k in self._get_primary_key(tablename):
-                upd = upd.where(getattr(diff_table.c, k) == bindparam(k))
-            upd = upd.values({key: bindparam(key) for key in diff_table.columns.keys() & items_for_update[0].keys()})
-            self._checked_execute(upd, [{**item} for item in items_for_update])
-        ins = diff_table.insert()
-        self._checked_execute(ins, [{**item} for item in items_for_insert])
-
-    def _update_wide_relationships(self, *items):
-        """Update relationships without checking integrity."""
-        items = self._items_with_type_id("relationship", *items)
-        ent_items = []
-        rel_ent_items = []
-        for item in items:
-            ent_item = item.copy()
-            object_class_id_list = ent_item.pop("object_class_id_list", [])
-            object_id_list = ent_item.pop("object_id_list", [])
-            ent_items.append(ent_item)
-            for dimension, (member_class_id, member_id) in enumerate(zip(object_class_id_list, object_id_list)):
-                rel_ent_item = ent_item.copy()
-                rel_ent_item["entity_class_id"] = rel_ent_item.pop("class_id", None)
-                rel_ent_item["entity_id"] = rel_ent_item.pop("id", None)
-                rel_ent_item["dimension"] = dimension
-                rel_ent_item["member_class_id"] = member_class_id
-                rel_ent_item["member_id"] = member_id
-                rel_ent_items.append(rel_ent_item)
+class DatabaseMappingCommitMixin:
+    _id_fields = {
+        "entity_class_dimension": "entity_class_id",
+        "entity_element": "entity_id",
+        "object_class": "entity_class_id",
+        "relationship_class": "entity_class_id",
+        "object": "entity_id",
+        "relationship": "entity_id",
+    }
+    composite_pks = {
+        "entity_element": ("entity_id", "position"),
+        "entity_class_dimension": ("entity_class_id", "position"),
+    }
+
+    def _do_add_items(self, connection, tablename, *items_to_add):
+        """Add items to DB without checking integrity."""
+        if not items_to_add:
+            return
         try:
-            ents_for_update, ents_for_insert, dirty_ent_ids, updated_ent_ids = self._get_items_for_update_and_insert(
-                "entity", ent_items
-            )
-            (
-                rel_ents_for_update,
-                rel_ents_for_insert,
-                dirty_rel_ent_ids,
-                updated_rel_ent_ids,
-            ) = self._get_items_for_update_and_insert("relationship_entity", rel_ent_items)
-            self._update_and_insert_items("entity", ents_for_update, ents_for_insert)
-            self._mark_as_dirty("entity", dirty_ent_ids)
-            self.updated_item_id["entity"].update(dirty_ent_ids)
-            self._update_and_insert_items("relationship_entity", rel_ents_for_update, rel_ents_for_insert)
-            self._mark_as_dirty("relationship_entity", dirty_rel_ent_ids)
-            self.updated_item_id["relationship_entity"].update(dirty_rel_ent_ids)
-            return updated_ent_ids.union(updated_rel_ent_ids)
+            table = self._metadata.tables[self.real_item_type(tablename)]
+            id_items, temp_id_items = [], []
+            for item in items_to_add:
+                if isinstance(item["id"], TempId):
+                    temp_id_items.append(item)
+                else:
+                    id_items.append(item)
+            if id_items:
+                connection.execute(table.insert(), [x.resolve() for x in id_items])
+            if temp_id_items:
+                current_ids = {x["id"] for x in connection.execute(table.select())}
+                next_id = max(current_ids, default=0) + 1
+                available_ids = set(range(1, next_id)) - current_ids
+                required_id_count = len(temp_id_items) - len(available_ids)
+                new_ids = set(range(next_id, next_id + required_id_count))
+                ids = sorted(available_ids | new_ids)
+                for id_, item in zip(ids, temp_id_items):
+                    temp_id = item["id"]
+                    temp_id.resolve(id_)
+                connection.execute(table.insert(), [x.resolve() for x in temp_id_items])
+            for tablename_, items_to_add_ in self._extra_items_to_add_per_table(tablename, items_to_add):
+                if not items_to_add_:
+                    continue
+                table = self._metadata.tables[self.real_item_type(tablename_)]
+                connection.execute(table.insert(), [resolve(x) for x in items_to_add_])
         except DBAPIError as e:
-            msg = "DBAPIError while updating relationships: {}".format(e.orig.args)
-            raise SpineDBAPIError(msg)
+            msg = f"DBAPIError while inserting {tablename} items: {e.orig.args}"
+            raise SpineDBAPIError(msg) from e
 
-    def remove_items(self, **kwargs):
-        """Removes items by id, *not in cascade*.
+    @staticmethod
+    def _dimensions_for_classes(classes):
+        return [
+            {"entity_class_id": x["id"], "position": position, "dimension_id": dimension_id}
+            for x in classes
+            for position, dimension_id in enumerate(x["dimension_id_list"])
+        ]
+
+    @staticmethod
+    def _elements_for_entities(entities):
+        return [
+            {
+                "entity_id": x["id"],
+                "entity_class_id": x["class_id"],
+                "position": position,
+                "element_id": element_id,
+                "dimension_id": dimension_id,
+            }
+            for x in entities
+            for position, (element_id, dimension_id) in enumerate(zip(x["element_id_list"], x["dimension_id_list"]))
+        ]
+
+    def _extra_items_to_add_per_table(self, tablename, items_to_add):
+        if tablename == "entity_class":
+            yield ("entity_class_dimension", self._dimensions_for_classes(items_to_add))
+        elif tablename == "entity":
+            yield ("entity_element", self._elements_for_entities(items_to_add))
+
+    def _extra_items_to_update_per_table(self, tablename, items_to_update):
+        if tablename == "entity":
+            yield ("entity_element", self._elements_for_entities(items_to_update))
+
+    def _get_primary_key(self, tablename):
+        pk = self.composite_pks.get(tablename)
+        if pk is None:
+            id_field = self._id_fields.get(tablename, "id")
+            pk = (id_field,)
+        return pk
+
+    def _make_update_stmt(self, tablename, keys):
+        table = self._metadata.tables[self.real_item_type(tablename)]
+        upd = table.update()
+        for k in self._get_primary_key(tablename):
+            upd = upd.where(getattr(table.c, k) == bindparam(k))
+        return upd.values({key: bindparam(key) for key in table.columns.keys() & keys})
+
+    def _do_update_items(self, connection, tablename, *items_to_update):
+        """Update items in DB without checking integrity."""
+        if not items_to_update:
+            return
+        try:
+            upd = self._make_update_stmt(tablename, items_to_update[0].keys())
+            connection.execute(upd, [x.resolve() for x in items_to_update])
+            for tablename_, items_to_update_ in self._extra_items_to_update_per_table(tablename, items_to_update):
+                if not items_to_update_:
+                    continue
+                upd = self._make_update_stmt(tablename_, items_to_update_[0].keys())
+                connection.execute(upd, [resolve(x) for x in items_to_update_])
+        except DBAPIError as e:
+            msg = f"DBAPIError while updating '{tablename}' items: {e.orig.args}"
+            raise SpineDBAPIError(msg) from e
+
+    def _do_remove_items(self, connection, tablename, *ids):
+        """Removes items from the db.
 
         Args:
-            **kwargs: keyword is table name, argument is list of ids to remove
+            *ids: ids to remove
         """
-        if self.committing:
-            for tablename, ids in kwargs.items():
-                table_id = self.table_ids.get(tablename, "id")
-                diff_table = self._diff_table(tablename)
-                delete = diff_table.delete().where(self.in_(getattr(diff_table.c, table_id), ids))
-                try:
-                    self.connection.execute(delete)
-                except DBAPIError as e:
-                    msg = f"DBAPIError while removing {tablename} items: {e.orig.args}"
-                    raise SpineDBAPIError(msg)
-        for tablename, ids in kwargs.items():
-            self.added_item_id[tablename].difference_update(ids)
-            self.updated_item_id[tablename].difference_update(ids)
-            self.removed_item_id[tablename].update(ids)
-            self._mark_as_dirty(tablename, ids)
+        tablename = self.real_item_type(tablename)
+        ids = {resolve(id_) for id_ in ids}
+        if tablename == "alternative":
+            # Do not remove the Base alternative
+            ids.discard(1)
+        if not ids:
+            return
+        tablenames = [tablename]
+        if tablename == "entity_class":
+            # Also remove the items corresponding to the id in entity_class_dimension
+            tablenames.append("entity_class_dimension")
+        elif tablename == "entity":
+            # Also remove the items corresponding to the id in entity_element
+            tablenames.append("entity_element")
+        for tablename_ in tablenames:
+            table = self._metadata.tables[tablename_]
+            delete = table.delete()
+            if Asterisk not in ids:
+                id_field = self._id_fields.get(tablename_, "id")
+                id_column = getattr(table.c, id_field)
+                cond = or_(*(and_(id_column >= first, id_column <= last) for first, last in group_consecutive(ids)))
+                delete = delete.where(cond)
+            try:
+                connection.execute(delete)
+            except DBAPIError as e:
+                msg = f"DBAPIError while removing {tablename_} items: {e.orig.args}"
+                raise SpineDBAPIError(msg) from e
```

### Comparing `spinedb_api-0.30.5/spinedb_api/exception.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/importers/sqlalchemy_connector.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,105 +1,95 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
+""" Contains SqlAlchemyConnector class. """
 
-"""
-Classes to handle exceptions while using the Spine database API.
 
-"""
+from sqlalchemy import create_engine, MetaData
+from sqlalchemy.orm import Session
+from .reader import SourceConnection
+
+
+class SqlAlchemyConnector(SourceConnection):
+    """Template class to read data from another QThread."""
+
+    DISPLAY_NAME = "SqlAlchemy"
+    """name of data source"""
+
+    OPTIONS = {}
+    """dict with option specification for source."""
+
+    FILE_EXTENSIONS = "*.sqlite"
+
+    def __init__(self, settings):
+        super().__init__(settings)
+        self._connection_string = None
+        self._engine = None
+        self._connection = None
+        self._session = None
+        self._schema = None
+        self._metadata = None
+
+    def connect_to_source(self, source, **extras):
+        """Saves source.
+
+        Args:
+            source (str): url
+            **extras: optional database schema
+        """
+        self._connection_string = source
+        self._engine = create_engine(source)
+        self._connection = self._engine.connect()
+        self._session = Session(self._engine)
+        self._schema = extras.get("schema")
+        self._metadata = MetaData(schema=self._schema)
+        self._metadata.reflect(bind=self._engine)
+
+    def disconnect(self):
+        """Disconnect from connected source."""
+        self._metadata = None
+        self._schema = None
+        self._session.close()
+        self._session = None
+        self._connection.close()
+        self._connection_string = None
+        self._engine = None
+        self._connection = None
+
+    def get_tables(self):
+        """Method that should return a list of table names, list(str)
+
+        Returns:
+            list of str: Table names in list
+        """
+        tables = list(self._engine.table_names(schema=self._schema))
+        return tables
+
+    def get_data_iterator(self, table, options, max_rows=-1):
+        """Creates an iterator for the database connection.
+
+        Args:
+            table (str): table name
+            options (dict): dict with options, not used
+            max_rows (int): how many rows of data to read, if -1 read all rows (default: {-1})
+
+        Returns:
+            tuple: iterator, header, column count
+        """
+        if self._schema is not None:
+            table = self._schema + "." + table
+        db_table = self._metadata.tables[table]
+        header = [str(name) for name in db_table.columns.keys()]
+
+        query = self._session.query(db_table)
+        if max_rows > 0:
+            query = query.limit(max_rows)
 
-
-class SpineDBAPIError(Exception):
-    """Basic exception for errors raised by the API."""
-
-    def __init__(self, msg=None):
-        super().__init__(msg)
-        self.msg = msg
-
-    def __str__(self):
-        return self.msg
-
-
-class SpineIntegrityError(SpineDBAPIError):
-    """Database integrity error while inserting/updating records.
-
-    Attributes:
-        msg (str): the message to be displayed
-        id (int): the id the instance that caused a unique violation
-    """
-
-    def __init__(self, msg=None, id=None):
-        super().__init__(msg)
-        self.id = id
-
-
-class SpineDBVersionError(SpineDBAPIError):
-    """Database version error."""
-
-    def __init__(self, url=None, current=None, expected=None, upgrade_available=True):
-        super().__init__(msg="The database at '{}' is not the expected version.".format(url))
-        self.url = url
-        self.current = current
-        self.expected = expected
-        self.upgrade_available = upgrade_available
-
-
-class SpineTableNotFoundError(SpineDBAPIError):
-    """Can't find one of the tables."""
-
-    def __init__(self, table, url=None):
-        super().__init__(msg="Table(s) '{}' couldn't be mapped from the database at '{}'.".format(table, url))
-        self.table = table
-
-
-class RecordNotFoundError(SpineDBAPIError):
-    """Can't find one record in one of the tables."""
-
-    def __init__(self, table, name=None, id=None):
-        super().__init__(msg="Unable to find item in table '{}'.".format(table))
-        self.table = table
-        self.name = name
-        self.id = id
-
-
-class ParameterValueError(SpineDBAPIError):
-    """The value given for a parameter does not fit the datatype."""
-
-    def __init__(self, value, data_type):
-        super().__init__(msg="The value {} does not fit the datatype '{}'.".format(value, data_type))
-        self.value = value
-        self.data_type = data_type
-
-
-class ParameterValueFormatError(SpineDBAPIError):
-    """
-    Failure in encoding/decoding a parameter value.
-
-    Attributes:
-        msg (str): an error message
-    """
-
-    def __init__(self, msg):
-        super().__init__(msg)
-
-
-class InvalidMapping(SpineDBAPIError):
-    """
-    Failure in import/export mapping
-    """
-
-    def __init__(self, msg):
-        super().__init__(msg)
-
-
-class InvalidMappingComponent(InvalidMapping):
-    def __init__(self, msg, rank=None, key=None):
-        super().__init__(msg)
-        self.rank = rank
-        self.key = key
+        return query, header
```

### Comparing `spinedb_api-0.30.5/spinedb_api/export_mapping/__init__.py` & `spinedb_api-0.31.0/spinedb_api/export_mapping/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -12,24 +13,17 @@
 This package contains facilities to map a Spine database into tables.
 
 """
 
 from .generator import rows, titles
 from .settings import (
     alternative_export,
-    feature_export,
-    object_export,
-    object_group_export,
-    object_parameter_default_value_export,
-    object_parameter_export,
+    entity_export,
+    entity_group_export,
+    entity_parameter_default_value_export,
+    entity_parameter_value_export,
     parameter_value_list_export,
-    relationship_export,
-    relationship_object_parameter_default_value_export,
-    relationship_object_parameter_export,
-    relationship_parameter_default_value_export,
-    relationship_parameter_export,
+    entity_dimension_parameter_default_value_export,
+    entity_dimension_parameter_value_export,
     scenario_alternative_export,
     scenario_export,
-    tool_export,
-    tool_feature_export,
-    tool_feature_method_export,
 )
```

### Comparing `spinedb_api-0.30.5/spinedb_api/export_mapping/export_mapping.py` & `spinedb_api-0.31.0/spinedb_api/export_mapping/export_mapping.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -175,29 +176,29 @@
 
     def add_query_columns(self, db_map, query):
         """Adds columns to the mapping query if needed, and returns the new query.
 
         The base class implementation just returns the same query without adding any new columns.
 
         Args:
-            db_map (DatabaseMappingBase)
+            db_map (DatabaseMapping)
             query (Alias or dict)
 
         Returns:
             Alias: expanded query, or the same if nothing to add.
         """
         return query
 
     def filter_query(self, db_map, query):
         """Filters the mapping query if needed, and returns the new query.
 
         The base class implementation just returns the same query without applying any new filters.
 
         Args:
-            db_map (DatabaseMappingBase)
+            db_map (DatabaseMapping)
             query (Alias or dict)
 
         Returns:
             Alias: filtered query, or the same if nothing to add.
         """
         return query
 
@@ -217,15 +218,15 @@
         """
         return query
 
     def _build_query(self, db_map, title_state):
         """Builds and returns the query to run for this mapping hierarchy.
 
         Args:
-            db_map (DatabaseMappingBase)
+            db_map (DatabaseMapping)
             title_state (dict)
 
         Returns:
             Query
         """
         mappings = self.flatten()
         # Start with empty query
@@ -247,15 +248,15 @@
             qry, lambda db_row: all(getattr(db_row, key) == value for key, value in title_state.items())
         )
 
     def _build_title_query(self, db_map):
         """Builds and returns the query to get titles for this mapping hierarchy.
 
         Args:
-            db_map (DatabaseMappingBase): database mapping
+            db_map (DatabaseMapping): database mapping
 
         Returns:
             Alias: title query
         """
         mappings = self.flatten()
         for _ in range(len(mappings)):
             if mappings[-1].position == Position.table_name:
@@ -271,26 +272,26 @@
             qry = m.filter_query(db_map, qry)
         return qry
 
     def _build_header_query(self, db_map, title_state, buddies):
         """Builds the header query for this mapping hierarchy.
 
         Args:
-            db_map (DatabaseMappingBase): database mapping
+            db_map (DatabaseMapping): database mapping
             title_state (dict): title state
             buddies (list of tuple): pairs of buddy mappings
 
         Returns:
             Alias: header query
         """
         mappings = self.flatten()
         flat_buddies = [b for pair in buddies for b in pair]
         for _ in range(len(mappings)):
             m = mappings[-1]
-            if m.position == Position.header or m.position == Position.table_name or m in flat_buddies:
+            if m.position in (Position.header, Position.table_name) or m in flat_buddies:
                 break
             mappings.pop(-1)
         # Start with empty query
         qry = db_map.query(literal(None))
         # Add columns
         for m in mappings:
             qry = m.add_query_columns(db_map, qry)
@@ -409,22 +410,22 @@
                 row.update(child_row)
                 yield row
 
     def rows(self, db_map, title_state):
         """Yields rows issued by this mapping and its children combined.
 
         Args:
-            db_map (DatabaseMappingBase)
+            db_map (DatabaseMapping)
             title_state (dict)
 
         Returns:
             generator(dict)
         """
         qry = self._build_query(db_map, title_state)
-        for db_row in qry.yield_per(1000):
+        for db_row in qry:
             yield from self.get_rows_recursive(db_row)
 
     def has_titles(self):
         """Returns True if this mapping or one of its children generates titles.
 
         Returns:
             bool: True if mappings generate titles, False otherwise
@@ -494,29 +495,29 @@
                 final_title = title + title_sep + child_title
                 yield final_title, {**title_state, **child_title_state}
 
     def _non_unique_titles(self, db_map, limit=None):
         """Yields all titles, not necessarily unique, and associated state dictionaries.
 
         Args:
-            db_map (DatabaseMappingBase): a database map
+            db_map (DatabaseMapping): a database map
             limit (int, optional): yield only this many items
 
         Yields:
             tuple(str,dict): title, and associated title state dictionary
         """
         qry = self._build_title_query(db_map)
-        for db_row in qry.yield_per(1000):
+        for db_row in qry:
             yield from self.get_titles_recursive(db_row, limit=limit)
 
     def titles(self, db_map, limit=None):
         """Yields unique titles and associated state dictionaries.
 
         Args:
-            db_map (DatabaseMappingBase): a database map
+            db_map (DatabaseMapping): a database map
             limit (int, optional): yield only this many items
 
         Yields:
             tuple(str,dict): unique title, and associated title state dictionary
         """
         titles = {}
         for title, title_state in self._non_unique_titles(db_map, limit=limit):
@@ -536,15 +537,15 @@
         return self.child.has_header()
 
     def make_header_recursive(self, query, buddies):
         """Builds the header recursively.
 
         Args:
             build_header_query (callable): a function that any mapping in the hierarchy can call to get the query
-            db_map (DatabaseMappingBase): database map
+            db_map (DatabaseMapping): database map
             title_state (dict): title state
             buddies (list of tuple): buddy mappings
 
         Returns
             dict: a mapping from column index to string header
         """
         if self.child is None:
@@ -553,32 +554,32 @@
             return {self.position: self.header}
         header = self.child.make_header_recursive(query, buddies)
         if self.position == Position.header:
             buddy = find_my_buddy(self, buddies)
             if buddy is not None:
                 query.rewind()
                 header[buddy.position] = next(
-                    (x for db_row in query for x in self._get_data_iterator(self._data(db_row))), ""
+                    (x for db_row in query for x in self._get_data_iterator(self._data(db_row)) if x), ""
                 )
         else:
             header[self.position] = self.header
         return header
 
     def make_header(self, db_map, title_state, buddies):
         """Returns the header for this mapping.
 
         Args:
-            db_map (DatabaseMappingBase): database map
+            db_map (DatabaseMapping): database map
             title_state (dict): title state
             buddies (list of tuple): buddy mappings
 
         Returns
             dict: a mapping from column index to string header
         """
-        query = _Rewindable(self._build_header_query(db_map, title_state, buddies).yield_per(1000))
+        query = _Rewindable(self._build_header_query(db_map, title_state, buddies))
         return self.make_header_recursive(query, buddies)
 
 
 def drop_non_positioned_tail(root_mapping):
     """Makes a modified mapping hierarchy without hidden tail mappings.
 
     This enables pivot tables to work correctly in certain situations.
@@ -620,313 +621,271 @@
         return None
 
     @staticmethod
     def id_field():
         return None
 
 
-class ObjectClassMapping(ExportMapping):
-    """Maps object classes.
+class EntityClassMapping(ExportMapping):
+    """Maps entity classes.
 
     Can be used as the topmost mapping.
     """
 
-    MAP_TYPE = "ObjectClass"
+    MAP_TYPE = "EntityClass"
+
+    def __init__(self, position, value=None, header="", filter_re="", highlight_position=None):
+        super().__init__(position, value, header, filter_re)
+        self.highlight_position = highlight_position
 
     def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.object_class_sq.c.id.label("object_class_id"),
-            db_map.object_class_sq.c.name.label("object_class_name"),
+        query = query.add_columns(
+            db_map.wide_entity_class_sq.c.id.label("entity_class_id"),
+            db_map.wide_entity_class_sq.c.name.label("entity_class_name"),
+            db_map.wide_entity_class_sq.c.dimension_id_list.label("dimension_id_list"),
+            db_map.wide_entity_class_sq.c.dimension_name_list.label("dimension_name_list"),
         )
+        if self.highlight_position is not None:
+            query = query.add_columns(db_map.entity_class_dimension_sq.c.dimension_id.label("highlighted_dimension_id"))
+        return query
+
+    def filter_query(self, db_map, query):
+        if any(isinstance(m, (DimensionMapping, ElementMapping)) for m in self.flatten()):
+            query = query.filter(db_map.wide_entity_class_sq.c.dimension_id_list != None)
+        else:
+            query = query.filter(db_map.wide_entity_class_sq.c.dimension_id_list == None)
+        if self.highlight_position is not None:
+            query = query.outerjoin(
+                db_map.entity_class_dimension_sq,
+                db_map.entity_class_dimension_sq.c.entity_class_id == db_map.wide_entity_class_sq.c.id,
+            ).filter(db_map.entity_class_dimension_sq.c.position == self.highlight_position)
+        return query
 
     @staticmethod
     def name_field():
-        return "object_class_name"
+        return "entity_class_name"
 
     @staticmethod
     def id_field():
         # Use the class name here, for the sake of the standard excel export
-        return "object_class_name"
+        return "entity_class_name"
 
+    def query_parents(self, what):
+        if what == "dimension":
+            return -1
+        if what == "highlight_position":
+            return self.highlight_position
+        return super().query_parents(what)
 
-class ObjectMapping(ExportMapping):
-    """Maps objects.
+    def _title_state(self, db_row):
+        state = super()._title_state(db_row)
+        state["dimension_id_list"] = getattr(db_row, "dimension_id_list")
+        return state
 
-    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectClassMapping`.
+    def to_dict(self):
+        mapping_dict = super().to_dict()
+        if self.highlight_position is not None:
+            mapping_dict["highlight_position"] = self.highlight_position
+        return mapping_dict
+
+    @classmethod
+    def reconstruct(cls, position, value, header, filter_re, ignorable, mapping_dict):
+        highlight_position = mapping_dict.get("highlight_position")
+        mapping = cls(position, value, header, filter_re, highlight_position)
+        mapping.set_ignorable(ignorable)
+        return mapping
+
+
+class EntityMapping(ExportMapping):
+    """Maps entities.
+
+    Cannot be used as the topmost mapping; one of the parents must be :class:`EntityClassMapping`.
     """
 
-    MAP_TYPE = "Object"
+    MAP_TYPE = "Entity"
 
     def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.object_sq.c.id.label("object_id"), db_map.object_sq.c.name.label("object_name"))
+        query = query.add_columns(
+            db_map.wide_entity_sq.c.id.label("entity_id"),
+            db_map.wide_entity_sq.c.name.label("entity_name"),
+            db_map.wide_entity_sq.c.element_id_list,
+            db_map.wide_entity_sq.c.element_name_list,
+        )
+        if self.query_parents("highlight_position") is not None:
+            query = query.add_columns(db_map.entity_element_sq.c.element_id.label("highlighted_element_id"))
+        return query
 
     def filter_query(self, db_map, query):
-        return query.outerjoin(db_map.object_sq, db_map.object_sq.c.class_id == db_map.object_class_sq.c.id)
+        query = query.outerjoin(
+            db_map.wide_entity_sq, db_map.wide_entity_sq.c.class_id == db_map.wide_entity_class_sq.c.id
+        )
+        if (highlight_position := self.query_parents("highlight_position")) is not None:
+            query = query.outerjoin(
+                db_map.entity_element_sq, db_map.entity_element_sq.c.entity_id == db_map.wide_entity_sq.c.id
+            ).filter(db_map.entity_element_sq.c.position == highlight_position)
+        return query
 
     @staticmethod
     def name_field():
-        return "object_name"
+        return "entity_name"
 
     @staticmethod
     def id_field():
-        return "object_id"
+        return "entity_id"
+
+    def query_parents(self, what):
+        if what == "dimension":
+            return -1
+        return super().query_parents(what)
+
+    def _title_state(self, db_row):
+        state = super()._title_state(db_row)
+        state["element_id_list"] = getattr(db_row, "element_id_list")
+        return state
 
     @staticmethod
     def is_buddy(parent):
-        return isinstance(parent, ObjectClassMapping)
+        return isinstance(parent, EntityClassMapping)
 
 
-class ObjectGroupMapping(ExportMapping):
-    """Maps object groups.
+class EntityGroupMapping(ExportMapping):
+    """Maps entity groups.
 
-    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectClassMapping`.
+    Cannot be used as the topmost mapping; one of the parents must be :class:`EntityClassMapping`.
     """
 
-    MAP_TYPE = "ObjectGroup"
+    MAP_TYPE = "EntityGroup"
 
     def add_query_columns(self, db_map, query):
         return query.add_columns(db_map.ext_entity_group_sq.c.group_id, db_map.ext_entity_group_sq.c.group_name)
 
     def filter_query(self, db_map, query):
         return query.outerjoin(
-            db_map.ext_entity_group_sq, db_map.ext_entity_group_sq.c.class_id == db_map.object_class_sq.c.id
+            db_map.ext_entity_group_sq, db_map.ext_entity_group_sq.c.class_id == db_map.wide_entity_class_sq.c.id
         ).distinct()
 
     @staticmethod
     def name_field():
         return "group_name"
 
     @staticmethod
     def id_field():
         return "group_id"
 
     @staticmethod
     def is_buddy(parent):
-        return isinstance(parent, ObjectClassMapping)
+        return isinstance(parent, EntityClassMapping)
 
 
-class ObjectGroupObjectMapping(ExportMapping):
-    """Maps objects in object groups.
+class EntityGroupEntityMapping(ExportMapping):
+    """Maps entities in objectentity groups.
 
-    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectGroupMapping`.
+    Cannot be used as the topmost mapping; one of the parents must be :class:`EntityGroupMapping`.
     """
 
-    MAP_TYPE = "ObjectGroupObject"
+    MAP_TYPE = "EntityGroupEntity"
 
     def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.object_sq.c.id.label("object_id"), db_map.object_sq.c.name.label("object_name"))
+        return query.add_columns(
+            db_map.wide_entity_sq.c.id.label("entity_id"), db_map.wide_entity_sq.c.name.label("entity_name")
+        )
 
     def filter_query(self, db_map, query):
-        return query.filter(db_map.ext_entity_group_sq.c.member_id == db_map.object_sq.c.id)
+        return query.filter(db_map.ext_entity_group_sq.c.member_id == db_map.wide_entity_sq.c.id)
 
     @staticmethod
     def name_field():
-        return "object_name"
+        return "entity_name"
 
     @staticmethod
     def id_field():
-        return "object_id"
+        return "entity_id"
 
     @staticmethod
     def is_buddy(parent):
-        return isinstance(parent, ObjectGroupMapping)
-
-
-class RelationshipClassMapping(ExportMapping):
-    """Maps relationships classes.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "RelationshipClass"
-
-    def __init__(self, position, value=None, header="", filter_re="", highlight_dimension=None):
-        super().__init__(position, value, header, filter_re)
-        self.highlight_dimension = highlight_dimension
-
-    def add_query_columns(self, db_map, query):
-        query = query.add_columns(
-            db_map.wide_relationship_class_sq.c.id.label("relationship_class_id"),
-            db_map.wide_relationship_class_sq.c.name.label("relationship_class_name"),
-            db_map.wide_relationship_class_sq.c.object_class_id_list,
-            db_map.wide_relationship_class_sq.c.object_class_name_list,
-        )
-        if self.highlight_dimension is not None:
-            query = query.add_columns(
-                db_map.ext_relationship_class_sq.c.object_class_id.label("highlighted_object_class_id")
-            )
-        return query
-
-    def filter_query(self, db_map, query):
-        if self.highlight_dimension is not None:
-            query = query.outerjoin(
-                db_map.ext_relationship_class_sq,
-                db_map.ext_relationship_class_sq.c.id == db_map.wide_relationship_class_sq.c.id,
-            ).filter(db_map.ext_relationship_class_sq.c.dimension == self.highlight_dimension)
-        return query
-
-    @staticmethod
-    def name_field():
-        return "relationship_class_name"
-
-    @staticmethod
-    def id_field():
-        # Use the class name here, for the sake of the standard excel export
-        return "relationship_class_name"
-
-    def query_parents(self, what):
-        if what == "dimension":
-            return -1
-        if what == "highlight_dimension":
-            return self.highlight_dimension
-        return super().query_parents(what)
-
-    def _title_state(self, db_row):
-        state = super()._title_state(db_row)
-        state["object_class_id_list"] = getattr(db_row, "object_class_id_list")
-        return state
-
-    def to_dict(self):
-        mapping_dict = super().to_dict()
-        if self.highlight_dimension is not None:
-            mapping_dict["highlight_dimension"] = self.highlight_dimension
-        return mapping_dict
-
-    @classmethod
-    def reconstruct(cls, position, value, header, filter_re, ignorable, mapping_dict):
-        highlight_dimension = mapping_dict.get("highlight_dimension")
-        mapping = cls(position, value, header, filter_re, highlight_dimension)
-        mapping.set_ignorable(ignorable)
-        return mapping
+        return isinstance(parent, EntityGroupMapping)
 
 
-class RelationshipClassObjectClassMapping(ExportMapping):
-    """Maps relationship class object classes.
+class DimensionMapping(ExportMapping):
+    """Maps dimensions.
 
-    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
+    Cannot be used as the topmost mapping; one of the parents must be :class:`EntityClassMapping`.
     """
 
-    MAP_TYPE = "RelationshipClassObjectClass"
+    MAP_TYPE = "Dimension"
     _cached_dimension = None
 
     @staticmethod
     def name_field():
-        return "object_class_name_list"
+        return "dimension_name_list"
 
     @staticmethod
     def id_field():
-        return "object_class_id_list"
+        return "dimension_id_list"
 
     def _data(self, db_row):
-        data = super()._data(db_row).split(",")
+        dimension_name_list = super()._data(db_row)
+        if dimension_name_list is None:
+            return None
+        data = dimension_name_list.split(",")
         if self._cached_dimension is None:
             self._cached_dimension = self.query_parents("dimension")
         try:
             return data[self._cached_dimension]
         except IndexError:
             return ""
 
     def query_parents(self, what):
         if what != "dimension":
             return super().query_parents(what)
         return self.parent.query_parents(what) + 1
 
     @staticmethod
     def is_buddy(parent):
-        return isinstance(parent, RelationshipClassMapping)
-
-
-class RelationshipMapping(ExportMapping):
-    """Maps relationships.
+        return isinstance(parent, EntityClassMapping)
 
-    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
-    """
 
-    MAP_TYPE = "Relationship"
+class ElementMapping(ExportMapping):
+    """Maps elements.
 
-    def add_query_columns(self, db_map, query):
-        query = query.add_columns(
-            db_map.wide_relationship_sq.c.id.label("relationship_id"),
-            db_map.wide_relationship_sq.c.name.label("relationship_name"),
-            db_map.wide_relationship_sq.c.object_id_list,
-            db_map.wide_relationship_sq.c.object_name_list,
-        )
-        if self.query_parents("highlight_dimension") is not None:
-            query = query.add_columns(db_map.ext_relationship_sq.c.object_id.label("highlighted_object_id"))
-        return query
-
-    def filter_query(self, db_map, query):
-        query = query.outerjoin(
-            db_map.wide_relationship_sq,
-            db_map.wide_relationship_sq.c.class_id == db_map.wide_relationship_class_sq.c.id,
-        )
-        if (highlight_dimension := self.query_parents("highlight_dimension")) is not None:
-            query = query.outerjoin(
-                db_map.ext_relationship_sq, db_map.ext_relationship_sq.c.id == db_map.wide_relationship_sq.c.id
-            ).filter(db_map.ext_relationship_sq.c.dimension == highlight_dimension)
-        return query
-
-    @staticmethod
-    def name_field():
-        return "relationship_name"
-
-    @staticmethod
-    def id_field():
-        return "relationship_id"
-
-    def query_parents(self, what):
-        if what != "dimension":
-            return super().query_parents(what)
-        return -1
-
-    def _title_state(self, db_row):
-        state = super()._title_state(db_row)
-        state["object_id_list"] = getattr(db_row, "object_id_list")
-        return state
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, RelationshipClassMapping)
-
-
-class RelationshipObjectMapping(ExportMapping):
-    """Maps relationship's objects.
-
-    Cannot be used as the topmost mapping; must have :class:`RelationshipClassMapping` and :class:`RelationshipMapping`
+    Cannot be used as the topmost mapping; must have :class:`EntityClassMapping` and :class:`EntityMapping`
     as parents.
     """
 
-    MAP_TYPE = "RelationshipObject"
+    MAP_TYPE = "Element"
     _cached_dimension = None
 
     @staticmethod
     def name_field():
-        return "object_name_list"
+        return "element_name_list"
 
     @staticmethod
     def id_field():
-        return "object_id_list"
+        return "element_id_list"
 
     def _data(self, db_row):
-        data = super()._data(db_row).split(",")
+        element_name_list = super()._data(db_row)
+        if element_name_list is None:
+            return None
+        data = element_name_list.split(",")
         if self._cached_dimension is None:
             self._cached_dimension = self.query_parents("dimension")
         try:
             return data[self._cached_dimension]
         except IndexError:
             return ""
 
     def query_parents(self, what):
         if what != "dimension":
             return super().query_parents(what)
         return self.parent.query_parents(what) + 1
 
     @staticmethod
     def is_buddy(parent):
-        return isinstance(parent, RelationshipClassObjectClassMapping)
+        return isinstance(parent, DimensionMapping)
 
 
 class ParameterDefinitionMapping(ExportMapping):
     """Maps parameter definitions.
 
     Cannot be used as the topmost mapping; must have an entity class mapping as one of parents.
     """
@@ -936,32 +895,23 @@
     def add_query_columns(self, db_map, query):
         return query.add_columns(
             db_map.parameter_definition_sq.c.id.label("parameter_definition_id"),
             db_map.parameter_definition_sq.c.name.label("parameter_definition_name"),
         )
 
     def filter_query(self, db_map, query):
-        column_names = {c["name"] for c in query.column_descriptions}
-        if "object_class_id" in column_names:
+        if self.query_parents("highlight_position") is not None:
             return query.outerjoin(
                 db_map.parameter_definition_sq,
-                db_map.parameter_definition_sq.c.object_class_id == db_map.object_class_sq.c.id,
+                db_map.parameter_definition_sq.c.entity_class_id == db_map.entity_class_dimension_sq.c.dimension_id,
             )
-        if "relationship_class_id" in column_names:
-            if self.query_parents("highlight_dimension") is not None:
-                return query.outerjoin(
-                    db_map.parameter_definition_sq,
-                    db_map.parameter_definition_sq.c.object_class_id
-                    == db_map.ext_relationship_class_sq.c.object_class_id,
-                )
-            return query.outerjoin(
-                db_map.parameter_definition_sq,
-                db_map.parameter_definition_sq.c.relationship_class_id == db_map.wide_relationship_class_sq.c.id,
-            )
-        raise RuntimeError("Logic error: this code should be unreachable.")
+        return query.outerjoin(
+            db_map.parameter_definition_sq,
+            db_map.parameter_definition_sq.c.entity_class_id == db_map.wide_entity_class_sq.c.id,
+        )
 
     @staticmethod
     def name_field():
         return "parameter_definition_name"
 
     @staticmethod
     def id_field():
@@ -1057,15 +1007,15 @@
 
     Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
     """
 
     MAP_TYPE = "ParameterDefaultValueIndex"
 
     def add_query_columns(self, db_map, query):
-        if "default_value" in {c["name"] for c in query.column_descriptions}:
+        if "default_value" in set(query.column_names()):
             return query
         return query.add_columns(
             db_map.parameter_definition_sq.c.default_value, db_map.parameter_definition_sq.c.default_type
         )
 
     def _expand_data(self, data):
         yield from _expand_indexed_data(data, self)
@@ -1117,60 +1067,50 @@
     an :class:`AlternativeMapping` as parents.
     """
 
     MAP_TYPE = "ParameterValue"
     _selects_value = False
 
     def add_query_columns(self, db_map, query):
-        if "value" in {c["name"] for c in query.column_descriptions}:
+        if "value" in set(query.column_names()):
             return query
         self._selects_value = True
         return query.add_columns(db_map.parameter_value_sq.c.value, db_map.parameter_value_sq.c.type)
 
     def filter_query(self, db_map, query):
         if not self._selects_value:
             return query
-        column_names = {c["name"] for c in query.column_descriptions}
-        if "object_id" in column_names:
+        if self.query_parents("highlight_position") is not None:
             return query.filter(
                 and_(
-                    db_map.parameter_value_sq.c.object_id == db_map.object_sq.c.id,
+                    db_map.parameter_value_sq.c.entity_id == db_map.entity_element_sq.c.element_id,
                     db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
                 )
             )
-        if "relationship_id" in column_names:
-            if self.query_parents("highlight_dimension") is not None:
-                return query.filter(
-                    and_(
-                        db_map.parameter_value_sq.c.object_id == db_map.ext_relationship_sq.c.object_id,
-                        db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
-                    )
-                )
-            return query.filter(
-                and_(
-                    db_map.parameter_value_sq.c.relationship_id == db_map.wide_relationship_sq.c.id,
-                    db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
-                )
+        return query.filter(
+            and_(
+                db_map.parameter_value_sq.c.entity_id == db_map.wide_entity_sq.c.id,
+                db_map.parameter_value_sq.c.parameter_definition_id == db_map.parameter_definition_sq.c.id,
             )
-        raise RuntimeError("Logic error: this code should be unreachable.")
+        )
 
     @staticmethod
     def name_field():
         return None
 
     @staticmethod
     def id_field():
         return None
 
     def _data(self, db_row):
         return from_database_to_single_value(db_row.value, db_row.type)
 
     @staticmethod
     def is_buddy(parent):
-        return isinstance(parent, (ParameterDefinitionMapping, ObjectMapping, RelationshipMapping, AlternativeMapping))
+        return isinstance(parent, (ParameterDefinitionMapping, EntityMapping, AlternativeMapping))
 
 
 class ParameterValueTypeMapping(ParameterValueMapping):
     """Maps parameter value types.
 
     Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping`, an entity mapping and
     an :class:`AlternativeMapping` as parents.
@@ -1189,15 +1129,15 @@
     def _title_state(self, db_row):
         return {"type_and_dimensions": (db_row.type, from_database_to_dimension_count(db_row.value, db_row.type))}
 
     def filter_query_by_title(self, query, title_state):
         pv = title_state.pop("type_and_dimensions", None)
         if pv is None:
             return query
-        if "value" not in {c["name"] for c in query.column_descriptions}:
+        if "value" not in set(query.column_names()):
             return query
         return _FilteredQuery(
             query, lambda db_row: (db_row.type, from_database_to_dimension_count(db_row.value, db_row.type) == pv)
         )
 
 
 class IndexNameMapping(_MappingWithLeafMixin, ParameterValueMapping):
@@ -1336,17 +1276,20 @@
         return query.add_columns(
             db_map.alternative_sq.c.id.label("alternative_id"),
             db_map.alternative_sq.c.name.label("alternative_name"),
             db_map.alternative_sq.c.description.label("description"),
         )
 
     def filter_query(self, db_map, query):
-        if self.parent is None:
-            return query
-        return query.filter(db_map.alternative_sq.c.id == db_map.parameter_value_sq.c.alternative_id)
+        parent = self.parent
+        while parent is not None:
+            if isinstance(parent, ParameterDefinitionMapping):
+                return query.filter(db_map.alternative_sq.c.id == db_map.parameter_value_sq.c.alternative_id)
+            parent = parent.parent
+        return query
 
     @staticmethod
     def name_field():
         return "alternative_name"
 
     @staticmethod
     def id_field():
@@ -1453,235 +1396,14 @@
         return "before_alternative_id"
 
     @staticmethod
     def is_buddy(parent):
         return isinstance(parent, ScenarioAlternativeMapping)
 
 
-class FeatureEntityClassMapping(ExportMapping):
-    """Maps feature entity classes.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "FeatureEntityClass"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.ext_feature_sq.c.entity_class_id, db_map.ext_feature_sq.c.entity_class_name)
-
-    @staticmethod
-    def name_field():
-        return "entity_class_name"
-
-    @staticmethod
-    def id_field():
-        return "entity_class_id"
-
-
-class FeatureParameterDefinitionMapping(ExportMapping):
-    """Maps feature parameter definitions.
-
-    Cannot be used as the topmost mapping; must have a :class:`FeatureEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "FeatureParameterDefinition"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_feature_sq.c.parameter_definition_id, db_map.ext_feature_sq.c.parameter_definition_name
-        )
-
-    @staticmethod
-    def name_field():
-        return "parameter_definition_name"
-
-    @staticmethod
-    def id_field():
-        return "parameter_definition_id"
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, FeatureEntityClassMapping)
-
-
-class ToolMapping(ExportMapping):
-    """Maps tools.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "Tool"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.tool_sq.c.id.label("tool_id"), db_map.tool_sq.c.name.label("tool_name"))
-
-    @staticmethod
-    def name_field():
-        return "tool_name"
-
-    @staticmethod
-    def id_field():
-        return "tool_id"
-
-
-class ToolFeatureEntityClassMapping(ExportMapping):
-    """Maps tool feature entity classes.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureEntityClass"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_tool_feature_sq.c.entity_class_id, db_map.ext_tool_feature_sq.c.entity_class_name
-        )
-
-    def filter_query(self, db_map, query):
-        return query.outerjoin(db_map.ext_tool_feature_sq, db_map.ext_tool_feature_sq.c.tool_id == db_map.tool_sq.c.id)
-
-    @staticmethod
-    def name_field():
-        return "entity_class_name"
-
-    @staticmethod
-    def id_field():
-        return "entity_class_id"
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, ToolMapping)
-
-
-class ToolFeatureParameterDefinitionMapping(ExportMapping):
-    """Maps tool feature parameter definitions.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureParameterDefinition"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_tool_feature_sq.c.parameter_definition_id, db_map.ext_tool_feature_sq.c.parameter_definition_name
-        )
-
-    @staticmethod
-    def name_field():
-        return "parameter_definition_name"
-
-    @staticmethod
-    def id_field():
-        return "parameter_definition_id"
-
-    @staticmethod
-    def is_buddy(parent):
-        return isinstance(parent, ToolFeatureEntityClassMapping)
-
-
-class ToolFeatureRequiredFlagMapping(ExportMapping):
-    """Maps tool feature required flags.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureRequiredFlag"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.ext_tool_feature_sq.c.required)
-
-    @staticmethod
-    def name_field():
-        return "required"
-
-    @staticmethod
-    def id_field():
-        return "required"
-
-
-class ToolFeatureMethodEntityClassMapping(ExportMapping):
-    """Maps tool feature method entity classes.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureMethodEntityClass"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_tool_feature_sq.c.entity_class_id, db_map.ext_tool_feature_sq.c.entity_class_name
-        )
-
-    def filter_query(self, db_map, query):
-        return query.outerjoin(db_map.ext_tool_feature_sq, db_map.ext_tool_feature_sq.c.tool_id == db_map.tool_sq.c.id)
-
-    @staticmethod
-    def name_field():
-        return "entity_class_name"
-
-    @staticmethod
-    def id_field():
-        return "entity_class_id"
-
-
-class ToolFeatureMethodParameterDefinitionMapping(ExportMapping):
-    """Maps tool feature method parameter definitions.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureMethodParameterDefinition"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(
-            db_map.ext_tool_feature_sq.c.parameter_definition_id, db_map.ext_tool_feature_sq.c.parameter_definition_name
-        )
-
-    @staticmethod
-    def name_field():
-        return "parameter_definition_name"
-
-    @staticmethod
-    def id_field():
-        return "parameter_definition_id"
-
-
-class ToolFeatureMethodMethodMapping(ExportMapping):
-    """Maps tool feature method methods.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
-    """
-
-    MAP_TYPE = "ToolFeatureMethodMethod"
-
-    def add_query_columns(self, db_map, query):
-        return query.add_columns(db_map.ext_tool_feature_method_sq.c.method)
-
-    def filter_query(self, db_map, query):
-        return query.outerjoin(
-            db_map.ext_tool_feature_method_sq,
-            and_(
-                db_map.ext_tool_feature_method_sq.c.tool_id == db_map.ext_tool_feature_sq.c.tool_id,
-                db_map.ext_tool_feature_method_sq.c.feature_id == db_map.ext_tool_feature_sq.c.feature_id,
-            ),
-        )
-
-    @staticmethod
-    def name_field():
-        return "method"
-
-    @staticmethod
-    def id_field():
-        return "method"
-
-    def _data(self, db_row):
-        data = super()._data(db_row)
-        return from_database_to_single_value(data, None)
-
-
 class _DescriptionMappingBase(ExportMapping):
     """Maps descriptions."""
 
     MAP_TYPE = "Description"
 
     @staticmethod
     def name_field():
@@ -1721,17 +1443,14 @@
         Args:
             query (Query): a query to filter
             condition (function): the filter condition
         """
         self._query = query
         self._condition = condition
 
-    def yield_per(self, count):
-        return _FilteredQuery(self._query.yield_per(count), self._condition)
-
     def filter(self, *args, **kwargs):
         return _FilteredQuery(self._query.filter(*args, **kwargs), self._condition)
 
     def __iter__(self):
         for db_row in self._query:
             if self._condition(db_row):
                 yield db_row
@@ -1815,56 +1534,68 @@
     """
     mappings = {
         klass.MAP_TYPE: klass
         for klass in (
             AlternativeDescriptionMapping,
             AlternativeMapping,
             DefaultValueIndexNameMapping,
+            DimensionMapping,
+            ElementMapping,
             ExpandedParameterDefaultValueMapping,
             ExpandedParameterValueMapping,
-            FeatureEntityClassMapping,
-            FeatureParameterDefinitionMapping,
             FixedValueMapping,
             IndexNameMapping,
-            ObjectClassMapping,
-            ObjectGroupMapping,
-            ObjectGroupObjectMapping,
-            ObjectMapping,
+            EntityClassMapping,
+            EntityGroupMapping,
+            EntityGroupEntityMapping,
+            EntityMapping,
             ParameterDefaultValueIndexMapping,
             ParameterDefaultValueMapping,
             ParameterDefaultValueTypeMapping,
             ParameterDefinitionMapping,
             ParameterValueIndexMapping,
             ParameterValueListMapping,
             ParameterValueListValueMapping,
             ParameterValueMapping,
             ParameterValueTypeMapping,
-            RelationshipClassMapping,
-            RelationshipClassObjectClassMapping,
-            RelationshipMapping,
-            RelationshipObjectMapping,
             ScenarioActiveFlagMapping,
             ScenarioAlternativeMapping,
             ScenarioBeforeAlternativeMapping,
             ScenarioDescriptionMapping,
             ScenarioMapping,
-            ToolMapping,
-            ToolFeatureEntityClassMapping,
-            ToolFeatureParameterDefinitionMapping,
-            ToolFeatureRequiredFlagMapping,
-            ToolFeatureMethodEntityClassMapping,
-            ToolFeatureMethodParameterDefinitionMapping,
+            # FIXME
+            # FeatureEntityClassMapping,
+            # FeatureParameterDefinitionMapping,
+            # ToolMapping,
+            # ToolFeatureEntityClassMapping,
+            # ToolFeatureParameterDefinitionMapping,
+            # ToolFeatureRequiredFlagMapping,
+            # ToolFeatureMethodEntityClassMapping,
+            # ToolFeatureMethodParameterDefinitionMapping,
         )
     }
-    # Legacy
-    mappings["ParameterIndex"] = ParameterValueIndexMapping
-    if any(m["map_type"] == "RelationshipClassObjectHighlightingMapping" for m in serialized):
-        _upgrade_legacy_object_highlighting_mapping(serialized)
+    legacy_mappings = {
+        "ParameterIndex": ParameterValueIndexMapping,
+        "ObjectClass": EntityClassMapping,
+        "ObjectGroup": EntityGroupMapping,
+        "ObjectGroupObject": EntityGroupEntityMapping,
+        "Object": EntityMapping,
+        "RelationshipClass": EntityClassMapping,
+        "RelationshipClassObjectClass": DimensionMapping,
+        "Relationship": EntityMapping,
+        "RelationshipObject": ElementMapping,
+        "RelationshipClassObjectHighlightingMapping": EntityClassMapping,
+        "RelationshipObjectHighlightingMapping": ElementMapping,
+    }
+    mappings.update(legacy_mappings)
     flattened = list()
     for mapping_dict in serialized:
+        if (highlight_position := mapping_dict.get("highlight_dimension")) is not None:
+            # legacy
+            mapping_dict["highlight_position"] = highlight_position
         position = mapping_dict["position"]
         if isinstance(position, str):
             position = Position(position)
         ignorable = mapping_dict.get("ignorable", False)
         value = mapping_dict.get("value")
         header = mapping_dict.get("header", "")
         filter_re = mapping_dict.get("filter_re", "")
@@ -1888,30 +1619,14 @@
     for mapping_dict in serialized:
         group_fn = mapping_dict.get("group_fn")
         if group_fn is not None:
             return group_fn
     return NoGroup.NAME
 
 
-def _upgrade_legacy_object_highlighting_mapping(serialized):
-    """Upgrades legacy object highlighting mappings in place.
-
-    ``RelationshipClassObjectHighlightingMapping`` and ``RelationshipObjectHighlightingMapping``
-    have been replaced by a ``highlight_dimension`` argument in ``RelationshipClassObjectClassMapping``.
-
-    Args:
-        serialized (list of dict): serialized mappings
-    """
-    for mapping_dict in serialized:
-        if mapping_dict["map_type"] == "RelationshipClassObjectHighlightingMapping":
-            mapping_dict["map_type"] = RelationshipClassMapping.MAP_TYPE
-        elif mapping_dict["map_type"] == "RelationshipObjectHighlightingMapping":
-            mapping_dict["map_type"] = RelationshipMapping.MAP_TYPE
-
-
 def _expand_indexed_data(data, mapping):
     """Expands indexed data and updates the current_leaf attribute.
 
     Args:
         data (Any): data to expand
         mapping (ExportMapping): mapping whose data is being expanded
```

### Comparing `spinedb_api-0.30.5/spinedb_api/export_mapping/generator.py` & `spinedb_api-0.31.0/spinedb_api/export_mapping/generator.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -21,15 +22,15 @@
 
 def rows(root_mapping, db_map, fixed_state=None, empty_data_header=True, group_fn=NoGroup.NAME):
     """
     Generates table's rows.
 
     Args:
         root_mapping (Mapping): root export mapping
-        db_map (DatabaseMappingBase): a database map
+        db_map (DatabaseMapping): a database map
         fixed_state (dict, optional): mapping state that fixes items
         empty_data_header (bool): True to yield at least header rows even if there is no data, False to yield nothing
         group_fn (str): group function name
 
     Yields:
         list: a list of row's cells
     """
@@ -94,15 +95,15 @@
 
 def titles(root_mapping, db_map, limit=None):
     """
     Generates titles.
 
     Args:
         root_mapping (Mapping): root export mapping
-        db_map (DatabaseMappingBase): a database map
+        db_map (DatabaseMapping): a database map
 
     Yield:
         tuple: title and title's fixed key
     """
     if not root_mapping.has_titles():
         yield None, None
         return
```

### Comparing `spinedb_api-0.30.5/spinedb_api/export_mapping/group_functions.py` & `spinedb_api-0.31.0/spinedb_api/export_mapping/group_functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/export_mapping/pivot.py` & `spinedb_api-0.31.0/spinedb_api/export_mapping/pivot.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 """
 Contains functions and methods to turn a regular export table into a pivot table
 
 """
 from copy import deepcopy
 
-from .export_mapping import RelationshipMapping
+from .export_mapping import EntityMapping
 from ..mapping import is_regular, is_pivoted, Position, unflatten, value_index
 from .group_functions import from_str as group_function_from_str, NoGroup
 
 
 def make_pivot(
     table, header, value_column, regular_columns, hidden_columns, pivot_columns, group_fn=None, empty_data_header=True
 ):
@@ -230,9 +231,9 @@
     for i, mapping in enumerate(mappings[value_i + 1 :]):
         mapping.position = value_column + i + 1
     mappings[value_i].position = value_column
     return unflatten(mappings), value_column, regular_columns, hidden_columns, sorted(pivot_columns)
 
 
 def _is_unhiddable(mapping):
-    """Returns True if mapping uhiddable for pivoting purposes."""
-    return not isinstance(mapping, RelationshipMapping)
+    """Returns True if mapping unhiddable for pivoting purposes."""
+    return not isinstance(mapping, EntityMapping)  # FIXME: Maybe also check that dimension_count > 0 ??
```

### Comparing `spinedb_api-0.30.5/spinedb_api/export_mapping/settings.py` & `spinedb_api-0.31.0/spinedb_api/export_mapping/settings.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -13,404 +14,296 @@
 
 """
 from itertools import takewhile
 
 from .export_mapping import (
     AlternativeMapping,
     AlternativeDescriptionMapping,
+    DimensionMapping,
+    ElementMapping,
     ExpandedParameterDefaultValueMapping,
     ExpandedParameterValueMapping,
-    FeatureEntityClassMapping,
-    FeatureParameterDefinitionMapping,
-    ObjectGroupMapping,
-    ObjectGroupObjectMapping,
-    ObjectMapping,
-    ObjectClassMapping,
+    EntityGroupMapping,
+    EntityGroupEntityMapping,
+    EntityMapping,
+    EntityClassMapping,
     ParameterDefaultValueMapping,
     ParameterDefaultValueIndexMapping,
     ParameterDefinitionMapping,
     ParameterValueIndexMapping,
     ParameterValueListMapping,
     ParameterValueListValueMapping,
     ParameterValueMapping,
     ParameterValueTypeMapping,
     Position,
-    RelationshipClassMapping,
-    RelationshipClassObjectClassMapping,
-    RelationshipMapping,
-    RelationshipObjectMapping,
     ScenarioActiveFlagMapping,
     ScenarioAlternativeMapping,
     ScenarioBeforeAlternativeMapping,
     ScenarioMapping,
     ScenarioDescriptionMapping,
-    ToolFeatureEntityClassMapping,
-    ToolFeatureMethodMethodMapping,
-    ToolFeatureMethodEntityClassMapping,
-    ToolFeatureMethodParameterDefinitionMapping,
-    ToolFeatureParameterDefinitionMapping,
-    ToolFeatureRequiredFlagMapping,
-    ToolMapping,
     IndexNameMapping,
     DefaultValueIndexNameMapping,
     ParameterDefaultValueTypeMapping,
 )
 from ..mapping import unflatten
 
 
-def object_export(class_position=Position.hidden, object_position=Position.hidden):
-    """
-    Sets up export mappings for exporting objects without parameters.
-
-    Args:
-        class_position (int or Position): position of object classes
-        object_position (int or Position): position of objects
-
-    Returns:
-        ExportMapping: root mapping
-    """
-    class_ = ObjectClassMapping(class_position)
-    object_ = ObjectMapping(object_position)
-    class_.child = object_
-    return class_
-
-
-def object_parameter_default_value_export(
-    class_position=Position.hidden,
-    definition_position=Position.hidden,
-    value_type_position=Position.hidden,
-    value_position=Position.hidden,
-    index_name_positions=None,
-    index_positions=None,
-):
-    """
-    Sets up export mappings for exporting objects classes and default parameter values.
-
-    Args:
-        class_position (int or Position): position of object classes
-        definition_position (int or Position): position of parameter names
-        value_type_position (int or Position): position of parameter value types
-        value_position (int or Position): position of parameter values
-        index_name_positions (list of int, optional): positions of index names
-        index_positions (list of int, optional): positions of parameter indexes
-
-    Returns:
-        ExportMapping: root mapping
-    """
-    class_ = ObjectClassMapping(class_position)
-    definition = ParameterDefinitionMapping(definition_position)
-    _generate_default_value_mappings(
-        definition, value_type_position, value_position, index_name_positions, index_positions
-    )
-    class_.child = definition
-    return class_
-
-
-def object_parameter_export(
-    class_position=Position.hidden,
-    definition_position=Position.hidden,
-    value_list_position=Position.hidden,
-    object_position=Position.hidden,
-    alternative_position=Position.hidden,
-    value_type_position=Position.hidden,
-    value_position=Position.hidden,
-    index_name_positions=None,
-    index_positions=None,
-):
-    """
-    Sets up export mappings for exporting objects and object parameters.
-
-    Args:
-        class_position (int or Position): position of object classes in a table
-        definition_position (int or Position): position of parameter names in a table
-        value_list_position (int or Position): position of parameter value lists
-        object_position (int or Position): position of objects in a table
-        alternative_position (int or Position): position of alternatives in a table
-        value_type_position (int or Position): position of parameter value types in a table
-        value_position (int or Position): position of parameter values in a table
-        index_name_positions (list of int, optional): positions of index names
-        index_positions (list of int, optional): positions of parameter indexes in a table
-
-    Returns:
-        ExportMapping: root mapping
-    """
-    class_ = ObjectClassMapping(class_position)
-    definition = ParameterDefinitionMapping(definition_position)
-    value_list = ParameterValueListMapping(value_list_position)
-    value_list.set_ignorable(True)
-    object_ = ObjectMapping(object_position)
-    _generate_parameter_value_mappings(
-        object_, alternative_position, value_type_position, value_position, index_name_positions, index_positions
-    )
-    value_list.child = object_
-    definition.child = value_list
-    class_.child = definition
-    return class_
-
-
-def object_group_export(
-    class_position=Position.hidden, group_position=Position.hidden, object_position=Position.hidden
+def entity_group_export(
+    entity_class_position=Position.hidden, group_position=Position.hidden, entity_position=Position.hidden
 ):
     """
-    Sets up export mappings for exporting object groups.
+    Sets up export mappings for exporting entity groups.
 
     Args:
-        class_position (int or Position): position of object classes
+        entity_class_position (int or Position): position of entity classes
         group_position (int or Position): position of groups
-        object_position (int or Position): position of objects
+        entity_position (int or Position): position of entities
 
     Returns:
         ExportMapping: root mapping
     """
-    class_ = ObjectClassMapping(class_position)
-    group = ObjectGroupMapping(group_position)
-    object_ = ObjectGroupObjectMapping(object_position)
-    group.child = object_
+    class_ = EntityClassMapping(entity_class_position)
+    group = EntityGroupMapping(group_position)
+    entity = EntityGroupEntityMapping(entity_position)
+    group.child = entity
     class_.child = group
     return class_
 
 
-def relationship_export(
-    relationship_class_position=Position.hidden,
-    relationship_position=Position.hidden,
-    object_class_positions=None,
-    object_positions=None,
+def entity_export(
+    entity_class_position=Position.hidden,
+    entity_position=Position.hidden,
+    dimension_positions=None,
+    element_positions=None,
 ):
     """
-    Sets up export items for exporting relationships without parameters.
+    Sets up export items for exporting entities without parameters.
 
     Args:
-        relationship_class_position (int or Position): position of relationship classes in a table
-        relationship_position (int or Position): position of relationships in a table
-        object_class_positions (Iterable, optional): positions of object classes in a table
-        object_positions (Iterable, optional): positions of object in a table
+        entity_class_position (int or Position): position of entity classes in a table
+        entity_position (int or Position): position of entities in a table
+        dimension_positions (Iterable, optional): positions of dimension in a table
+        element_positions (Iterable, optional): positions of element in a table
 
     Returns:
         ExportMapping: root mapping
     """
-    if object_class_positions is None:
-        object_class_positions = list()
-    if object_positions is None:
-        object_positions = list()
-    relationship_class = RelationshipClassMapping(relationship_class_position)
-    object_or_relationship_class = _generate_dimensions(
-        relationship_class, RelationshipClassObjectClassMapping, object_class_positions
-    )
-    relationship = RelationshipMapping(relationship_position)
-    object_or_relationship_class.child = relationship
-    _generate_dimensions(relationship, RelationshipObjectMapping, object_positions)
-    return relationship_class
+    if dimension_positions is None:
+        dimension_positions = list()
+    if element_positions is None:
+        element_positions = list()
+    entity_class = EntityClassMapping(entity_class_position)
+    dimension = _generate_dimensions(entity_class, DimensionMapping, dimension_positions)
+    entity = EntityMapping(entity_position)
+    dimension.child = entity
+    _generate_dimensions(entity, ElementMapping, element_positions)
+    return entity_class
 
 
-def relationship_parameter_default_value_export(
-    relationship_class_position=Position.hidden,
+def entity_parameter_default_value_export(
+    entity_class_position=Position.hidden,
     definition_position=Position.hidden,
     value_type_position=Position.hidden,
     value_position=Position.hidden,
     index_name_positions=None,
     index_positions=None,
 ):
     """
-    Sets up export mappings for exporting relationship classes and default parameter values.
+    Sets up export mappings for exporting entity classes and default parameter values.
 
     Args:
-        relationship_class_position (int or Position): position of relationship classes
+        entity_class_position (int or Position): position of relationship classes
         definition_position (int or Position): position of parameter definitions
         value_type_position (int or Position): position of parameter value types
         value_position (int or Position): position of parameter values
         index_name_positions (list of int, optional): positions of index names
         index_positions (list of int, optional): positions of parameter indexes
 
     Returns:
         ExportMapping: root mapping
     """
-    relationship_class = RelationshipClassMapping(relationship_class_position)
+    entity_class = EntityClassMapping(entity_class_position)
     definition = ParameterDefinitionMapping(definition_position)
     _generate_default_value_mappings(
         definition, value_type_position, value_position, index_name_positions, index_positions
     )
-    relationship_class.child = definition
-    return relationship_class
+    entity_class.child = definition
+    return entity_class
 
 
-def relationship_parameter_export(
-    relationship_class_position=Position.hidden,
+def entity_parameter_value_export(
+    entity_class_position=Position.hidden,
     definition_position=Position.hidden,
     value_list_position=Position.hidden,
-    relationship_position=Position.hidden,
-    object_class_positions=None,
-    object_positions=None,
+    entity_position=Position.hidden,
+    dimension_positions=None,
+    element_positions=None,
     alternative_position=Position.hidden,
     value_type_position=Position.hidden,
     value_position=Position.hidden,
     index_name_positions=None,
     index_positions=None,
 ):
     """
-    Sets up export mappings for exporting relationships and relationship parameters.
+    Sets up export mappings for exporting entities and parameter values.
 
     Args:
-        relationship_class_position (int or Position): position of relationship classes
+        entity_class_position (int or Position): position of entity classes
         definition_position (int or Position): position of parameter definitions
         value_list_position (int or Position): position of parameter value lists
-        relationship_position (int or Position): position of relationships
-        object_class_positions (list of int, optional): positions of object classes
-        object_positions (list of int, optional): positions of objects
+        entity_position (int or Position): position of entities
+        dimension_positions (list of int, optional): positions of dimensions
+        element_positions (list of int, optional): positions of elements
         alternative_position (int or Position): positions of alternatives
         value_type_position (int or Position): position of parameter value types
         value_position (int or Position): position of parameter values
         index_name_positions (list of int, optional): positions of index names
         index_positions (list of int, optional): positions of parameter indexes
 
     Returns:
         ExportMapping: root mapping
     """
-    if object_class_positions is None:
-        object_class_positions = list()
-    if object_positions is None:
-        object_positions = list()
-    relationship_class = RelationshipClassMapping(relationship_class_position)
-    object_or_relationship_class = _generate_dimensions(
-        relationship_class, RelationshipClassObjectClassMapping, object_class_positions
-    )
+    if dimension_positions is None:
+        dimension_positions = list()
+    if element_positions is None:
+        element_positions = list()
+    entity_class = EntityClassMapping(entity_class_position)
+    dimension = _generate_dimensions(entity_class, DimensionMapping, dimension_positions)
     value_list = ParameterValueListMapping(value_list_position)
     value_list.set_ignorable(True)
     definition = ParameterDefinitionMapping(definition_position)
-    object_or_relationship_class.child = definition
-    relationship = RelationshipMapping(relationship_position)
+    dimension.child = definition
+    relationship = EntityMapping(entity_position)
     definition.child = value_list
     value_list.child = relationship
-    object_or_relationship = _generate_dimensions(relationship, RelationshipObjectMapping, object_positions)
+    element = _generate_dimensions(relationship, ElementMapping, element_positions)
     _generate_parameter_value_mappings(
-        object_or_relationship,
+        element,
         alternative_position,
         value_type_position,
         value_position,
         index_name_positions,
         index_positions,
     )
-    return relationship_class
+    return entity_class
 
 
-def relationship_object_parameter_default_value_export(
-    relationship_class_position=Position.hidden,
+def entity_dimension_parameter_default_value_export(
+    entity_class_position=Position.hidden,
     definition_position=Position.hidden,
-    object_class_positions=None,
+    dimension_positions=None,
     value_type_position=Position.hidden,
     value_position=Position.hidden,
     index_name_positions=None,
     index_positions=None,
-    highlight_dimension=0,
+    highlight_position=0,
 ):
     """
-    Sets up export mappings for exporting relationship classes but with default object parameter values.
+    Sets up export mappings for exporting entity classes but with default dimension parameter values.
 
     Args:
-        relationship_class_position (int or Position): position of relationship classes
+        entity_class_position (int or Position): position of entity classes
         definition_position (int or Position): position of parameter definitions
-        object_class_positions (list of int, optional): positions of object classes
+        dimension_positions (list of int, optional): positions of dimensions
         value_type_position (int or Position): position of parameter value types
         value_position (int or Position): position of parameter values
         index_name_positions (list of int, optional): positions of index names
         index_positions (list of int, optional): positions of parameter indexes
-        highlight_dimension (int): selected object class' relationship dimension
+        highlight_position (int): selected dimension
 
     Returns:
         ExportMapping: root mapping
     """
     root_mapping = unflatten(
         [
-            RelationshipClassMapping(relationship_class_position, highlight_dimension=highlight_dimension),
+            EntityClassMapping(entity_class_position, highlight_position=highlight_position),
             ParameterDefinitionMapping(definition_position),
         ]
     )
-    _generate_dimensions(root_mapping.tail_mapping(), RelationshipClassObjectClassMapping, object_class_positions)
+    _generate_dimensions(root_mapping.tail_mapping(), DimensionMapping, dimension_positions)
     _generate_default_value_mappings(
         root_mapping.tail_mapping(), value_type_position, value_position, index_name_positions, index_positions
     )
     return root_mapping
 
 
-def relationship_object_parameter_export(
-    relationship_class_position=Position.hidden,
+def entity_dimension_parameter_value_export(
+    entity_class_position=Position.hidden,
     definition_position=Position.hidden,
     value_list_position=Position.hidden,
-    relationship_position=Position.hidden,
-    object_class_positions=None,
-    object_positions=None,
+    entity_position=Position.hidden,
+    dimension_positions=None,
+    element_positions=None,
     alternative_position=Position.hidden,
     value_type_position=Position.hidden,
     value_position=Position.hidden,
     index_name_positions=None,
     index_positions=None,
-    highlight_dimension=0,
+    highlight_position=0,
 ):
     """
-    Sets up export mappings for exporting relationships and relationship parameters.
+    Sets up export mappings for exporting entities and element parameter values.
 
     Args:
-        relationship_class_position (int or Position): position of relationship classes
+        entity_class_position (int or Position): position of entity classes
         definition_position (int or Position): position of parameter definitions
         value_list_position (int or Position): position of parameter value lists
-        relationship_position (int or Position): position of relationships
-        object_class_positions (list of int, optional): positions of object classes
-        object_positions (list of int, optional): positions of objects
+        entity_position (int or Position): position of relationships
+        dimension_positions (list of int, optional): positions of object classes
+        element_positions (list of int, optional): positions of objects
         alternative_position (int or Position): positions of alternatives
         value_type_position (int or Position): position of parameter value types
         value_position (int or Position): position of parameter values
         index_name_positions (list of int, optional): positions of index names
         index_positions (list of int, optional): positions of parameter indexes
-        highlight_dimension (int): selected object class' relationship dimension
+        highlight_position (int): selected dimension position
 
     Returns:
         ExportMapping: root mapping
     """
-    if object_class_positions is None:
-        object_class_positions = list()
-    if object_positions is None:
-        object_positions = list()
-    relationship_class = RelationshipClassMapping(relationship_class_position, highlight_dimension=highlight_dimension)
-    object_or_relationship_class = _generate_dimensions(
-        relationship_class, RelationshipClassObjectClassMapping, object_class_positions
-    )
+    # TODO fix dimension highlighting
+    if dimension_positions is None:
+        dimension_positions = list()
+    if element_positions is None:
+        element_positions = list()
+    entity_class = EntityClassMapping(entity_class_position, highlight_position=highlight_position)
+    dimension = _generate_dimensions(entity_class, DimensionMapping, dimension_positions)
     value_list = ParameterValueListMapping(value_list_position)
     value_list.set_ignorable(True)
     definition = ParameterDefinitionMapping(definition_position)
-    object_or_relationship_class.child = definition
-    relationship = RelationshipMapping(relationship_position)
+    dimension.child = definition
+    entity = EntityMapping(entity_position)
     definition.child = value_list
-    value_list.child = relationship
-    object_or_relationship = _generate_dimensions(relationship, RelationshipObjectMapping, object_positions)
+    value_list.child = entity
+    element = _generate_dimensions(entity, ElementMapping, element_positions)
     _generate_parameter_value_mappings(
-        object_or_relationship,
+        element,
         alternative_position,
         value_type_position,
         value_position,
         index_name_positions,
         index_positions,
     )
-    return relationship_class
+    return entity_class
 
 
-def set_relationship_dimensions(relationship_mapping, dimensions):
+def set_entity_dimensions(entity_mapping, dimensions):
     """
-    Modifies given relationship mapping's dimensions (number of object classes and objects).
+    Modifies given entity mapping's dimensions.
 
     Args:
-        relationship_mapping (ExportMapping): a relationship mapping
+        entity_mapping (ExportMapping): an entity mapping
         dimensions (int): number of dimensions
     """
-    mapping_list = relationship_mapping.flatten()
+    mapping_list = entity_mapping.flatten()
     mapping_list = _change_amount_of_consecutive_mappings(
-        mapping_list, RelationshipClassMapping, RelationshipClassObjectClassMapping, dimensions
+        mapping_list, EntityClassMapping, DimensionMapping, dimensions
     )
-    if any(isinstance(m, RelationshipMapping) for m in mapping_list):
-        mapping_list = _change_amount_of_consecutive_mappings(
-            mapping_list, RelationshipMapping, RelationshipObjectMapping, dimensions
-        )
+    if any(isinstance(m, EntityMapping) for m in mapping_list):
+        mapping_list = _change_amount_of_consecutive_mappings(mapping_list, EntityMapping, ElementMapping, dimensions)
     unflatten(mapping_list)
 
 
 def alternative_export(alternative_position=Position.hidden, alternative_description_position=Position.hidden):
     """
     Sets up export mappings for exporting alternatives.
 
@@ -518,100 +411,14 @@
         ParameterDefaultValueMapping,
         ExpandedParameterDefaultValueMapping,
         ParameterDefaultValueIndexMapping,
         DefaultValueIndexNameMapping,
     )
 
 
-def feature_export(class_position=Position.hidden, definition_position=Position.hidden):
-    """
-    Sets up export mappings for exporting features.
-
-    Args:
-        class_position (int or Position): position of entity classes
-        definition_position (int or Position): position of parameter definitions
-
-    Returns:
-        ExportMapping: root mapping
-    """
-    class_ = FeatureEntityClassMapping(class_position)
-    definition = FeatureParameterDefinitionMapping(definition_position)
-    class_.child = definition
-    return class_
-
-
-def tool_export(tool_position=Position.hidden):
-    """
-    Sets up export mappings for exporting tools.
-
-    Args:
-        tool_position (int or Position): position of tools
-
-    Returns:
-        ExportMapping: root mapping
-    """
-    return ToolMapping(tool_position)
-
-
-def tool_feature_export(
-    tool_position=Position.hidden,
-    class_position=Position.hidden,
-    definition_position=Position.hidden,
-    required_flag_position=Position.hidden,
-):
-    """
-    Sets up export mappings for exporting tool features.
-
-    Args:
-        tool_position (int or Position): position of tools
-        class_position (int or Position): position of entity classes
-        definition_position (int or Position): position of parameter definitions
-        required_flag_position (int or Position): position of required flags
-
-    Returns:
-        ExportMapping: root mapping
-    """
-    tool = ToolMapping(tool_position)
-    class_ = ToolFeatureEntityClassMapping(class_position)
-    definition = ToolFeatureParameterDefinitionMapping(definition_position)
-    required_flag = ToolFeatureRequiredFlagMapping(required_flag_position)
-    definition.child = required_flag
-    class_.child = definition
-    tool.child = class_
-    return tool
-
-
-def tool_feature_method_export(
-    tool_position=Position.hidden,
-    class_position=Position.hidden,
-    definition_position=Position.hidden,
-    method_position=Position.hidden,
-):
-    """
-    Sets up export mappings for exporting tool feature methods.
-
-    Args:
-        tool_position (int or Position): position of tools
-        class_position (int or Position): position of entity classes
-        definition_position (int or Position): position of parameter definitions
-        method_position (int or Position): position of methods
-
-    Returns:
-        ExportMapping: root mapping
-    """
-    tool = ToolMapping(tool_position)
-    class_ = ToolFeatureMethodEntityClassMapping(class_position)
-    definition = ToolFeatureMethodParameterDefinitionMapping(definition_position)
-    method = ToolFeatureMethodMethodMapping(method_position)
-    definition.child = method
-    class_.child = definition
-    tool.child = class_
-    return tool
-
-
 def _generate_dimensions(parent, cls, positions):
     """
     Nests mappings of same type as children of given ``parent``.
 
     Args:
         parent (ExportMapping): parent mapping
         cls (Type): mapping type
```

### Comparing `spinedb_api-0.30.5/spinedb_api/filters/__init__.py` & `spinedb_api-0.31.0/spinedb_api/filters/__init__.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/filters/alternative_filter.py` & `spinedb_api-0.31.0/spinedb_api/filters/alternative_filter.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -22,15 +23,15 @@
 
 
 def apply_alternative_filter_to_parameter_value_sq(db_map, alternatives):
     """
     Replaces parameter value subquery properties in ``db_map`` such that they return only values of given alternatives.
 
     Args:
-        db_map (DatabaseMappingBase): a database map to alter
+        db_map (DatabaseMapping): a database map to alter
         alternatives (Iterable of str or int, optional): alternative names or ids;
     """
     state = _AlternativeFilterState(db_map, alternatives)
     filtering = partial(_make_alternative_filtered_parameter_value_sq, state=state)
     db_map.override_parameter_value_sq_maker(filtering)
 
 
@@ -48,15 +49,15 @@
 
 
 def alternative_filter_from_dict(db_map, config):
     """
     Applies alternative filter to given database map.
 
     Args:
-        db_map (DatabaseMappingBase): target database map
+        db_map (DatabaseMapping): target database map
         config (dict): alternative filter configuration
     """
     apply_alternative_filter_to_parameter_value_sq(db_map, config["alternatives"])
 
 
 def alternative_filter_config_to_shorthand(config):
     """
@@ -95,15 +96,15 @@
 
     Args:
         shorthand (str): a shorthand string
 
     Returns:
         dict: alternative filter configuration
     """
-    filter_type, separator, tokens = shorthand.partition(":'")
+    _filter_type, _separator, tokens = shorthand.partition(":'")
     alternatives = tokens.split("':'")
     alternatives[-1] = alternatives[-1][:-1]
     return alternative_filter_config(alternatives)
 
 
 class _AlternativeFilterState:
     """
@@ -113,65 +114,63 @@
         original_parameter_value_sq (Alias): previous ``parameter_value_sq``
         alternatives (Iterable of int): ids of alternatives
     """
 
     def __init__(self, db_map, alternatives):
         """
         Args:
-            db_map (DatabaseMappingBase): database the state applies to
+            db_map (DatabaseMapping): database the state applies to
             alternatives (Iterable of str or int): alternative names or ids;
         """
         self.original_parameter_value_sq = db_map.parameter_value_sq
         self.alternatives = self._alternative_ids(db_map, alternatives) if alternatives is not None else None
 
     @staticmethod
     def _alternative_ids(db_map, alternatives):
         """
         Finds ids for given alternatives.
 
         Args:
-            db_map (DatabaseMappingBase): a database map
+            db_map (DatabaseMapping): a database map
             alternatives (Iterable): alternative names or ids
 
         Returns:
             list of int: alternative ids
         """
         alternative_names = [name for name in alternatives if isinstance(name, str)]
         ids_from_db = (
             db_map.query(db_map.alternative_sq.c.id, db_map.alternative_sq.c.name)
-            .filter(db_map.in_(db_map.alternative_sq.c.name, alternative_names))
+            .filter(db_map.alternative_sq.c.name.in_(alternative_names))
             .all()
         )
         names_in_db = [i.name for i in ids_from_db]
         if len(alternative_names) != len(names_in_db):
             missing_names = tuple(name for name in alternative_names if name not in names_in_db)
             raise SpineDBAPIError(f"Alternative(s) {missing_names} not found")
         ids = [i.id for i in ids_from_db]
         alternative_ids = [id_ for id_ in alternatives if isinstance(id_, int)]
         ids_from_db = (
-            db_map.query(db_map.alternative_sq.c.id)
-            .filter(db_map.in_(db_map.alternative_sq.c.id, alternative_ids))
-            .all()
+            db_map.query(db_map.alternative_sq.c.id).filter(db_map.alternative_sq.c.id.in_(alternative_ids)).all()
         )
         ids_in_db = [i.id for i in ids_from_db]
         if len(alternative_ids) != len(ids_from_db):
             missing_ids = tuple(i for i in alternative_ids if i not in ids_in_db)
             raise SpineDBAPIError(f"Alternative id(s) {missing_ids} not found")
         ids += ids_in_db
         return ids
 
 
 def _make_alternative_filtered_parameter_value_sq(db_map, state):
     """
-    Returns an alternative filtering subquery similar to :func:`DatabaseMappingBase.parameter_value_sq`.
+    Returns an alternative filtering subquery similar to :func:`DatabaseMapping.parameter_value_sq`.
 
-    This function can be used as replacement for parameter value subquery maker in :class:`DatabaseMappingBase`.
+    This function can be used as replacement for parameter value subquery maker in :class:`DatabaseMapping`.
 
     Args:
-        db_map (DatabaseMappingBase): a database map
+        db_map (DatabaseMapping): a database map
         state (_AlternativeFilterState): a state bound to ``db_map``
 
     Returns:
         Alias: a subquery for parameter value filtered by selected alternatives
     """
     subquery = state.original_parameter_value_sq
-    return db_map.query(subquery).filter(db_map.in_(subquery.c.alternative_id, state.alternatives)).subquery()
+    return db_map.query(subquery).filter(subquery.c.alternative_id.in_(state.alternatives)).subquery()
```

### Comparing `spinedb_api-0.30.5/spinedb_api/filters/execution_filter.py` & `spinedb_api-0.31.0/spinedb_api/filters/execution_filter.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -24,15 +25,15 @@
 
 
 def apply_execution_filter(db_map, execution):
     """
     Replaces the import alternative in ``db_map`` with a dedicated alternative for an execution.
 
     Args:
-        db_map (DatabaseMappingBase): a database map to alter
+        db_map (DatabaseMapping): a database map to alter
         execution (dict): execution descriptor
     """
     state = _ExecutionFilterState(db_map, execution)
     create_import_alternative = partial(_create_import_alternative, state=state)
     db_map.override_create_import_alternative(create_import_alternative)
 
 
@@ -50,15 +51,15 @@
 
 
 def execution_filter_from_dict(db_map, config):
     """
     Applies execution filter to given database map.
 
     Args:
-        db_map (DatabaseMappingBase): target database map
+        db_map (DatabaseMapping): target database map
         config (dict): execution filter configuration
     """
     apply_execution_filter(db_map, config["execution"])
 
 
 def execution_descriptor_from_dict(config):
     """
@@ -112,61 +113,59 @@
         scenarios (list of str): scenarios involved in the execution
         timestamp (str): timestamp of execution
     """
 
     def __init__(self, db_map, execution):
         """
         Args:
-            db_map (DatabaseMappingBase): database the state applies to
+            db_map (DatabaseMapping): database the state applies to
             execution (dict): execution descriptor
         """
         self.original_create_import_alternative = db_map._create_import_alternative
         self.execution_item, self.scenarios, self.timestamp = self._parse_execution_descriptor(execution)
 
-    def _parse_execution_descriptor(self, execution):
-        """Raises ``SpineDBAPIError`` if descriptor not good.
+    @staticmethod
+    def _parse_execution_descriptor(execution):
+        """Parses data from execution descriptor.
 
         Args:
             execution (dict): execution descriptor
 
         Returns:
-            str: the execution item
-            list: scenarios
+            tuple: execution item name, list of scenario names, timestamp string
+
+        Raises:
+            SpineDBAPIError: raised when execution descriptor is invalid
         """
         try:
             execution_item = execution["execution_item"]
             scenarios = execution["scenarios"]
             timestamp = execution["timestamp"]
         except KeyError as e:
             raise SpineDBAPIError(f"Key '{e}' not found in execution filter descriptor.") from e
         if not isinstance(scenarios, list):
             raise SpineDBAPIError("Key 'scenarios' should contain a list.")
         return execution_item, scenarios, timestamp
 
 
-def _create_import_alternative(db_map, state, cache=None):
+def _create_import_alternative(db_map, state):
     """
     Creates an alternative to use as default for all import operations on the given db_map.
 
     Args:
-        db_map (DatabaseMappingBase): database the state applies to
+        db_map (DatabaseMapping): database the state applies to
         state (_ExecutionFilterState): a state bound to ``db_map``
     """
     execution_item = state.execution_item
     scenarios = state.scenarios
     timestamp = state.timestamp
     sep = "__" if scenarios else ""
     db_map._import_alternative_name = f"{'_'.join(scenarios)}{sep}{execution_item}@{timestamp}"
-    alt_ids, _ = db_map.add_alternatives({"name": db_map._import_alternative_name}, return_dups=True)
-    db_map._import_alternative_id = next(iter(alt_ids))
-    scenarios = [{"name": scenario} for scenario in scenarios]
-    scen_ids, _ = db_map.add_scenarios(*scenarios, return_dups=True)
-    for scen_id in scen_ids:
-        max_rank = (
-            db_map.query(func.max(db_map.scenario_alternative_sq.c.rank))
-            .filter(db_map.scenario_alternative_sq.c.scenario_id == scen_id)
-            .scalar()
-        )
-        rank = max_rank + 1 if max_rank else 1
-        db_map.add_scenario_alternatives(
-            {"scenario_id": scen_id, "alternative_id": db_map._import_alternative_id, "rank": rank}
+    db_map.add_item("alternative", name=db_map._import_alternative_name)
+    for scen_name in scenarios:
+        db_map.add_item("scenario", name=scen_name)
+    for scen_name in scenarios:
+        scen = db_map.get_item("scenario", name=scen_name)
+        rank = len(scen["sorted_scenario_alternatives"]) + 1  # ranks are 1-based
+        db_map.add_item(
+            "scenario_alternative", scenario_name=scen_name, alternative_name=db_map._import_alternative_name, rank=rank
         )
```

### Comparing `spinedb_api-0.30.5/spinedb_api/filters/renamer.py` & `spinedb_api-0.31.0/spinedb_api/filters/renamer.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,22 +1,19 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
-
-"""
-Provides a database query manipulator that renames database items.
-
-"""
+""" Provides a database query manipulator that renames database items. """
 from functools import partial
 from sqlalchemy import case
 
 
 ENTITY_CLASS_RENAMER_TYPE = "entity_class_renamer"
 ENTITY_CLASS_RENAMER_SHORTHAND_TAG = "entity_class_rename"
 PARAMETER_RENAMER_TYPE = "parameter_renamer"
@@ -24,15 +21,15 @@
 
 
 def apply_renaming_to_entity_class_sq(db_map, name_map):
     """
     Applies renaming to entity class subquery.
 
     Args:
-        db_map (DatabaseMappingBase): a database map
+        db_map (DatabaseMapping): a database map
         name_map (dict): a map from old name to new name
     """
     state = _EntityClassRenamerState(db_map, name_map)
     renaming = partial(_make_renaming_entity_class_sq, state=state)
     db_map.override_entity_class_sq_maker(renaming)
 
 
@@ -50,15 +47,15 @@
 
 
 def entity_class_renamer_from_dict(db_map, config):
     """
     Applies entity class renamer manipulator to given database map.
 
     Args:
-        db_map (DatabaseMappingBase): target database map
+        db_map (DatabaseMapping): target database map
         config (dict): renamer configuration
     """
     apply_renaming_to_entity_class_sq(db_map, config["name_map"])
 
 
 def entity_class_renamer_config_to_shorthand(config):
     """
@@ -94,15 +91,15 @@
 
 
 def apply_renaming_to_parameter_definition_sq(db_map, name_map):
     """
     Applies renaming to parameter definition subquery.
 
     Args:
-        db_map (DatabaseMappingBase): a database map
+        db_map (DatabaseMapping): a database map
         name_map (dict): a map from old name to new name
     """
     state = _ParameterRenamerState(db_map, name_map)
     renaming = partial(_make_renaming_parameter_definition_sq, state=state)
     db_map.override_parameter_definition_sq_maker(renaming)
 
 
@@ -120,15 +117,15 @@
 
 
 def parameter_renamer_from_dict(db_map, config):
     """
     Applies parameter renamer manipulator to given database map.
 
     Args:
-        db_map (DatabaseMappingBase): target database map
+        db_map (DatabaseMapping): target database map
         config (dict): renamer configuration
     """
     apply_renaming_to_parameter_definition_sq(db_map, config["name_map"])
 
 
 def parameter_renamer_config_to_shorthand(config):
     """
@@ -164,26 +161,26 @@
     return parameter_renamer_config(name_map)
 
 
 class _EntityClassRenamerState:
     def __init__(self, db_map, name_map):
         """
         Args:
-            db_map (DatabaseMappingBase): a database map
+            db_map (DatabaseMapping): a database map
             name_map (dict): a mapping from original name to a new name.
         """
         name_map = {old: new for old, new in name_map.items() if old != new}
         self.id_to_name = self._ids(db_map, name_map)
         self.original_entity_class_sq = db_map.entity_class_sq
 
     @staticmethod
     def _ids(db_map, name_map):
         """
         Args:
-            db_map (DatabaseMappingBase): a database map
+            db_map (DatabaseMapping): a database map
             name_map (dict): a mapping from original name to a new name
 
         Returns:
             dict: a mapping from entity class id to a new name
         """
         names = set(name_map.keys())
         return {
@@ -193,53 +190,52 @@
 
 
 def _make_renaming_entity_class_sq(db_map, state):
     """
     Returns an entity class subquery which renames classes.
 
     Args:
-        db_map (DatabaseMappingBase): a database map
+        db_map (DatabaseMapping): a database map
         state (_EntityClassRenamerState):
 
     Returns:
         Alias: a renaming entity class subquery
     """
     subquery = state.original_entity_class_sq
     if not state.id_to_name:
         return subquery
     cases = [(subquery.c.id == id, new_name) for id, new_name in state.id_to_name.items()]
     new_class_name = case(cases, else_=subquery.c.name)  # if not in the name map, just keep the original name
     entity_class_sq = db_map.query(
         subquery.c.id,
-        subquery.c.type_id,
         new_class_name.label("name"),
         subquery.c.description,
         subquery.c.display_order,
         subquery.c.display_icon,
         subquery.c.hidden,
-        subquery.c.commit_id,
+        subquery.c.active_by_default,
     ).subquery()
     return entity_class_sq
 
 
 class _ParameterRenamerState:
     def __init__(self, db_map, name_map):
         """
         Args:
-            db_map (DatabaseMappingBase): a database map
+            db_map (DatabaseMapping): a database map
             name_map (dict): mapping from entity class name to mapping from parameter name to new name
         """
         self.id_to_name = self._ids(db_map, name_map)
         self.original_parameter_definition_sq = db_map.parameter_definition_sq
 
     @staticmethod
     def _ids(db_map, name_map):
         """
         Args:
-            db_map (DatabaseMappingBase): a database map
+            db_map (DatabaseMapping): a database map
             name_map (dict): a mapping from original name to a new name
 
         Returns:
             dict: a mapping from entity class id to a new name
         """
         class_names = set(name_map.keys())
         param_names = set(old_name for renaming in name_map.values() for old_name in renaming)
@@ -254,15 +250,15 @@
 
 
 def _make_renaming_parameter_definition_sq(db_map, state):
     """
     Returns an entity class subquery which renames parameters.
 
     Args:
-        db_map (DatabaseMappingBase): a database map
+        db_map (DatabaseMapping): a database map
         state (_ParameterRenamerState):
 
     Returns:
         Alias: a renaming parameter definition subquery
     """
     subquery = state.original_parameter_definition_sq
     if not state.id_to_name:
@@ -270,16 +266,14 @@
     cases = [(subquery.c.id == id, new_name) for id, new_name in state.id_to_name.items()]
     new_parameter_name = case(cases, else_=subquery.c.name)  # if not in the name map, just keep the original name
     parameter_definition_sq = db_map.query(
         subquery.c.id,
         new_parameter_name.label("name"),
         subquery.c.description,
         subquery.c.entity_class_id,
-        subquery.c.object_class_id,
-        subquery.c.relationship_class_id,
         subquery.c.default_value,
         subquery.c.default_type,
         subquery.c.list_value_id,
         subquery.c.commit_id,
         subquery.c.parameter_value_list_id,
     ).subquery()
     return parameter_definition_sq
```

### Comparing `spinedb_api-0.30.5/spinedb_api/filters/tools.py` & `spinedb_api-0.31.0/spinedb_api/filters/tools.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -41,23 +42,14 @@
     SCENARIO_SHORTHAND_TAG,
     scenario_filter_config,
     scenario_filter_config_to_shorthand,
     scenario_filter_from_dict,
     scenario_filter_shorthand_to_config,
     scenario_name_from_dict,
 )
-from .tool_filter import (
-    TOOL_SHORTHAND_TAG,
-    TOOL_FILTER_TYPE,
-    tool_filter_config,
-    tool_filter_config_to_shorthand,
-    tool_filter_from_dict,
-    tool_filter_shorthand_to_config,
-    tool_name_from_dict,
-)
 from .value_transformer import (
     VALUE_TRANSFORMER_SHORTHAND_TAG,
     VALUE_TRANSFORMER_TYPE,
     value_transformer_shorthand_to_config,
     value_transformer_from_dict,
     value_transformer_config_to_shorthand,
 )
@@ -75,24 +67,23 @@
 
 
 def apply_filter_stack(db_map, stack):
     """
     Applies stack of filters and manipulator to given database map.
 
     Args:
-        db_map (DatabaseMappingBase): a database map
+        db_map (DatabaseMapping): a database map
         stack (list): a stack of database filters and manipulators
     """
     appliers = {
         ALTERNATIVE_FILTER_TYPE: alternative_filter_from_dict,
         ENTITY_CLASS_RENAMER_TYPE: entity_class_renamer_from_dict,
         EXECUTION_FILTER_TYPE: execution_filter_from_dict,
         PARAMETER_RENAMER_TYPE: parameter_renamer_from_dict,
         SCENARIO_FILTER_TYPE: scenario_filter_from_dict,
-        TOOL_FILTER_TYPE: tool_filter_from_dict,
         VALUE_TRANSFORMER_TYPE: value_transformer_from_dict,
     }
     for filter_ in stack:
         appliers[filter_["type"]](db_map, filter_)
 
 
 def load_filters(configs):
@@ -135,15 +126,14 @@
         value (object): the filter value (e.g. scenario name)
 
     Returns:
         dict: filter configuration
     """
     return {
         SCENARIO_FILTER_TYPE: scenario_filter_config,
-        TOOL_FILTER_TYPE: tool_filter_config,
         ALTERNATIVE_FILTER_TYPE: alternative_filter_config,
         EXECUTION_FILTER_TYPE: execution_filter_config,
     }[filter_type](value)
 
 
 def append_filter_config(url, config):
     """
@@ -284,15 +274,14 @@
         str: config shorthand
     """
     shorthands = {
         ALTERNATIVE_FILTER_TYPE: alternative_filter_config_to_shorthand,
         ENTITY_CLASS_RENAMER_TYPE: entity_class_renamer_config_to_shorthand,
         PARAMETER_RENAMER_TYPE: parameter_renamer_config_to_shorthand,
         SCENARIO_FILTER_TYPE: scenario_filter_config_to_shorthand,
-        TOOL_FILTER_TYPE: tool_filter_config_to_shorthand,
         EXECUTION_FILTER_TYPE: execution_filter_config_to_shorthand,
         VALUE_TRANSFORMER_TYPE: value_transformer_config_to_shorthand,
     }
     return SHORTHAND_TAG + shorthands[config["type"]](config)
 
 
 def _parse_shorthand(shorthand):
@@ -306,29 +295,28 @@
         dict: filter configuration dictionary
     """
     shorthand_parsers = {
         ALTERNATIVE_FILTER_SHORTHAND_TAG: alternative_filter_shorthand_to_config,
         ENTITY_CLASS_RENAMER_SHORTHAND_TAG: entity_class_renamer_shorthand_to_config,
         PARAMETER_RENAMER_SHORTHAND_TAG: parameter_renamer_shorthand_to_config,
         SCENARIO_SHORTHAND_TAG: scenario_filter_shorthand_to_config,
-        TOOL_SHORTHAND_TAG: tool_filter_shorthand_to_config,
         EXECUTION_SHORTHAND_TAG: execution_filter_shorthand_to_config,
         VALUE_TRANSFORMER_SHORTHAND_TAG: value_transformer_shorthand_to_config,
     }
     tag, _, _ = shorthand.partition(":")
     return shorthand_parsers[tag](shorthand)
 
 
 def name_from_dict(config):
     """
-    Returns scenario or tool name from filter config.
+    Returns scenario name from filter config.
 
     Args:
         config (dict): filter configuration
 
     Returns:
         str: name or None if ``config`` is not a valid 'name' filter configuration
     """
-    func = {SCENARIO_FILTER_TYPE: scenario_name_from_dict, TOOL_FILTER_TYPE: tool_name_from_dict}.get(config["type"])
+    func = {SCENARIO_FILTER_TYPE: scenario_name_from_dict}.get(config["type"])
     if func is None:
         return None
     return func(config)
```

### Comparing `spinedb_api-0.30.5/spinedb_api/filters/value_transformer.py` & `spinedb_api-0.31.0/spinedb_api/filters/value_transformer.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -12,30 +13,30 @@
 """
 Provides a database query manipulator that applies mathematical transformations to parameter values.
 
 """
 from functools import partial
 from numbers import Number
 from sqlalchemy import case, literal, Integer, LargeBinary, String
-from sqlalchemy.sql.expression import label, select, cast, union_all
+from sqlalchemy.sql.expression import select, cast, union_all
 
 from ..exception import SpineDBAPIError
 from ..helpers import LONGTEXT_LENGTH
 from ..parameter_value import from_database, IndexedValue, to_database, Map
 
 VALUE_TRANSFORMER_TYPE = "value_transformer"
 VALUE_TRANSFORMER_SHORTHAND_TAG = "value_transform"
 
 
 def apply_value_transform_to_parameter_value_sq(db_map, instructions):
     """
     Applies renaming to parameter definition subquery.
 
     Args:
-        db_map (DatabaseMappingBase): a database map
+        db_map (DatabaseMapping): a database map
         instructions (dict): mapping from entity class name to mapping from parameter name to list of
             instructions
     """
     state = _ValueTransformerState(db_map, instructions)
     transform = partial(_make_parameter_value_transforming_sq, state=state)
     db_map.override_parameter_value_sq_maker(transform)
 
@@ -55,15 +56,15 @@
 
 
 def value_transformer_from_dict(db_map, config):
     """
     Applies value transformer manipulator to given database map.
 
     Args:
-        db_map (DatabaseMappingBase): target database map
+        db_map (DatabaseMapping): target database map
         config (dict): transformer configuration
     """
     apply_value_transform_to_parameter_value_sq(db_map, config["instructions"])
 
 
 def value_transformer_config_to_shorthand(config):
     """
@@ -116,26 +117,26 @@
     return value_transformer_config(instructions)
 
 
 class _ValueTransformerState:
     def __init__(self, db_map, instructions):
         """
         Args:
-            db_map (DatabaseMappingBase): a database map
+            db_map (DatabaseMapping): a database map
             instructions (dict): mapping from entity class name to parameter name to list of instructions
         """
         self.original_parameter_value_sq = db_map.parameter_value_sq
         self.transformed = self._transform(db_map, instructions)
 
     @staticmethod
     def _transform(db_map, instructions):
         """Transforms applicable parameter values for caching.
 
         Args:
-            db_map (DatabaseMappingBase): a database map
+            db_map (DatabaseMapping): a database map
             instructions (dict): mapping from entity class name to parameter name to list of instructions
 
         Returns:
             dict: mapping from parameter value ids to transformed values
         """
         class_names = set(instructions.keys())
         param_names = set(name for class_instructions in instructions.values() for name in class_instructions)
@@ -160,15 +161,15 @@
 
 
 def _make_parameter_value_transforming_sq(db_map, state):
     """
     Returns subquery which applies transformations to parameter values.
 
     Args:
-        db_map (DatabaseMappingBase): a database map
+        db_map (DatabaseMapping): a database map
         state (_ValueTransformerState): state
 
     Returns:
         Alias: a value transforming parameter value subquery
     """
     subquery = state.original_parameter_value_sq
     if not state.transformed:
@@ -184,36 +185,20 @@
             ]
         )
     ]
     statements += [select([literal(i), literal(v), literal(t)]) for i, v, t in transformed_rows[1:]]
     temp_sq = union_all(*statements).alias("transformed_values")
     new_value = case([(temp_sq.c.transformed_value != None, temp_sq.c.transformed_value)], else_=subquery.c.value)
     new_type = case([(temp_sq.c.transformed_type != None, temp_sq.c.transformed_type)], else_=subquery.c.type)
-    object_class_case = case(
-        [(db_map.entity_class_sq.c.type_id == db_map.object_class_type, subquery.c.entity_class_id)], else_=None
-    )
-    rel_class_case = case(
-        [(db_map.entity_class_sq.c.type_id == db_map.relationship_class_type, subquery.c.entity_class_id)], else_=None
-    )
-    object_entity_case = case(
-        [(db_map.entity_sq.c.type_id == db_map.object_entity_type, subquery.c.entity_id)], else_=None
-    )
-    rel_entity_case = case(
-        [(db_map.entity_sq.c.type_id == db_map.relationship_entity_type, subquery.c.entity_id)], else_=None
-    )
     parameter_value_sq = (
         db_map.query(
             subquery.c.id.label("id"),
             subquery.c.parameter_definition_id,
             subquery.c.entity_class_id,
             subquery.c.entity_id,
-            label("object_class_id", object_class_case),
-            label("relationship_class_id", rel_class_case),
-            label("object_id", object_entity_case),
-            label("relationship_id", rel_entity_case),
             new_value.label("value"),
             new_type.label("type"),
             subquery.c.list_value_id,
             subquery.c.alternative_id,
             subquery.c.commit_id.label("commit_id"),
         )
         .join(temp_sq, subquery.c.id == temp_sq.c.id, isouter=True)
```

### Comparing `spinedb_api-0.30.5/spinedb_api/graph_layout_generator.py` & `spinedb_api-0.31.0/spinedb_api/graph_layout_generator.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,47 +1,67 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Engine.
 # Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
 # Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
 # any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
 """
-Contains the GraphLayoutGenerator class.
-
+This module defines the :class:`.GraphLayoutGenerator` class.
 """
-
 import math
 import numpy as np
 from numpy import atleast_1d as arr
 from scipy.sparse.csgraph import dijkstra
 
 
 class GraphLayoutGenerator:
-    """Computes the layout for the Entity Graph View."""
+    """A class to build an optimised layout for an undirected graph.
+    This can help visualizing the Spine data structure of multi-dimensional entities.
+    """
 
     def __init__(
         self,
         vertex_count,
         src_inds=(),
         dst_inds=(),
         spread=0,
         heavy_positions=None,
         max_iters=12,
         weight_exp=-2,
         is_stopped=lambda: False,
         preview_available=lambda x, y: None,
         layout_available=lambda x, y: None,
         layout_progressed=lambda iter: None,
-        message_available=lambda msg: None,
     ):
+        """
+        Args:
+            vertex_count (int): The number of vertices in the graph. Graph vertices will have indices 0, 1, 2, ...
+            src_inds (tuple, optional): The indices of the source vertices of each edge.
+            dst_inds (tuple, optional): The indices of the destination vertices of each edge.
+            spread (int, optional): the ideal edge length.
+            heavy_positions (dict, optional): a dictionary mapping vertex indices to another dictionary
+                with keys "x" and "y" specifying the position it should have in the generated layout.
+            max_iters (int, optional): the maximum numbers of iterations of the layout generation algorithm.
+            weight_exp (int, optional): The exponential decay rate of attraction between vertices. The higher this
+                number, the lesser the attraction between distant vertices.
+            is_stopped (function, optional): A function to call without arguments, that returns a boolean indicating
+                whether the layout generation process needs to be stopped.
+            preview_available (function, optional): A function to call after every iteration with two lists, x and y,
+                representing the current layout.
+            layout_available (function, optional): A function to call after the last iteration with two lists, x and y,
+                representing the final layout.
+            layout_progressed (function, optional): A function to call after each iteration with the current iteration
+                number.
+        """
         super().__init__()
         if vertex_count == 0:
             vertex_count = 1
         if heavy_positions is None:
             heavy_positions = dict()
         self.vertex_count = vertex_count
         self.src_inds = src_inds
@@ -51,51 +71,44 @@
         self.max_iters = max(3, round(max_iters * (1 - len(heavy_positions) / self.vertex_count)))
         self.weight_exp = weight_exp
         self.initial_diameter = (self.vertex_count ** (0.5)) * self.spread
         self._is_stopped = is_stopped
         self._preview_available = preview_available
         self._layout_available = layout_available
         self._layout_progressed = layout_progressed
-        self._message_available = message_available
 
     def shortest_path_matrix(self):
-        """Returns the shortest-path matrix."""
         if not self.src_inds:
             # Graph with no edges, just vertices. Introduce fake pair of edges to help 'spreadness'.
             self.src_inds = [self.vertex_count, self.vertex_count]
             self.dst_inds = [np.random.randint(0, self.vertex_count), np.random.randint(0, self.vertex_count)]
             self.vertex_count += 1
         dist = np.zeros((self.vertex_count, self.vertex_count))
         src_inds = arr(self.src_inds)
         dst_inds = arr(self.dst_inds)
         try:
             dist[src_inds, dst_inds] = dist[dst_inds, src_inds] = self.spread
         except IndexError:
             pass
         start = 0
         slices = []
-        iteration = 0
-        self._message_available("Step 1 of 2: Computing shortest-path matrix...")
         while start < self.vertex_count:
             if self._is_stopped():
                 return None
-            self._layout_progressed(iteration)
             stop = min(self.vertex_count, start + math.ceil(self.vertex_count / 10))
             slice_ = dijkstra(dist, directed=False, indices=range(start, stop))
             slices.append(slice_)
             start = stop
-            iteration += 1
         matrix = np.vstack(slices)
         # Remove infinites and zeros
         matrix[matrix == np.inf] = self.spread * self.vertex_count ** (0.5)
         matrix[matrix == 0] = self.spread * 1e-6
         return matrix
 
     def sets(self):
-        """Returns sets of vertex pairs indices."""
         sets = []
         for n in range(1, self.vertex_count):
             pairs = np.zeros((self.vertex_count - n, 2), int)  # pairs on diagonal n
             pairs[:, 0] = np.arange(self.vertex_count - n)
             pairs[:, 1] = pairs[:, 0] + n
             mask = np.mod(range(self.vertex_count - n), 2 * n) < n
             s1 = pairs[mask]
@@ -103,22 +116,31 @@
             if s1.any():
                 sets.append(s1)
             if s2.any():
                 sets.append(s2)
         return sets
 
     def compute_layout(self):
-        """Computes and returns x and y coordinates for each vertex in the graph, using VSGD-MS."""
+        """Computes the layout using VSGD-MS and returns x and y coordinates for each vertex in the graph.
+
+        Returns:
+            tuple(list,list): x and y coordinates
+        """
+        if len(self.heavy_positions) == self.vertex_count:
+            x, y = zip(*[(pos["x"], pos["y"]) for pos in self.heavy_positions.values()])
+            self._layout_available(x, y)
+            return x, y
         if self.vertex_count <= 1:
             x, y = np.array([0.0]), np.array([0.0])
             self._layout_available(x, y)
-            return
+            return x, y
         matrix = self.shortest_path_matrix()
+        self._layout_progressed(1)
         if matrix is None:
-            return
+            return [], []
         mask = np.ones((self.vertex_count, self.vertex_count)) == 1 - np.tril(
             np.ones((self.vertex_count, self.vertex_count))
         )  # Upper triangular except diagonal
         np.random.seed(0)
         layout = np.random.rand(self.vertex_count, 2) * self.initial_diameter - self.initial_diameter / 2
         heavy_ind_list = list()
         heavy_pos_list = list()
@@ -130,21 +152,21 @@
         if heavy_ind.any():
             layout[heavy_ind, :] = heavy_pos
         weights = matrix ** self.weight_exp  # bus-pair weights (lower for distant buses)
         maxstep = 1 / np.min(weights[mask])
         minstep = 1 / np.max(weights[mask])
         lambda_ = np.log(minstep / maxstep) / (self.max_iters - 1)  # exponential decay of allowed adjustment
         sets = self.sets()  # construct sets of bus pairs
-        self._message_available("Step 2 of 2: Generating layout...")
+        self._layout_progressed(2)
         for iteration in range(self.max_iters):
             if self._is_stopped():
                 break
             x, y = layout[:, 0], layout[:, 1]
             self._preview_available(x, y)
-            self._layout_progressed(iteration)
+            self._layout_progressed(3 + iteration)
             # FIXME
             step = maxstep * np.exp(lambda_ * iteration)  # how big adjustments are allowed?
             rand_order = np.random.permutation(
                 self.vertex_count
             )  # we don't want to use the same pair order each iteration
             for s in sets:
                 v1, v2 = rand_order[s[:, 0]], rand_order[s[:, 1]]  # arrays of vertex1 and vertex2
@@ -155,7 +177,8 @@
                 dx2 = -dx1
                 layout[v1, :] += dx1  # update position
                 layout[v2, :] += dx2
                 if heavy_ind.any():
                     layout[heavy_ind, :] = heavy_pos
         x, y = layout[:, 0], layout[:, 1]
         self._layout_available(x, y)
+        return x, y
```

### Comparing `spinedb_api-0.30.5/spinedb_api/helpers.py` & `spinedb_api-0.31.0/spinedb_api/helpers.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,28 +1,25 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
-
-"""
-General helper functions and classes.
-
-"""
+""" General helper functions. """
 
 import os
 import json
 import warnings
 from operator import itemgetter
-from urllib.parse import urlparse, urlunparse
+from itertools import groupby
 from sqlalchemy import (
     Boolean,
     BigInteger,
     CheckConstraint,
     Column,
     DateTime,
     Float,
@@ -37,52 +34,80 @@
     LargeBinary,
     UniqueConstraint,
     create_engine,
     false,
     func,
     inspect,
     null,
+    true,
     select,
 )
 from sqlalchemy.ext.automap import generate_relationship
 from sqlalchemy.ext.compiler import compiles
-from sqlalchemy.exc import DatabaseError, IntegrityError, OperationalError
+from sqlalchemy.exc import DatabaseError, IntegrityError
 from sqlalchemy.dialects.mysql import TINYINT, DOUBLE
 from sqlalchemy.sql.expression import FunctionElement, bindparam, cast
 from alembic.config import Config
 from alembic.script import ScriptDirectory
 from alembic.migration import MigrationContext
 from alembic.environment import EnvironmentContext
 from .exception import SpineDBAPIError, SpineDBVersionError
 
-# Supported dialects and recommended dbapi. Restricted to mysql and sqlite for now:
-# - sqlite works
-# - mysql is trying to work
 SUPPORTED_DIALECTS = {
     "mysql": "pymysql",
     "sqlite": "sqlite3",
 }
+"""Currently supported dialects and recommended dbapi."""
+
 
 UNSUPPORTED_DIALECTS = {
     "mssql": "pyodbc",
     "postgresql": "psycopg2",
 }
+"""Dialects and recommended dbapi that are not supported by DatabaseMapping but are supported by SqlAlchemy."""
+
 
 naming_convention = {
     "pk": "pk_%(table_name)s",
     "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
     "uq": "uq_%(table_name)s_%(column_0N_name)s",
     "ck": "ck_%(table_name)s_%(constraint_name)s",
 }
 
 model_meta = MetaData(naming_convention=naming_convention)
 
 LONGTEXT_LENGTH = 2 ** 32 - 1
 
 
+def name_from_elements(elements):
+    """Creates an entity name by combining element names.
+
+    Args:
+        elements (Sequence of str): element names
+
+    Returns:
+        str: entity name
+    """
+    if len(elements) == 1:
+        return elements[0] + "__"
+    return "__".join(elements)
+
+
+def name_from_dimensions(dimensions):
+    """Creates an entity class name by combining dimension names.
+
+    Args:
+        dimensions (Sequence of str): dimension names
+
+    Returns:
+        str: entity class name
+    """
+    return name_from_elements(dimensions)
+
+
 # NOTE: Deactivated since foreign keys are too difficult to get right in the diff tables.
 # For example, the diff_object table would need a `class_id` field and a `diff_class_id` field,
 # plus a CHECK constraint that at least one of the two is NOT NULL.
 # @event.listens_for(Engine, "connect")
 def set_sqlite_pragma(dbapi_connection, connection_record):
     module_name = dbapi_connection.__class__.__module__
     if not module_name.lower().startswith("sqlite"):
@@ -102,15 +127,15 @@
 def compile_DOUBLE_mysql_sqlite(element, compiler, **kw):
     """Handles mysql DOUBLE datatype as REAL in sqlite."""
     return compiler.visit_REAL(element, **kw)
 
 
 class group_concat(FunctionElement):
     type = String()
-    name = 'group_concat'
+    name = "group_concat"
 
 
 def _parse_group_concat_clauses(clauses):
     keys = ("group_concat_column", "order_by_column", "separator")
     d = dict(zip(keys, clauses))
     return d["group_concat_column"], d.get("order_by_column"), d.get("separator", bindparam("sep", ","))
 
@@ -150,32 +175,26 @@
         if isinstance(value, list):
             for val in value:
                 yield (key, str(val))
             continue
         yield (key, str(value))
 
 
-def is_head(db_url, upgrade=False):
-    """Check whether or not db_url is head.
+def _is_head(db_url, upgrade=False):
+    """Check whether or not db_url is at the head revision.
 
     Args:
         db_url (str): database url
         upgrade (Bool): if True, upgrade db to head
     """
     engine = create_engine(db_url)
     return is_head_engine(engine, upgrade=upgrade)
 
 
 def is_head_engine(engine, upgrade=False):
-    """Check whether or not engine is head.
-
-    Args:
-        engine (Engine): database engine
-        upgrade (Bool): if True, upgrade db to head
-    """
     config = Config()
     config.set_main_option("script_location", "spinedb_api:alembic")
     script = ScriptDirectory.from_config(config)
     head = script.get_current_head()
     with engine.connect() as connection:
         migration_context = MigrationContext.configure(connection)
         current_rev = migration_context.get_current_revision()
@@ -194,16 +213,25 @@
             environment_context.configure(connection=connection, target_metadata=model_meta)
             with environment_context.begin_transaction():
                 environment_context.run_migrations()
     return True
 
 
 def copy_database(dest_url, source_url, overwrite=True, upgrade=False, only_tables=(), skip_tables=()):
-    """Copy the database from source_url into dest_url."""
-    if not is_head(source_url, upgrade=upgrade):
+    """Copy the database from one url to another.
+
+    Args:
+        dest_url (str): The destination url.
+        source_url (str): The source url.
+        overwrite (bool, optional): whether to overwrite the destination.
+        upgrade (bool, optional): whether to upgrade the source to the latest Spine schema revision.
+        only_tables (tuple, optional): If given, only these tables are copied.
+        skip_tables (tuple, optional): If given, these tables are skipped.
+    """
+    if not _is_head(source_url, upgrade=upgrade):
         raise SpineDBVersionError(url=source_url)
     source_engine = create_engine(source_url)
     dest_engine = create_engine(dest_url)
     copy_database_bind(
         dest_engine,
         source_engine,
         overwrite=overwrite,
@@ -241,41 +269,27 @@
         data = result.fetchall()
         if not data:
             continue
         ins = dest_table.insert()
         try:
             dest_bind.execute(ins, data)
         except IntegrityError as e:
-            warnings.warn("Skipping table {0}: {1}".format(source_table.name, e.orig.args))
+            warnings.warn(f"Skipping table {source_table.name}: {e.orig.args}")
 
 
 def custom_generate_relationship(base, direction, return_fn, attrname, local_cls, referred_cls, **kw):
     """Make all relationships view only to avoid warnings."""
     kw["viewonly"] = True
     kw["cascade"] = ""
     kw["passive_deletes"] = False
     kw["sync_backref"] = False
     return generate_relationship(base, direction, return_fn, attrname, local_cls, referred_cls, **kw)
 
 
-def is_unlocked(db_url, timeout=0):
-    """Return True if the SQLite db_url is unlocked, after waiting at most timeout seconds.
-    Otherwise return False."""
-    if not db_url.startswith("sqlite"):
-        return False
-    try:
-        engine = create_engine(db_url, connect_args={"timeout": timeout})
-        engine.execute("BEGIN IMMEDIATE")
-        return True
-    except OperationalError:
-        return False
-
-
 def compare_schemas(left_engine, right_engine):
-    """Whether or not the left and right engine have the same schema."""
     left_insp = inspect(left_engine)
     right_insp = inspect(right_engine)
     left_dict = schema_dict(left_insp)
     right_dict = schema_dict(right_insp)
     return str(left_dict) == str(right_dict)
 
 
@@ -298,14 +312,21 @@
         raise SpineDBAPIError("Could not connect to '{}': {}".format(db_url, e.orig.args)) from None
     insp = inspect(engine)
     if insp.get_table_names():
         return False
     return True
 
 
+def get_head_alembic_version():
+    config = Config()
+    config.set_main_option("script_location", "spinedb_api:alembic")
+    script = ScriptDirectory.from_config(config)
+    return script.get_current_head()
+
+
 def create_spine_metadata():
     meta = MetaData(naming_convention=naming_convention)
     Table(
         "commit",
         meta,
         Column("id", Integer, primary_key=True),
         Column("comment", String(255), nullable=False),
@@ -346,137 +367,91 @@
         ),
         Column("rank", Integer, nullable=False),
         Column("commit_id", Integer, ForeignKey("commit.id")),
         UniqueConstraint("scenario_id", "rank"),
         UniqueConstraint("scenario_id", "alternative_id"),
     )
     Table(
-        "entity_class_type",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(255), nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
-        "entity_type",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(255), nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
         "entity_class",
         meta,
         Column("id", Integer, primary_key=True),
-        Column(
-            "type_id",
-            Integer,
-            ForeignKey("entity_class_type.id", onupdate="CASCADE", ondelete="CASCADE"),
-            nullable=False,
-        ),
         Column("name", String(255), nullable=False),
         Column("description", Text(), server_default=null()),
         Column("display_order", Integer, server_default="99"),
         Column("display_icon", BigInteger, server_default=null()),
         Column("hidden", Integer, server_default="0"),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("id", "type_id"),
-        UniqueConstraint("type_id", "name"),
+        Column("active_by_default", Boolean(name="active_by_default"), server_default=false(), nullable=False),
     )
     Table(
-        "object_class",
+        "superclass_subclass",
         meta,
-        Column("entity_class_id", Integer, primary_key=True),
-        Column("type_id", Integer, nullable=False),
-        ForeignKeyConstraint(
-            ("entity_class_id", "type_id"), ("entity_class.id", "entity_class.type_id"), ondelete="CASCADE"
+        Column("id", Integer, primary_key=True),
+        Column(
+            "superclass_id",
+            Integer,
+            ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE"),
+            nullable=False,
         ),
-        CheckConstraint("`type_id` = 1", name="type_id"),  # make sure object class can only have object type
-    )
-    Table(
-        "relationship_class",
-        meta,
-        Column("entity_class_id", Integer, primary_key=True),
-        Column("type_id", Integer, nullable=False),
-        ForeignKeyConstraint(
-            ("entity_class_id", "type_id"), ("entity_class.id", "entity_class.type_id"), ondelete="CASCADE"
+        Column(
+            "subclass_id",
+            Integer,
+            ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE"),
+            nullable=False,
+            unique=True,
         ),
-        CheckConstraint("`type_id` = 2", name="type_id"),
     )
     Table(
-        "relationship_entity_class",
+        "entity_class_dimension",
         meta,
         Column(
             "entity_class_id",
             Integer,
-            ForeignKey("relationship_class.entity_class_id", onupdate="CASCADE", ondelete="CASCADE"),
+            ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE"),
             primary_key=True,
         ),
-        Column("dimension", Integer, primary_key=True),
-        Column("member_class_id", Integer, nullable=False),
-        Column("member_class_type_id", Integer, nullable=False),
-        UniqueConstraint("entity_class_id", "dimension", "member_class_id", name="uq_relationship_entity_class"),
-        ForeignKeyConstraint(("member_class_id", "member_class_type_id"), ("entity_class.id", "entity_class.type_id")),
-        CheckConstraint("`member_class_type_id` != 2", name="member_class_type_id"),
+        Column(
+            "dimension_id",
+            Integer,
+            ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE"),
+            primary_key=True,
+        ),
+        Column("position", Integer, primary_key=True),
+        UniqueConstraint("entity_class_id", "dimension_id", "position", name="uq_entity_class_dimension"),
     )
     Table(
         "entity",
         meta,
         Column("id", Integer, primary_key=True),
-        Column("type_id", Integer, ForeignKey("entity_type.id", onupdate="CASCADE", ondelete="CASCADE")),
         Column("class_id", Integer, ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE")),
         Column("name", String(255), nullable=False),
         Column("description", Text(), server_default=null()),
         Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("id", "class_id"),
-        UniqueConstraint("id", "type_id", "class_id"),
         UniqueConstraint("class_id", "name"),
     )
     Table(
-        "object",
-        meta,
-        Column("entity_id", Integer, primary_key=True),
-        Column("type_id", Integer, nullable=False),
-        ForeignKeyConstraint(("entity_id", "type_id"), ("entity.id", "entity.type_id"), ondelete="CASCADE"),
-        CheckConstraint("`type_id` = 1", name="type_id"),  # make sure object can only have object type
-    )
-    Table(
-        "relationship",
-        meta,
-        Column("entity_id", Integer, primary_key=True),
-        Column("entity_class_id", Integer, nullable=False),
-        Column("type_id", Integer, nullable=False),
-        UniqueConstraint("entity_id", "entity_class_id"),
-        ForeignKeyConstraint(("entity_id", "type_id"), ("entity.id", "entity.type_id"), ondelete="CASCADE"),
-        CheckConstraint("`type_id` = 2", name="type_id"),
-    )
-    Table(
-        "relationship_entity",
+        "entity_element",
         meta,
         Column("entity_id", Integer, primary_key=True),
         Column("entity_class_id", Integer, nullable=False),
-        Column("dimension", Integer, primary_key=True),
-        Column("member_id", Integer, nullable=False),
-        Column("member_class_id", Integer, nullable=False),
+        Column("element_id", Integer, nullable=False),
+        Column("dimension_id", Integer, nullable=False),
+        Column("position", Integer, primary_key=True),
         ForeignKeyConstraint(
-            ("member_id", "member_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
+            ("entity_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
         ),
         ForeignKeyConstraint(
-            ("entity_class_id", "dimension", "member_class_id"),
-            (
-                "relationship_entity_class.entity_class_id",
-                "relationship_entity_class.dimension",
-                "relationship_entity_class.member_class_id",
-            ),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
+            ("element_id", "dimension_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
         ),
         ForeignKeyConstraint(
-            ("entity_id", "entity_class_id"),
-            ("relationship.entity_id", "relationship.entity_class_id"),
+            ("entity_class_id", "dimension_id", "position"),
+            (
+                "entity_class_dimension.entity_class_id",
+                "entity_class_dimension.dimension_id",
+                "entity_class_dimension.position",
+            ),
             onupdate="CASCADE",
             ondelete="CASCADE",
         ),
     )
     Table(
         "entity_group",
         meta,
@@ -489,14 +464,29 @@
             ("entity_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
         ),
         ForeignKeyConstraint(
             ("member_id", "entity_class_id"), ("entity.id", "entity.class_id"), onupdate="CASCADE", ondelete="CASCADE"
         ),
     )
     Table(
+        "entity_alternative",
+        meta,
+        Column("id", Integer, primary_key=True),
+        Column("entity_id", Integer, ForeignKey("entity.id", onupdate="CASCADE", ondelete="CASCADE"), nullable=False),
+        Column(
+            "alternative_id",
+            Integer,
+            ForeignKey("alternative.id", onupdate="CASCADE", ondelete="CASCADE"),
+            nullable=False,
+        ),
+        Column("active", Boolean(name="active"), server_default=true(), nullable=False),
+        Column("commit_id", Integer, ForeignKey("commit.id")),
+        UniqueConstraint("entity_id", "alternative_id"),
+    )
+    Table(
         "parameter_definition",
         meta,
         Column("id", Integer, primary_key=True),
         Column(
             "entity_class_id",
             Integer,
             ForeignKey("entity_class.id", onupdate="CASCADE", ondelete="CASCADE"),
@@ -571,79 +561,14 @@
         Column("index", Integer, nullable=False),
         Column("type", String(255)),
         Column("value", LargeBinary(LONGTEXT_LENGTH), server_default=null()),
         Column("commit_id", Integer, ForeignKey("commit.id")),
         UniqueConstraint("parameter_value_list_id", "index"),
     )
     Table(
-        "tool",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("name", String(155), nullable=False),
-        Column("description", Text(), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-    )
-    Table(
-        "feature",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("parameter_definition_id", Integer, nullable=False),
-        Column("parameter_value_list_id", Integer, nullable=False),
-        Column("description", Text(), server_default=null()),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("parameter_definition_id", "parameter_value_list_id"),
-        UniqueConstraint("id", "parameter_value_list_id"),
-        ForeignKeyConstraint(
-            ("parameter_definition_id", "parameter_value_list_id"),
-            ("parameter_definition.id", "parameter_definition.parameter_value_list_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    Table(
-        "tool_feature",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("tool_id", Integer, ForeignKey("tool.id")),
-        Column("feature_id", Integer, nullable=False),
-        Column("parameter_value_list_id", Integer, nullable=False),
-        Column("required", Boolean(name="required"), server_default=false(), nullable=False),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("tool_id", "feature_id"),
-        UniqueConstraint("id", "parameter_value_list_id"),
-        ForeignKeyConstraint(
-            ("feature_id", "parameter_value_list_id"),
-            ("feature.id", "feature.parameter_value_list_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    Table(
-        "tool_feature_method",
-        meta,
-        Column("id", Integer, primary_key=True),
-        Column("tool_feature_id", Integer, nullable=False),
-        Column("parameter_value_list_id", Integer, nullable=False),
-        Column("method_index", Integer),
-        Column("commit_id", Integer, ForeignKey("commit.id")),
-        UniqueConstraint("tool_feature_id", "method_index"),
-        ForeignKeyConstraint(
-            ("tool_feature_id", "parameter_value_list_id"),
-            ("tool_feature.id", "tool_feature.parameter_value_list_id"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-        ForeignKeyConstraint(
-            ("parameter_value_list_id", "method_index"),
-            ("list_value.parameter_value_list_id", "list_value.index"),
-            onupdate="CASCADE",
-            ondelete="CASCADE",
-        ),
-    )
-    Table(
         "metadata",
         meta,
         Column("id", Integer, primary_key=True),
         Column("name", String(155), nullable=False),
         Column("value", String(255), nullable=False),
         Column("commit_id", Integer, ForeignKey("commit.id")),
         UniqueConstraint("name", "value"),
@@ -681,45 +606,55 @@
         Column("version_num", String(32), nullable=False),
         PrimaryKeyConstraint("version_num", name="alembic_version_pkc"),
     )
     return meta
 
 
 def create_new_spine_database(db_url):
-    """Create a new Spine database at the given url."""
+    """Create a new Spine database at the given url.
+
+    Args:
+        db_url (str): The url.
+
+    Returns:
+        Engine
+    """
     try:
         engine = create_engine(db_url)
     except DatabaseError as e:
-        raise SpineDBAPIError("Could not connect to '{}': {}".format(db_url, e.orig.args)) from None
+        raise SpineDBAPIError(f"Could not connect to '{db_url}': {e.orig.args}") from None
+    create_new_spine_database_from_bind(engine)
+    return engine
+
+
+def create_new_spine_database_from_bind(bind):
     # Drop existing tables. This is a Spine db now...
-    meta = MetaData(engine)
+    meta = MetaData(bind)
     meta.reflect()
     meta.drop_all()
     # Create new tables
     meta = create_spine_metadata()
+    version = get_head_alembic_version()
     try:
-        meta.create_all(engine)
-        engine.execute("INSERT INTO `commit` VALUES (1, 'Create the database', CURRENT_TIMESTAMP, 'spinedb_api')")
-        engine.execute("INSERT INTO alternative VALUES (1, 'Base', 'Base alternative', 1)")
-        engine.execute("INSERT INTO entity_class_type VALUES (1, 'object', 1), (2, 'relationship', 1)")
-        engine.execute("INSERT INTO entity_type VALUES (1, 'object', 1), (2, 'relationship', 1)")
-        engine.execute("INSERT INTO alembic_version VALUES ('989fccf80441')")
+        meta.create_all(bind)
+        bind.execute("INSERT INTO `commit` VALUES (1, 'Create the database', CURRENT_TIMESTAMP, 'spinedb_api')")
+        bind.execute("INSERT INTO alternative VALUES (1, 'Base', 'Base alternative', 1)")
+        bind.execute(f"INSERT INTO alembic_version VALUES ('{version}')")
     except DatabaseError as e:
-        raise SpineDBAPIError("Unable to create Spine database: {}".format(e)) from None
-    return engine
+        raise SpineDBAPIError(f"Unable to create Spine database: {e}") from None
 
 
 def _create_first_spine_database(db_url):
     """Creates a Spine database with the very first version at the given url.
     Used internally.
     """
     try:
         engine = create_engine(db_url)
     except DatabaseError as e:
-        raise SpineDBAPIError("Could not connect to '{}': {}".format(db_url, e.orig.args)) from None
+        raise SpineDBAPIError(f"Could not connect to '{db_url}': {e.orig.args}") from None
     # Drop existing tables. This is a Spine db now...
     meta = MetaData(engine)
     meta.reflect()
     meta.drop_all(engine)
     # Create new tables
     meta = MetaData(naming_convention=naming_convention)
     Table(
@@ -853,26 +788,26 @@
             onupdate="CASCADE",
             ondelete="CASCADE",
         ),
     )
     try:
         meta.create_all(engine)
     except DatabaseError as e:
-        raise SpineDBAPIError("Unable to create Spine database: {}".format(e.orig.args))
+        raise SpineDBAPIError(f"Unable to create Spine database: {e.orig.args}")
     return engine
 
 
-def forward_sweep(root, fn):
+def forward_sweep(root, fn, *args):
     """Recursively visit, using `get_children()`, the given sqlalchemy object.
     Apply `fn` on every visited node."""
     current = root
     parent = {}
     children = {current: iter(current.get_children(column_collections=False))}
     while True:
-        fn(current)
+        fn(current, *args)
         # Try and visit next children
         next_ = next(children[current], None)
         if next_ is not None:
             parent[next_] = current
             children[next_] = iter(next_.get_children(column_collections=False))
             current = next_
             continue
@@ -886,43 +821,14 @@
             continue
         # No (more) siblings, go back to parent
         current = current_parent
         if current == root:
             break
 
 
-def get_relationship_entity_class_items(item, object_class_type):
-    return [
-        {
-            "entity_class_id": item["id"],
-            "dimension": dimension,
-            "member_class_id": object_class_id,
-            "member_class_type_id": object_class_type,
-        }
-        for dimension, object_class_id in enumerate(item["object_class_id_list"])
-    ]
-
-
-def get_relationship_entity_items(item, relationship_entity_type, object_entity_type):
-    return [
-        {
-            "entity_id": item["id"],
-            "type_id": relationship_entity_type,
-            "entity_class_id": item["class_id"],
-            "dimension": dimension,
-            "member_id": object_id,
-            "member_class_type_id": object_entity_type,
-            "member_class_id": object_class_id,
-        }
-        for dimension, (object_id, object_class_id) in enumerate(
-            zip(item["object_id_list"], item["object_class_id_list"])
-        )
-    ]
-
-
 def labelled_columns(table):
     return [c.label(c.name) for c in table.columns]
 
 
 class AsteriskType:
     def __repr__(self):
         return "Asterisk"
@@ -965,11 +871,43 @@
 
     Args:
         url (str): URL
 
     Returns:
         str: sanitized URL
     """
-    parsed = urlparse(url)
-    if parsed.username is None:
+    if "@" not in url:
         return url
-    return urlunparse(parsed._replace(netloc=parsed.netloc.partition("@")[-1]))
+    head, tail = url.rsplit("@", maxsplit=1)
+    scheme, credentials = head.split("://", maxsplit=1)
+    return scheme + "://" + tail
+
+
+def group_consecutive(list_of_numbers):
+    for _k, g in groupby(enumerate(sorted(list_of_numbers)), lambda x: x[0] - x[1]):
+        group = list(map(itemgetter(1), g))
+        yield group[0], group[-1]
+
+
+_TRUTHS = {s.casefold() for s in ("yes", "true", "y", "t", "1")}
+_FALSES = {s.casefold() for s in ("no", "false", "n", "f", "0")}
+
+
+def string_to_bool(string):
+    """Converts string to boolean.
+
+    Recognizes "yes", "true", "y", "t" and "1" as True, "no", "false", "n", "f" and "0" as False.
+    Case insensitive.
+    Raises Value error if value is not recognized.
+
+    Args:
+        string (str): string to convert
+
+    Returns:
+        bool: True or False
+    """
+    string = string.casefold()
+    if string in _TRUTHS:
+        return True
+    if string in _FALSES:
+        return False
+    raise ValueError(string)
```

### Comparing `spinedb_api-0.30.5/spinedb_api/import_mapping/__init__.py` & `spinedb_api-0.31.0/tests/export_mapping/__init__.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/import_mapping/generator.py` & `spinedb_api-0.31.0/spinedb_api/import_mapping/generator.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -12,14 +13,15 @@
 """
 Contains `get_mapped_data()` that converts rows of tabular data into a dictionary for import to a Spine DB,
 using ``import_functions.import_data()``
 
 """
 
 from copy import deepcopy
+from operator import itemgetter
 from .import_mapping_compat import import_mapping_from_dict
 from .import_mapping import ImportMapping, check_validity
 from ..mapping import Position
 from ..parameter_value import (
     convert_leaf_maps_to_specialized_containers,
     Map,
     TimeSeriesVariableResolution,
@@ -78,24 +80,25 @@
         elif not isinstance(mapping, ImportMapping):
             raise TypeError(f"mapping must be a dict or ImportMapping subclass, instead got: {type(mapping).__name__}")
     mapped_data = {}
     errors = []
     rows = list(data_source)
     if not rows:
         return mapped_data, errors
+    column_count = len(max(rows, key=lambda x: len(x) if x else 0))
     if column_convert_fns is None:
         column_convert_fns = {}
     if row_convert_fns is None:
         row_convert_fns = {}
     if default_column_convert_fn is None:
         default_column_convert_fn = column_convert_fns[max(column_convert_fns)] if column_convert_fns else identity
     for mapping in mappings:
         read_state = {}
         mapping = deepcopy(mapping)
-        mapping.polish(table_name, data_header)
+        mapping.polish(table_name, data_header, column_count)
         mapping_errors = check_validity(mapping)
         if mapping_errors:
             errors += mapping_errors
             continue
         # Find pivoted and unpivoted mappings
         pivoted, non_pivoted, pivoted_from_header, last = _split_mapping(mapping)
         # If there are no pivoted mappings, we can just feed the rows to our mapping directly
@@ -153,15 +156,16 @@
             for column_pos, unpivoted_row in zip(unpivoted_column_pos, unpivoted_rows):
                 if not _is_valid_row(unpivoted_row):
                     continue
                 unpivoted_row = _convert_row(unpivoted_row, row_convert_fns, k, errors)
                 full_row = non_pivoted_row + unpivoted_row
                 full_row.append(row[column_pos])
                 mapping.import_row(full_row, read_state, mapped_data)
-    _make_relationship_classes(mapped_data)
+    _make_entity_classes(mapped_data)
+    _make_entities(mapped_data)
     _make_parameter_values(mapped_data, unparse_value)
     return mapped_data, errors
 
 
 def _is_valid_row(row):
     return row is not None and not all(i is None for i in row)
 
@@ -258,45 +262,52 @@
             for data in row[last_pivoted_position + 1 :]:
                 expanded_pivoted_rows.append(head + [data])
         unpivoted_rows = expanded_pivoted_rows
     unpivoted_column_pos = [k for k in range(len(rows[0])) if k not in skip_pos] if rows else []
     return unpivoted_rows, pivoted_pos, non_pivoted_pos, unpivoted_column_pos
 
 
-def _make_relationship_classes(mapped_data):
-    rows = mapped_data.get("relationship_classes")
+def _make_entity_classes(mapped_data):
+    rows = mapped_data.get("entity_classes")
     if rows is None:
         return
-    full_rows = []
-    for class_name, object_classes in rows.items():
-        full_rows.append((class_name, object_classes))
-    mapped_data["relationship_classes"] = full_rows
+    rows = [(class_name, tuple(dimension_names)) for class_name, dimension_names in rows.items()]
+    rows.sort(key=itemgetter(1))
+    mapped_data["entity_classes"] = final_rows = []
+    for class_name, dimension_names in rows:
+        row = (class_name, tuple(dimension_names)) if dimension_names else (class_name,)
+        final_rows.append(row)
+
+
+def _make_entities(mapped_data):
+    rows = mapped_data.get("entities")
+    if rows is None:
+        return
+    mapped_data["entities"] = list(rows)
 
 
 def _make_parameter_values(mapped_data, unparse_value):
     value_pos = 3
-    for key in ("object_parameter_values", "relationship_parameter_values"):
-        rows = mapped_data.get(key)
-        if rows is None:
-            continue
+    key = "parameter_values"
+    rows = mapped_data.get(key)
+    if rows is not None:
         valued_rows = []
         for row in rows:
             raw_value = _make_value(row, value_pos)
             if raw_value is _NO_VALUE:
                 continue
             value = unparse_value(raw_value)
             if value is not None:
                 row[value_pos] = value
                 valued_rows.append(row)
         mapped_data[key] = valued_rows
     value_pos = 0
-    for key in ("object_parameters", "relationship_parameters"):
-        rows = mapped_data.get(key)
-        if rows is None:
-            continue
+    key = "parameter_definitions"
+    rows = mapped_data.get(key)
+    if rows is not None:
         full_rows = []
         for entity_definition, extras in rows.items():
             if extras:
                 value = unparse_value(_make_value(extras, value_pos))
                 if value is not None:
                     extras[value_pos] = value
                     full_rows.append(entity_definition + tuple(extras))
```

### Comparing `spinedb_api-0.30.5/spinedb_api/import_mapping/import_mapping.py` & `spinedb_api-0.31.0/spinedb_api/import_mapping/import_mapping.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,77 +1,64 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
-"""
-Contains import mappings for database items such as entities, entity classes and parameter values.
+""" Contains import mappings for database items such as entities, entity classes and parameter values. """
 
-"""
-
-from distutils.util import strtobool
 from enum import auto, Enum, unique
+
+from spinedb_api.helpers import string_to_bool
 from spinedb_api.mapping import Mapping, Position, unflatten, is_pivoted
 from spinedb_api.exception import InvalidMappingComponent
 
 
 @unique
 class ImportKey(Enum):
-    CLASS_NAME = auto()
-    RELATIONSHIP_DIMENSION_COUNT = auto()
-    OBJECT_CLASS_NAME = auto()
-    OBJECT_NAME = auto()
+    DIMENSION_COUNT = auto()
+    ENTITY_CLASS_NAME = auto()
+    ENTITY_NAME = auto()
     GROUP_NAME = auto()
     MEMBER_NAME = auto()
     PARAMETER_NAME = auto()
     PARAMETER_DEFINITION = auto()
     PARAMETER_DEFINITION_EXTRAS = auto()
     PARAMETER_DEFAULT_VALUES = auto()
     PARAMETER_DEFAULT_VALUE_INDEXES = auto()
     PARAMETER_VALUES = auto()
     PARAMETER_VALUE_INDEXES = auto()
-    RELATIONSHIP_CLASS_NAME = auto()
-    OBJECT_CLASS_NAMES = auto()
-    OBJECT_NAMES = auto()
+    DIMENSION_NAMES = auto()
+    ELEMENT_NAMES = auto()
     ALTERNATIVE_NAME = auto()
     SCENARIO_NAME = auto()
     SCENARIO_ALTERNATIVE = auto()
-    FEATURE = auto()
-    TOOL_NAME = auto()
-    TOOL_FEATURE = auto()
-    TOOL_FEATURE_METHOD = auto()
     PARAMETER_VALUE_LIST_NAME = auto()
 
     def __str__(self):
         name = {
             self.ALTERNATIVE_NAME.value: "Alternative names",
-            self.CLASS_NAME.value: "Class names",
-            self.OBJECT_CLASS_NAME.value: "Object class names",
-            self.OBJECT_NAME.value: "Object names",
+            self.ENTITY_CLASS_NAME.value: "Entity class names",
+            self.ENTITY_NAME.value: "Entity names",
             self.GROUP_NAME.value: "Group names",
             self.MEMBER_NAME.value: "Member names",
             self.PARAMETER_NAME.value: "Parameter names",
             self.PARAMETER_DEFINITION.value: "Parameter names",
             self.PARAMETER_DEFAULT_VALUE_INDEXES.value: "Parameter indexes",
             self.PARAMETER_VALUE_INDEXES.value: "Parameter indexes",
-            self.RELATIONSHIP_CLASS_NAME.value: "Relationship class names",
-            self.OBJECT_CLASS_NAMES.value: "Object class names",
-            self.OBJECT_NAMES.value: "Object names",
+            self.DIMENSION_NAMES.value: "Dimension names",
+            self.ELEMENT_NAMES.value: "Element names",
             self.PARAMETER_VALUE_LIST_NAME.value: "Parameter value lists",
             self.SCENARIO_NAME.value: "Scenario names",
             self.SCENARIO_ALTERNATIVE.value: "Alternative names",
-            self.TOOL_NAME.value: "Tool names",
-            self.FEATURE.value: "Entity class names",
-            self.TOOL_FEATURE.value: "Entity class names",
-            self.TOOL_FEATURE_METHOD.value: "Entity class names",
         }.get(self.value)
         if name is not None:
             return name
         return super().__str__()
 
 
 class KeyFix(Exception):
@@ -143,30 +130,52 @@
     def read_start_row(self, row):
         if not isinstance(row, int):
             raise TypeError(f"row must be int, instead got {type(row).__name__}")
         if row < 0:
             raise ValueError(f"row must be >= 0 ({row})")
         self._read_start_row = row
 
-    def polish(self, table_name, source_header, for_preview=False):
+    def check_for_invalid_column_refs(self, header, table_name):
+        """Checks that the mappings column refs are not out of range for the source table
+
+        Args:
+            header (list): The header of the table as a list
+            table_name (str): The name of the source table
+
+        Returns:
+            str: Error message if a column ref exceeds the column count of the source table,
+            empty string otherwise
+        """
+        if self.child is not None:
+            error = self.child.check_for_invalid_column_refs(header, table_name)
+            if error:
+                return error
+        if isinstance(self.position, int) and self.position >= len(header) > 0:
+            msg = f'Column ref {self.position + 1} is out of range for the source table "{table_name}"'
+            return msg
+        return ""
+
+    def polish(self, table_name, source_header, column_count=0, for_preview=False):
         """Polishes the mapping before an import operation.
         'Expands' transient ``position`` and ``value`` attributes into their final value.
 
         Args:
             table_name (str)
             source_header (list(str))
+            column_count (int, optional)
+            for_preview (bool, optional)
         """
-        self._polish_for_import(table_name, source_header)
+        self._polish_for_import(table_name, source_header, column_count)
         if for_preview:
             self._polish_for_preview(source_header)
 
-    def _polish_for_import(self, table_name, source_header):
+    def _polish_for_import(self, table_name, source_header, column_count):
         # FIXME: Polish skip columns
         if self.child is not None:
-            self.child._polish_for_import(table_name, source_header)
+            self.child._polish_for_import(table_name, source_header, column_count)
         if isinstance(self.position, str):
             # Column mapping with string position, we need to find the index in the header
             try:
                 self.position = source_header.index(self.position)
                 return
             except ValueError:
                 msg = f"'{self.position}' is not in '{source_header}'"
@@ -193,14 +202,17 @@
             # Integer value, we try and get the actual value from that index in the header
             try:
                 self._index = self.value
                 self.value = source_header[self.value]
             except IndexError:
                 msg = f"'{self.value}' is not a valid index in header '{source_header}'"
                 raise InvalidMappingComponent(msg)
+        if isinstance(self.position, int) and self.position >= column_count > 0:
+            msg = f'Column ref {self.position + 1} is out of range for the source table "{table_name}"'
+            raise InvalidMappingComponent(msg)
 
     def _polish_for_preview(self, source_header):
         if self.position == Position.header and self.value is not None:
             self.value = self._index
         if self.child is not None:
             self.child._polish_for_preview(source_header)
 
@@ -259,15 +271,15 @@
                     for key in fix.args:
                         indexes |= {k for k, err in enumerate(errors) if err.key == key}
                     for k in sorted(indexes, reverse=True):
                         errors.pop(k)
         if self.child is not None:
             self.child.import_row(source_row, state, mapped_data, errors=errors)
 
-    def _data(self, source_row):  # pylint: disable=arguments-differ
+    def _data(self, source_row):  # pylint: disable=arguments-renamed
         if source_row is None:
             return None
         return source_row[self.position]
 
     def _import_row(self, source_data, state, mapped_data):
         raise NotImplementedError()
 
@@ -310,29 +322,29 @@
         Returns:
             Mapping: reconstructed mapping
         """
         mapping = cls(position, value, skip_columns, read_start_row, filter_re)
         return mapping
 
 
-class ImportObjectsMixin:
-    def __init__(self, position, value=None, skip_columns=None, read_start_row=0, filter_re="", import_objects=False):
+class ImportEntitiesMixin:
+    def __init__(self, position, value=None, skip_columns=None, read_start_row=0, filter_re="", import_entities=False):
         super().__init__(position, value, skip_columns, read_start_row, filter_re)
-        self.import_objects = import_objects
+        self.import_entities = import_entities
 
     def to_dict(self):
         d = super().to_dict()
-        if self.import_objects:
-            d["import_objects"] = True
+        if self.import_entities:
+            d["import_entities"] = True
         return d
 
     @classmethod
     def reconstruct(cls, position, value, skip_columns, read_start_row, filter_re, mapping_dict):
-        import_objects = mapping_dict.get("import_objects", False)
-        mapping = cls(position, value, skip_columns, read_start_row, filter_re, import_objects)
+        import_entities = mapping_dict.get("import_entities", False)
+        mapping = cls(position, value, skip_columns, read_start_row, filter_re, import_entities)
         return mapping
 
 
 class IndexedValueMixin:
     def __init__(
         self, position, value=None, skip_columns=None, read_start_row=0, filter_re="", compress=False, options=None
     ):
@@ -354,194 +366,149 @@
     def reconstruct(cls, position, value, skip_columns, read_start_row, filter_re, mapping_dict):
         compress = mapping_dict.get("compress", False)
         options = mapping_dict.get("options")
         mapping = cls(position, value, skip_columns, read_start_row, filter_re, compress, options)
         return mapping
 
 
-class ObjectClassMapping(ImportMapping):
-    """Maps object classes.
+class EntityClassMapping(ImportMapping):
+    """Maps entity classes.
 
     Can be used as the topmost mapping.
     """
 
-    MAP_TYPE = "ObjectClass"
+    MAP_TYPE = "EntityClass"
 
     def _import_row(self, source_data, state, mapped_data):
-        object_class_name = state[ImportKey.OBJECT_CLASS_NAME] = str(source_data)
-        object_classes = mapped_data.setdefault("object_classes", set())
-        object_classes.add(object_class_name)
+        dim_count = len([m for m in self.flatten() if isinstance(m, DimensionMapping)])
+        state[ImportKey.DIMENSION_COUNT] = dim_count
+        entity_class_name = state[ImportKey.ENTITY_CLASS_NAME] = str(source_data)
+        dimension_names = state[ImportKey.DIMENSION_NAMES] = []
+        entity_classes = mapped_data.setdefault("entity_classes", {})
+        entity_classes[entity_class_name] = dimension_names
+        if dim_count:
+            raise KeyError(ImportKey.DIMENSION_NAMES)
 
 
-class ObjectMapping(ImportMapping):
-    """Maps objects.
+class EntityMapping(ImportMapping):
+    """Maps entities.
 
-    Cannot be used as the topmost mapping; one of the parents must be :class:`ObjectClassMapping`.
+    Cannot be used as the topmost mapping; one of the parents must be :class:`EntityClassMapping`.
     """
 
-    MAP_TYPE = "Object"
+    MAP_TYPE = "Entity"
+
+    def import_row(self, source_row, state, mapped_data, errors=None):
+        state[ImportKey.ELEMENT_NAMES] = ()
+        super().import_row(source_row, state, mapped_data, errors=errors)
 
     def _import_row(self, source_data, state, mapped_data):
-        object_class_name = state[ImportKey.OBJECT_CLASS_NAME]
-        object_name = state[ImportKey.OBJECT_NAME] = str(source_data)
-        if isinstance(self.child, ObjectGroupMapping):
+        if state[ImportKey.DIMENSION_COUNT]:
+            return
+        entity_class_name = state[ImportKey.ENTITY_CLASS_NAME]
+        entity_name = state[ImportKey.ENTITY_NAME] = str(source_data)
+        if isinstance(self.child, EntityGroupMapping):
             raise KeyError(ImportKey.MEMBER_NAME)
-        mapped_data.setdefault("objects", set()).add((object_class_name, object_name))
+        mapped_data.setdefault("entities", {})[entity_class_name, entity_name] = None
 
 
-class ObjectMetadataMapping(ImportMapping):
-    """Maps object metadata.
+class EntityMetadataMapping(ImportMapping):
+    """Maps entity metadata.
 
-    Cannot be used as the topmost mapping; must have :class:`ObjectClassMapping` and :class:`ObjectMapping` as parents.
+    Cannot be used as the topmost mapping; must have :class:`EntityClassMapping` and :class:`EntityMapping` as parents.
     """
 
-    MAP_TYPE = "ObjectMetadata"
+    MAP_TYPE = "EntityMetadata"
 
     def _import_row(self, source_data, state, mapped_data):
         pass
 
 
-class ObjectGroupMapping(ImportObjectsMixin, ImportMapping):
-    """Maps object groups.
+class EntityGroupMapping(ImportEntitiesMixin, ImportMapping):
+    """Maps entity groups.
 
-    Cannot be used as the topmost mapping; must have :class:`ObjectClassMapping` and :class:`ObjectMapping` as parents.
+    Cannot be used as the topmost mapping; must have :class:`EntityClassMapping` and :class:`EntityMapping` as parents.
     """
 
-    MAP_TYPE = "ObjectGroup"
+    MAP_TYPE = "EntityGroup"
 
     def _import_row(self, source_data, state, mapped_data):
-        object_class_name = state[ImportKey.OBJECT_CLASS_NAME]
-        group_name = state.get(ImportKey.OBJECT_NAME)
+        entity_class_name = state[ImportKey.ENTITY_CLASS_NAME]
+        group_name = state.get(ImportKey.ENTITY_NAME)
         if group_name is None:
             raise KeyError(ImportKey.GROUP_NAME)
         member_name = str(source_data)
-        mapped_data.setdefault("object_groups", set()).add((object_class_name, group_name, member_name))
-        if self.import_objects:
-            objects = (object_class_name, group_name), (object_class_name, member_name)
-            mapped_data.setdefault("objects", set()).update(objects)
+        mapped_data.setdefault("entity_groups", set()).add((entity_class_name, group_name, member_name))
+        if self.import_entities:
+            entities = mapped_data.setdefault("entities", {})
+            entities[entity_class_name, group_name] = None
+            entities[entity_class_name, member_name] = None
         raise KeyFix(ImportKey.MEMBER_NAME)
 
 
-class RelationshipClassMapping(ImportMapping):
-    """Maps relationships classes.
-
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "RelationshipClass"
-
-    def _import_row(self, source_data, state, mapped_data):
-        dim_count = len([m for m in self.flatten() if isinstance(m, RelationshipClassObjectClassMapping)])
-        state[ImportKey.RELATIONSHIP_DIMENSION_COUNT] = dim_count
-        relationship_class_name = state[ImportKey.RELATIONSHIP_CLASS_NAME] = str(source_data)
-        object_class_names = state[ImportKey.OBJECT_CLASS_NAMES] = []
-        relationship_classes = mapped_data.setdefault("relationship_classes", dict())
-        relationship_classes[relationship_class_name] = object_class_names
-        raise KeyError(ImportKey.OBJECT_CLASS_NAMES)
-
-
-class RelationshipClassObjectClassMapping(ImportMapping):
-    """Maps relationship class object classes.
+class DimensionMapping(ImportMapping):
+    """Maps dimensions.
 
-    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
+    Cannot be used as the topmost mapping; one of the parents must be :class:`EntityClassMapping`.
     """
 
-    MAP_TYPE = "RelationshipClassObjectClass"
+    MAP_TYPE = "Dimension"
 
     def _import_row(self, source_data, state, mapped_data):
-        _ = state[ImportKey.RELATIONSHIP_CLASS_NAME]
-        object_class_names = state[ImportKey.OBJECT_CLASS_NAMES]
-        object_class_name = str(source_data)
-        object_class_names.append(object_class_name)
-        if len(object_class_names) == state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
-            raise KeyFix(ImportKey.OBJECT_CLASS_NAMES)
+        _ = state[ImportKey.ENTITY_CLASS_NAME]
+        dimension_name = str(source_data)
+        state[ImportKey.DIMENSION_NAMES].append(dimension_name)
+        dimension_names = state[ImportKey.DIMENSION_NAMES]
+        if len(dimension_names) == state[ImportKey.DIMENSION_COUNT]:
+            raise KeyFix(ImportKey.DIMENSION_NAMES)
 
 
-class RelationshipMapping(ImportMapping):
-    """Maps relationships.
+class ElementMapping(ImportEntitiesMixin, ImportMapping):
+    """Maps elements.
 
-    Cannot be used as the topmost mapping; one of the parents must be :class:`RelationshipClassMapping`.
-    """
-
-    MAP_TYPE = "Relationship"
-
-    def _import_row(self, source_data, state, mapped_data):
-        # Don't access state[ImportKey.RELATIONSHIP_CLASS_NAME], we don't want to catch errors here
-        # because this one's invisible.
-        state[ImportKey.OBJECT_NAMES] = []
-
-
-class RelationshipObjectMapping(ImportObjectsMixin, ImportMapping):
-    """Maps relationship's objects.
-
-    Cannot be used as the topmost mapping; must have :class:`RelationshipClassMapping` and :class:`RelationshipMapping`
+    Cannot be used as the topmost mapping; must have :class:`EntityClassMapping` and :class:`EntityMapping`
     as parents.
     """
 
-    MAP_TYPE = "RelationshipObject"
+    MAP_TYPE = "Element"
 
     def _import_row(self, source_data, state, mapped_data):
-        relationship_class_name = state[ImportKey.RELATIONSHIP_CLASS_NAME]
-        object_class_names = state[ImportKey.OBJECT_CLASS_NAMES]
-        if len(object_class_names) != state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
-            raise KeyError(ImportKey.OBJECT_CLASS_NAMES)
-        object_names = state[ImportKey.OBJECT_NAMES]
-        object_name = str(source_data)
-        object_names.append(object_name)
-        if self.import_objects:
-            k = len(object_names) - 1
-            object_class_name = object_class_names[k]
-            mapped_data.setdefault("object_classes", set()).add(object_class_name)
-            mapped_data.setdefault("objects", set()).add((object_class_name, object_name))
-        if len(object_names) == state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
-            relationships = mapped_data.setdefault("relationships", set())
-            relationships.add((relationship_class_name, tuple(object_names)))
-            raise KeyFix(ImportKey.OBJECT_NAMES)
-        raise KeyError(ImportKey.OBJECT_NAMES)
-
-
-class RelationshipMetadataMapping(ImportMapping):
-    """Maps relationship metadata.
-
-    Cannot be used as the topmost mapping; must have :class:`RelationshipClassMapping`, a :class:`RelationshipMapping`
-    and one or more :class:`RelationshipObjectMapping` as parents.
-    """
-
-    MAP_TYPE = "RelationshipMetadata"
-
-    def _import_row(self, source_data, state, mapped_data):
-        pass
+        entity_class_name = state[ImportKey.ENTITY_CLASS_NAME]
+        dimension_names = state[ImportKey.DIMENSION_NAMES]
+        if len(dimension_names) != state[ImportKey.DIMENSION_COUNT]:
+            raise KeyError(ImportKey.DIMENSION_NAMES)
+        element_name = str(source_data)
+        element_names = state[ImportKey.ELEMENT_NAMES] = state[ImportKey.ELEMENT_NAMES] + (element_name,)
+        if self.import_entities:
+            k = len(element_names) - 1
+            dimension_name = dimension_names[k]
+            mapped_data.setdefault("entity_classes", {}).update({dimension_name: ()})
+            mapped_data.setdefault("entities", {})[dimension_name, element_name] = None
+        if len(element_names) == state[ImportKey.DIMENSION_COUNT]:
+            mapped_data.setdefault("entities", {})[entity_class_name, tuple(element_names)] = None
+            raise KeyFix(ImportKey.ELEMENT_NAMES)
+        raise KeyError(ImportKey.ELEMENT_NAMES)
 
 
 class ParameterDefinitionMapping(ImportMapping):
     """Maps parameter definitions.
 
     Cannot be used as the topmost mapping; must have an entity class mapping as one of parents.
     """
 
     MAP_TYPE = "ParameterDefinition"
 
     def _import_row(self, source_data, state, mapped_data):
-        object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
-        if object_class_name is not None:
-            class_name = object_class_name
-            map_key = "object_parameters"
-        else:
-            relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
-            if relationship_class_name is not None:
-                class_name = relationship_class_name
-                map_key = "relationship_parameters"
-            else:
-                raise KeyError(ImportKey.CLASS_NAME)
+        entity_class_name = state.get(ImportKey.ENTITY_CLASS_NAME)
         parameter_name = state[ImportKey.PARAMETER_NAME] = str(source_data)
         definition_extras = state[ImportKey.PARAMETER_DEFINITION_EXTRAS] = []
-        parameter_definition_key = state[ImportKey.PARAMETER_DEFINITION] = class_name, parameter_name
+        parameter_definition_key = state[ImportKey.PARAMETER_DEFINITION] = entity_class_name, parameter_name
         default_values = state.get(ImportKey.PARAMETER_DEFAULT_VALUES)
         if default_values is None or parameter_definition_key not in default_values:
-            mapped_data.setdefault(map_key, dict())[parameter_definition_key] = definition_extras
+            mapped_data.setdefault("parameter_definitions", dict())[parameter_definition_key] = definition_extras
 
 
 class ParameterDefaultValueMapping(ImportMapping):
     """Maps scalar (non-indexed) default values
 
     Cannot be used as the topmost mapping; must have a :class:`ParameterDefinitionMapping` as parent.
     """
@@ -672,73 +639,43 @@
 
     MAP_TYPE = "ParameterValue"
 
     def _import_row(self, source_data, state, mapped_data):
         value = source_data
         if value == "":
             return
-        object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
-        relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
-        if object_class_name is not None:
-            class_name = object_class_name
-            entity_name = state[ImportKey.OBJECT_NAME]
-            map_key = "object_parameter_values"
-        elif relationship_class_name is not None:
-            object_names = state[ImportKey.OBJECT_NAMES]
-            if len(object_names) != state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
-                raise KeyError(ImportKey.OBJECT_NAMES)
-            class_name = relationship_class_name
-            entity_name = object_names
-            map_key = "relationship_parameter_values"
-        else:
-            raise KeyError(ImportKey.CLASS_NAME)
-        parameter_name = state[ImportKey.PARAMETER_NAME]
-        parameter_value = [class_name, entity_name, parameter_name, value]
-        alternative_name = state.get(ImportKey.ALTERNATIVE_NAME)
+        entity_class_name, entity_byname, parameter_name, alternative_name = _parameter_value_key(state)
+        parameter_value = [entity_class_name, entity_byname, parameter_name, value]
         if alternative_name is not None:
             parameter_value.append(alternative_name)
-        mapped_data.setdefault(map_key, list()).append(parameter_value)
+        mapped_data.setdefault("parameter_values", []).append(parameter_value)
 
 
 class ParameterValueTypeMapping(IndexedValueMixin, ImportMapping):
     MAP_TYPE = "ParameterValueType"
 
     def _import_row(self, source_data, state, mapped_data):
-        parameter_name = state.get(ImportKey.PARAMETER_NAME)
-        if parameter_name is None:
+        if ImportKey.PARAMETER_NAME not in state:
             # Don't catch errors here, this one's invisible
             return
-        object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
+        key = _parameter_value_key(state)
         values = state.setdefault(ImportKey.PARAMETER_VALUES, {})
-        if object_class_name is not None:
-            class_name = object_class_name
-            entity_name = state[ImportKey.OBJECT_NAME]
-            map_key = "object_parameter_values"
-        else:
-            relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
-            if relationship_class_name is not None:
-                class_name = relationship_class_name
-                entity_name = tuple(state[ImportKey.OBJECT_NAMES])
-                map_key = "relationship_parameter_values"
-            else:
-                raise KeyError(ImportKey.CLASS_NAME)
-        alternative_name = state.get(ImportKey.ALTERNATIVE_NAME)
-        key = (class_name, entity_name, parameter_name, alternative_name)
         if key in values:
             return
+        entity_class_name, entity_byname, parameter_name, alternative_name = key
         value_type = str(source_data)
         value = values[key] = {"type": value_type}  # See import_mapping.generator._parameter_value_from_dict()
         if self.compress and value_type == "map":
             value["compress"] = self.compress
         if self.options and value_type == "time_series":
             value["options"] = self.options
-        parameter_value = [class_name, entity_name, parameter_name, value]
+        parameter_value = [entity_class_name, entity_byname, parameter_name, value]
         if alternative_name is not None:
             parameter_value.append(alternative_name)
-        mapped_data.setdefault(map_key, list()).append(parameter_value)
+        mapped_data.setdefault("parameter_values", []).append(parameter_value)
 
 
 class ParameterValueMetadataMapping(ImportMapping):
     """Maps relationship metadata.
 
     Cannot be used as the topmost mapping; must have a :class:`ParameterValueMapping` or
     a :class:`ParameterValueTypeMapping` as parent.
@@ -830,15 +767,15 @@
     MAP_TYPE = "ParameterValueListValue"
 
     def _import_row(self, source_data, state, mapped_data):
         list_value = source_data
         if list_value == "":
             return
         value_list_name = state[ImportKey.PARAMETER_VALUE_LIST_NAME]
-        mapped_data.setdefault("parameter_value_lists", list()).append([value_list_name, list_value])
+        mapped_data.setdefault("parameter_value_lists", []).append([value_list_name, list_value])
 
 
 class AlternativeMapping(ImportMapping):
     """Maps alternatives.
 
     Can be used as the topmost mapping.
     """
@@ -868,15 +805,15 @@
     Cannot be used as the topmost mapping; must have a :class:`ScenarioMapping` as parent.
     """
 
     MAP_TYPE = "ScenarioActiveFlag"
 
     def _import_row(self, source_data, state, mapped_data):
         scenario = state[ImportKey.SCENARIO_NAME]
-        active = bool(strtobool(str(source_data)))
+        active = string_to_bool(str(source_data))
         mapped_data.setdefault("scenarios", set()).add((scenario, active))
 
 
 class ScenarioAlternativeMapping(ImportMapping):
     """Maps scenario alternatives.
 
     Cannot be used as the topmost mapping; must have a :class:`ScenarioMapping` as parent.
@@ -886,15 +823,15 @@
 
     def _import_row(self, source_data, state, mapped_data):
         alternative = str(source_data)
         if not alternative:
             return
         scenario = state[ImportKey.SCENARIO_NAME]
         scen_alt = state[ImportKey.SCENARIO_ALTERNATIVE] = [scenario, alternative]
-        mapped_data.setdefault("scenario_alternatives", list()).append(scen_alt)
+        mapped_data.setdefault("scenario_alternatives", []).append(scen_alt)
 
 
 class ScenarioBeforeAlternativeMapping(ImportMapping):
     """Maps scenario 'before' alternatives.
 
     Cannot be used as the topmost mapping; must have a :class:`ScenarioAlternativeMapping` as parent.
     """
@@ -917,130 +854,100 @@
 
     def _import_row(self, source_data, state, mapped_data):
         tool = state[ImportKey.TOOL_NAME] = str(source_data)
         if self.child is None:
             mapped_data.setdefault("tools", set()).add(tool)
 
 
-class FeatureEntityClassMapping(ImportMapping):
-    """Maps feature entity classes.
+def default_import_mapping(map_type):
+    """Creates default mappings for given map type.
 
-    Can be used as the topmost mapping.
-    """
-
-    MAP_TYPE = "FeatureEntityClass"
-
-    def _import_row(self, source_data, state, mapped_data):
-        entity_class = str(source_data)
-        state[ImportKey.FEATURE] = [entity_class]
-
-
-class FeatureParameterDefinitionMapping(ImportMapping):
-    """Maps feature parameter definitions.
+    Args:
+        map_type (str): map type
 
-    Cannot be used as the topmost mapping; must have a :class:`FeatureEntityClassMapping` as parent.
+    Returns:
+        ImportMapping: root mapping of desired type
     """
+    make_root_mapping = {
+        "EntityClass": _default_entity_class_mapping,
+        "Alternative": _default_alternative_mapping,
+        "Scenario": _default_scenario_mapping,
+        "ScenarioAlternative": _default_scenario_alternative_mapping,
+        "EntityGroup": _default_entity_group_mapping,
+        "ParameterValueList": _default_parameter_value_list_mapping,
+    }[map_type]
+    return make_root_mapping()
 
-    MAP_TYPE = "FeatureParameterDefinition"
-
-    def _import_row(self, source_data, state, mapped_data):
-        feature = state[ImportKey.FEATURE]
-        parameter = str(source_data)
-        feature.append(parameter)
-        mapped_data.setdefault("features", set()).add(tuple(feature))
 
+def _default_entity_class_mapping():
+    """Creates default entity class mappings.
 
-class ToolFeatureEntityClassMapping(ImportMapping):
-    """Maps tool feature entity classes.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
+    Returns:
+        EntityClassMapping: root mapping
     """
-
-    MAP_TYPE = "ToolFeatureEntityClass"
-
-    def _import_row(self, source_data, state, mapped_data):
-        tool = state[ImportKey.TOOL_NAME]
-        entity_class = str(source_data)
-        tool_feature = [tool, entity_class]
-        state[ImportKey.TOOL_FEATURE] = tool_feature
-        mapped_data.setdefault("tool_features", list()).append(tool_feature)
+    root_mapping = EntityClassMapping(Position.hidden)
+    object_mapping = root_mapping.child = EntityMapping(Position.hidden)
+    object_mapping.child = EntityMetadataMapping(Position.hidden)
+    return root_mapping
 
 
-class ToolFeatureParameterDefinitionMapping(ImportMapping):
-    """Maps tool feature parameter definitions.
+def _default_alternative_mapping():
+    """Creates default alternative mappings.
 
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
+    Returns:
+        AlternativeMapping: root mapping
     """
-
-    MAP_TYPE = "ToolFeatureParameterDefinition"
-
-    def _import_row(self, source_data, state, mapped_data):
-        tool_feature = state[ImportKey.TOOL_FEATURE]
-        parameter = str(source_data)
-        tool_feature.append(parameter)
+    root_mapping = AlternativeMapping(Position.hidden)
+    return root_mapping
 
 
-class ToolFeatureRequiredFlagMapping(ImportMapping):
-    """Maps tool feature required flags.
+def _default_scenario_mapping():
+    """Creates default scenario mappings.
 
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureEntityClassMapping` as parent.
+    Returns:
+        ScenarioMapping: root mapping
     """
+    root_mapping = ScenarioMapping(Position.hidden)
+    root_mapping.child = ScenarioActiveFlagMapping(Position.hidden)
+    return root_mapping
 
-    MAP_TYPE = "ToolFeatureRequiredFlag"
-
-    def _import_row(self, source_data, state, mapped_data):
-        required = bool(strtobool(str(source_data)))
-        tool_feature = state[ImportKey.TOOL_FEATURE]
-        tool_feature.append(required)
 
+def _default_scenario_alternative_mapping():
+    """Creates default scenario alternative mappings.
 
-class ToolFeatureMethodEntityClassMapping(ImportMapping):
-    """Maps tool feature method entity classes.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolMapping` as parent.
+    Returns:
+        ScenarioAlternativeMapping: root mapping
     """
+    root_mapping = ScenarioMapping(Position.hidden)
+    scen_alt_mapping = root_mapping.child = ScenarioAlternativeMapping(Position.hidden)
+    scen_alt_mapping.child = ScenarioBeforeAlternativeMapping(Position.hidden)
+    return root_mapping
 
-    MAP_TYPE = "ToolFeatureMethodEntityClass"
 
-    def _import_row(self, source_data, state, mapped_data):
-        tool_name = state[ImportKey.TOOL_NAME]
-        entity_class = str(source_data)
-        tool_feature_method = [tool_name, entity_class]
-        state[ImportKey.TOOL_FEATURE_METHOD] = tool_feature_method
-        mapped_data.setdefault("tool_feature_methods", list()).append(tool_feature_method)
-
-
-class ToolFeatureMethodParameterDefinitionMapping(ImportMapping):
-    """Maps tool feature method parameter definitions.
+def _default_entity_group_mapping():
+    """Creates default entity group mappings.
 
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
+    Returns:
+        EntityClassMapping: root mapping
     """
+    root_mapping = EntityClassMapping(Position.hidden)
+    object_mapping = root_mapping.child = EntityMapping(Position.hidden)
+    object_mapping.child = EntityGroupMapping(Position.hidden)
+    return root_mapping
 
-    MAP_TYPE = "ToolFeatureMethodParameterDefinition"
 
-    def _import_row(self, source_data, state, mapped_data):
-        tool_feature_method = state[ImportKey.TOOL_FEATURE_METHOD]
-        parameter = str(source_data)
-        tool_feature_method.append(parameter)
+def _default_parameter_value_list_mapping():
+    """Creates default parameter value list mappings.
 
-
-class ToolFeatureMethodMethodMapping(ImportMapping):
-    """Maps tool feature method methods.
-
-    Cannot be used as the topmost mapping; must have :class:`ToolFeatureMethodEntityClassMapping` as parent.
+    Returns:
+        ParameterValueListMapping: root mapping
     """
-
-    MAP_TYPE = "ToolFeatureMethodMethod"
-
-    def _import_row(self, source_data, state, mapped_data):
-        method = source_data
-        if method == "":
-            return
-        tool_feature_method = state[ImportKey.TOOL_FEATURE_METHOD]
-        tool_feature_method.append(method)
+    root_mapping = ParameterValueListMapping(Position.hidden)
+    root_mapping.child = ParameterValueListValueMapping(Position.hidden)
+    return root_mapping
 
 
 def from_dict(serialized):
     """
     Deserializes mappings.
 
     Args:
@@ -1048,23 +955,20 @@
 
     Returns:
         Mapping: root mapping
     """
     mappings = {
         klass.MAP_TYPE: klass
         for klass in (
-            ObjectClassMapping,
-            ObjectMapping,
-            ObjectMetadataMapping,
-            ObjectGroupMapping,
-            RelationshipClassMapping,
-            RelationshipClassObjectClassMapping,
-            RelationshipMapping,
-            RelationshipObjectMapping,
-            RelationshipMetadataMapping,
+            EntityClassMapping,
+            EntityMapping,
+            EntityMetadataMapping,
+            EntityGroupMapping,
+            DimensionMapping,
+            ElementMapping,
             ParameterDefinitionMapping,
             ParameterDefaultValueMapping,
             ParameterDefaultValueTypeMapping,
             ParameterDefaultValueIndexMapping,
             ExpandedParameterDefaultValueMapping,
             ParameterValueMapping,
             ParameterValueTypeMapping,
@@ -1076,35 +980,50 @@
             ParameterValueListValueMapping,
             AlternativeMapping,
             ScenarioMapping,
             ScenarioActiveFlagMapping,
             ScenarioAlternativeMapping,
             ScenarioBeforeAlternativeMapping,
             ToolMapping,
-            FeatureEntityClassMapping,
-            FeatureParameterDefinitionMapping,
-            ToolFeatureEntityClassMapping,
-            ToolFeatureParameterDefinitionMapping,
-            ToolFeatureRequiredFlagMapping,
-            ToolFeatureMethodEntityClassMapping,
-            ToolFeatureMethodParameterDefinitionMapping,
-            ToolFeatureMethodMethodMapping,
+            # FIXME
+            # FeatureEntityClassMapping,
+            # FeatureParameterDefinitionMapping,
+            # ToolFeatureEntityClassMapping,
+            # ToolFeatureParameterDefinitionMapping,
+            # ToolFeatureRequiredFlagMapping,
+            # ToolFeatureMethodEntityClassMapping,
+            # ToolFeatureMethodParameterDefinitionMapping,
+            # ToolFeatureMethodMethodMapping,
         )
     }
-    # Legacy
-    mappings["ParameterIndex"] = ParameterValueIndexMapping
-    flattened = list()
+    legacy_mappings = {
+        "ParameterIndex": ParameterValueIndexMapping,
+        "ObjectClass": EntityClassMapping,
+        "Object": EntityMapping,
+        "ObjectMetadata": EntityMetadataMapping,
+        "ObjectGroup": EntityGroupMapping,
+        "RelationshipClass": EntityClassMapping,
+        "RelationshipClassObjectClass": DimensionMapping,
+        "Relationship": EntityMapping,
+        "RelationshipObject": ElementMapping,
+        "RelationshipMetadata": EntityMetadataMapping,
+    }
+    mappings.update(legacy_mappings)
+    flattened = []
     for mapping_dict in serialized:
         position = mapping_dict["position"]
         value = mapping_dict.get("value")
         skip_columns = mapping_dict.get("skip_columns")
         read_start_row = mapping_dict.get("read_start_row", 0)
         filter_re = mapping_dict.get("filter_re", "")
         if isinstance(position, str):
             position = Position(position)
+        if "import_objects" in mapping_dict:
+            # Legacy
+            mapping_dict["import_entities"] = mapping_dict.pop("import_objects")
         flattened.append(
             mappings[mapping_dict["map_type"]].reconstruct(
                 position, value, skip_columns, read_start_row, filter_re, mapping_dict
             )
         )
         if mapping_dict["map_type"] == "ObjectGroup":
             # Legacy: dropping parameter mappings from object groups
@@ -1115,43 +1034,32 @@
 def _parameter_value_key(state):
     """Creates parameter value's key from current state.
 
     Args:
         state (dict): import state
 
     Returns:
-        tuple of str: class name, entity name and parameter name
+        tuple of str: class name, entity byname, parameter name, and alternative name
     """
-    object_class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
-    if object_class_name is not None:
-        class_name = object_class_name
-        entity_name = state[ImportKey.OBJECT_NAME]
+    entity_class_name = state.get(ImportKey.ENTITY_CLASS_NAME)
+    if state.get(ImportKey.DIMENSION_COUNT):
+        element_names = state[ImportKey.ELEMENT_NAMES]
+        if len(element_names) != state[ImportKey.DIMENSION_COUNT]:
+            raise KeyError(ImportKey.ELEMENT_NAMES)
+        entity_byname = element_names
     else:
-        relationship_class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
-        if relationship_class_name is None:
-            raise KeyError(ImportKey.CLASS_NAME)
-        object_names = state[ImportKey.OBJECT_NAMES]
-        if len(object_names) != state[ImportKey.RELATIONSHIP_DIMENSION_COUNT]:
-            raise KeyError(ImportKey.OBJECT_NAMES)
-        class_name = relationship_class_name
-        entity_name = tuple(object_names)
+        entity_byname = state[ImportKey.ENTITY_NAME]
     parameter_name = state[ImportKey.PARAMETER_NAME]
     alternative_name = state.get(ImportKey.ALTERNATIVE_NAME)
-    return class_name, entity_name, parameter_name, alternative_name
+    return entity_class_name, entity_byname, parameter_name, alternative_name
 
 
 def _default_value_key(state):
     """Creates parameter default value's key from current state.
 
     Args:
         state (dict): import state
 
     Returns:
         tuple of str: class name and parameter name
     """
-    class_name = state.get(ImportKey.OBJECT_CLASS_NAME)
-    if class_name is None:
-        class_name = state.get(ImportKey.RELATIONSHIP_CLASS_NAME)
-        if class_name is None:
-            raise KeyError(ImportKey.CLASS_NAME)
-    parameter_name = state[ImportKey.PARAMETER_NAME]
-    return class_name, parameter_name
+    return state[ImportKey.ENTITY_CLASS_NAME], state[ImportKey.PARAMETER_NAME]
```

### Comparing `spinedb_api-0.30.5/spinedb_api/import_mapping/import_mapping_compat.py` & `spinedb_api-0.31.0/spinedb_api/import_mapping/import_mapping_compat.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,32 +1,27 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
-"""
-Functions for creating import mappings from dicts.
-
-"""
+""" Functions for creating import mappings from dicts. """
 from .import_mapping import (
     Position,
-    ObjectClassMapping,
-    RelationshipClassMapping,
-    RelationshipClassObjectClassMapping,
-    ObjectMapping,
-    ObjectMetadataMapping,
-    RelationshipMapping,
-    RelationshipObjectMapping,
-    RelationshipMetadataMapping,
+    EntityClassMapping,
+    DimensionMapping,
+    EntityMapping,
+    EntityMetadataMapping,
+    ElementMapping,
     ParameterDefinitionMapping,
     ParameterDefaultValueMapping,
     ParameterDefaultValueTypeMapping,
     ParameterDefaultValueIndexMapping,
     ExpandedParameterDefaultValueMapping,
     ParameterValueMapping,
     ParameterValueTypeMapping,
@@ -34,24 +29,15 @@
     ParameterValueIndexMapping,
     ExpandedParameterValueMapping,
     AlternativeMapping,
     ScenarioMapping,
     ScenarioActiveFlagMapping,
     ScenarioAlternativeMapping,
     ScenarioBeforeAlternativeMapping,
-    ToolMapping,
-    FeatureEntityClassMapping,
-    FeatureParameterDefinitionMapping,
-    ToolFeatureEntityClassMapping,
-    ToolFeatureParameterDefinitionMapping,
-    ToolFeatureRequiredFlagMapping,
-    ToolFeatureMethodEntityClassMapping,
-    ToolFeatureMethodParameterDefinitionMapping,
-    ToolFeatureMethodMethodMapping,
-    ObjectGroupMapping,
+    EntityGroupMapping,
     ParameterValueListMapping,
     ParameterValueListValueMapping,
     from_dict as mapping_from_dict,
     IndexNameMapping,
     DefaultValueIndexNameMapping,
 )
 from ..mapping import to_dict as import_mapping_to_dict
@@ -83,25 +69,25 @@
     map_type = map_dict.get("map_type")
     legacy_mapping_from_dict = {
         "ObjectClass": _object_class_mapping_from_dict,
         "RelationshipClass": _relationship_class_mapping_from_dict,
         "Alternative": _alternative_mapping_from_dict,
         "Scenario": _scenario_mapping_from_dict,
         "ScenarioAlternative": _scenario_alternative_mapping_from_dict,
-        "Tool": _tool_mapping_from_dict,
-        "Feature": _feature_mapping_from_dict,
-        "ToolFeature": _tool_feature_mapping_from_dict,
-        "ToolFeatureMethod": _tool_feature_method_mapping_from_dict,
         "ObjectGroup": _object_group_mapping_from_dict,
         "ParameterValueList": _parameter_value_list_mapping_from_dict,
     }
     from_dict = legacy_mapping_from_dict.get(map_type)
     if from_dict is not None:
         return from_dict(map_dict)
-    raise ValueError(f'invalid "map_type" value, expected any of {", ".join(legacy_mapping_from_dict)}, got {map_type}')
+    obsolete_types = ("Tool", "Feature", "ToolFeature", "ToolFeatureMethod")
+    invalid = "obsolete" if map_type in obsolete_types else "unknown"
+    raise ValueError(
+        f'{invalid} "map_type" value, expected any of {", ".join(legacy_mapping_from_dict)}, got {map_type}'
+    )
 
 
 def _parameter_value_list_mapping_from_dict(map_dict):
     name = map_dict.get("name")
     value = map_dict.get("value")
     skip_columns = map_dict.get("skip_columns", [])
     read_start_row = map_dict.get("read_start_row", 0)
@@ -140,123 +126,67 @@
         *_pos_and_val(scenario_name), skip_columns=skip_columns, read_start_row=read_start_row
     )
     scen_alt_mapping = root_mapping.child = ScenarioAlternativeMapping(*_pos_and_val(alternative_name))
     scen_alt_mapping.child = ScenarioBeforeAlternativeMapping(*_pos_and_val(before_alternative_name))
     return root_mapping
 
 
-def _tool_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ToolMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    return root_mapping
-
-
-def _feature_mapping_from_dict(map_dict):
-    entity_class_name = map_dict.get("entity_class_name")
-    parameter_definition_name = map_dict.get("parameter_definition_name")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = FeatureEntityClassMapping(
-        *_pos_and_val(entity_class_name), skip_columns=skip_columns, read_start_row=read_start_row
-    )
-    root_mapping.child = FeatureParameterDefinitionMapping(*_pos_and_val(parameter_definition_name))
-    return root_mapping
-
-
-def _tool_feature_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    entity_class_name = map_dict.get("entity_class_name")
-    parameter_definition_name = map_dict.get("parameter_definition_name")
-    required = map_dict.get("required", "false")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ToolMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    root_mapping.child = ent_class_mapping = ToolFeatureEntityClassMapping(*_pos_and_val(entity_class_name))
-    ent_class_mapping.child = param_def_mapping = ToolFeatureParameterDefinitionMapping(
-        *_pos_and_val(parameter_definition_name)
-    )
-    param_def_mapping.child = ToolFeatureRequiredFlagMapping(*_pos_and_val(required))
-    return root_mapping
-
-
-def _tool_feature_method_mapping_from_dict(map_dict):
-    name = map_dict.get("name")
-    entity_class_name = map_dict.get("entity_class_name")
-    parameter_definition_name = map_dict.get("parameter_definition_name")
-    method = map_dict.get("method")
-    skip_columns = map_dict.get("skip_columns", [])
-    read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ToolMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    root_mapping.child = ent_class_mapping = ToolFeatureMethodEntityClassMapping(*_pos_and_val(entity_class_name))
-    ent_class_mapping.child = param_def_mapping = ToolFeatureMethodParameterDefinitionMapping(
-        *_pos_and_val(parameter_definition_name)
-    )
-    param_def_mapping.child = ToolFeatureMethodMethodMapping(*_pos_and_val(method))
-    return root_mapping
-
-
 def _object_class_mapping_from_dict(map_dict):
     name = map_dict.get("name")
     objects = map_dict.get("objects", map_dict.get("object"))
     object_metadata = map_dict.get("object_metadata", None)
     parameters = map_dict.get("parameters")
     skip_columns = map_dict.get("skip_columns", [])
     read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ObjectClassMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    object_mapping = root_mapping.child = ObjectMapping(*_pos_and_val(objects))
-    object_metadata_mapping = object_mapping.child = ObjectMetadataMapping(*_pos_and_val(object_metadata))
+    root_mapping = EntityClassMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
+    object_mapping = root_mapping.child = EntityMapping(*_pos_and_val(objects))
+    object_metadata_mapping = object_mapping.child = EntityMetadataMapping(*_pos_and_val(object_metadata))
     object_metadata_mapping.child = parameter_mapping_from_dict(parameters)
     return root_mapping
 
 
 def _object_group_mapping_from_dict(map_dict):
     name = map_dict.get("name")
     groups = map_dict.get("groups")
     members = map_dict.get("members")
-    import_objects = map_dict.get("import_objects", False)
+    import_entities = map_dict.get("import_objects", False)
     skip_columns = map_dict.get("skip_columns", [])
     read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = ObjectClassMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
-    object_mapping = root_mapping.child = ObjectMapping(*_pos_and_val(groups))
-    object_mapping.child = ObjectGroupMapping(*_pos_and_val(members), import_objects=import_objects)
+    root_mapping = EntityClassMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
+    object_mapping = root_mapping.child = EntityMapping(*_pos_and_val(groups))
+    object_mapping.child = EntityGroupMapping(*_pos_and_val(members), import_entities=import_entities)
     return root_mapping
 
 
 def _relationship_class_mapping_from_dict(map_dict):
     name = map_dict.get("name")
     objects = map_dict.get("objects")
     if objects is None:
         objects = [None]
     object_classes = map_dict.get("object_classes")
     if object_classes is None:
         object_classes = [None]
     relationship_metadata = map_dict.get("relationship_metadata")
     parameters = map_dict.get("parameters")
-    import_objects = map_dict.get("import_objects", False)
+    import_entities = map_dict.get("import_objects", False)
     skip_columns = map_dict.get("skip_columns", [])
     read_start_row = map_dict.get("read_start_row", 0)
-    root_mapping = RelationshipClassMapping(
-        *_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row
-    )
+    root_mapping = EntityClassMapping(*_pos_and_val(name), skip_columns=skip_columns, read_start_row=read_start_row)
     parent_mapping = root_mapping
     for klass in object_classes:
-        class_mapping = RelationshipClassObjectClassMapping(*_pos_and_val(klass))
+        class_mapping = DimensionMapping(*_pos_and_val(klass))
         parent_mapping.child = class_mapping
         parent_mapping = class_mapping
-    relationship_mapping = parent_mapping.child = RelationshipMapping(Position.hidden, value="relationship")
+    relationship_mapping = parent_mapping.child = EntityMapping(Position.hidden)
     parent_mapping = relationship_mapping
     for obj in objects:
-        object_mapping = RelationshipObjectMapping(*_pos_and_val(obj), import_objects=import_objects)
+        object_mapping = ElementMapping(*_pos_and_val(obj), import_entities=import_entities)
         parent_mapping.child = object_mapping
         parent_mapping = object_mapping
-    relationship_metadata_mapping = parent_mapping.child = RelationshipMetadataMapping(
-        *_pos_and_val(relationship_metadata)
-    )
+    relationship_metadata_mapping = parent_mapping.child = EntityMetadataMapping(*_pos_and_val(relationship_metadata))
     relationship_metadata_mapping.child = parameter_mapping_from_dict(parameters)
     return root_mapping
 
 
 def parameter_mapping_from_dict(map_dict):
     if map_dict is None:
         return None
```

### Comparing `spinedb_api-0.30.5/spinedb_api/import_mapping/type_conversion.py` & `spinedb_api-0.31.0/spinedb_api/import_mapping/type_conversion.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,25 +1,24 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
-"""
-Type conversion functions.
-
-"""
+""" Type conversion functions. """
 
 import re
-from distutils.util import strtobool
+
+from spinedb_api.helpers import string_to_bool
 from spinedb_api.parameter_value import DateTime, Duration, ParameterValueFormatError
 
 
 def value_to_convert_spec(value):
     if isinstance(value, ConvertSpec):
         return value
     if isinstance(value, str):
@@ -76,15 +75,15 @@
 
 
 class BooleanConvertSpec(ConvertSpec):
     DISPLAY_NAME = "boolean"
     RETURN_TYPE = bool
 
     def __call__(self, value):
-        return self.RETURN_TYPE(strtobool(str(value)))
+        return self.RETURN_TYPE(string_to_bool(str(value)))
 
 
 class IntegerSequenceDateTimeConvertSpec(ConvertSpec):
     DISPLAY_NAME = "integer sequence datetime"
     RETURN_TYPE = DateTime
 
     def __init__(self, start_datetime, start_int, duration):
```

### Comparing `spinedb_api-0.30.5/spinedb_api/mapping.py` & `spinedb_api-0.31.0/spinedb_api/mapping.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,20 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
-"""
-Contains export mappings for database items such as entities, entity classes and parameter values.
 
-"""
+"""Base class for import and export mappings."""
 
 from enum import Enum, unique
 from itertools import takewhile
 import re
 
 
 @unique
```

### Comparing `spinedb_api-0.30.5/spinedb_api/parameter_value.py` & `spinedb_api-0.31.0/spinedb_api/parameter_value.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,34 +1,88 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
 """
-Support utilities and classes to deal with Spine data (relationship)
-parameter values.
+Parameter values in a Spine DB can be of different types (see :ref:`parameter_value_format`).
+For each of these types, this module provides a Python class to represent values of that type.
 
-The `from_database` function reads the database's value format returning
-a float, Datatime, Duration, TimePattern, TimeSeriesFixedResolution
-TimeSeriesVariableResolution or Map objects.
-
-The above objects can be converted back to the database format by the `to_database` free function
-or by their `to_database` member functions.
-
-Individual datetimes are represented as datetime objects from the standard Python library.
-Individual time steps are represented as relativedelta objects from the dateutil package.
-Datetime indexes (as returned by TimeSeries.indexes()) are represented as
-numpy.ndarray arrays holding numpy.datetime64 objects.
+.. list-table:: Parameter value type and Python class
+   :header-rows: 1
 
+   * - type
+     - Python class
+   * - ``date_time``
+     - :class:`DateTime`
+   * - ``duration``
+     - :class:`Duration`
+   * - ``array``
+     - :class:`Array`
+   * - ``time_pattern``
+     - :class:`TimePattern`
+   * - ``time_series``
+     - :class:`TimeSeriesFixedResolution` and :class:`TimeSeriesVariableResolution`
+   * - ``map``
+     - :class:`Map`
+
+The module also provides the functions :func:`to_database` and :func:`from_database`
+to translate between instances of the above classes and their DB representation (namely, the `value` and `type` fields
+that would go in the ``parameter_value`` table).
+
+For example, to write a Python object into a parameter value in the DB::
+
+    # Create the Python object
+    parsed_value = TimeSeriesFixedResolution(
+        "2023-01-01T00:00",             # start
+        "1D",                           # resolution
+        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0], # values
+        ignore_year=False,
+        repeat=False,
+    )
+    # Translate it to value and type
+    value, type_ = to_database(parsed_value)
+    # Add a parameter_value to the DB with that value and type
+    with DatabaseMapping(url) as db_map:
+        db_map.add_parameter_value_item(
+            entity_class_name="cat",
+            entity_byname=("Tom",),
+            parameter_definition_name="number_of_lives",
+            alternative_name="Base",
+            value=value,
+            type=type_,
+        )
+        db_map.commit_session("Tom is living one day at a time")
+
+The value can be accessed as a Python object using the ``parsed_value`` field::
+
+    # Get the parameter_value from the DB
+    with DatabaseMapping(url) as db_map:
+        pval_item = db_map.get_parameter_value_item(
+            entity_class_name="cat",
+            entity_byname=("Tom",),
+            parameter_definition_name="number_of_lives",
+            alternative_name="Base",
+        )
+    value = pval_item["parsed_value"]
+
+In the rare case where a manual conversion from a DB value to Python object is needed,
+use :func:`.from_database`::
+
+    # Obtain value and type
+    value, type_ = pval_item["value"], pval_item["type"]
+    # Translate value and type to a Python object manually
+    parsed_value = from_database(value, type_)
 """
 
 from collections.abc import Sequence
 from copy import copy
 from datetime import datetime
 import json
 from json.decoder import JSONDecodeError
@@ -46,23 +100,62 @@
 _TIME_SERIES_DEFAULT_START = "0001-01-01T00:00:00"
 # Default resolution if it is omitted from the index entry.
 _TIME_SERIES_DEFAULT_RESOLUTION = "1h"
 # Default unit if resolution is given as a number instead of a string.
 _TIME_SERIES_PLAIN_INDEX_UNIT = "m"
 
 
+def from_database(value, type_=None):
+    """
+    Converts a parameter value from the DB into a Python object.
+
+    Args:
+        value (bytes or None): the `value` field from the ``parameter_value`` table.
+        type_ (str, optional): the `type` field from the ``parameter_value`` table.
+
+    Returns:
+        :class:`ParameterValue`, float, str, bool or None: a Python object representing the parameter value.
+    """
+    parsed = load_db_value(value, type_)
+    if isinstance(parsed, dict):
+        return from_dict(parsed)
+    if isinstance(parsed, bool):
+        return parsed
+    if isinstance(parsed, Number):
+        return float(parsed)
+    return parsed
+
+
+def to_database(parsed_value):
+    """
+    Converts a Python object representing a parameter value into their DB representation.
+
+    Args:
+        parsed_value (any): the Python object.
+
+    Returns:
+        tuple(bytes,str): the `value` and `type` fields that would go in the ``parameter_value`` table.
+    """
+    if hasattr(parsed_value, "to_database"):
+        return parsed_value.to_database()
+    db_value = json.dumps(parsed_value).encode("UTF8")
+    return db_value, None
+
+
 def duration_to_relativedelta(duration):
     """
     Converts a duration to a relativedelta object.
 
+    :meta private:
+
     Args:
-        duration (str): a duration specification
+        duration (str): a duration string.
 
     Returns:
-        a relativedelta object corresponding to the given duration
+        :class:`~dateutil.relativedelta.relativedelta`: a relativedelta object corresponding to the given duration.
     """
     try:
         count, abbreviation, full_unit = re.split("\\s|([a-z]|[A-Z])", duration, maxsplit=1)
         count = int(count)
     except ValueError:
         raise ParameterValueFormatError(f'Could not parse duration "{duration}"')
     unit = abbreviation if abbreviation is not None else full_unit
@@ -81,19 +174,21 @@
     raise ParameterValueFormatError(f'Could not parse duration "{duration}"')
 
 
 def relativedelta_to_duration(delta):
     """
     Converts a relativedelta to duration.
 
+    :meta private:
+
     Args:
-        delta (relativedelta): the relativedelta to convert
+        delta (:class:`~dateutil.relativedelta.relativedelta`): the relativedelta to convert.
 
     Returns:
-        a duration string
+        str: a duration string
     """
     if delta.seconds > 0:
         seconds = delta.seconds
         seconds += 60 * delta.minutes
         seconds += 60 * 60 * delta.hours
         seconds += 60 * 60 * 24 * delta.days
         # Skipping months and years since dateutil does not use them here
@@ -115,192 +210,166 @@
         months += 12 * delta.years
         return f"{months}M"
     if delta.years > 0:
         return f"{delta.years}Y"
     return "0h"
 
 
-def load_db_value(db_value, value_type=None):
+def load_db_value(db_value, type_=None):
     """
-    Loads a database parameter value into a Python object using JSON.
-    Adds the "type" property to dicts representing complex types.
+    Parses a database representation of a parameter value (value and type) into a Python object, using JSON.
+    If the result is a dict, adds the "type" property to it.
+
+    :meta private:
 
     Args:
-        db_value (bytes, optional): a value in the database
-        value_type (str, optional): the type in case of complex ones
+        db_value (bytes, optional): the database value.
+        type_ (str, optional): the value type.
 
     Returns:
-        Any: the parsed parameter value
+        any: the parsed parameter value
     """
     if db_value is None:
         return None
     try:
         parsed = json.loads(db_value)
     except JSONDecodeError as err:
         raise ParameterValueFormatError(f"Could not decode the value: {err}") from err
     if isinstance(parsed, dict):
-        return {"type": value_type, **parsed}
+        return {"type": type_, **parsed}
     return parsed
 
 
 def dump_db_value(parsed_value):
     """
-    Dumps a Python object into a database parameter value using JSON.
-    Extracts the "type" property from dicts representing complex types.
+    Unparses a Python object into a database representation of a parameter value (value and type), using JSON.
+    If the given object is a dict, extracts the "type" property from it.
+
+    :meta private:
 
     Args:
-        parsed_value (Any): the Python object
+        parsed_value (any): a Python object, typically obtained by calling :func:`load_db_value`.
 
     Returns:
-        str: the database parameter value
-        str: the type
+        tuple(str,str): database representation (value and type).
     """
     value_type = parsed_value.pop("type") if isinstance(parsed_value, dict) else None
     db_value = json.dumps(parsed_value).encode("UTF8")
     if isinstance(parsed_value, dict) and value_type is not None:
         parsed_value["type"] = value_type
     return db_value, value_type
 
 
-def from_database(database_value, value_type=None):
-    """
-    Converts a parameter value from its database representation into an encoded Python object.
-
-    Args:
-        database_value (bytes, optional): a value in the database
-        value_type (str, optional): the type in case of complex ones
-
-    Returns:
-        Any: the encoded parameter value
-    """
-    parsed = load_db_value(database_value, value_type)
-    if isinstance(parsed, dict):
-        return from_dict(parsed)
-    if isinstance(parsed, bool):
-        return parsed
-    if isinstance(parsed, Number):
-        return float(parsed)
-    return parsed
-
-
 def from_database_to_single_value(database_value, value_type):
     """
-    Converts a value from its database representation into a single value.
+    Same as :func:`from_database`, but in the case of indexed types it returns just the type as a string.
 
-    Indexed values get converted to their type string.
+    :meta private:
 
     Args:
-        database_value (bytes): a value in the database
-        value_type (str, optional): value's type
+        database_value (bytes): the database value
+        value_type (str, optional): the value type
 
     Returns:
-        Any: single-value representation
+        :class:`ParameterValue`, float, str, bool or None: the encoded parameter value or its type.
     """
     if value_type is None or value_type not in ("map", "time_series", "time_pattern", "array"):
         return from_database(database_value, value_type)
     return value_type
 
 
 def from_database_to_dimension_count(database_value, value_type):
     """
-    Counts dimensions of value's database representation
+    Counts the dimensions in a database representation of a parameter value (value and type).
+
+    :meta private:
 
     Args:
-        database_value (bytes): a value in the database
-        value_type (str, optional): value's type
+        database_value (bytes): the database value
+        value_type (str, optional): the value type
 
     Returns:
         int: number of dimensions
     """
 
     if value_type in {"time_series", "time_pattern", "array"}:
         return 1
     if value_type == "map":
         map_value = from_database(database_value, value_type)
         return map_dimensions(map_value)
     return 0
 
 
-def to_database(parsed_value):
-    """
-    Converts an encoded Python object into its database representation.
-
-    Args:
-        value: the value to convert. It can be the result of either ``load_db_value`` or ``from_database```.
-
-    Returns:
-        bytes: value's database representation as bytes
-        str: the value type
+def from_dict(value):
     """
-    if hasattr(parsed_value, "to_database"):
-        return parsed_value.to_database()
-    db_value = json.dumps(parsed_value).encode("UTF8")
-    return db_value, None
-
+    Converts a dictionary representation of a parameter value into an encoded parameter value.
 
-def from_dict(value_dict):
-    """
-    Converts a complex (relationship) parameter value from its dictionary representation to a Python object.
+    :meta private:
 
     Args:
-        value_dict (dict): value's dictionary; a parsed JSON object with the "type" key
+        value (dict): the value dictionary including the "type" key.
 
     Returns:
-        the encoded (relationship) parameter value
+        :class:`ParameterValue`, float, str, bool or None: the encoded parameter value.
     """
-    value_type = value_dict["type"]
+    value_type = value["type"]
     try:
         if value_type == "date_time":
-            return _datetime_from_database(value_dict["data"])
+            return _datetime_from_database(value["data"])
         if value_type == "duration":
-            return _duration_from_database(value_dict["data"])
+            return _duration_from_database(value["data"])
         if value_type == "map":
-            return _map_from_database(value_dict)
+            return _map_from_database(value)
         if value_type == "time_pattern":
-            return _time_pattern_from_database(value_dict)
+            return _time_pattern_from_database(value)
         if value_type == "time_series":
-            return _time_series_from_database(value_dict)
+            return _time_series_from_database(value)
         if value_type == "array":
-            return _array_from_database(value_dict)
+            return _array_from_database(value)
         raise ParameterValueFormatError(f'Unknown parameter value type "{value_type}"')
     except KeyError as error:
         raise ParameterValueFormatError(f'"{error.args[0]}" is missing in the parameter value description')
 
 
 def fix_conflict(new, old, on_conflict="merge"):
     """Resolves conflicts between parameter values:
 
+    :meta private:
+
     Args:
-        new (any): new parameter value to write
-        old (any): existing parameter value in the db
+        new (:class:`ParameterValue`, float, str, bool or None): new parameter value to be written.
+        old (:class:`ParameterValue`, float, str, bool or None): an existing parameter value in the db.
         on_conflict (str): conflict resolution strategy:
-            - 'merge': Merge indexes if possible, otherwise replace
-            - 'replace': Replace old with new
-            - 'keep': keep old
+            - 'merge': Merge indexes if possible, otherwise replace.
+            - 'replace': Replace old with new.
+            - 'keep': Keep old.
 
     Returns:
-        any: a parameter value with conflicts resolved
+        :class:`ParameterValue`, float, str, bool or None: a new parameter value with conflicts resolved.
     """
     funcs = {"keep": lambda new, old: old, "replace": lambda new, old: new, "merge": merge}
     func = funcs.get(on_conflict)
     if func is None:
         raise RuntimeError(
             f"Invalid conflict resolution strategy {on_conflict}, valid strategies are {', '.join(funcs)}"
         )
     return func(new, old)
 
 
 def merge(value, other):
-    """Merges other into value, returns the result.
+    """Merges the DB representation of two parameter values.
+
+    :meta private:
+
     Args:
-        value (tuple): recipient value and type
-        other (tuple): other value and type
+        value (tuple(bytes,str)): recipient value and type.
+        other (tuple(bytes,str)): other value and type.
 
     Returns:
-        tuple: value and type of merged value
+        tuple(bytes,str): the DB representation of the merged value.
     """
     parsed_value = from_database(*value)
     if not hasattr(parsed_value, "merge"):
         return value
     parsed_other = from_database(*other)
     return to_database(parsed_value.merge(parsed_other))
 
@@ -320,17 +389,20 @@
     indexes, values = zip(*data.items())
     return list(indexes), np.array(values)
 
 
 def _datetime_from_database(value):
     """Converts a datetime database value into a DateTime object."""
     try:
-        stamp = dateutil.parser.parse(value)
+        stamp = datetime.fromisoformat(value)
     except ValueError:
-        raise ParameterValueFormatError(f'Could not parse datetime from "{value}"')
+        try:
+            stamp = dateutil.parser.parse(value)
+        except ValueError:
+            raise ParameterValueFormatError(f'Could not parse datetime from "{value}"')
     return DateTime(stamp)
 
 
 def _duration_from_database(value):
     """
     Converts a duration database value into a Duration object.
 
@@ -448,17 +520,20 @@
         resolution = [resolution]
     relativedeltas = list()
     for duration in resolution:
         if not isinstance(duration, str):
             duration = str(duration) + _TIME_SERIES_PLAIN_INDEX_UNIT
         relativedeltas.append(duration_to_relativedelta(duration))
     try:
-        start = dateutil.parser.parse(start)
+        start = datetime.fromisoformat(start)
     except ValueError:
-        raise ParameterValueFormatError(f'Could not decode start value "{start}"')
+        try:
+            start = dateutil.parser.parse(start)
+        except ValueError:
+            raise ParameterValueFormatError(f'Could not decode start value "{start}"')
     values = np.array(value_dict["data"])
     return TimeSeriesFixedResolution(
         start, relativedeltas, values, ignore_year, repeat, value_dict.get("index_name", "")
     )
 
 
 def _time_series_from_two_columns(value_dict):
@@ -493,28 +568,28 @@
     Args:
         value_dict (dict): time pattern dictionary
 
     Returns:
         TimePattern: restored time pattern
     """
     patterns, values = _break_dictionary(value_dict["data"])
-    return TimePattern(patterns, values, value_dict.get("index_name", "p"))
+    return TimePattern(patterns, values, value_dict.get("index_name", TimePattern.DEFAULT_INDEX_NAME))
 
 
 def _map_from_database(value_dict):
     """Converts a map from its database representation to a Map object.
 
     Args:
         value_dict (dict): Map dictionary
 
     Returns:
         Map: restored Map
     """
     index_type = _map_index_type_from_database(value_dict["index_type"])
-    index_name = value_dict.get("index_name", "x")
+    index_name = value_dict.get("index_name", Map.DEFAULT_INDEX_NAME)
     data = value_dict["data"]
     if isinstance(data, dict):
         indexes = _map_indexes_from_database(data.keys(), index_type)
         values = _map_values_from_database(data.values())
     elif isinstance(data, Sequence):
         if not data:
             indexes = list()
@@ -610,58 +685,92 @@
         value_type_id, None
     )
     if value_type is None:
         raise ParameterValueFormatError(f'Unsupported value type for Array: "{value_type_id}".')
     try:
         data = [value_type(x) for x in value_dict["data"]]
     except (TypeError, ParameterValueFormatError) as error:
-        raise ParameterValueFormatError(f'Failed to read values for Array: {error}')
+        raise ParameterValueFormatError(f"Failed to read values for Array: {error}")
     else:
-        index_name = value_dict.get("index_name", "i")
+        index_name = value_dict.get("index_name", Array.DEFAULT_INDEX_NAME)
         return Array(data, value_type, index_name)
 
 
+class ParameterValue:
+    """Base for all classes representing parameter values."""
+
+    VALUE_TYPE = NotImplemented
+
+    def to_dict(self):
+        """Returns a dictionary representation of this parameter value.
+
+        :meta private:
+
+        Returns:
+            dict: a dictionary including the "type" key.
+        """
+        raise NotImplementedError()
+
+    @staticmethod
+    def type_():
+        """Returns the type of the parameter value represented by this object.
+
+        Returns:
+            str
+        """
+        raise NotImplementedError()
+
+    def to_database(self):
+        """Returns the DB representation of this object. Equivalent to calling :func:`to_database` with it.
+
+        Returns:
+            tuple(bytes,str): the `value` and `type` fields that would go in the ``parameter_value`` table.
+        """
+        return json.dumps(self.to_dict()).encode("UTF8"), self.type_()
+
+
 class ListValueRef:
     def __init__(self, list_value_id):
         self._list_value_id = list_value_id
 
     @staticmethod
     def type_():
         return "list_value_ref"
 
     def to_database(self):
-        """Returns the database representation of this object as JSON."""
         return json.dumps(self._list_value_id).encode("UTF8"), self.type_()
 
 
-class DateTime:
-    """A single datetime value."""
+class DateTime(ParameterValue):
+    """A parameter value of type 'date_time'. A point in time."""
 
     VALUE_TYPE = "single value"
 
     def __init__(self, value=None):
         """
         Args:
-            value (DataTime or str or datetime.datetime): a timestamp
+            value (:class:`DateTime` or str or :class:`~datetime.datetime`): the `date_time` value.
         """
         if value is None:
             value = datetime(year=2000, month=1, day=1)
         elif isinstance(value, str):
             try:
-                value = dateutil.parser.parse(value)
+                value = datetime.fromisoformat(value)
             except ValueError:
-                raise ParameterValueFormatError(f'Could not parse datetime from "{value}"')
+                try:
+                    value = dateutil.parser.parse(value)
+                except ValueError:
+                    raise ParameterValueFormatError(f'Could not parse datetime from "{value}"')
         elif isinstance(value, DateTime):
             value = copy(value._value)
         elif not isinstance(value, datetime):
             raise ParameterValueFormatError(f'"{type(value).__name__}" cannot be converted to DateTime.')
         self._value = value
 
     def __eq__(self, other):
-        """Returns True if other is equal to this object."""
         if not isinstance(other, DateTime):
             return NotImplemented
         return self._value == other._value
 
     def __lt__(self, other):
         if not isinstance(other, DateTime):
             return NotImplemented
@@ -670,90 +779,99 @@
     def __hash__(self):
         return hash(self._value)
 
     def __str__(self):
         return self._value.isoformat()
 
     def value_to_database_data(self):
-        """Returns the database representation of the datetime."""
+        """Returns the database representation of the datetime.
+
+        :meta private:
+        """
         return self._value.isoformat()
 
     def to_dict(self):
-        """Returns the database representation of this object."""
         return {"data": self.value_to_database_data()}
 
     @staticmethod
     def type_():
-        return "date_time"
+        """See base class
 
-    def to_database(self):
-        """Returns the database representation of this object as JSON."""
-        return json.dumps(self.to_dict()).encode("UTF8"), self.type_()
+        :meta private:
+        """
+        return "date_time"
 
     @property
     def value(self):
-        """Returns the value as a datetime object."""
+        """The value.
+
+        Returns:
+            :class:`~datetime.datetime`
+        """
         return self._value
 
 
-class Duration:
+class Duration(ParameterValue):
     """
-    This class represents a duration in time.
-
-    Durations are always handled as relativedeltas.
+    A parameter value of type 'duration'. An extension of time.
     """
 
     VALUE_TYPE = "single value"
 
     def __init__(self, value=None):
         """
         Args:
-            value (str or relativedelta): the time step
+            value (str or :class:`Duration` or :class:`~dateutil.dateutil.relativedelta`): the `duration` value.
         """
         if value is None:
             value = relativedelta(hours=1)
         elif isinstance(value, str):
             value = duration_to_relativedelta(value)
         elif isinstance(value, Duration):
             value = copy(value._value)
         if not isinstance(value, relativedelta):
             raise ParameterValueFormatError(f'Could not parse duration from "{value}"')
         self._value = value
 
     def __eq__(self, other):
-        """Returns True if other is equal to this object."""
         if not isinstance(other, Duration):
             return NotImplemented
         return self._value == other._value
 
     def __hash__(self):
         return hash(self._value)
 
     def __str__(self):
         return str(relativedelta_to_duration(self._value))
 
     def value_to_database_data(self):
-        """Returns the 'data' attribute part of Duration's database representation."""
+        """Returns the 'data' property of this object's database representation.
+
+        :meta private:
+        """
         return relativedelta_to_duration(self._value)
 
     def to_dict(self):
-        """Returns the database representation of the duration."""
         return {"data": self.value_to_database_data()}
 
     @staticmethod
     def type_():
-        return "duration"
+        """See base class
 
-    def to_database(self):
-        """Returns the database representation of the duration as JSON."""
-        return json.dumps(self.to_dict()).encode("UTF8"), self.type_()
+        :meta private:
+        """
+        return "duration"
 
     @property
     def value(self):
-        """Returns the duration as a :class:`relativedelta`."""
+        """The value.
+
+        Returns
+            :class:`~dateutil.dateutil.relativedelta`
+        """
         return self._value
 
 
 class _Indexes(np.ndarray):
     """
     A subclass of numpy.ndarray that keeps a lookup dictionary from elements to positions.
     Used by methods get_value and set_value of IndexedValue, to avoid something like
@@ -768,413 +886,432 @@
         obj.position_lookup = {index: k for k, index in enumerate(other)}
         return obj
 
     def __array_finalize__(self, obj):
         if obj is None:
             return
         # pylint: disable=attribute-defined-outside-init
-        self.position_lookup = getattr(obj, 'position_lookup', {})
+        self.position_lookup = getattr(obj, "position_lookup", {})
 
     def __setitem__(self, position, index):
         old_index = self.__getitem__(position)
-        self.position_lookup[index] = self.position_lookup.pop(old_index, '')
+        self.position_lookup[index] = self.position_lookup.pop(old_index, "")
         super().__setitem__(position, index)
 
     def __eq__(self, other):
-        return np.all(super().__eq__(other))
+        return len(self) == len(other) and np.all(super().__eq__(other))
 
     def __bool__(self):
         return np.size(self) != 0
 
 
-class IndexedValue:
+class IndexedValue(ParameterValue):
     """
-    An abstract base class for indexed values.
-
-    Attributes:
-        index_name (str): index name
+    Base for all classes representing indexed parameter values.
     """
 
-    VALUE_TYPE = NotImplemented
+    DEFAULT_INDEX_NAME = NotImplemented
 
-    def __init__(self, index_name):
+    def __init__(self, values, value_type=None, index_name=""):
         """
+        :meta private:
+
         Args:
-            index_name (str): index name
+            index_name (str): a label for the index.
         """
+        self._value_type = value_type
         self._indexes = None
         self._values = None
-        self.index_name = index_name
+        self.values = values
+        self.index_name = index_name if index_name else self.DEFAULT_INDEX_NAME
 
     def __bool__(self):
         # NOTE: Use self.indexes rather than self._indexes, otherwise TimeSeriesFixedResolution gives wrong result
         return bool(self.indexes)
 
     def __len__(self):
-        """Returns the number of values."""
         return len(self.indexes)
 
     @staticmethod
     def type_():
-        """Returns a type identifier string.
+        """See base class
 
-        Returns:
-            str: type identifier
+        :meta private:
         """
         raise NotImplementedError()
 
     @property
     def indexes(self):
-        """Returns the indexes."""
+        """The indexes.
+
+        Returns:
+            :class:`~numpy.ndarray`
+        """
         return self._indexes
 
     @indexes.setter
     def indexes(self, indexes):
-        """Sets the indexes."""
-        self._indexes = _Indexes(indexes)
+        """Sets the indexes.
 
-    def to_database(self):
-        """Return the database representation of the value."""
-        return json.dumps(self.to_dict()).encode("UTF8"), self.type_()
+        Args:
+            indexes (:class:`~numpy.ndarray`)
+        """
+        self._indexes = _Indexes(indexes)
 
     @property
     def values(self):
-        """Returns the data values."""
+        """The values.
+
+        Returns:
+            :class:`~numpy.ndarray`
+        """
         return self._values
 
     @values.setter
     def values(self, values):
-        """Sets the values."""
+        """Sets the values.
+
+        Args:
+            values (:class:`~numpy.ndarray`)
+        """
+        if isinstance(self._value_type, np.dtype) and (
+            not isinstance(values, np.ndarray) or not values.dtype == self._value_type
+        ):
+            values = np.array(values, dtype=self._value_type)
         self._values = values
 
+    @property
+    def value_type(self):
+        """The type of the values.
+
+        Returns:
+            type:
+        """
+        return self._value_type
+
     def get_nearest(self, index):
+        """Returns the value at the nearest index to the given one.
+
+        Args:
+            index (any): The index.
+
+        Returns:
+            any: The value.
+        """
         pos = np.searchsorted(self.indexes, index)
         return self.values[pos]
 
     def get_value(self, index):
-        """Returns the value at the given index."""
+        """Returns the value at the given index.
+
+        Args:
+            index (any): The index.
+
+        Returns:
+            any: The value.
+        """
         pos = self.indexes.position_lookup.get(index)
         if pos is None:
             return None
         return self.values[pos]
 
     def set_value(self, index, value):
-        """Sets the value at the given index."""
+        """Sets the value at the given index.
+
+        Args:
+            index (any): The index.
+            value (any): The value.
+        """
         pos = self.indexes.position_lookup.get(index)
         if pos is not None:
             self.values[pos] = value
 
     def to_dict(self):
-        """Converts the value to a Python dictionary.
-
-        Returns:
-            dict(): mapping from indexes to values
-        """
         raise NotImplementedError()
 
     def merge(self, other):
         if not isinstance(other, type(self)):
             return self
         new_indexes = np.unique(np.concatenate((self.indexes, other.indexes)))
-        new_indexes.sort(kind='mergesort')
+        new_indexes.sort(kind="mergesort")
         _merge = lambda value, other: other if value is None else merge_parsed(value, other)
         new_values = [_merge(self.get_value(index), other.get_value(index)) for index in new_indexes]
         self.indexes = new_indexes
         self.values = new_values
         return self
 
 
 class Array(IndexedValue):
-    """A one dimensional array with zero based indexing."""
+    """A parameter value of type 'array'. A one dimensional array with zero based indexing."""
 
     VALUE_TYPE = "array"
     DEFAULT_INDEX_NAME = "i"
 
     def __init__(self, values, value_type=None, index_name=""):
         """
         Args:
-            values (Sequence): array's values
-            value_type (Type, optional): array element type; will be deduced from the array if not given
-                and defaults to float if ``values`` is empty
-            index_name (str): index name
+            values (Sequence): the array values.
+            value_type (type, optional): the type of the values; if not given, it will be deduced from `values`.
+                Defaults to float if `values` is empty.
+            index_name (str): the name you would give to the array index in your application.
         """
-        super().__init__(index_name if index_name else Array.DEFAULT_INDEX_NAME)
         if value_type is None:
             value_type = type(values[0]) if values else float
-            if value_type == int:
-                try:
-                    values = [float(x) for x in values]
-                except ValueError:
-                    raise ParameterValueFormatError("Cannot convert array's values to float.")
-                value_type = float
-        if any(not isinstance(x, value_type) for x in values):
+        if value_type == int:
+            value_type = float
+            try:
+                values = [value_type(x) for x in values]
+            except ValueError:
+                raise ParameterValueFormatError("Cannot convert array's values to float.")
+        if not all(isinstance(x, value_type) for x in values):
             try:
                 values = [value_type(x) for x in values]
             except ValueError:
                 raise ParameterValueFormatError("Not all array's values are of the same type.")
+        super().__init__(values, value_type=value_type, index_name=index_name)
         self.indexes = range(len(values))
-        self.values = list(values)
-        self._value_type = value_type
 
     def __eq__(self, other):
         if not isinstance(other, Array):
             return NotImplemented
-        return np.array_equal(self._values, other._values) and self.index_name == other.index_name
+        try:
+            return np.array_equal(self._values, other._values, equal_nan=True) and self.index_name == other.index_name
+        except TypeError:
+            return np.array_equal(self._values, other._values) and self.index_name == other.index_name
 
     @staticmethod
     def type_():
         return "array"
 
     def to_dict(self):
-        """See base class."""
         value_type_id = {
             float: "float",
             str: "str",  # String could also mean time_period but we don't have any way to distinguish that, yet.
             DateTime: "date_time",
             Duration: "duration",
         }.get(self._value_type)
         if value_type_id is None:
             raise ParameterValueFormatError(f"Cannot write unsupported array value type: {self._value_type.__name__}")
         if value_type_id in ("float", "str"):
             data = self._values
         else:
             data = [x.value_to_database_data() for x in self._values]
         value_dict = {"value_type": value_type_id, "data": data}
-        if self.index_name != "i":
+        if self.index_name != self.DEFAULT_INDEX_NAME:
             value_dict["index_name"] = self.index_name
         return value_dict
 
-    @property
-    def value_type(self):
-        """Returns the type of array's elements."""
-        return self._value_type
-
-
-class IndexedNumberArray(IndexedValue):
-    """
-    An abstract base class for indexed floats.
-
-    The indexes and numbers are stored in numpy.ndarrays.
-    """
-
-    def __init__(self, index_name, values):
-        """
-        Args:
-            index_name (str): index name
-            values (Sequence): array's values; index handling should be implemented by subclasses
-        """
-        super().__init__(index_name)
-        self.values = values
 
-    @IndexedValue.values.setter
-    def values(self, values):
-        """Sets the values."""
-        if not isinstance(values, np.ndarray) or not values.dtype == np.dtype(float):
-            values = np.array(values, dtype=float)
-        self._values = values
+class _TimePatternIndexes(_Indexes):
+    """An array of *checked* time pattern indexes."""
 
     @staticmethod
-    def type_():
-        raise NotImplementedError()
-
-    def to_dict(self):
-        """Return the database representation of the value."""
-        raise NotImplementedError()
-
-
-class TimeSeries(IndexedNumberArray):
-    """An abstract base class for time series."""
-
-    VALUE_TYPE = "time series"
-    DEFAULT_INDEX_NAME = "t"
-
-    def __init__(self, values, ignore_year, repeat, index_name=""):
+    def _check_index(union_str):
         """
-        Args:
-            values (Sequence): an array of values
-            ignore_year (bool): True if the year should be ignored in the time stamps
-            repeat (bool): True if the series should be repeated from the beginning
-            index_name (str): index name
-        """
-        if len(values) < 1:
-            raise ParameterValueFormatError("Time series too short. Must have one or more values")
-        super().__init__(index_name if index_name else TimeSeries.DEFAULT_INDEX_NAME, values)
-        self._ignore_year = ignore_year
-        self._repeat = repeat
-
-    def __len__(self):
-        """Returns the number of values."""
-        return len(self._values)
-
-    @property
-    def ignore_year(self):
-        """Returns True if the year should be ignored."""
-        return self._ignore_year
-
-    @ignore_year.setter
-    def ignore_year(self, ignore_year):
-        self._ignore_year = bool(ignore_year)
-
-    @property
-    def repeat(self):
-        """Returns True if the series should be repeated."""
-        return self._repeat
-
-    @repeat.setter
-    def repeat(self, repeat):
-        self._repeat = bool(repeat)
-
-    @staticmethod
-    def type_():
-        return "time_series"
-
-    def to_dict(self):
-        """Return the database representation of the value."""
-        raise NotImplementedError()
-
-
-def _check_time_pattern_index(union_str):
-    """
-    Checks if a time pattern index has the right format.
-
-    Args:
-        union_str (str): The time pattern index to check. Generally assumed to be a union of interval intersections.
-
-    Raises:
-        ParameterValueFormatError: If the given string doesn't comply with time pattern spec.
-    """
-    if not union_str:
-        # We accept empty strings so we can add empty rows in the parameter value editor UI
-        return
-    union_dlm = ","
-    intersection_dlm = ";"
-    range_dlm = "-"
-    regexp = r"(Y|M|D|WD|h|m|s)"
-    for intersection_str in union_str.split(union_dlm):
-        for interval_str in intersection_str.split(intersection_dlm):
-            m = re.match(regexp, interval_str)
-            if m is None:
-                raise ParameterValueFormatError(
-                    f"Invalid interval {interval_str}, it should start with either Y, M, D, WD, h, m, or s."
-                )
-            key = m.group(0)
-            lower_upper_str = interval_str[len(key) :]
-            lower_upper = lower_upper_str.split(range_dlm)
-            if len(lower_upper) != 2:
-                raise ParameterValueFormatError(
-                    f"Invalid interval bounds {lower_upper_str}, it should be two integers separated by dash (-)."
-                )
-            lower_str, upper_str = lower_upper
-            try:
-                lower = int(lower_str)
-            except:
-                raise ParameterValueFormatError(f"Invalid lower bound {lower_str}, must be an integer.")
-            try:
-                upper = int(upper_str)
-            except:
-                raise ParameterValueFormatError(f"Invalid upper bound {upper_str}, must be an integer.")
-            if lower > upper:
-                raise ParameterValueFormatError(f"Lower bound {lower} can't be higher than upper bound {upper}.")
+        Checks if a time pattern index has the right format.
 
+        Args:
+            union_str (str): The time pattern index to check. Generally assumed to be a union of interval intersections.
 
-class _TimePatternIndexes(_Indexes):
-    """An array of *checked* time pattern indexes."""
+        Raises:
+            ParameterValueFormatError: If the given string doesn't comply with time pattern spec.
+        """
+        if not union_str:
+            # We accept empty strings so we can add empty rows in the parameter value editor UI
+            return
+        union_dlm = ","
+        intersection_dlm = ";"
+        range_dlm = "-"
+        regexp = r"(Y|M|D|WD|h|m|s)"
+        for intersection_str in union_str.split(union_dlm):
+            for interval_str in intersection_str.split(intersection_dlm):
+                m = re.match(regexp, interval_str)
+                if m is None:
+                    raise ParameterValueFormatError(
+                        f"Invalid interval {interval_str}, it should start with either Y, M, D, WD, h, m, or s."
+                    )
+                key = m.group(0)
+                lower_upper_str = interval_str[len(key) :]
+                lower_upper = lower_upper_str.split(range_dlm)
+                if len(lower_upper) != 2:
+                    raise ParameterValueFormatError(
+                        f"Invalid interval bounds {lower_upper_str}, it should be two integers separated by dash (-)."
+                    )
+                lower_str, upper_str = lower_upper
+                try:
+                    lower = int(lower_str)
+                except:
+                    raise ParameterValueFormatError(f"Invalid lower bound {lower_str}, must be an integer.")
+                try:
+                    upper = int(upper_str)
+                except:
+                    raise ParameterValueFormatError(f"Invalid upper bound {upper_str}, must be an integer.")
+                if lower > upper:
+                    raise ParameterValueFormatError(f"Lower bound {lower} can't be higher than upper bound {upper}.")
 
     def __array_finalize__(self, obj):
         """Checks indexes when building the array."""
         for x in obj:
-            _check_time_pattern_index(x)
+            self._check_index(x)
         super().__array_finalize__(obj)
 
     def __eq__(self, other):
         return list(self) == list(other)
 
     def __setitem__(self, position, index):
         """Checks indexes when setting and item."""
-        _check_time_pattern_index(index)
+        self._check_index(index)
         super().__setitem__(position, index)
 
 
-class TimePattern(IndexedNumberArray):
-    """Represents a time pattern (relationship) parameter value."""
+class TimePattern(IndexedValue):
+    """A parameter value of type 'time_pattern'.
+    A mapping from time patterns strings to numerical values.
+    """
 
     VALUE_TYPE = "time pattern"
     DEFAULT_INDEX_NAME = "p"
 
     def __init__(self, indexes, values, index_name=""):
         """
         Args:
-            indexes (list): a list of time pattern strings
-            values (Sequence): an array of values corresponding to the time patterns
+            indexes (list): the time pattern strings.
+            values (Sequence): the values associated to different patterns.
             index_name (str): index name
         """
         if len(indexes) != len(values):
             raise ParameterValueFormatError("Length of values does not match length of indexes")
         if not indexes:
             raise ParameterValueFormatError("Empty time pattern not allowed")
-        super().__init__(index_name if index_name else TimePattern.DEFAULT_INDEX_NAME, values)
+        super().__init__(values, value_type=np.dtype(float), index_name=index_name)
         self.indexes = indexes
 
     def __eq__(self, other):
-        """Returns True if other is equal to this object."""
         if not isinstance(other, TimePattern):
             return NotImplemented
         return (
             self._indexes == other._indexes
             and np.all(self._values == other._values)
             and self.index_name == other.index_name
         )
 
-    @IndexedNumberArray.indexes.setter
+    @IndexedValue.indexes.setter
     def indexes(self, indexes):
-        """Sets the indexes."""
         self._indexes = _TimePatternIndexes(indexes, dtype=np.object_)
 
     @staticmethod
     def type_():
         return "time_pattern"
 
     def to_dict(self):
-        """Returns the database representation of this time pattern."""
         value_dict = {"data": dict(zip(self._indexes, self._values))}
-        if self.index_name != "p":
+        if self.index_name != self.DEFAULT_INDEX_NAME:
             value_dict["index_name"] = self.index_name
         return value_dict
 
 
+class TimeSeries(IndexedValue):
+    """Base for all classes representing 'time_series' parameter values."""
+
+    VALUE_TYPE = "time series"
+    DEFAULT_INDEX_NAME = "t"
+
+    def __init__(self, values, ignore_year, repeat, index_name=""):
+        """
+        :meta private:
+
+        Args:
+            values (Sequence): the values in the time-series.
+            ignore_year (bool): True if the year should be ignored.
+            repeat (bool): True if the series is repeating.
+            index_name (str): index name.
+        """
+        if len(values) < 1:
+            raise ParameterValueFormatError("Time series too short. Must have one or more values")
+        super().__init__(values, value_type=np.dtype(float), index_name=index_name)
+        self._ignore_year = ignore_year
+        self._repeat = repeat
+
+    def __len__(self):
+        return len(self._values)
+
+    @property
+    def ignore_year(self):
+        """Whether the year should be ignored.
+
+        Returns:
+            bool:
+        """
+        return self._ignore_year
+
+    @ignore_year.setter
+    def ignore_year(self, ignore_year):
+        """Sets the ignore_year property.
+
+        Args:
+            bool: new value.
+        """
+        self._ignore_year = bool(ignore_year)
+
+    @property
+    def repeat(self):
+        """Whether the series is repeating.
+
+        Returns:
+            bool:
+        """
+        return self._repeat
+
+    @repeat.setter
+    def repeat(self, repeat):
+        """Sets the repeat property.
+
+        Args:
+            bool: new value.
+        """
+        self._repeat = bool(repeat)
+
+    @staticmethod
+    def type_():
+        return "time_series"
+
+    def to_dict(self):
+        raise NotImplementedError()
+
+
 class TimeSeriesFixedResolution(TimeSeries):
     """
-    A time series with fixed durations between the time stamps.
+    A parameter value of type 'time_series'.
+    A mapping from time stamps to numerical values, with fixed durations between the time stamps.
 
     When getting the indexes the durations are applied cyclically.
 
     Currently, there is no support for the `ignore_year` and `repeat` options
     other than having getters for their values.
     """
 
     _memoized_indexes = {}
 
     def __init__(self, start, resolution, values, ignore_year, repeat, index_name=""):
         """
         Args:
-            start (str or datetime or datetime64): the first time stamp
-            resolution (str, relativedelta, list): duration(s) between the time stamps
-            values (Sequence): data values at each time stamp
-            ignore_year (bool): whether or not the time-series should apply to any year
-            repeat (bool): whether or not the time series should repeat cyclically
-            index_name (str): index name
+            start (str or :class:`~datetime.datetime` or :class:`~numpy.datetime64`): the first time stamp
+            resolution (str, :class:`~dateutil.relativedelta.relativedelta`, list): duration(s) between the time stamps.
+            values (Sequence): the values in the time-series.
+            ignore_year (bool): True if the year should be ignored.
+            repeat (bool): True if the series is repeating.
+            index_name (str): index name.
         """
         super().__init__(values, ignore_year, repeat, index_name)
         self._start = None
         self._resolution = None
         self.start = start
         self.resolution = resolution
 
     def __eq__(self, other):
-        """Returns True if other is equal to this object."""
         if not isinstance(other, TimeSeriesFixedResolution):
             return NotImplemented
         return (
             self._start == other._start
             and self._resolution == other._resolution
             and np.array_equal(self._values, other._values, equal_nan=True)
             and self._ignore_year == other._ignore_year
@@ -1183,124 +1320,125 @@
         )
 
     def _get_memoized_indexes(self):
         key = (self.start, tuple(self.resolution), len(self))
         memoized_indexes = self._memoized_indexes.get(key)
         if memoized_indexes is not None:
             return memoized_indexes
-        step_index = 0
-        step_cycle_index = 0
-        full_cycle_duration = sum(self._resolution, relativedelta())
-        stamps = np.empty(len(self), dtype=_NUMPY_DATETIME_DTYPE)
-        stamps[0] = self._start
-        for stamp_index in range(1, len(self._values)):
-            if step_index >= len(self._resolution):
-                step_index = 0
-                step_cycle_index += 1
-            current_cycle_duration = sum(self._resolution[: step_index + 1], relativedelta())
-            duration_from_start = step_cycle_index * full_cycle_duration + current_cycle_duration
-            stamps[stamp_index] = self._start + duration_from_start
-            step_index += 1
-        memoized_indexes = self._memoized_indexes[key] = np.array(stamps, dtype=_NUMPY_DATETIME_DTYPE)
+        cycle_count = -(-len(self) // len(self.resolution))
+        resolution = (cycle_count * self.resolution)[: len(self) - 1]
+        resolution.insert(0, self._start)
+        resolution_arr = np.array(resolution)
+        memoized_indexes = self._memoized_indexes[key] = resolution_arr.cumsum().astype(_NUMPY_DATETIME_DTYPE)
         return memoized_indexes
 
     @property
     def indexes(self):
-        """Returns the time stamps as a numpy.ndarray of numpy.datetime64 objects."""
         if self._indexes is None:
             self.indexes = self._get_memoized_indexes()
         return IndexedValue.indexes.fget(self)
 
     @indexes.setter
     def indexes(self, indexes):
-        """Sets the indexes."""
         # Needed because we redefine the setter
         self._indexes = _Indexes(indexes)
 
     @property
     def start(self):
-        """Returns the start index."""
+        """Returns the start index.
+
+        Returns:
+            :class:`~numpy.datetime64`:
+        """
         return self._start
 
     @start.setter
     def start(self, start):
         """
-        Sets the start datetime.
+        Sets the start index.
 
         Args:
-            start (datetime or datetime64 or str): the start of the series
+            start (:class:`~datetime.datetime` or :class:`~numpy.datetime64` or str): the start of the series
         """
         if isinstance(start, str):
             try:
-                self._start = dateutil.parser.parse(start)
+                self._start = datetime.fromisoformat(start)
             except ValueError:
-                raise ParameterValueFormatError(f'Cannot parse start time "{start}"')
+                try:
+                    self._start = dateutil.parser.parse(start)
+                except ValueError:
+                    raise ParameterValueFormatError(f'Cannot parse start time "{start}"')
         elif isinstance(start, np.datetime64):
             self._start = start.tolist()
         else:
             self._start = start
         self._indexes = None
 
     @property
     def resolution(self):
-        """Returns the resolution as list of durations."""
+        """Returns the resolution as list of durations.
+
+        Returns:
+            list(:class:`Duration`):
+        """
         return self._resolution
 
     @resolution.setter
     def resolution(self, resolution):
         """
         Sets the resolution.
 
         Args:
-            resolution (str, relativedelta, list): resolution or a list thereof
+            resolution (str, :class:`~.dateutil.relativedelta.relativedelta`, list): resolution or a list thereof
         """
         if isinstance(resolution, str):
             resolution = [duration_to_relativedelta(resolution)]
         elif not isinstance(resolution, Sequence):
             resolution = [resolution]
         else:
-            for i in range(len(resolution)):
-                if isinstance(resolution[i], str):
-                    resolution[i] = duration_to_relativedelta(resolution[i])
+            for i, r in enumerate(resolution):
+                if isinstance(r, str):
+                    resolution[i] = duration_to_relativedelta(r)
         if not resolution:
             raise ParameterValueFormatError("Resolution cannot be zero.")
         self._resolution = resolution
         self._indexes = None
 
     def to_dict(self):
-        """Returns the value in its database representation."""
         if len(self._resolution) > 1:
             resolution_as_json = [relativedelta_to_duration(step) for step in self._resolution]
         else:
             resolution_as_json = relativedelta_to_duration(self._resolution[0])
         value_dict = {
             "index": {
                 "start": str(self._start),
                 "resolution": resolution_as_json,
                 "ignore_year": self._ignore_year,
                 "repeat": self._repeat,
             },
             "data": self._values.tolist(),
         }
-        if self.index_name != "t":
+        if self.index_name != self.DEFAULT_INDEX_NAME:
             value_dict["index_name"] = self.index_name
         return value_dict
 
 
 class TimeSeriesVariableResolution(TimeSeries):
-    """A class representing time series data with variable time steps."""
+    """A parameter value of type 'time_series'.
+    A mapping from time stamps to numerical values with arbitrary time steps.
+    """
 
     def __init__(self, indexes, values, ignore_year, repeat, index_name=""):
         """
         Args:
-            indexes (Sequence): time stamps as numpy.datetime64 objects
-            values (Sequence): the values corresponding to the time stamps
-            ignore_year (bool): True if the stamp year should be ignored
-            repeat (bool): True if the series should be repeated from the beginning
-            index_name (str): index name
+            indexes (Sequence(:class:`~numpy.datetime64`)): the time stamps.
+            values (Sequence): the value for each time stamp.
+            ignore_year (bool): True if the year should be ignored.
+            repeat (bool): True if the series is repeating.
+            index_name (str): index name.
         """
         super().__init__(values, ignore_year, repeat, index_name)
         if len(indexes) != len(values):
             raise ParameterValueFormatError("Length of values does not match length of indexes")
         if not isinstance(indexes, np.ndarray):
             date_times = np.empty(len(indexes), dtype=_NUMPY_DATETIME_DTYPE)
             for i, index in enumerate(indexes):
@@ -1313,71 +1451,79 @@
                         raise ParameterValueFormatError(
                             f'Cannot convert "{index}" of type {type(index).__name__} to time stamp.'
                         )
             indexes = date_times
         self.indexes = indexes
 
     def __eq__(self, other):
-        """Returns True if other is equal to this object."""
         if not isinstance(other, TimeSeriesVariableResolution):
             return NotImplemented
         return (
             np.array_equal(self._indexes, other._indexes)
             and np.array_equal(self._values, other._values, equal_nan=True)
             and self._ignore_year == other._ignore_year
             and self._repeat == other._repeat
             and self.index_name == other.index_name
         )
 
     def to_dict(self):
-        """Returns the value in its database representation"""
         value_dict = dict()
         value_dict["data"] = {str(index): float(value) for index, value in zip(self._indexes, self._values)}
         # Add "index" entry only if its contents are not set to their default values.
         if self._ignore_year:
             value_dict.setdefault("index", dict())["ignore_year"] = self._ignore_year
         if self._repeat:
             value_dict.setdefault("index", dict())["repeat"] = self._repeat
-        if self.index_name != "t":
+        if self.index_name != self.DEFAULT_INDEX_NAME:
             value_dict["index_name"] = self.index_name
         return value_dict
 
 
 class Map(IndexedValue):
-    """A nested general purpose indexed value."""
+    """A parameter value of type 'map'. A mapping from key to value, where the values can be other instances
+    of :class:`ParameterValue`.
+    """
 
     VALUE_TYPE = "map"
     DEFAULT_INDEX_NAME = "x"
 
     def __init__(self, indexes, values, index_type=None, index_name=""):
         """
         Args:
-            indexes (Sequence): map's indexes
-            values (Sequence): map's values
-            index_type (type or NoneType): index type or None to deduce from indexes
-            index_name (str): index name
+            indexes (Sequence): the indexes in the map.
+            values (Sequence): the value for each index.
+            index_type (type or NoneType): index type or None to deduce from ``indexes``.
+            index_name (str): index name.
         """
         if not indexes and index_type is None:
             raise ParameterValueFormatError("Cannot deduce index type from empty indexes list.")
         if indexes and index_type is not None and not isinstance(indexes[0], index_type):
             raise ParameterValueFormatError('Type of index does not match "index_type" argument.')
         if len(indexes) != len(values):
             raise ParameterValueFormatError("Length of values does not match length of indexes")
-        super().__init__(index_name if index_name else Map.DEFAULT_INDEX_NAME)
+        super().__init__(values, index_name=index_name)
         self.indexes = indexes
         self._index_type = index_type if index_type is not None else type(indexes[0])
         self._values = values
 
     def __eq__(self, other):
         if not isinstance(other, Map):
             return NotImplemented
         return other._indexes == self._indexes and other._values == self._values and self.index_name == other.index_name
 
+    @property
+    def index_type(self):
+        return self._index_type
+
     def is_nested(self):
-        """Returns True if any of the values is also a map."""
+        """Whether any of the values is also a map.
+
+        Returns:
+            bool:
+        """
         return any(isinstance(value, Map) for value in self._values)
 
     def value_to_database_data(self):
         """Returns map's database representation's 'data' dictionary."""
         data = list()
         for index, value in zip(self._indexes, self._values):
             index_in_db = _map_index_to_database(index)
@@ -1386,29 +1532,30 @@
         return data
 
     @staticmethod
     def type_():
         return "map"
 
     def to_dict(self):
-        """Returns map's database representation."""
         value_dict = {
             "index_type": _map_index_type_to_database(self._index_type),
             "data": self.value_to_database_data(),
         }
-        if self.index_name != "x":
+        if self.index_name != self.DEFAULT_INDEX_NAME:
             value_dict["index_name"] = self.index_name
         return value_dict
 
 
 def map_dimensions(map_):
-    """Counts Map's dimensions.
+    """Counts the dimensions in a map.
+
+    :meta private:
 
     Args:
-        map_ (Map): a Map
+        map_ (:class:`Map`): the map to process.
 
     Returns:
         int: number of dimensions
     """
     nested = 0
     for v in map_.values:
         if isinstance(v, Map):
@@ -1416,25 +1563,28 @@
         elif isinstance(v, IndexedValue):
             nested = max(nested, 1)
     return 1 + nested
 
 
 def convert_leaf_maps_to_specialized_containers(map_):
     """
-    Converts suitable leaf maps to corresponding specialized containers.
+    Converts leafs to specialized containers.
 
-    Currently supported conversions:
+    Current conversion rules:
 
-    - index_type: :class:`DateTime`, all values ``float`` -> :class"`TimeSeries`
+    - If the ``index_type`` is a :class:`DateTime` and all ``values`` are float,
+      then the leaf is converted to a :class:`TimeSeries`.
+
+    :meta private:
 
     Args:
-        map_ (Map): a map to process
+        map_ (:class:`Map`): a map to process.
 
     Returns:
-        IndexedValue: a map with leaves converted or specialized container if map was convertible in itself
+        :class:`IndexedValue`: a new map with leaves converted.
     """
     converted_container = _try_convert_to_container(map_)
     if converted_container is not None:
         return converted_container
     new_values = list()
     for _, value in zip(map_.indexes, map_.values):
         if isinstance(value, Map):
@@ -1445,21 +1595,23 @@
     return Map(map_.indexes, new_values, index_name=map_.index_name)
 
 
 def convert_containers_to_maps(value):
     """
     Converts indexed values into maps.
 
-    if ``value`` is :class:`Map` converts leaf values into Maps recursively.
+    If ``value`` is a :class:`Map` then converts leaf values into maps recursively.
+
+    :meta private:
 
     Args:
-        value (IndexedValue): a value to convert
+        value (:class:`IndexedValue`): an indexed value to convert.
 
     Returns:
-        Map: converted Map
+        :class:`Map`: converted Map
     """
     if isinstance(value, Map):
         if not value:
             return value
         new_values = list()
         for _, x in zip(value.indexes, value.values):
             if isinstance(x, IndexedValue):
@@ -1476,19 +1628,21 @@
     return value
 
 
 def convert_map_to_table(map_, make_square=True, row_this_far=None, empty=None):
     """
     Converts :class:`Map` into list of rows recursively.
 
+    :meta private:
+
     Args:
-        map_ (Map): map to convert
-        make_square (bool): if True, append None to shorter rows, otherwise leave the row as is
-        row_this_far (list, optional): current row; used for recursion
-        empty (Any, optional): object to fill empty cells with
+        map_ (:class:`Map`): map to convert.
+        make_square (bool): if True, then pad rows with None so they all have the same length.
+        row_this_far (list, optional): current row; used for recursion.
+        empty (any, optional): object to fill empty cells with.
 
     Returns:
         list of list: map's rows
     """
     if row_this_far is None:
         row_this_far = list()
     rows = list()
@@ -1507,21 +1661,23 @@
             equal_length_rows.append(equal_length_row)
         return equal_length_rows
     return rows
 
 
 def convert_map_to_dict(map_):
     """
-    Converts :class:`Map` to nested dictionaries.
+    Converts a :class:`Map` to a nested dictionary.
+
+    :meta private:
 
     Args:
-        map_ (Map): map to convert
+        map_ (:class:`Map`): map to convert
 
     Returns:
-        dict: Map as a dict
+        dict:
     """
     d = dict()
     for index, x in zip(map_.indexes, map_.values):
         if isinstance(x, Map):
             x = convert_map_to_dict(x)
         d[index] = x
     return d
@@ -1555,37 +1711,90 @@
 
 def join_value_and_type(db_value, db_type):
     """Joins database value and type into a string.
     The resulting string is a JSON string.
     In case of complex types (duration, date_time, time_series, time_pattern, array, map),
     the type is just added as top-level key.
 
+    :meta private:
+
     Args:
         db_value (bytes): database value
         db_type (str, optional): value type
 
     Returns:
-        str: parameter value as JSON with an additional `type` field.
+        str: parameter value as JSON with an additional ``type`` field.
     """
     try:
         parsed = load_db_value(db_value, db_type)
     except ParameterValueFormatError:
         parsed = None
     return json.dumps(parsed)
 
 
 def split_value_and_type(value_and_type):
     """Splits the given string into value and type.
-    The string must be the result of calling ``join_value_and_type`` or have the same form.
+
+    :meta private:
 
     Args:
-        value_and_type (str)
+        value_and_type (str): a string joining value and type, as obtained by calling :func:`join_value_and_type`.
 
     Returns:
-        bytes
-        str or NoneType
+        tuple(bytes,str): database value and type.
     """
     try:
         parsed = json.loads(value_and_type)
     except (TypeError, json.JSONDecodeError):
         parsed = value_and_type
     return dump_db_value(parsed)
+
+
+def deep_copy_value(value):
+    """Copies a value.
+    The operation is deep meaning that nested Maps will be copied as well.
+
+    :meta private:
+
+    Args:
+        value (Any): value to copy
+
+    Returns:
+        Any: deep-copied value
+    """
+    if isinstance(value, (Number, str)) or value is None:
+        return value
+    if isinstance(value, Array):
+        return Array(value.values, value.value_type, value.index_name)
+    if isinstance(value, DateTime):
+        return DateTime(value)
+    if isinstance(value, Duration):
+        return Duration(value)
+    if isinstance(value, Map):
+        return deep_copy_map(value)
+    if isinstance(value, TimePattern):
+        return TimePattern(value.indexes.copy(), value.values.copy(), value.index_name)
+    if isinstance(value, TimeSeriesFixedResolution):
+        return TimeSeriesFixedResolution(
+            value.start, value.resolution, value.values.copy(), value.ignore_year, value.repeat, value.index_name
+        )
+    if isinstance(value, TimeSeriesVariableResolution):
+        return TimeSeriesVariableResolution(
+            value.indexes.copy(), value.values.copy(), value.ignore_year, value.repeat, value.index_name
+        )
+    raise ValueError("unknown value")
+
+
+def deep_copy_map(value):
+    """Deep copies a Map value.
+
+    :meta private:
+
+    Args:
+        value (Map): Map to copy
+
+    Returns:
+        Map: deep-copied Map
+    """
+    xs = value.indexes.copy()
+    ys = [deep_copy_value(y) for y in value.values]
+    return Map(xs, ys, index_type=value.index_type, index_name=value.index_name)
```

### Comparing `spinedb_api-0.30.5/spinedb_api/perfect_split.py` & `spinedb_api-0.31.0/spinedb_api/perfect_split.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,43 +1,43 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
 # Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
 # any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
 """
-Functions for the perfect db split.
-
+This module provides the :func:`perfect_split` function.
 """
 from .db_mapping import DatabaseMapping
 from .export_functions import export_data
 from .import_functions import import_data
 
 
 def perfect_split(input_urls, intersection_url, diff_urls):
-    """Splits dbs into disjoint subsets.
+    """Splits DBs into disjoint subsets.
 
     Args:
-        input_urls (list(str)): List of urls to split
-        intersection_url (str): A url to store the data common to all input urls
-        diff_urls (list(str)): List of urls to store the differences of each input with respect to the intersection.
+        input_urls (list(str)): List of urls of DBs to split.
+        intersection_url (str): The url of a DB to store the data common to all input DBs (i.e., their intersection).
+        diff_urls (list(str)): List of urls of DBs to store the differences between each input and the intersection.
     """
     diff_url_lookup = dict(zip(input_urls, diff_urls))
     input_data_sets = {}
     db_names = {}
     for input_url in diff_url_lookup:
         input_db_map = DatabaseMapping(input_url)
         input_data_sets[input_url] = export_data(input_db_map)
         db_names[input_url] = input_db_map.codename
-        input_db_map.connection.close()
+        input_db_map.close()
     intersection_data = {}
     input_data_set_iter = iter(input_data_sets)
     left_url = next(iter(input_data_set_iter))
     right_urls = list(input_data_set_iter)
     left_data_set = input_data_sets[left_url]
     for tablename, left in left_data_set.items():
         intersection = [x for x in left if all(x in input_data_sets[url][tablename] for url in right_urls)]
@@ -51,27 +51,27 @@
             left_diff = [x for x in left if all(x not in input_data_sets[url][tablename] for url in right_urls)]
             if left_diff:
                 diff_data = diffs_data.setdefault(left_url, {})
                 diff_data[tablename] = left_diff
     if intersection_data:
         db_map_intersection = DatabaseMapping(intersection_url)
         import_data(db_map_intersection, **intersection_data)
-        all_db_names = ', '.join(db_names.values())
+        all_db_names = ", ".join(db_names.values())
         db_map_intersection.commit_session(f"Add intersection of {all_db_names}")
         db_map_intersection.connection.close()
     lookup = _make_lookup(intersection_data)
     for input_url, diff_data in diffs_data.items():
         diff_url = diff_url_lookup[input_url]
         diff_db_map = DatabaseMapping(diff_url)
         _add_references(diff_data, lookup)
         import_data(diff_db_map, **diff_data)
         db_name = db_names[input_url]
-        other_db_names = ', '.join([name for url, name in db_names.items() if url != input_url])
+        other_db_names = ", ".join([name for url, name in db_names.items() if url != input_url])
         diff_db_map.commit_session(f"Add differences between {db_name} and {other_db_names}")
-        diff_db_map.connection.close()
+        diff_db_map.close()
 
 
 def _make_lookup(data):
     lookup = {}
     if "object_classes" in data:
         lookup["object_classes"] = {(x[0],): x for x in data["object_classes"]}
     if "relationship_classes" in data:
```

### Comparing `spinedb_api-0.30.5/spinedb_api/purge.py` & `spinedb_api-0.31.0/spinedb_api/purge.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,80 +1,80 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Toolbox is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
 # Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
 # any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
 """
-Functions to purge dbs.
-
+Functions to purge DBs.
 """
-
 from .db_mapping import DatabaseMapping
 from .exception import SpineDBAPIError, SpineDBVersionError
 from .filters.tools import clear_filter_configs
 from .helpers import remove_credentials_from_url
 
 
-def _ids_for_item_type(db_map, item_type):
-    """Queries ids for given database item type.
+def purge_url(url, purge_settings, logger=None):
+    """Removes all items of selected types from the database at a given URL.
+
+    Purges everything if ``purge_settings`` is None.
 
     Args:
-        db_map (DatabaseMapping): database map
-        item_type (str): database item type
+        url (str): database URL
+        purge_settings (dict, optional): mapping from item type to a boolean indicating whether to remove them or not
+        logger (LoggerInterface, optional): logger
 
     Returns:
-        set of int: item ids
+        bool: True if operation was successful, False otherwise
     """
-    sq_attr = db_map.cache_sqs[item_type]
-    return {row.id for row in db_map.query(getattr(db_map, sq_attr))}
-
-
-def purge_url(url, purge_settings, logger=None):
     try:
         db_map = DatabaseMapping(url)
     except (SpineDBAPIError, SpineDBVersionError) as err:
         sanitized_url = clear_filter_configs(remove_credentials_from_url(url))
         if logger:
             logger.msg_warning.emit(f"Failed to purge url <b>{sanitized_url}</b>: {err}")
         return False
     success = purge(db_map, purge_settings, logger=logger)
-    db_map.connection.close()
+    db_map.close()
     return success
 
 
 def purge(db_map, purge_settings, logger=None):
-    """Removes items from database.
+    """Removes all items of selected types from a database.
+
+    Purges everything if ``purge_settings`` is None.
 
     Args:
         db_map (DatabaseMapping): target database mapping
-        purge_settings (dict, optional): mapping from item type to purge flag
+        purge_settings (dict, optional): mapping from item type to a boolean indicating whether to remove them or not
         logger (LoggerInterface): logger
 
     Returns:
         bool: True if operation was successful, False otherwise
     """
     if purge_settings is None:
         # Bring all the pain
-        purge_settings = {item_type: True for item_type in DatabaseMapping.ITEM_TYPES}
+        purge_settings = {item_type: True for item_type in DatabaseMapping.item_types()}
     removable_db_map_data = {
-        item_type: _ids_for_item_type(db_map, item_type) for item_type, checked in purge_settings.items() if checked
+        DatabaseMapping.real_item_type(item_type) for item_type, checked in purge_settings.items() if checked
     }
-    removable_db_map_data = {item_type: ids for item_type, ids in removable_db_map_data.items() if ids}
     if removable_db_map_data:
         try:
             if logger:
                 logger.msg.emit("Purging database...")
-            db_map.cascade_remove_items(**removable_db_map_data)
+            for item_type in removable_db_map_data & set(DatabaseMapping.item_types()):
+                db_map.purge_items(item_type)
             db_map.commit_session("Purge database")
             if logger:
                 logger.msg.emit("Database purged")
-        except SpineDBAPIError:
+        except SpineDBAPIError as err:
             if logger:
-                logger.msg_error.emit("Failed to purge database.")
+                sanitized_url = clear_filter_configs(remove_credentials_from_url(db_map.db_url))
+                logger.msg_error.emit(f"Failed to purge {sanitized_url}: {err}")
             return False
     return True
```

### Comparing `spinedb_api-0.30.5/spinedb_api/server_client_helpers.py` & `spinedb_api-0.31.0/spinedb_api/server_client_helpers.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,39 +1,34 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
-"""
-Helpers for server and client.
-
-"""
-
 import json
-from .import_functions import ImportErrorLogItem
 from .exception import SpineDBAPIError
+from .db_mapping_base import PublicItem
+from .temp_id import TempId
 
 # Encode decode server messages
-_START_OF_TAIL = '\u001f'  # Unit separator
-_START_OF_ADDRESS = '\u0091'  # Private Use 1
-_ADDRESS_SEP = ':'
+_START_OF_TAIL = "\u001f"  # Unit separator
+_START_OF_ADDRESS = "\u0091"  # Private Use 1
+_ADDRESS_SEP = ":"
 
 
 class ReceiveAllMixing:
-    """Provides _recvall, to read everything from a socket until the _EOT character is found."""
-
     _ENCODING = "utf-8"
     _BUFF_SIZE = 4096
-    _EOT = '\u0004'  # End of transmission
+    _EOT = "\u0004"  # End of transmission
     _BEOT = _EOT.encode(_ENCODING)
     """End of message character"""
 
     def _recvall(self):
         """
         Receives and returns all data in the request.
 
@@ -48,15 +43,15 @@
                 break
         return b"".join(fragments)[:-1]
 
 
 class _TailJSONEncoder(json.JSONEncoder):
     """
     A custom JSON encoder that accummulates bytes objects into a tail.
-    The bytes object are encoded as a string pointing to the address in the tail.
+    Each bytes object is encoded as a string pointing to the address in the tail.
     """
 
     def __init__(self):
         super().__init__()
         self._tail_parts = []
         self._tip = 0
 
@@ -66,59 +61,63 @@
             new_tip = self._tip + len(o)
             fr, to = self._tip, new_tip - 1
             address = f"{_START_OF_ADDRESS}{fr}{_ADDRESS_SEP}{to}"
             self._tip = new_tip
             return address
         if isinstance(o, set):
             return list(o)
-        if isinstance(o, (SpineDBAPIError, ImportErrorLogItem)):
+        if isinstance(o, SpineDBAPIError):
             return str(o)
+        if isinstance(o, PublicItem):
+            return o._extended()
+        if isinstance(o, TempId):
+            return o.private_id
         return super().default(o)
 
     @property
     def tail(self):
         return b"".join(self._tail_parts)
 
 
 def encode(o):
     """
-    Encodes given object (representing a server response) into a message with the following structure:
+    Encodes given object into a message to be sent via a socket, with the following structure:
 
         body | start of tail character | tail
 
     The body is obtained by JSON-encoding the argument, while replacing all `bytes` objects by addresses in the tail.
     The tail is computed by concatenating all `bytes` objects in the argument.
     See class:`_TailJSONEncoder`.
 
     Args:
-        o (any): A Python object representing a server response.
+        o (any): A Python object to encode.
 
     Returns:
-        bytes: A message to the client.
+        bytes: Encoded message.
     """
     encoder = _TailJSONEncoder()
     s = encoder.encode(o)
     return s.encode() + _START_OF_TAIL.encode() + encoder.tail
 
 
 def decode(b):
     """
-    Decodes given message (representing a client request) into a Python object.
+    Decodes given message received via a socket into a Python object.
     The message must have the following structure:
 
         body | start of tail character | tail
 
     The result is obtained by JSON-decoding the body, and then replacing all the addresses with the referred `bytes`
     from the tail.
 
     Args:
-        b (bytes): A message from the client.
+        b (bytes): A message to decode.
 
     Returns:
-        any: A Python object representing a client request.
+        any: Decoded object.
     """
     body, tail = b.split(_START_OF_TAIL.encode())
     o = json.loads(body)
     return _expand_addresses_in_place(o, tail)
 
 
 def _expand_addresses_in_place(o, tail):
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_db_client.py` & `spinedb_api-0.31.0/spinedb_api/spine_db_client.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,70 +1,99 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Engine.
 # Spine Engine is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser General
 # Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option)
 # any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
 """
-Contains the SpineDBClient class.
-
+This module defines the :class:`SpineDBClient` class.
 """
 
 from urllib.parse import urlparse
 import socket
 from sqlalchemy.engine.url import URL
 from .server_client_helpers import ReceiveAllMixing, encode, decode
 
-client_version = 6
+client_version = 7
 
 
 class SpineDBClient(ReceiveAllMixing):
     def __init__(self, server_address):
-        """
+        """Enables sending requests to a Spine DB server.
+
         Args:
-            server_address (tuple(str,int)): hostname and port
+            server_address (tuple(str,int)): the hostname and port where the server is listening.
         """
         self._server_address = server_address
         self.request = None
 
     @classmethod
     def from_server_url(cls, url):
+        """Creates a client from a server's URL.
+
+        Args:
+            url (str, URL): the URL where the server is listening.
+        """
         parsed = urlparse(url)
         if parsed.scheme != "http":
             raise ValueError(f"unable to create client, invalid server url {url}")
         return cls((parsed.hostname, parsed.port))
 
     def get_db_url(self):
-        """
+        """Returns the URL of the target Spine DB - the one the server is set to communicate with.
+
         Returns:
-            str: The underlying db url from the server
+            str
         """
         return self._send("get_db_url")
 
     def db_checkin(self):
+        """Blocks until all the servers that need to write to the same DB before this one
+        have reported all their writes."""
         return self._send("db_checkin")
 
     def db_checkout(self):
+        """Reports one write for this server."""
         return self._send("db_checkout")
 
     def cancel_db_checkout(self):
+        """Reverts the last write report for this server."""
         return self._send("cancel_db_checkout")
 
     def import_data(self, data, comment):
+        """Imports data to the DB using :func:`~spinedb_api.import_functions.import_data` and commits the changes.
+
+        Args:
+            data (dict): to be splatted into keyword arguments to :func:`~spinedb_api.import_functions.import_data`
+            comment (str): a commit message.
+        """
         return self._send("import_data", args=(data, comment))
 
     def export_data(self, **kwargs):
+        """Exports data from the DB using :func:`~spinedb_api.export_functions.export_data`.
+
+        Args:
+            **kwargs: keyword arguments passed to :func:`~spinedb_api.import_functions.export_data`
+        """
         return self._send("export_data", kwargs=kwargs)
 
     def call_method(self, method_name, *args, **kwargs):
+        """Calls a method from :class:`~spinedb_api.db_mapping.DatabaseMapping`.
+
+        Args:
+            method_name (str): the name of the method to call.
+            *args: positional arguments passed to the method call.
+            **kwargs: keyword arguments passed to the method call.
+        """
         return self._send("call_method", args=(method_name, *args), kwargs=kwargs)
 
     def open_db_map(self, db_url, upgrade, memory):
         return self._send("open_db_map", args=(db_url, upgrade, memory))
 
     def close_db_map(self):
         return self._send("close_db_map")
@@ -89,23 +118,13 @@
             self.request.sendall(msg + self._BEOT)
             if receive:
                 response = self._recvall()
                 return decode(response)
 
 
 def get_db_url_from_server(url):
-    """Returns the underlying db url associated with the given url, if it's a server url.
-    Otherwise, it assumes it's the url of DB and returns it unaltered.
-    Used by ``DatabaseMappingBase()``.
-
-    Args:
-        url (str, URL): a url, either from a Spine DB or from a Spine DB server.
-
-    Returns:
-        str
-    """
     if isinstance(url, URL):
         return url
     parsed = urlparse(url)
     if parsed.scheme != "http":
         return url
     return SpineDBClient((parsed.hostname, parsed.port)).get_db_url()
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_db_server.py` & `spinedb_api-0.31.0/spinedb_api/spine_db_server.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,21 +1,101 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
 """
-Contains the SpineDBServer class.
+This module provides a mechanism to create a socket server interface to a Spine DB.
+The server exposes most of the functionality of :class:`~spinedb_api.db_mapping.DatabaseMapping`,
+and can eventually remove the ``spinedb_api`` requirement (and the Python requirement altogether)
+from third-party applications that want to interact with Spine DBs. (Of course, they would need to have
+access to sockets instead.)
+
+Typically, you would start the server in a background Python process by specifying the URL of the target Spine DB,
+getting back the URL where the server is listening.
+You can then use that URL in any number of instances of your application that would connect to the server
+via a socket and then send requests to retrieve or modify the data in the DB.
+
+Requests to the server must be encoded using JSON.
+Each request must be a JSON array with the following elements:
+
+#. A JSON string with one of the available request names:
+   ``get_db_url``, ``import_data``, ``export_data``, ``query``, ``filtered_query``, ``apply_filters``,
+   ``clear_filters``, ``call_method``, ``db_checkin``, ``db_checkout``.
+#. A JSON array with positional arguments to the request.
+#. A JSON object with keyword arguments to the request.
+#. A JSON integer indicating the version of the server you want to talk to.
+
+The positional and keyword arguments to the different requests are documented
+in the :class:`~spinedb_api.spine_db_client.SpineDBClient` class
+(just look for a member function named after the request).
+
+The point of the server version is to allow client developers to adapt to changes in the Spine DB server API.
+Say we update ``spinedb_api`` and change the signature of one of the requests - in this case, we will
+also bump the current server version to the next integer.
+If you then upgrade your ``spinedb_api`` installation but not your client, the server will be able to respond
+with an error message saying that you need to update your client.
+The current server version can be queried by calling :func:`get_current_server_version`.
+
+The order in which multiple servers should write to the same DB can also be controlled using DB servers.
+This is particularly useful in high-concurrency scenarios.
+
+The server is started using :func:`closing_spine_db_server`.
+To control the order of writing you need to provide a queue, that you would obtain by calling :func:`db_server_manager`.
+
+The below example illustrates most of the functionality of the module.
+We create two DB servers targeting the same DB, and set the second to write before the first
+(via the ``ordering`` argument to :func:`closing_spine_db_server`).
+Then we spawn two threads that connect to those two servers and import an entity class.
+We make sure to call :meth:`~spinedb_api.spine_db_client.SpineDBClient.db_checkin` before importing,
+and :meth:`~spinedb_api.spine_db_client.SpineDBClient.db_checkout` after so the order of writing is respected.
+When the first thread attemps to write to the DB, it hangs because the second one hasn't written yet.
+Only after the second writes, the first one also writes and the program finishes::
+
+    import threading
+    from spinedb_api.spine_db_server import db_server_manager, closing_spine_db_server
+    from spinedb_api.spine_db_client import SpineDBClient
+    from spinedb_api.db_mapping import DatabaseMapping
+
+
+    def _import_entity_class(server_url, class_name):
+        client = SpineDBClient.from_server_url(server_url)
+        client.db_checkin()
+        _answer = client.import_data({"entity_classes": [(class_name, ())]}, f"Import {class_name}")
+        client.db_checkout()
+
+
+    db_url = 'sqlite:///somedb.sqlite'
+    with db_server_manager() as mngr_queue:
+        first_ordering = {"id": "second_before_first", "current": "first", "precursors": {"second"}, "part_count": 1}
+        second_ordering = {"id": "second_before_first", "current": "second", "precursors": set(), "part_count": 1}
+        with closing_spine_db_server(
+            db_url, server_manager_queue=mngr_queue, ordering=first_ordering
+        ) as first_server_url:
+            with closing_spine_db_server(
+                db_url, server_manager_queue=mngr_queue, ordering=second_ordering
+            ) as second_server_url:
+                t1 = threading.Thread(target=_import_entity_class, args=(first_server_url, "monkey"))
+                t2 = threading.Thread(target=_import_entity_class, args=(second_server_url, "donkey"))
+                t1.start()
+                with DatabaseMapping(db_url) as db_map:
+                    assert db_map.get_items("entity_class") == []  # Nothing written yet
+                t2.start()
+                t1.join()
+                t2.join()
 
+    with DatabaseMapping(db_url) as db_map:
+        assert [x["name"] for x in db_map.get_items("entity_class")] == ["donkey", "monkey"]
 """
 
 from urllib.parse import urlunsplit
 from contextlib import contextmanager
 import socketserver
 import multiprocessing as mp
 from multiprocessing.queues import Queue as MPQueue
@@ -29,24 +109,32 @@
 from spinedb_api import __version__ as spinedb_api_version
 from .db_mapping import DatabaseMapping
 from .import_functions import import_data
 from .export_functions import export_data
 from .parameter_value import dump_db_value
 from .server_client_helpers import ReceiveAllMixing, encode, decode
 from .filters.scenario_filter import scenario_filter_config
-from .filters.tool_filter import tool_filter_config
 from .filters.alternative_filter import alternative_filter_config
 from .filters.tools import apply_filter_stack
 from .spine_db_client import SpineDBClient
 
-_required_client_version = 6
+_current_server_version = 7
 
 
-def _parse_value(v, value_type=None):
-    return (v, value_type)
+def get_current_server_version():
+    """Returns the current client version.
+
+    Returns:
+        int: current client version
+    """
+    return _current_server_version
+
+
+def _parse_value(v, type_=None):
+    return (v, type_)
 
 
 def _unparse_value(value_and_type):
     if isinstance(value_and_type, (tuple, list)) and len(value_and_type) == 2:
         value, type_ = value_and_type
         if value is None or (isinstance(value, bytes) and (isinstance(type_, str) or type_ is None)):
             # Tuple of value and type ready to go
@@ -265,15 +353,15 @@
             self._out_queue.put(None)
         except Exception as error:  # pylint: disable=broad-except
             self._out_queue.put(error)
             return
         while True:
             input_ = self._in_queue.get()
             if input_ == self._CLOSE:
-                self._db_map.connection.close()
+                self._db_map.close()
                 break
             request, args, kwargs = input_
             handler = {
                 "query": self._do_query,
                 "filtered_query": self._do_filtered_query,
                 "import_data": self._do_import_data,
                 "export_data": self._do_export_data,
@@ -291,15 +379,15 @@
 
     def _do_query(self, *args):
         result = {}
         for sq_name in args:
             sq = getattr(self._db_map, sq_name, None)
             if sq is None:
                 continue
-            result[sq_name] = [x._asdict() for x in self._db_map.query(sq)]
+            result[sq_name] = [dict(x) for x in self._db_map.query(sq)]
         return dict(result=result)
 
     def _do_filtered_query(self, **kwargs):
         result = {}
         for sq_name, filters in kwargs.items():
             sq = getattr(self._db_map, sq_name, None)
             if sq is None:
@@ -319,20 +407,24 @@
                 self._db_map.rollback_session()
         return dict(result=(count, errors))
 
     def _do_export_data(self, **kwargs):
         return dict(result=export_data(self._db_map, parse_value=_parse_value, **kwargs))
 
     def _do_call_method(self, method_name, *args, **kwargs):
-        method = getattr(self._db_map, method_name)
-        result = method(*args, **kwargs)
-        return dict(result=result)
+        try:
+            method = getattr(self._db_map, method_name)
+            result = method(*args, **kwargs)
+            return dict(result=result)
+        except Exception as err:
+            return dict(error=str(err))
 
     def _do_clear_filters(self):
         self._db_map.restore_entity_sq_maker()
+        self._db_map.restore_entity_element_sq_maker()
         self._db_map.restore_entity_class_sq_maker()
         self._db_map.restore_parameter_definition_sq_maker()
         self._db_map.restore_parameter_value_sq_maker()
         self._db_map.restore_alternative_sq_maker()
         self._db_map.restore_scenario_sq_maker()
         self._db_map.restore_scenario_alternative_sq_maker()
         return dict(result=True)
@@ -457,19 +549,19 @@
 
         Returns:
             dict: where result is the return value of the method
         """
         return _db_manager.call_method(self.server_address, method_name, *args, **kwargs)
 
     def apply_filters(self, filters):
+        obsolete = ("tool",)
         configs = [
-            {"scenario": scenario_filter_config, "tool": tool_filter_config, "alternatives": alternative_filter_config}[
-                key
-            ](value)
+            {"scenario": scenario_filter_config, "alternatives": alternative_filter_config}[key](value)
             for key, value in filters.items()
+            if key not in obsolete
         ]
         return _db_manager.apply_filters(self.server_address, configs)
 
     def clear_filters(self):
         return _db_manager.clear_filters(self.server_address)
 
     def db_checkin(self):
@@ -498,16 +590,16 @@
         response = {"get_api_version": spinedb_api_version, "get_db_url": self.get_db_url()}.get(request)
         if response is not None:
             return response
         try:
             args, kwargs, client_version = extras
         except ValueError:
             client_version = 0
-        if client_version < _required_client_version:
-            return dict(error=1, result=_required_client_version)
+        if client_version < _current_server_version:
+            return dict(error=1, result=_current_server_version)
         handler = {
             "query": self.query,
             "filtered_query": self.filtered_query,
             "import_data": self.import_data,
             "export_data": self.export_data,
             "call_method": self.call_method,
             "apply_filters": self.apply_filters,
@@ -539,18 +631,14 @@
         atexit.register(self.close)
 
     def close(self):
         _db_manager.close_db_map(self.server_address)
 
 
 class DBRequestHandler(ReceiveAllMixing, HandleDBMixin, socketserver.BaseRequestHandler):
-    """
-    The request handler class for our server.
-    """
-
     @property
     def server_address(self):
         return self.server.server_address
 
     @property
     def server_manager_queue(self):
         return self.server.manager_queue
@@ -562,43 +650,66 @@
 
 
 def quick_db_checkout(server_manager_queue, ordering):
     _ManagerRequestHandler(server_manager_queue).quick_db_checkout(ordering)
 
 
 def start_spine_db_server(server_manager_queue, db_url, upgrade=False, memory=False, ordering=None):
-    """
-    Args:
-        db_url (str): Spine db url
-        upgrade (bool): Whether to upgrade db or not
-        memory (bool): Whether to use an in-memory database together with a persistent connection to it
-
-    Returns:
-        tuple: server address (e.g. (127.0.0.1, 54321))
-    """
     handler = _ManagerRequestHandler(server_manager_queue)
     server_address = handler.start_server(db_url, upgrade, memory, ordering)
     return server_address
 
 
 def shutdown_spine_db_server(server_manager_queue, server_address):
     handler = _ManagerRequestHandler(server_manager_queue)
     handler.shutdown_server(server_address)
 
 
 @contextmanager
-def closing_spine_db_server(server_manager_queue, db_url, upgrade=False, memory=False, ordering=None):
-    server_address = start_spine_db_server(server_manager_queue, db_url, memory=memory, ordering=ordering)
-    host, port = server_address
+def db_server_manager():
+    """Creates a DB server manager that can be used to control the order in which different servers
+    write to the same DB.
+
+    Yields:
+        :class:`~multiprocessing.queues.Queue`: a queue that can be passed to :func:`.closing_spine_db_server`
+        in order to control write order.
+    """
+    mngr = _DBServerManager()
     try:
-        yield urlunsplit(("http", f"{host}:{port}", "", "", ""))
+        yield mngr.queue
     finally:
-        shutdown_spine_db_server(server_manager_queue, server_address)
+        mngr.shutdown()
 
 
 @contextmanager
-def db_server_manager():
-    mngr = _DBServerManager()
+def closing_spine_db_server(db_url, upgrade=False, memory=False, ordering=None, server_manager_queue=None):
+    """Creates a Spine DB server.
+
+    Args:
+        db_url (str): the URL of a Spine DB.
+        upgrade (bool): Whether to upgrade the DB to the last revision.
+        memory (bool): Whether to use an in-memory database together with a persistent connection.
+        server_manager_queue (Queue, optional): A queue that can be used to control order of writing.
+            Only needed if you also specify `ordering` below.
+        ordering (dict, optional): A dictionary specifying an ordering to be followed by multiple concurrent servers
+            writing to the same DB. It must have the following keys:
+                - "id": an identifier for the ordering, shared by all the servers in the ordering.
+                - "current": an identifier for this server within the ordering.
+                - "precursors": a set of identifiers of other servers that must have checked out from the DB before this one can check in.
+                - "part_count": the number of times this server needs to check out from the DB before their successors can check in.
+
+    Yields:
+        str: server url
+    """
+    if server_manager_queue is None:
+        mngr = _DBServerManager()
+        server_manager_queue = mngr.queue
+    else:
+        mngr = None
+    server_address = start_spine_db_server(server_manager_queue, db_url, memory=memory, ordering=ordering)
+    host, port = server_address
     try:
-        yield mngr.queue
+        yield urlunsplit(("http", f"{host}:{port}", "", "", ""))
     finally:
-        mngr.shutdown()
+        shutdown_spine_db_server(server_manager_queue, server_address)
+        if mngr is not None:
+            mngr.shutdown()
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/__init__.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/exporters/__init__.py` & `spinedb_api-0.31.0/tests/spine_io/importers/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
 """
-Init file for spine_io.exporters package. Intentionally empty.
+Init file for tests.spine_io.importers package. Intentionally empty.
 
 """
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/exporters/csv_writer.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/exporters/csv_writer.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/exporters/excel.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/exporters/excel.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -14,30 +15,28 @@
 
 """
 from spinedb_api.export_mapping.group_functions import GroupOneOrNone
 from spinedb_api.export_mapping.export_mapping import (
     Position,
     AlternativeMapping,
     AlternativeDescriptionMapping,
-    ObjectClassMapping,
-    ObjectGroupMapping,
-    ObjectMapping,
+    EntityClassMapping,
+    EntityGroupMapping,
+    EntityMapping,
     FixedValueMapping,
     ScenarioMapping,
     ScenarioAlternativeMapping,
     ScenarioBeforeAlternativeMapping,
     ScenarioDescriptionMapping,
     ParameterDefinitionMapping,
     ParameterValueIndexMapping,
     ParameterValueTypeMapping,
     ParameterValueMapping,
     ExpandedParameterValueMapping,
-    RelationshipClassMapping,
-    RelationshipMapping,
-    RelationshipObjectMapping,
+    ElementMapping,
 )
 from ...parameter_value import from_database_to_dimension_count
 from .excel_writer import ExcelWriter
 from .writer import write
 
 # FIXME: Use multiple sheets if data doesn't fit
 
@@ -52,33 +51,30 @@
             return True
         return False
 
     @staticmethod
     def _make_preamble(table_name, title_key):
         if table_name in ("alternative", "scenario", "scenario_alternative"):
             return {"sheet_type": table_name}
-        class_name = title_key.get("object_class_name") or title_key.get("relationship_class_name")
+        class_name = title_key["entity_class_name"]
         if table_name.endswith(",group"):
             return {"sheet_type": "object_group", "class_name": class_name}
-        object_class_id_list = title_key.get("object_class_id_list")
-        if object_class_id_list is None:
-            entity_type = "object"
-            entity_dim_count = 1
+        dimension_id_list = title_key.get("dimension_id_list")
+        if dimension_id_list is None:
+            entity_dim_count = 0
         else:
-            entity_type = "relationship"
-            entity_dim_count = len(object_class_id_list.split(","))
+            entity_dim_count = len(dimension_id_list.split(","))
         preamble = {
             "sheet_type": "entity",
-            "entity_type": entity_type,
             "class_name": class_name,
             "entity_dim_count": entity_dim_count,
         }
         td = title_key.get("type_and_dimensions")
         if td is not None:
-            preamble["value_type"] = td[0]
+            preamble["value_type"] = td[0] if td[0] else "single_value"
             preamble["index_dim_count"] = td[1]
         return preamble
 
     def _set_current_sheet(self):
         super()._set_current_sheet()
         if not self._preamble:
             return
@@ -120,19 +116,19 @@
     scenario_mapping = root_mapping.child = ScenarioMapping(0, header="scenario")
     alternative_mapping = scenario_mapping.child = ScenarioAlternativeMapping(1, header="alternative")
     alternative_mapping.child = ScenarioBeforeAlternativeMapping(2, header="before alternative")
     return root_mapping
 
 
 def _make_object_group_mappings(db_map):
-    for obj_grp in db_map.query(db_map.ext_entity_group_sq).group_by(db_map.ext_entity_group_sq.c.class_name):
-        root_mapping = ObjectClassMapping(Position.table_name, filter_re=obj_grp.class_name)
+    for obj_grp in db_map.query(db_map.ext_entity_group_sq).group_by(db_map.ext_entity_group_sq.c.entity_class_name):
+        root_mapping = EntityClassMapping(Position.table_name, filter_re=obj_grp.entity_class_name)
         group_mapping = root_mapping.child = FixedValueMapping(Position.table_name, value="group")
-        object_mapping = group_mapping.child = ObjectMapping(1, header="member")
-        object_mapping.child = ObjectGroupMapping(0, header="group")
+        object_mapping = group_mapping.child = EntityMapping(1, header="member")
+        object_mapping.child = EntityGroupMapping(0, header="group")
         yield root_mapping
 
 
 def _make_scalar_parameter_value_mapping(alt_pos=1):
     alternative_mapping = AlternativeMapping(alt_pos, header="alternative")
     param_def_mapping = alternative_mapping.child = ParameterDefinitionMapping(-1)
     type_mapping = param_def_mapping.child = ParameterValueTypeMapping(Position.table_name, filter_re="single_value")
@@ -150,17 +146,17 @@
         index_mapping.set_ignorable(True)
         parent_mapping = index_mapping
     parent_mapping.child = ExpandedParameterValueMapping(420)
     return alternative_mapping
 
 
 def _make_object_mapping(object_class_name, pivoted=False):
-    root_mapping = ObjectClassMapping(Position.table_name, filter_re=f"^{object_class_name}$")
+    root_mapping = EntityClassMapping(Position.table_name, filter_re=f"^{object_class_name}$")
     pos = 0 if not pivoted else -1
-    root_mapping.child = ObjectMapping(pos, header=object_class_name)
+    root_mapping.child = EntityMapping(pos, header=object_class_name)
     return root_mapping
 
 
 def _make_object_scalar_parameter_value_mapping(object_class_name):
     root_mapping = _make_object_mapping(object_class_name)
     object_mapping = root_mapping.child
     object_mapping.child = _make_scalar_parameter_value_mapping(alt_pos=1)
@@ -179,21 +175,21 @@
     object_mapping = root_mapping.child
     filter_re = f"{dim_count}d_map"
     object_mapping.child = _make_indexed_parameter_value_mapping(alt_pos=-2, filter_re=filter_re, dim_count=dim_count)
     return root_mapping
 
 
 def _make_relationship_mapping(relationship_class_name, object_class_name_list, pivoted=False):
-    root_mapping = RelationshipClassMapping(Position.table_name, filter_re=f"^{relationship_class_name}$")
-    relationship_mapping = root_mapping.child = RelationshipMapping(Position.hidden)
+    root_mapping = EntityClassMapping(Position.table_name, filter_re=f"^{relationship_class_name}$")
+    relationship_mapping = root_mapping.child = EntityMapping(Position.hidden)
     parent_mapping = relationship_mapping
-    for d, class_name in enumerate(object_class_name_list):
+    for d, entity_class_name in enumerate(object_class_name_list):
         if pivoted:
             d = -(d + 1)
-        object_mapping = parent_mapping.child = RelationshipObjectMapping(d, header=class_name)
+        object_mapping = parent_mapping.child = ElementMapping(d, header=entity_class_name)
         parent_mapping = object_mapping
     return root_mapping
 
 
 def _make_relationship_scalar_parameter_value_mapping(relationship_class_name, object_class_name_list):
     root_mapping = _make_relationship_mapping(relationship_class_name, object_class_name_list)
     parent_mapping = root_mapping.flatten()[-1]
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/exporters/excel_writer.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/exporters/excel_writer.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/exporters/gdx_writer.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/exporters/gdx_writer.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/exporters/sql_writer.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/exporters/sql_writer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/exporters/writer.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/exporters/writer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -23,15 +24,15 @@
 
 
 def write(db_map, writer, *mappings, empty_data_header=True, max_tables=None, max_rows=None, group_fns=NoGroup.NAME):
     """
     Writes given mapping.
 
     Args:
-        db_map (DatabaseMappingBase): database map
+        db_map (DatabaseMapping): database map
         writer (Writer): target writer
         mappings (Mapping): root mappings
         empty_data_header (bool or Iterable of bool): True to write at least header rows even if there is no data,
             False to write nothing; a list of booleans applies to each mapping individually
         max_tables (int, optional): maximum number of tables to write
         max_rows (int, optional): maximum number of rows/table to write
         group_fns (str or Iterable of str): group function names for each mappings
@@ -126,15 +127,15 @@
 def _new_table(writer, table_name, title_key):
     """
     Manages table contexts.
 
     Args:
         writer (Writer): a writer
         table_name (str): table's name
-        title_key (dict,optional)
+        title_key (dict, optional)
 
     Yields:
         bool: whether or not the new table was successfully started
     """
     try:
         table_started = writer.start_table(table_name, title_key)
         yield table_started
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/gdx_utils.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/gdx_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/importers/__init__.py` & `spinedb_api-0.31.0/tests/filters/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,11 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
-
-"""
-Intentionally empty.
-
-"""
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/importers/csv_reader.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/importers/csv_reader.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/importers/datapackage_reader.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/importers/datapackage_reader.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -12,16 +13,18 @@
 """
 Contains DataPackageConnector class.
 
 """
 import threading
 from itertools import chain
 
+import tabulator.exceptions
 from datapackage import Package
 from .reader import SourceConnection
+from ...exception import ConnectorError
 
 
 class DataPackageConnector(SourceConnection):
     """Template class to read data from another QThread."""
 
     # name of data source, ex: "Text/CSV"
     DISPLAY_NAME = "Datapackage"
@@ -93,20 +96,25 @@
         """
         Return data read from data source table in table. If max_rows is
         specified only that number of rows.
         """
         if not self._datapackage:
             return iter([]), []
 
+        def iterator(r):
+            try:
+                yield from (item for row, item in enumerate(r.iter(cast=False)) if row != max_rows)
+            except tabulator.exceptions.TabulatorException as error:
+                raise ConnectorError(str(error)) from error
+
         has_header = options.get("has_header", True)
         for resource in self._datapackage.resources:
             with self._resource_name_lock:
                 if resource.name is None:
                     resource.infer()
             if table == resource.name:
-                iterator = (item for row, item in enumerate(resource.iter(cast=False)) if row != max_rows)
                 if has_header:
                     header = resource.schema.field_names
-                    return iterator, header
-                return chain([resource.headers], iterator), None
+                    return iterator(resource), header
+                return chain([resource.headers], iterator(resource)), None
         # table not found
         return iter([]), []
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/importers/excel_reader.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/importers/excel_reader.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -236,27 +237,24 @@
         header.append(label)
         column += 1
     return header
 
 
 def _create_entity_mappings(metadata, header, index_dim_count):
     class_name = metadata["class_name"]
-    entity_type = metadata["entity_type"]
+    entity_dim_count = int(metadata["entity_dim_count"])
     map_dict = {"name": class_name}
     ent_alt_map_type = "row" if index_dim_count else "column"
-    if entity_type == "object":
+    if entity_dim_count == 0:
         map_dict["map_type"] = "ObjectClass"
         map_dict["objects"] = {"map_type": ent_alt_map_type, "reference": 0}
-    elif entity_type == "relationship":
-        entity_dim_count = int(metadata["entity_dim_count"])
+    else:
         map_dict["map_type"] = "RelationshipClass"
         map_dict["object_classes"] = header[:entity_dim_count]
         map_dict["objects"] = [{"map_type": ent_alt_map_type, "reference": i} for i in range(entity_dim_count)]
-    else:
-        return None, None
     value_type = metadata.get("value_type")
     if value_type is not None:
         value = {"value_type": value_type}
         if index_dim_count:
             value["extra_dimensions"] = list(range(index_dim_count))
         p_ref = len(header) if index_dim_count else -1
         map_dict["parameters"] = {
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/importers/gdx_connector.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/importers/gdx_connector.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/importers/json_reader.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/importers/json_reader.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/spinedb_api/spine_io/importers/reader.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/importers/reader.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -11,14 +12,16 @@
 
 """
 Contains a class template for a data source connector used in import ui.
 
 """
 
 from itertools import islice
+
+from spinedb_api.exception import ConnectorError, InvalidMappingComponent
 from spinedb_api.import_mapping.generator import get_mapped_data, identity
 from spinedb_api.import_mapping.import_mapping_compat import parse_named_mapping_spec
 from spinedb_api import DateTime, Duration, ParameterValueFormatError
 
 TYPE_STRING_TO_CLASS = {"string": str, "datetime": DateTime, "duration": Duration, "float": float, "boolean": bool}
 
 TYPE_CLASS_TO_STRING = {type_class: string for string, type_class in TYPE_STRING_TO_CLASS.items()}
@@ -146,14 +149,14 @@
                     header,
                     table,
                     column_convert_fns,
                     default_column_convert_fn,
                     row_convert_fns,
                     unparse_value,
                 )
-            except ParameterValueFormatError as error:
+            except (ConnectorError, ParameterValueFormatError, InvalidMappingComponent) as error:
                 errors.append(str(error))
                 continue
             for key, value in data.items():
                 mapped_data.setdefault(key, []).extend(value)
             errors.extend([(table, err) for err in t_errors])
         return mapped_data, errors
```

### Comparing `spinedb_api-0.30.5/spinedb_api.egg-info/PKG-INFO` & `spinedb_api-0.31.0/spinedb_api.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 Metadata-Version: 2.1
 Name: spinedb_api
-Version: 0.30.5
+Version: 0.31.0
 Summary: An API to talk to Spine databases.
 Author-email: Spine Project consortium <spine_info@vtt.fi>
 License: LGPL-3.0-or-later
 Project-URL: Repository, https://github.com/spine-tools/Spine-Database-API
 Keywords: energy system modelling,workflow,optimisation,database
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)
 Classifier: Operating System :: OS Independent
-Requires-Python: <3.12,>=3.8.1
+Requires-Python: >=3.8.1
 Description-Content-Type: text/markdown
 License-File: COPYING
 License-File: COPYING.LESSER
 Requires-Dist: sqlalchemy<1.4,>=1.3
 Requires-Dist: alembic>=1.7
 Requires-Dist: faker>=8.1.2
 Requires-Dist: datapackage>=1.15.2
@@ -24,14 +24,15 @@
 Requires-Dist: gdx2py>=2.1.1
 Requires-Dist: ijson>=3.1.4
 Requires-Dist: chardet>=4.0.0
 Requires-Dist: pymysql>=1.0.2
 Requires-Dist: psycopg2
 Provides-Extra: dev
 Requires-Dist: coverage[toml]; extra == "dev"
+Requires-Dist: pyperf; extra == "dev"
 
 # Spine Database API
 
 [![Documentation Status](https://readthedocs.org/projects/spine-database-api/badge/?version=latest)](https://spine-database-api.readthedocs.io/en/latest/?badge=latest)
 [![Unit tests](https://github.com/spine-tools/Spine-Database-API/workflows/Unit%20tests/badge.svg)](https://github.com/spine-tools/Spine-Database-API/actions?query=workflow%3A"Unit+tests")
 [![codecov](https://codecov.io/gh/spine-tools/Spine-Database-API/branch/master/graph/badge.svg)](https://codecov.io/gh/spine-tools/Spine-Database-API)
 [![PyPI version](https://badge.fury.io/py/spinedb-api.svg)](https://badge.fury.io/py/spinedb-api)
```

#### html2text {}

```diff
@@ -1,27 +1,27 @@
-Metadata-Version: 2.1 Name: spinedb_api Version: 0.30.5 Summary: An API to talk
+Metadata-Version: 2.1 Name: spinedb_api Version: 0.31.0 Summary: An API to talk
 to Spine databases. Author-email: Spine Project consortium
 vtt.fi> License: LGPL-3.0-or-later Project-URL: Repository, https://github.com/
 spine-tools/Spine-Database-API Keywords: energy system
 modelling,workflow,optimisation,database Classifier: Programming Language ::
 Python :: 3 Classifier: License :: OSI Approved :: GNU Lesser General Public
 License v3 (LGPLv3) Classifier: Operating System :: OS Independent Requires-
-Python: <3.12,>=3.8.1 Description-Content-Type: text/markdown License-File:
-COPYING License-File: COPYING.LESSER Requires-Dist: sqlalchemy<1.4,>=1.3
-Requires-Dist: alembic>=1.7 Requires-Dist: faker>=8.1.2 Requires-Dist:
-datapackage>=1.15.2 Requires-Dist: python-dateutil>=2.8.1 Requires-Dist:
-numpy>=1.20.2 Requires-Dist: scipy>=1.7.1 Requires-Dist:
-openpyxl!=3.1.1,>=3.0.7 Requires-Dist: gdx2py>=2.1.1 Requires-Dist:
-ijson>=3.1.4 Requires-Dist: chardet>=4.0.0 Requires-Dist: pymysql>=1.0.2
-Requires-Dist: psycopg2 Provides-Extra: dev Requires-Dist: coverage[toml];
-extra == "dev" # Spine Database API [![Documentation Status](https://
-readthedocs.org/projects/spine-database-api/badge/?version=latest)](https://
-spine-database-api.readthedocs.io/en/latest/?badge=latest) [![Unit tests]
-(https://github.com/spine-tools/Spine-Database-API/workflows/Unit%20tests/
-badge.svg)](https://github.com/spine-tools/Spine-Database-API/
+Python: >=3.8.1 Description-Content-Type: text/markdown License-File: COPYING
+License-File: COPYING.LESSER Requires-Dist: sqlalchemy<1.4,>=1.3 Requires-Dist:
+alembic>=1.7 Requires-Dist: faker>=8.1.2 Requires-Dist: datapackage>=1.15.2
+Requires-Dist: python-dateutil>=2.8.1 Requires-Dist: numpy>=1.20.2 Requires-
+Dist: scipy>=1.7.1 Requires-Dist: openpyxl!=3.1.1,>=3.0.7 Requires-Dist:
+gdx2py>=2.1.1 Requires-Dist: ijson>=3.1.4 Requires-Dist: chardet>=4.0.0
+Requires-Dist: pymysql>=1.0.2 Requires-Dist: psycopg2 Provides-Extra: dev
+Requires-Dist: coverage[toml]; extra == "dev" Requires-Dist: pyperf; extra ==
+"dev" # Spine Database API [![Documentation Status](https://readthedocs.org/
+projects/spine-database-api/badge/?version=latest)](https://spine-database-
+api.readthedocs.io/en/latest/?badge=latest) [![Unit tests](https://github.com/
+spine-tools/Spine-Database-API/workflows/Unit%20tests/badge.svg)](https://
+github.com/spine-tools/Spine-Database-API/
 actions?query=workflow%3A"Unit+tests") [![codecov](https://codecov.io/gh/spine-
 tools/Spine-Database-API/branch/master/graph/badge.svg)](https://codecov.io/gh/
 spine-tools/Spine-Database-API) [![PyPI version](https://badge.fury.io/py/
 spinedb-api.svg)](https://badge.fury.io/py/spinedb-api) A Python package to
 access and manipulate Spine databases in a customary, unified way. ## License
 Spine Database API is released under the GNU Lesser General Public License
 (LGPL) license. All accompanying documentation and manual are released under
```

### Comparing `spinedb_api-0.30.5/spinedb_api.egg-info/SOURCES.txt` & `spinedb_api-0.31.0/spinedb_api.egg-info/SOURCES.txt`

 * *Files 6% similar despite different names*

```diff
@@ -1,62 +1,64 @@
 .gitattributes
 .gitignore
 .readthedocs.yml
+CHANGELOG.md
 COPYING
 COPYING.LESSER
 MANIFEST.in
 README.md
 deploy-key.enc
 pylintrc
 pyproject.toml
 requirements.txt
 .github/pull_request_template.md
 .github/workflows/run_unit_tests.yml
+benchmarks/README.md
+benchmarks/__init__.py
+benchmarks/datetime_from_database.py
+benchmarks/map_from_database.py
+benchmarks/mapped_item_getitem.py
+benchmarks/update_default_value_to_different_value.py
+benchmarks/update_default_value_to_same_value.py
+benchmarks/utils.py
 bin/build_doc.bat
 bin/build_doc.py
-bin/update_copyrights.py
 docs/Makefile
 docs/make.bat
 docs/requirements.txt
 docs/source/conf.py
 docs/source/front_matter.rst
 docs/source/index.rst
-docs/source/metadata_description.rst
+docs/source/metadata.rst
 docs/source/parameter_value_format.rst
-docs/source/results_metadata_description.rst
 docs/source/tutorial.rst
 docs/source/img/spinetoolbox_on_wht.svg
 fig/eu-emblem-low-res.jpg
 spinedb_api/__init__.py
 spinedb_api/alembic.ini
-spinedb_api/check_functions.py
-spinedb_api/db_cache.py
+spinedb_api/compatibility.py
 spinedb_api/db_mapping.py
-spinedb_api/db_mapping_add_mixin.py
 spinedb_api/db_mapping_base.py
-spinedb_api/db_mapping_check_mixin.py
 spinedb_api/db_mapping_commit_mixin.py
 spinedb_api/db_mapping_query_mixin.py
-spinedb_api/db_mapping_remove_mixin.py
-spinedb_api/db_mapping_update_mixin.py
-spinedb_api/diff_db_mapping.py
-spinedb_api/diff_db_mapping_base.py
-spinedb_api/diff_db_mapping_commit_mixin.py
 spinedb_api/exception.py
 spinedb_api/export_functions.py
 spinedb_api/graph_layout_generator.py
 spinedb_api/helpers.py
 spinedb_api/import_functions.py
+spinedb_api/mapped_items.py
 spinedb_api/mapping.py
 spinedb_api/parameter_value.py
 spinedb_api/perfect_split.py
 spinedb_api/purge.py
+spinedb_api/query.py
 spinedb_api/server_client_helpers.py
 spinedb_api/spine_db_client.py
 spinedb_api/spine_db_server.py
+spinedb_api/temp_id.py
 spinedb_api/version.py
 spinedb_api.egg-info/PKG-INFO
 spinedb_api.egg-info/SOURCES.txt
 spinedb_api.egg-info/dependency_links.txt
 spinedb_api.egg-info/not-zip-safe
 spinedb_api.egg-info/requires.txt
 spinedb_api.egg-info/top_level.txt
@@ -65,36 +67,39 @@
 spinedb_api/alembic/script.py.mako
 spinedb_api/alembic/versions/070a0eb89e88_drop_category_tables.py
 spinedb_api/alembic/versions/0c7d199ae915_add_list_value_table.py
 spinedb_api/alembic/versions/1892adebc00f_create_metadata_tables.py
 spinedb_api/alembic/versions/1e4997105288_separate_type_from_value.py
 spinedb_api/alembic/versions/39e860a11b05_add_alternatives_and_scenarios.py
 spinedb_api/alembic/versions/51fd7b69acf7_add_parameter_tag_and_parameter_value_list.py
+spinedb_api/alembic/versions/5385f063bef2_create_superclass_subclass_table.py
+spinedb_api/alembic/versions/6b7c994c1c61_drop_object_and_relationship_tables.py
 spinedb_api/alembic/versions/738d494a08ac_fix_foreign_key_constraints_in_object_.py
 spinedb_api/alembic/versions/7d0b467f2f4e_fix_foreign_key_constraints_in_entity_.py
+spinedb_api/alembic/versions/8b0eff478bcb_add_active_by_default_to_entity_class.py
 spinedb_api/alembic/versions/8c19c53d5701_rename_parameter_to_parameter_definition.py
 spinedb_api/alembic/versions/989fccf80441_replace_values_with_reference_to_list_.py
 spinedb_api/alembic/versions/9da58d2def22_create_entity_group_table.py
 spinedb_api/alembic/versions/bba1e2ef5153_move_to_entity_based_design.py
 spinedb_api/alembic/versions/bf255c179bce_get_rid_of_unused_fields_in_parameter_.py
+spinedb_api/alembic/versions/ce9faa82ed59_create_entity_alternative_table.py
 spinedb_api/alembic/versions/defbda3bf2b5_add_tool_feature_tables.py
 spinedb_api/alembic/versions/fbb540efbf15_add_support_for_mysql.py
 spinedb_api/alembic/versions/fd542cebf699_drop_on_update_clauses_from_object_and_.py
 spinedb_api/export_mapping/__init__.py
 spinedb_api/export_mapping/export_mapping.py
 spinedb_api/export_mapping/generator.py
 spinedb_api/export_mapping/group_functions.py
 spinedb_api/export_mapping/pivot.py
 spinedb_api/export_mapping/settings.py
 spinedb_api/filters/__init__.py
 spinedb_api/filters/alternative_filter.py
 spinedb_api/filters/execution_filter.py
 spinedb_api/filters/renamer.py
 spinedb_api/filters/scenario_filter.py
-spinedb_api/filters/tool_filter.py
 spinedb_api/filters/tools.py
 spinedb_api/filters/value_transformer.py
 spinedb_api/import_mapping/__init__.py
 spinedb_api/import_mapping/generator.py
 spinedb_api/import_mapping/import_mapping.py
 spinedb_api/import_mapping/import_mapping_compat.py
 spinedb_api/import_mapping/type_conversion.py
@@ -112,29 +117,33 @@
 spinedb_api/spine_io/importers/datapackage_reader.py
 spinedb_api/spine_io/importers/excel_reader.py
 spinedb_api/spine_io/importers/gdx_connector.py
 spinedb_api/spine_io/importers/json_reader.py
 spinedb_api/spine_io/importers/reader.py
 spinedb_api/spine_io/importers/sqlalchemy_connector.py
 tests/__init__.py
+tests/custom_db_mapping.py
 tests/test_DatabaseMapping.py
-tests/test_DiffDatabaseMapping.py
-tests/test_check_functions.py
+tests/test_check_integrity.py
+tests/test_db_mapping_base.py
+tests/test_db_server.py
 tests/test_export_functions.py
 tests/test_helpers.py
 tests/test_import_functions.py
 tests/test_mapping.py
 tests/test_migration.py
 tests/test_parameter_value.py
+tests/test_purge.py
 tests/export_mapping/__init__.py
 tests/export_mapping/test_export_mapping.py
 tests/export_mapping/test_pivot.py
 tests/export_mapping/test_settings.py
 tests/filters/__init__.py
 tests/filters/test_alternative_filter.py
+tests/filters/test_execution_filter.py
 tests/filters/test_renamer.py
 tests/filters/test_scenario_filter.py
 tests/filters/test_tool_filter.py
 tests/filters/test_tools.py
 tests/filters/test_value_transformer.py
 tests/import_mapping/__init__.py
 tests/import_mapping/test_generator.py
@@ -150,8 +159,9 @@
 tests/spine_io/exporters/test_writer.py
 tests/spine_io/importers/__init__.py
 tests/spine_io/importers/test_CSVConnector.py
 tests/spine_io/importers/test_GdxConnector.py
 tests/spine_io/importers/test_datapackage_reader.py
 tests/spine_io/importers/test_excel_reader.py
 tests/spine_io/importers/test_json_reader.py
+tests/spine_io/importers/test_reader.py
 tests/spine_io/importers/test_sqlalchemy_connector.py
```

### Comparing `spinedb_api-0.30.5/tests/__init__.py` & `spinedb_api-0.31.0/tests/spine_io/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
 """
-Unit tests package for :mod:`spinedb_api`.
+Init file for tests.spine_io package. Intentionally empty.
 
 """
```

### Comparing `spinedb_api-0.30.5/tests/export_mapping/__init__.py` & `spinedb_api-0.31.0/tests/import_mapping/__init__.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/tests/export_mapping/test_export_mapping.py` & `spinedb_api-0.31.0/tests/export_mapping/test_export_mapping.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -12,316 +13,303 @@
 Unit tests for export mappings.
 
 """
 
 import unittest
 from spinedb_api import (
     DatabaseMapping,
-    DiffDatabaseMapping,
     import_alternatives,
-    import_features,
     import_object_classes,
     import_object_parameter_values,
     import_object_parameters,
     import_objects,
     import_parameter_value_lists,
     import_relationship_classes,
     import_relationships,
     import_scenario_alternatives,
     import_scenarios,
-    import_tool_features,
-    import_tool_feature_methods,
-    import_tools,
     Map,
 )
 from spinedb_api.import_functions import import_object_groups
 from spinedb_api.mapping import Position, to_dict
 from spinedb_api.export_mapping import (
     rows,
     titles,
-    object_parameter_default_value_export,
-    object_parameter_export,
-    relationship_export,
-    relationship_parameter_export,
+    entity_parameter_default_value_export,
+    entity_parameter_value_export,
+    entity_export,
 )
 from spinedb_api.export_mapping.export_mapping import (
+    AlternativeDescriptionMapping,
     AlternativeMapping,
     drop_non_positioned_tail,
     FixedValueMapping,
     ExpandedParameterValueMapping,
     ExpandedParameterDefaultValueMapping,
-    FeatureEntityClassMapping,
-    FeatureParameterDefinitionMapping,
     from_dict,
-    ObjectGroupMapping,
-    ObjectGroupObjectMapping,
-    ObjectMapping,
-    ObjectClassMapping,
+    EntityGroupMapping,
+    EntityGroupEntityMapping,
+    EntityMapping,
+    EntityClassMapping,
     ParameterDefaultValueMapping,
     ParameterDefaultValueIndexMapping,
     ParameterDefinitionMapping,
     ParameterValueIndexMapping,
     ParameterValueListMapping,
     ParameterValueListValueMapping,
     ParameterValueMapping,
     ParameterValueTypeMapping,
-    RelationshipClassMapping,
-    RelationshipClassObjectClassMapping,
-    RelationshipMapping,
-    RelationshipObjectMapping,
+    DimensionMapping,
+    ElementMapping,
     ScenarioActiveFlagMapping,
     ScenarioAlternativeMapping,
+    ScenarioDescriptionMapping,
     ScenarioMapping,
-    ToolMapping,
-    ToolFeatureEntityClassMapping,
-    ToolFeatureParameterDefinitionMapping,
-    ToolFeatureRequiredFlagMapping,
-    ToolFeatureMethodEntityClassMapping,
-    ToolFeatureMethodMethodMapping,
-    ToolFeatureMethodParameterDefinitionMapping,
 )
 from spinedb_api.mapping import unflatten
 
 
 class TestExportMapping(unittest.TestCase):
     def test_export_empty_table(self):
         db_map = DatabaseMapping("sqlite://", create=True)
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         self.assertEqual(list(rows(object_class_mapping, db_map)), [])
-        db_map.connection.close()
+        db_map.close()
 
     def test_export_single_object_class(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("object_class",))
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         self.assertEqual(list(rows(object_class_mapping, db_map)), [["object_class"]])
-        db_map.connection.close()
+        db_map.close()
 
     def test_export_objects(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         import_objects(
             db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31"), ("oc3", "o32"), ("oc3", "o33"))
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        object_class_mapping.child = ObjectMapping(1)
+        object_class_mapping = EntityClassMapping(0)
+        object_class_mapping.child = EntityMapping(1)
         self.assertEqual(
             list(rows(object_class_mapping, db_map)),
             [["oc1", "o11"], ["oc1", "o12"], ["oc2", "o21"], ["oc3", "o31"], ["oc3", "o32"], ["oc3", "o33"]],
         )
-        db_map.connection.close()
+        db_map.close()
 
     def test_hidden_tail(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1",))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
-        object_class_mapping.child = ObjectMapping(Position.hidden)
+        object_class_mapping = EntityClassMapping(0)
+        object_class_mapping.child = EntityMapping(Position.hidden)
         self.assertEqual(list(rows(object_class_mapping, db_map)), [["oc1"], ["oc1"]])
-        db_map.connection.close()
+        db_map.close()
 
     def test_pivot_without_values(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1",))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(-1)
-        object_class_mapping.child = ObjectMapping(Position.hidden)
+        object_class_mapping = EntityClassMapping(-1)
+        object_class_mapping.child = EntityMapping(Position.hidden)
         self.assertEqual(list(rows(object_class_mapping, db_map)), [])
-        db_map.connection.close()
+        db_map.close()
 
     def test_hidden_tail_pivoted(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p1"), ("oc", "p2")))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(db_map, (("oc", "o1", "p1", -11.0), ("oc", "o1", "p2", -12.0)))
         db_map.commit_session("Add test data.")
         root_mapping = unflatten(
             [
-                ObjectClassMapping(0),
+                EntityClassMapping(0),
                 ParameterDefinitionMapping(-1),
-                ObjectMapping(1),
+                EntityMapping(1),
                 AlternativeMapping(2),
                 ParameterValueMapping(Position.hidden),
             ]
         )
         expected = [[None, None, "p1", "p2"], ["oc", "o1", "Base", "Base"]]
         self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_hidden_leaf_item_in_regular_table_valid(self):
-        object_class_mapping = ObjectClassMapping(0)
-        object_class_mapping.child = ObjectMapping(Position.hidden)
+        object_class_mapping = EntityClassMapping(0)
+        object_class_mapping.child = EntityMapping(Position.hidden)
         self.assertEqual(object_class_mapping.check_validity(), [])
 
     def test_hidden_leaf_item_in_pivot_table_not_valid(self):
-        object_class_mapping = ObjectClassMapping(-1)
-        object_class_mapping.child = ObjectMapping(Position.hidden)
+        object_class_mapping = EntityClassMapping(-1)
+        object_class_mapping.child = EntityMapping(Position.hidden)
         self.assertEqual(object_class_mapping.check_validity(), ["Cannot be pivoted."])
 
     def test_object_groups(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2"), ("oc", "o3"), ("oc", "g1"), ("oc", "g2")))
         import_object_groups(db_map, (("oc", "g1", "o1"), ("oc", "g1", "o2"), ("oc", "g2", "o3")))
         db_map.commit_session("Add test data.")
-        flattened = [ObjectClassMapping(0), ObjectGroupMapping(1)]
+        flattened = [EntityClassMapping(0), EntityGroupMapping(1)]
         mapping = unflatten(flattened)
         self.assertEqual(list(rows(mapping, db_map)), [["oc", "g1"], ["oc", "g2"]])
-        db_map.connection.close()
+        db_map.close()
 
     def test_object_groups_with_objects(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2"), ("oc", "o3"), ("oc", "g1"), ("oc", "g2")))
         import_object_groups(db_map, (("oc", "g1", "o1"), ("oc", "g1", "o2"), ("oc", "g2", "o3")))
         db_map.commit_session("Add test data.")
-        flattened = [ObjectClassMapping(0), ObjectGroupMapping(1), ObjectGroupObjectMapping(2)]
+        flattened = [EntityClassMapping(0), EntityGroupMapping(1), EntityGroupEntityMapping(2)]
         mapping = unflatten(flattened)
         self.assertEqual(list(rows(mapping, db_map)), [["oc", "g1", "o1"], ["oc", "g1", "o2"], ["oc", "g2", "o3"]])
-        db_map.connection.close()
+        db_map.close()
 
     def test_object_groups_with_parameter_values(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2"), ("oc", "o3"), ("oc", "g1"), ("oc", "g2")))
         import_object_groups(db_map, (("oc", "g1", "o1"), ("oc", "g1", "o2"), ("oc", "g2", "o3")))
         import_object_parameters(db_map, (("oc", "p"),))
         import_object_parameter_values(
             db_map, (("oc", "o1", "p", -11.0), ("oc", "o2", "p", -12.0), ("oc", "o3", "p", -13.0))
         )
         db_map.commit_session("Add test data.")
         flattened = [
-            ObjectClassMapping(0),
-            ObjectGroupMapping(1),
-            ObjectGroupObjectMapping(2),
+            EntityClassMapping(0),
+            EntityGroupMapping(1),
+            EntityGroupEntityMapping(2),
             ParameterDefinitionMapping(Position.hidden),
             AlternativeMapping(Position.hidden),
             ParameterValueMapping(3),
         ]
         mapping = unflatten(flattened)
         self.assertEqual(
             list(rows(mapping, db_map)),
             [["oc", "g1", "o1", -11.0], ["oc", "g1", "o2", -12.0], ["oc", "g2", "o3", -13.0]],
         )
-        db_map.connection.close()
+        db_map.close()
 
     def test_export_parameter_definitions(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21")))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         parameter_definition_mapping = ParameterDefinitionMapping(1)
-        parameter_definition_mapping.child = ObjectMapping(2)
+        parameter_definition_mapping.child = EntityMapping(2)
         object_class_mapping.child = parameter_definition_mapping
         expected = [
             ["oc1", "p11", "o11"],
             ["oc1", "p11", "o12"],
             ["oc1", "p12", "o11"],
             ["oc1", "p12", "o12"],
             ["oc2", "p21", "o21"],
         ]
         self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_export_single_parameter_value_when_there_are_multiple_objects(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21")))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
         import_object_parameter_values(db_map, (("oc1", "o11", "p12", -11.0),))
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         parameter_definition_mapping = ParameterDefinitionMapping(1)
         alternative_mapping = AlternativeMapping(Position.hidden)
         parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(2)
+        object_mapping = EntityMapping(2)
         alternative_mapping.child = object_mapping
         value_mapping = ParameterValueMapping(3)
         object_mapping.child = value_mapping
         object_class_mapping.child = parameter_definition_mapping
         self.assertEqual(list(rows(object_class_mapping, db_map)), [["oc1", "p12", "o11", -11.0]])
-        db_map.connection.close()
+        db_map.close()
 
     def test_export_single_parameter_value_pivoted_by_object_name(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1",))
         import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12")))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
         import_object_parameter_values(
             db_map,
             (
                 ("oc1", "o11", "p11", -11.0),
                 ("oc1", "o11", "p12", -12.0),
                 ("oc1", "o12", "p11", -21.0),
                 ("oc1", "o12", "p12", -22.0),
             ),
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         parameter_definition_mapping = ParameterDefinitionMapping(1)
         alternative_mapping = AlternativeMapping(Position.hidden)
         parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(-1)
+        object_mapping = EntityMapping(-1)
         alternative_mapping.child = object_mapping
         value_mapping = ParameterValueMapping(-2)
         object_mapping.child = value_mapping
         object_class_mapping.child = parameter_definition_mapping
         expected = [[None, None, "o11", "o12"], ["oc1", "p11", -11.0, -21.0], ["oc1", "p12", -12.0, -22.0]]
         self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_minimum_pivot_index_need_not_be_minus_one(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_alternatives(db_map, ("alt",))
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o"),))
         import_object_parameter_values(
             db_map,
             (
                 ("oc", "o", "p", Map(["A", "B"], [-1.1, -2.2]), "Base"),
                 ("oc", "o", "p", Map(["A", "B"], [-5.5, -6.6]), "alt"),
             ),
         )
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(1, 2, Position.hidden, 0, -2, Position.hidden, 4, [Position.hidden], [3])
+        mapping = entity_parameter_value_export(
+            1, 2, Position.hidden, 0, None, None, -2, Position.hidden, 4, [Position.hidden], [3]
+        )
         expected = [
             [None, None, None, None, "Base", "alt"],
             ["o", "oc", "p", "A", -1.1, -5.5],
             ["o", "oc", "p", "B", -2.2, -6.6],
         ]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_pivot_row_order(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1",))
         import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12")))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12")))
         import_object_parameter_values(
             db_map,
             (
                 ("oc1", "o11", "p11", -11.0),
                 ("oc1", "o11", "p12", -12.0),
                 ("oc1", "o12", "p11", -21.0),
                 ("oc1", "o12", "p12", -22.0),
             ),
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         parameter_definition_mapping = ParameterDefinitionMapping(-1)
         alternative_mapping = AlternativeMapping(Position.hidden)
-        object_mapping = ObjectMapping(-2)
+        object_mapping = EntityMapping(-2)
         value_mapping = ParameterValueMapping(3)
         mappings = [
             object_class_mapping,
             parameter_definition_mapping,
             alternative_mapping,
             object_mapping,
             value_mapping,
@@ -337,71 +325,71 @@
         object_mapping.position = -1
         expected = [
             [None, "o11", "o11", "o12", "o12"],
             [None, "p11", "p12", "p11", "p12"],
             ["oc1", -11.0, -12.0, -21.0, -22.0],
         ]
         self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_export_parameter_indexes(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p1"), ("oc", "p2")))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map,
             (
                 ("oc", "o1", "p1", Map(["a", "b"], [5.0, 5.0])),
                 ("oc", "o1", "p2", Map(["c", "d"], [5.0, 5.0])),
                 ("oc", "o2", "p1", Map(["e", "f"], [5.0, 5.0])),
                 ("oc", "o2", "p2", Map(["g", "h"], [5.0, 5.0])),
             ),
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         parameter_definition_mapping = ParameterDefinitionMapping(2)
         alternative_mapping = AlternativeMapping(Position.hidden)
         parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(1)
+        object_mapping = EntityMapping(1)
         alternative_mapping.child = object_mapping
         index_mapping = ParameterValueIndexMapping(3)
         object_mapping.child = index_mapping
         object_class_mapping.child = parameter_definition_mapping
         expected = [
             ["oc", "o1", "p1", "a"],
             ["oc", "o1", "p1", "b"],
-            ["oc", "o1", "p2", "c"],
-            ["oc", "o1", "p2", "d"],
             ["oc", "o2", "p1", "e"],
             ["oc", "o2", "p1", "f"],
+            ["oc", "o1", "p2", "c"],
+            ["oc", "o1", "p2", "d"],
             ["oc", "o2", "p2", "g"],
             ["oc", "o2", "p2", "h"],
         ]
         self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_export_nested_parameter_indexes(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map,
             (
                 ("oc", "o1", "p", Map(["A", "B"], [23.0, Map(["a", "b"], [-1.1, -2.2])])),
                 ("oc", "o2", "p", Map(["C", "D"], [Map(["c", "d"], [-3.3, -4.4]), 2.3])),
             ),
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         parameter_definition_mapping = ParameterDefinitionMapping(2)
         alternative_mapping = AlternativeMapping(Position.hidden)
         parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(1)
+        object_mapping = EntityMapping(1)
         alternative_mapping.child = object_mapping
         index_mapping_1 = ParameterValueIndexMapping(3)
         index_mapping_2 = ParameterValueIndexMapping(4)
         index_mapping_1.child = index_mapping_2
         object_mapping.child = index_mapping_1
         object_class_mapping.child = parameter_definition_mapping
         expected = [
@@ -409,64 +397,64 @@
             ["oc", "o1", "p", "B", "a"],
             ["oc", "o1", "p", "B", "b"],
             ["oc", "o2", "p", "C", "c"],
             ["oc", "o2", "p", "C", "d"],
             ["oc", "o2", "p", "D", None],
         ]
         self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_export_nested_map_values_only(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map,
             (
                 ("oc", "o1", "p", Map(["A", "B"], [23.0, Map(["a", "b"], [-1.1, -2.2])])),
                 ("oc", "o2", "p", Map(["C", "D"], [Map(["c", "d"], [-3.3, -4.4]), 2.3])),
             ),
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(Position.hidden)
+        object_class_mapping = EntityClassMapping(Position.hidden)
         parameter_definition_mapping = ParameterDefinitionMapping(Position.hidden)
-        object_mapping = ObjectMapping(Position.hidden)
+        object_mapping = EntityMapping(Position.hidden)
         parameter_definition_mapping.child = object_mapping
         alternative_mapping = AlternativeMapping(Position.hidden)
         object_mapping.child = alternative_mapping
         index_mapping_1 = ParameterValueIndexMapping(Position.hidden)
         index_mapping_2 = ParameterValueIndexMapping(Position.hidden)
         value_mapping = ExpandedParameterValueMapping(0)
         index_mapping_2.child = value_mapping
         index_mapping_1.child = index_mapping_2
         alternative_mapping.child = index_mapping_1
         object_class_mapping.child = parameter_definition_mapping
         expected = [[23.0], [-1.1], [-2.2], [-3.3], [-4.4], [2.3]]
         self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_full_pivot_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map,
             (
                 ("oc", "o1", "p", Map(["A", "B"], [Map(["a", "b"], [-1.1, -2.2]), Map(["a", "b"], [-3.3, -4.4])])),
                 ("oc", "o2", "p", Map(["A", "B"], [Map(["a", "b"], [-5.5, -6.6]), Map(["a", "b"], [-7.7, -8.8])])),
             ),
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         parameter_definition_mapping = ParameterDefinitionMapping(1)
         alternative_mapping = AlternativeMapping(Position.hidden)
         parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(-1)
+        object_mapping = EntityMapping(-1)
         alternative_mapping.child = object_mapping
         index_mapping_1 = ParameterValueIndexMapping(2)
         index_mapping_2 = ParameterValueIndexMapping(3)
         value_mapping = ExpandedParameterValueMapping(-2)
         index_mapping_2.child = value_mapping
         index_mapping_1.child = index_mapping_2
         object_mapping.child = index_mapping_1
@@ -475,81 +463,87 @@
             [None, None, None, None, "o1", "o2"],
             ["oc", "p", "A", "a", -1.1, -5.5],
             ["oc", "p", "A", "b", -2.2, -6.6],
             ["oc", "p", "B", "a", -3.3, -7.7],
             ["oc", "p", "B", "b", -4.4, -8.8],
         ]
         self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_full_pivot_table_with_hidden_columns(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map, (("oc", "o1", "p", Map(["A", "B"], [-1.1, -2.2])), ("oc", "o2", "p", Map(["A", "B"], [-5.5, -6.6])))
         )
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 2, Position.hidden, -1, 3, Position.hidden, 5, [Position.hidden], [4])
+        mapping = entity_parameter_value_export(
+            0, 2, Position.hidden, -1, None, None, 3, Position.hidden, 5, [Position.hidden], [4]
+        )
         expected = [
             [None, None, None, None, None, "o1", "o2"],
             ["oc", None, "p", "Base", "A", -1.1, -5.5],
             ["oc", None, "p", "Base", "B", -2.2, -6.6],
         ]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_objects_as_pivot_header_for_indexed_values_with_alternatives(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_alternatives(db_map, ("alt",))
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map,
             (
                 ("oc", "o1", "p", Map(["A", "B"], [-1.1, -2.2]), "Base"),
                 ("oc", "o1", "p", Map(["A", "B"], [-3.3, -4.4]), "alt"),
                 ("oc", "o2", "p", Map(["A", "B"], [-5.5, -6.6]), "Base"),
                 ("oc", "o2", "p", Map(["A", "B"], [-7.7, -8.8]), "alt"),
             ),
         )
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 2, Position.hidden, -1, 3, Position.hidden, 5, [Position.hidden], [4])
+        mapping = entity_parameter_value_export(
+            0, 2, Position.hidden, -1, None, None, 3, Position.hidden, 5, [Position.hidden], [4]
+        )
         expected = [
             [None, None, None, None, None, "o1", "o2"],
             ["oc", None, "p", "Base", "A", -1.1, -5.5],
             ["oc", None, "p", "Base", "B", -2.2, -6.6],
             ["oc", None, "p", "alt", "A", -3.3, -7.7],
             ["oc", None, "p", "alt", "B", -4.4, -8.8],
         ]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_objects_and_indexes_as_pivot_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map, (("oc", "o1", "p", Map(["A", "B"], [-1.1, -2.2])), ("oc", "o2", "p", Map(["A", "B"], [-3.3, -4.4])))
         )
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 2, Position.hidden, -1, 3, Position.hidden, 4, [Position.hidden], [-2])
+        mapping = entity_parameter_value_export(
+            0, 2, Position.hidden, -1, None, None, 3, Position.hidden, 4, [Position.hidden], [-2]
+        )
         expected = [
             [None, None, None, None, "o1", "o1", "o2", "o2"],
             [None, None, None, None, "A", "B", "A", "B"],
             ["oc", None, "p", "Base", -1.1, -2.2, -3.3, -4.4],
         ]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_objects_and_indexes_as_pivot_header_with_multiple_alternatives_and_parameters(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_alternatives(db_map, ("alt",))
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p1"),))
         import_object_parameters(db_map, (("oc", "p2"),))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map,
@@ -561,289 +555,301 @@
                 ("oc", "o2", "p1", Map(["A", "B"], [-9.9, -10.1]), "Base"),
                 ("oc", "o2", "p1", Map(["A", "B"], [-11.1, -12.2]), "alt"),
                 ("oc", "o2", "p2", Map(["A", "B"], [-13.3, -14.4]), "Base"),
                 ("oc", "o2", "p2", Map(["A", "B"], [-15.5, -16.6]), "alt"),
             ),
         )
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 1, Position.hidden, -1, -2, Position.hidden, 2, [Position.hidden], [-3])
+        mapping = entity_parameter_value_export(
+            0, 1, Position.hidden, -1, None, None, -2, Position.hidden, 2, [Position.hidden], [-3]
+        )
         expected = [
             [None, None, "o1", "o1", "o1", "o1", "o2", "o2", "o2", "o2"],
             [None, None, "Base", "Base", "alt", "alt", "Base", "Base", "alt", "alt"],
             [None, None, "A", "B", "A", "B", "A", "B", "A", "B"],
             ["oc", "p1", -1.1, -2.2, -3.3, -4.4, -9.9, -10.1, -11.1, -12.2],
             ["oc", "p2", -5.5, -6.6, -7.7, -8.8, -13.3, -14.4, -15.5, -16.6],
         ]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_empty_column_while_pivoted_handled_gracefully(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_alternatives(db_map, ("alt",))
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o"),))
         db_map.commit_session("Add test data.")
-        mapping = ObjectClassMapping(0)
+        mapping = EntityClassMapping(0)
         definition = ParameterDefinitionMapping(1)
         value_list = ParameterValueListMapping(2)
-        object_ = ObjectMapping(-1)
+        object_ = EntityMapping(-1)
         value_list.child = object_
         definition.child = value_list
         mapping.child = definition
         self.assertEqual(list(rows(mapping, db_map)), [])
-        db_map.connection.close()
+        db_map.close()
 
     def test_object_classes_as_header_row_and_objects_in_columns(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         import_objects(
             db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31"), ("oc3", "o32"), ("oc3", "o33"))
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(-1)
-        object_class_mapping.child = ObjectMapping(0)
+        object_class_mapping = EntityClassMapping(-1)
+        object_class_mapping.child = EntityMapping(0)
         self.assertEqual(
             list(rows(object_class_mapping, db_map)),
             [["oc1", "oc2", "oc3"], ["o11", "o21", "o31"], ["o12", None, "o32"], [None, None, "o33"]],
         )
-        db_map.connection.close()
+        db_map.close()
 
     def test_object_classes_as_table_names(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         import_objects(
             db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31"), ("oc3", "o32"), ("oc3", "o33"))
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(Position.table_name)
-        object_class_mapping.child = ObjectMapping(0)
+        object_class_mapping = EntityClassMapping(Position.table_name)
+        object_class_mapping.child = EntityMapping(0)
         tables = dict()
         for title, title_key in titles(object_class_mapping, db_map):
             tables[title] = list(rows(object_class_mapping, db_map, title_key))
         self.assertEqual(tables, {"oc1": [["o11"], ["o12"]], "oc2": [["o21"]], "oc3": [["o31"], ["o32"], ["o33"]]})
-        db_map.connection.close()
+        db_map.close()
 
     def test_object_class_and_parameter_definition_as_table_name(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         import_object_parameters(db_map, (("oc1", "p11"), ("oc2", "p21"), ("oc2", "p22")))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31")))
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(Position.table_name)
+        object_class_mapping = EntityClassMapping(Position.table_name)
         definition_mapping = ParameterDefinitionMapping(Position.table_name)
-        object_mapping = ObjectMapping(0)
+        object_mapping = EntityMapping(0)
         object_class_mapping.child = definition_mapping
         definition_mapping.child = object_mapping
         tables = dict()
         for title, title_key in titles(object_class_mapping, db_map):
             tables[title] = list(rows(object_class_mapping, db_map, title_key))
         self.assertEqual(
             tables, {"oc1,p11": [["o11"], ["o12"]], "oc2,p21": [["o21"]], "oc2,p22": [["o21"]], "oc3": [["o31"]]}
         )
-        db_map.connection.close()
+        db_map.close()
 
     def test_object_relationship_name_as_table_name(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_objects(db_map, (("oc1", "o1"), ("oc1", "o2"), ("oc2", "O")))
         import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
         import_relationships(db_map, (("rc", ("o1", "O")), ("rc", ("o2", "O"))))
         db_map.commit_session("Add test data.")
-        mappings = relationship_export(0, Position.table_name, [1, 2], [Position.table_name, 3])
+        mappings = entity_export(0, Position.table_name, [1, 2], [Position.table_name, 3])
         tables = dict()
         for title, title_key in titles(mappings, db_map):
             tables[title] = list(rows(mappings, db_map, title_key))
-        self.assertEqual(
-            tables, {"rc_o1__O,o1": [["rc", "oc1", "oc2", "O"]], "rc_o2__O,o2": [["rc", "oc1", "oc2", "O"]]}
-        )
-        db_map.connection.close()
+        self.assertEqual(tables, {"o1__O,o1": [["rc", "oc1", "oc2", "O"]], "o2__O,o2": [["rc", "oc1", "oc2", "O"]]})
+        db_map.close()
 
     def test_parameter_definitions_with_value_lists(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_parameter_value_lists(db_map, (("vl1", -1.0), ("vl2", -2.0)))
         import_object_parameters(db_map, (("oc", "p1", None, "vl1"), ("oc", "p2")))
         db_map.commit_session("Add test data.")
-        class_mapping = ObjectClassMapping(0)
+        class_mapping = EntityClassMapping(0)
         definition_mapping = ParameterDefinitionMapping(1)
         value_list_mapping = ParameterValueListMapping(2)
         definition_mapping.child = value_list_mapping
         class_mapping.child = definition_mapping
         tables = dict()
         for title, title_key in titles(class_mapping, db_map):
             tables[title] = list(rows(class_mapping, db_map, title_key))
         self.assertEqual(tables, {None: [["oc", "p1", "vl1"]]})
-        db_map.connection.close()
+        db_map.close()
 
     def test_parameter_definitions_and_values_and_value_lists(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_parameter_value_lists(db_map, (("vl", -1.0),))
         import_object_parameters(db_map, (("oc", "p1", None, "vl"), ("oc", "p2")))
         import_objects(db_map, (("oc", "o"),))
         import_object_parameter_values(db_map, (("oc", "o", "p1", -1.0), ("oc", "o", "p2", 5.0)))
         db_map.commit_session("Add test data.")
         flattened = [
-            ObjectClassMapping(0),
+            EntityClassMapping(0),
             ParameterDefinitionMapping(1),
             AlternativeMapping(Position.hidden),
             ParameterValueListMapping(2),
-            ObjectMapping(3),
+            EntityMapping(3),
             ParameterValueMapping(4),
         ]
         mapping = unflatten(flattened)
         tables = dict()
         for title, title_key in titles(mapping, db_map):
             tables[title] = list(rows(mapping, db_map, title_key))
         self.assertEqual(tables, {None: [["oc", "p1", "vl", "o", -1.0]]})
-        db_map.connection.close()
+        db_map.close()
 
     def test_parameter_definitions_and_values_and_ignorable_value_lists(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_parameter_value_lists(db_map, (("vl", -1.0),))
         import_object_parameters(db_map, (("oc", "p1", None, "vl"), ("oc", "p2")))
         import_objects(db_map, (("oc", "o"),))
         import_object_parameter_values(db_map, (("oc", "o", "p1", -1.0), ("oc", "o", "p2", 5.0)))
         db_map.commit_session("Add test data.")
         value_list_mapping = ParameterValueListMapping(2)
         value_list_mapping.set_ignorable(True)
         flattened = [
-            ObjectClassMapping(0),
+            EntityClassMapping(0),
             ParameterDefinitionMapping(1),
             AlternativeMapping(Position.hidden),
             value_list_mapping,
-            ObjectMapping(3),
+            EntityMapping(3),
             ParameterValueMapping(4),
         ]
         mapping = unflatten(flattened)
         tables = dict()
         for title, title_key in titles(mapping, db_map):
             tables[title] = list(rows(mapping, db_map, title_key))
         self.assertEqual(tables, {None: [["oc", "p1", "vl", "o", -1.0], ["oc", "p2", None, "o", 5.0]]})
-        db_map.connection.close()
+        db_map.close()
 
     def test_parameter_value_lists(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_parameter_value_lists(db_map, (("vl1", -1.0), ("vl2", -2.0)))
         db_map.commit_session("Add test data.")
         value_list_mapping = ParameterValueListMapping(0)
         tables = dict()
         for title, title_key in titles(value_list_mapping, db_map):
             tables[title] = list(rows(value_list_mapping, db_map, title_key))
         self.assertEqual(tables, {None: [["vl1"], ["vl2"]]})
-        db_map.connection.close()
+        db_map.close()
 
     def test_parameter_value_list_values(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_parameter_value_lists(db_map, (("vl1", -1.0), ("vl2", -2.0)))
         db_map.commit_session("Add test data.")
         value_list_mapping = ParameterValueListMapping(Position.table_name)
         value_mapping = ParameterValueListValueMapping(0)
         value_list_mapping.child = value_mapping
         tables = dict()
         for title, title_key in titles(value_list_mapping, db_map):
             tables[title] = list(rows(value_list_mapping, db_map, title_key))
         self.assertEqual(tables, {"vl1": [[-1.0]], "vl2": [[-2.0]]})
-        db_map.connection.close()
+        db_map.close()
 
     def test_no_item_declared_as_title_gives_full_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         import_object_parameters(db_map, (("oc1", "p11"), ("oc2", "p21"), ("oc2", "p22")))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc3", "o31")))
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(Position.hidden)
+        object_class_mapping = EntityClassMapping(Position.hidden)
         definition_mapping = ParameterDefinitionMapping(Position.hidden)
-        object_mapping = ObjectMapping(0)
+        object_mapping = EntityMapping(0)
         object_class_mapping.child = definition_mapping
         definition_mapping.child = object_mapping
         tables = dict()
         for title, title_key in titles(object_class_mapping, db_map):
             tables[title] = list(rows(object_class_mapping, db_map, title_key))
         self.assertEqual(tables, {None: [["o11"], ["o12"], ["o21"], ["o21"]]})
-        db_map.connection.close()
+        db_map.close()
 
     def test_missing_values_for_alternatives(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p1"), ("oc", "p2")))
         import_alternatives(db_map, ("alt1", "alt2"))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map,
             (
                 ("oc", "o1", "p1", -1.1, "alt1"),
                 ("oc", "o1", "p1", -1.2, "alt2"),
                 ("oc", "o1", "p2", -2.2, "alt1"),
                 ("oc", "o2", "p2", -5.5, "alt2"),
             ),
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         definition_mapping = ParameterDefinitionMapping(2)
-        object_mapping = ObjectMapping(1)
+        object_mapping = EntityMapping(1)
         alternative_mapping = AlternativeMapping(3)
         value_mapping = ParameterValueMapping(4)
         object_class_mapping.child = definition_mapping
         definition_mapping.child = object_mapping
         object_mapping.child = alternative_mapping
         alternative_mapping.child = value_mapping
         expected = [
             ["oc", "o1", "p1", "alt1", -1.1],
             ["oc", "o1", "p1", "alt2", -1.2],
             ["oc", "o1", "p2", "alt1", -2.2],
             ["oc", "o2", "p2", "alt2", -5.5],
         ]
         self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_export_relationship_classes(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         import_relationship_classes(
             db_map, (("rc1", ("oc1",)), ("rc2", ("oc3", "oc2")), ("rc3", ("oc2", "oc3", "oc1")))
         )
         db_map.commit_session("Add test data.")
-        relationship_class_mapping = RelationshipClassMapping(0)
-        self.assertEqual(list(rows(relationship_class_mapping, db_map)), [["rc1"], ["rc2"], ["rc3"]])
-        db_map.connection.close()
+        relationship_class_mapping = EntityClassMapping(0)
+        dimension_mapping = relationship_class_mapping.child = DimensionMapping(1)
+        dimension_mapping.child = DimensionMapping(2)
+        self.assertEqual(
+            list(rows(relationship_class_mapping, db_map)),
+            [["rc1", "oc1", ""], ["rc2", "oc3", "oc2"], ["rc3", "oc2", "oc3"]],
+        )
+        db_map.close()
 
     def test_export_relationships(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
         import_relationship_classes(db_map, (("rc1", ("oc1",)), ("rc2", ("oc2", "oc1"))))
         import_relationships(db_map, (("rc1", ("o11",)), ("rc2", ("o21", "o11")), ("rc2", ("o21", "o12"))))
         db_map.commit_session("Add test data.")
-        relationship_class_mapping = RelationshipClassMapping(0)
-        relationship_mapping = RelationshipMapping(1)
-        relationship_class_mapping.child = relationship_mapping
-        expected = [["rc1", "rc1_o11"], ["rc2", "rc2_o21__o11"], ["rc2", "rc2_o21__o12"]]
+        relationship_class_mapping = EntityClassMapping(0)
+        dimension1_mapping = relationship_class_mapping.child = DimensionMapping(1)
+        dimension2_mapping = dimension1_mapping.child = DimensionMapping(2)
+        relationship_mapping = dimension2_mapping.child = EntityMapping(3)
+        element1_mapping = relationship_mapping.child = ElementMapping(4)
+        element1_mapping.child = ElementMapping(5)
+        expected = [
+            ["rc1", "oc1", "", "o11__", "o11", ""],
+            ["rc2", "oc2", "oc1", "o21__o11", "o21", "o11"],
+            ["rc2", "oc2", "oc1", "o21__o12", "o21", "o12"],
+        ]
         self.assertEqual(list(rows(relationship_class_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_relationships_with_different_dimensions(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc2", "o22")))
         import_relationship_classes(db_map, (("rc1D", ("oc1",)), ("rc2D", ("oc1", "oc2"))))
         import_relationships(db_map, (("rc1D", ("o11",)), ("rc1D", ("o12",))))
         import_relationships(
             db_map,
             (("rc2D", ("o11", "o21")), ("rc2D", ("o11", "o22")), ("rc2D", ("o12", "o21")), ("rc2D", ("o12", "o22"))),
         )
         db_map.commit_session("Add test data.")
-        relationship_class_mapping = RelationshipClassMapping(0)
-        object_class_mapping1 = RelationshipClassObjectClassMapping(1)
-        object_class_mapping2 = RelationshipClassObjectClassMapping(2)
-        relationship_mapping = RelationshipMapping(Position.hidden)
-        object_mapping1 = RelationshipObjectMapping(3)
-        object_mapping2 = RelationshipObjectMapping(4)
+        relationship_class_mapping = EntityClassMapping(0)
+        object_class_mapping1 = DimensionMapping(1)
+        object_class_mapping2 = DimensionMapping(2)
+        relationship_mapping = EntityMapping(Position.hidden)
+        object_mapping1 = ElementMapping(3)
+        object_mapping2 = ElementMapping(4)
         object_mapping1.child = object_mapping2
         relationship_mapping.child = object_mapping1
         object_class_mapping2.child = relationship_mapping
         object_class_mapping1.child = object_class_mapping2
         relationship_class_mapping.child = object_class_mapping1
         tables = dict()
         for title, title_key in titles(relationship_class_mapping, db_map):
@@ -853,43 +859,43 @@
             ["rc1D", "oc1", "", "o12", ""],
             ["rc2D", "oc1", "oc2", "o11", "o21"],
             ["rc2D", "oc1", "oc2", "o11", "o22"],
             ["rc2D", "oc1", "oc2", "o12", "o21"],
             ["rc2D", "oc1", "oc2", "o12", "o22"],
         ]
         self.assertEqual(tables[None], expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_default_parameter_values(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         import_object_parameters(db_map, (("oc1", "p11", 3.14), ("oc2", "p21", 14.3), ("oc2", "p22", -1.0)))
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         definition_mapping = ParameterDefinitionMapping(1)
         default_value_mapping = ParameterDefaultValueMapping(2)
         definition_mapping.child = default_value_mapping
         object_class_mapping.child = definition_mapping
         table = list(rows(object_class_mapping, db_map))
         self.assertEqual(table, [["oc1", "p11", 3.14], ["oc2", "p21", 14.3], ["oc2", "p22", -1.0]])
-        db_map.connection.close()
+        db_map.close()
 
     def test_indexed_default_parameter_values(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         import_object_parameters(
             db_map,
             (
                 ("oc1", "p11", Map(["a", "b"], [-6.28, -3.14])),
                 ("oc2", "p21", Map(["A", "B"], [1.1, 2.2])),
                 ("oc2", "p22", Map(["D"], [-1.0])),
             ),
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         definition_mapping = ParameterDefinitionMapping(1)
         index_mapping = ParameterDefaultValueIndexMapping(2)
         value_mapping = ExpandedParameterDefaultValueMapping(3)
         index_mapping.child = value_mapping
         definition_mapping.child = index_mapping
         object_class_mapping.child = definition_mapping
         table = list(rows(object_class_mapping, db_map))
@@ -897,315 +903,203 @@
             ["oc1", "p11", "a", -6.28],
             ["oc1", "p11", "b", -3.14],
             ["oc2", "p21", "A", 1.1],
             ["oc2", "p21", "B", 2.2],
             ["oc2", "p22", "D", -1.0],
         ]
         self.assertEqual(table, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_replace_parameter_indexes_by_external_data(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p1"),))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map, (("oc", "o1", "p1", Map(["a", "b"], [5.0, -5.0])), ("oc", "o2", "p1", Map(["a", "b"], [2.0, -2.0])))
         )
         db_map.commit_session("Add test data.")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         parameter_definition_mapping = ParameterDefinitionMapping(2)
         alternative_mapping = AlternativeMapping(Position.hidden)
         parameter_definition_mapping.child = alternative_mapping
-        object_mapping = ObjectMapping(1)
+        object_mapping = EntityMapping(1)
         alternative_mapping.child = object_mapping
         index_mapping = ParameterValueIndexMapping(3)
         value_mapping = ExpandedParameterValueMapping(4)
         index_mapping.child = value_mapping
         index_mapping.replace_data(["c", "d"])
         object_mapping.child = index_mapping
         object_class_mapping.child = parameter_definition_mapping
         expected = [
             ["oc", "o1", "p1", "c", 5.0],
             ["oc", "o1", "p1", "d", -5.0],
             ["oc", "o2", "p1", "c", 2.0],
             ["oc", "o2", "p1", "d", -2.0],
         ]
         self.assertEqual(list(rows(object_class_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_constant_mapping_as_title(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         db_map.commit_session("Add test data.")
         constant_mapping = FixedValueMapping(Position.table_name, "title_text")
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         constant_mapping.child = object_class_mapping
         tables = dict()
         for title, title_key in titles(constant_mapping, db_map):
             tables[title] = list(rows(constant_mapping, db_map, title_key))
         self.assertEqual(tables, {"title_text": [["oc1"], ["oc2"], ["oc3"]]})
-        db_map.connection.close()
+        db_map.close()
 
     def test_scenario_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_scenarios(db_map, ("s1", "s2"))
         db_map.commit_session("Add test data.")
         scenario_mapping = ScenarioMapping(0)
         tables = dict()
         for title, title_key in titles(scenario_mapping, db_map):
             tables[title] = list(rows(scenario_mapping, db_map, title_key))
         self.assertEqual(tables, {None: [["s1"], ["s2"]]})
-        db_map.connection.close()
+        db_map.close()
 
     def test_scenario_active_flag_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_scenarios(db_map, (("s1", True), ("s2", False)))
         db_map.commit_session("Add test data.")
         scenario_mapping = ScenarioMapping(0)
         active_flag_mapping = ScenarioActiveFlagMapping(1)
         scenario_mapping.child = active_flag_mapping
         tables = dict()
         for title, title_key in titles(scenario_mapping, db_map):
             tables[title] = list(rows(scenario_mapping, db_map, title_key))
         self.assertEqual(tables, {None: [["s1", True], ["s2", False]]})
-        db_map.connection.close()
+        db_map.close()
 
     def test_scenario_alternative_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_alternatives(db_map, ("a1", "a2", "a3"))
         import_scenarios(db_map, ("s1", "s2", "empty"))
         import_scenario_alternatives(db_map, (("s1", "a2"), ("s1", "a1", "a2"), ("s2", "a2"), ("s2", "a3", "a2")))
         db_map.commit_session("Add test data.")
         scenario_mapping = ScenarioMapping(0)
         scenario_alternative_mapping = ScenarioAlternativeMapping(1)
         scenario_mapping.child = scenario_alternative_mapping
         tables = dict()
         for title, title_key in titles(scenario_mapping, db_map):
             tables[title] = list(rows(scenario_mapping, db_map, title_key))
         self.assertEqual(tables, {None: [["s1", "a1"], ["s1", "a2"], ["s2", "a2"], ["s2", "a3"]]})
-        db_map.connection.close()
-
-    def test_tool_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_tools(db_map, ("tool1", "tool2"))
-        db_map.commit_session("Add test data.")
-        tool_mapping = ToolMapping(0)
-        tables = dict()
-        for title, title_key in titles(tool_mapping, db_map):
-            tables[title] = list(rows(tool_mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["tool1"], ["tool2"]]})
-        db_map.connection.close()
-
-    def test_feature_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_parameter_value_lists(db_map, (("features", "feat1"), ("features", "feat2")))
-        import_object_parameters(
-            db_map,
-            (
-                ("oc1", "p1", "feat1", "features"),
-                ("oc1", "p2", "feat1", "features"),
-                ("oc2", "p3", "feat2", "features"),
-            ),
-        )
-        import_features(db_map, (("oc1", "p2"), ("oc2", "p3")))
-        db_map.commit_session("Add test data.")
-        class_mapping = FeatureEntityClassMapping(0)
-        parameter_mapping = FeatureParameterDefinitionMapping(1)
-        class_mapping.child = parameter_mapping
-        tables = dict()
-        for title, title_key in titles(class_mapping, db_map):
-            tables[title] = list(rows(class_mapping, db_map, title_key))
-        self.assertEqual(tables, {None: [["oc1", "p2"], ["oc2", "p3"]]})
-        db_map.connection.close()
-
-    def test_tool_feature_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_parameter_value_lists(db_map, (("features", "feat1"), ("features", "feat2")))
-        import_object_parameters(
-            db_map,
-            (
-                ("oc1", "p1", "feat1", "features"),
-                ("oc1", "p2", "feat1", "features"),
-                ("oc2", "p3", "feat2", "features"),
-            ),
-        )
-        import_features(db_map, (("oc1", "p1"), ("oc1", "p2"), ("oc2", "p3")))
-        import_tools(db_map, ("tool1", "tool2"))
-        import_tool_features(
-            db_map, (("tool1", "oc1", "p1", True), ("tool1", "oc2", "p3", False), ("tool2", "oc1", "p1", True))
-        )
-        db_map.commit_session("Add test data.")
-        mapping = unflatten(
-            [
-                ToolMapping(Position.table_name),
-                ToolFeatureEntityClassMapping(0),
-                ToolFeatureParameterDefinitionMapping(1),
-                ToolFeatureRequiredFlagMapping(2),
-            ]
-        )
-        tables = dict()
-        for title, title_key in titles(mapping, db_map):
-            tables[title] = list(rows(mapping, db_map, title_key))
-        expected = {"tool1": [["oc1", "p1", True], ["oc2", "p3", False]], "tool2": [["oc1", "p1", True]]}
-        self.assertEqual(tables, expected)
-        db_map.connection.close()
-
-    def test_tool_feature_method_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc1", "oc2"))
-        import_parameter_value_lists(db_map, (("features", "feat1"), ("features", "feat2")))
-        import_object_parameters(
-            db_map,
-            (
-                ("oc1", "p1", "feat1", "features"),
-                ("oc1", "p2", "feat1", "features"),
-                ("oc2", "p3", "feat2", "features"),
-            ),
-        )
-        import_features(db_map, (("oc1", "p1"), ("oc1", "p2"), ("oc2", "p3")))
-        import_tools(db_map, ("tool1", "tool2"))
-        import_tool_features(
-            db_map, (("tool1", "oc1", "p1", True), ("tool1", "oc2", "p3", False), ("tool2", "oc1", "p1", True))
-        )
-        import_tool_feature_methods(
-            db_map,
-            (
-                ("tool1", "oc1", "p1", "feat1"),
-                ("tool1", "oc1", "p1", "feat2"),
-                ("tool2", "oc1", "p1", "feat1"),
-                ("tool2", "oc1", "p1", "feat2"),
-            ),
-        )
-        db_map.commit_session("Add test data.")
-        mapping = unflatten(
-            [
-                ToolMapping(Position.table_name),
-                ToolFeatureMethodEntityClassMapping(0),
-                ToolFeatureMethodParameterDefinitionMapping(1),
-                ToolFeatureMethodMethodMapping(2),
-            ]
-        )
-        tables = dict()
-        for title, title_key in titles(mapping, db_map):
-            tables[title] = list(rows(mapping, db_map, title_key))
-        expected = {
-            "tool1": [["oc1", "p1", "feat1"], ["oc1", "p1", "feat2"]],
-            "tool2": [["oc1", "p1", "feat1"], ["oc1", "p1", "feat2"]],
-        }
-        self.assertEqual(tables, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"),))
         db_map.commit_session("Add test data.")
-        root = unflatten([ObjectClassMapping(0, header="class"), ObjectMapping(1, header="entity")])
+        root = unflatten([EntityClassMapping(0, header="class"), EntityMapping(1, header="entity")])
         expected = [["class", "entity"], ["oc", "o1"]]
         self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_header_without_data_still_creates_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root = unflatten([ObjectClassMapping(0, header="class"), ObjectMapping(1, header="object")])
+        db_map = DatabaseMapping("sqlite://", create=True)
+        root = unflatten([EntityClassMapping(0, header="class"), EntityMapping(1, header="object")])
         expected = [["class", "object"]]
         self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_header_in_half_pivot_table_without_data_still_creates_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root = unflatten([ObjectClassMapping(-1, header="class"), ObjectMapping(9, header="object")])
+        db_map = DatabaseMapping("sqlite://", create=True)
+        root = unflatten([EntityClassMapping(-1, header="class"), EntityMapping(9, header="object")])
         expected = [["class"]]
         self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_header_in_pivot_table_without_data_still_creates_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         root = unflatten(
             [
-                ObjectClassMapping(-1, header="class"),
+                EntityClassMapping(-1, header="class"),
                 ParameterDefinitionMapping(0, header="parameter"),
-                ObjectMapping(-2, header="object"),
+                EntityMapping(-2, header="object"),
                 AlternativeMapping(1, header="alternative"),
                 ParameterValueMapping(0),
             ]
         )
         expected = [[None, "class"], ["parameter", "alternative"]]
         self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_disabled_empty_data_header(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root = unflatten([ObjectClassMapping(0, header="class"), ObjectMapping(1, header="object")])
+        db_map = DatabaseMapping("sqlite://", create=True)
+        root = unflatten([EntityClassMapping(0, header="class"), EntityMapping(1, header="object")])
         expected = []
         self.assertEqual(list(rows(root, db_map, empty_data_header=False)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_disabled_empty_data_header_in_pivot_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root = unflatten([ObjectClassMapping(-1, header="class"), ObjectMapping(0)])
+        db_map = DatabaseMapping("sqlite://", create=True)
+        root = unflatten([EntityClassMapping(-1, header="class"), EntityMapping(0)])
         expected = []
         self.assertEqual(list(rows(root, db_map, empty_data_header=False)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_header_position(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"),))
         db_map.commit_session("Add test data.")
-        root = unflatten([ObjectClassMapping(Position.header), ObjectMapping(0)])
+        root = unflatten([EntityClassMapping(Position.header), EntityMapping(0)])
         expected = [["oc"], ["o1"]]
         self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_header_position_with_relationships(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_objects(db_map, (("oc1", "o11"), ("oc2", "o21")))
         import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
         import_relationships(db_map, (("rc", ("o11", "o21")),))
         db_map.commit_session("Add test data.")
         root = unflatten(
             [
-                RelationshipClassMapping(0),
-                RelationshipClassObjectClassMapping(Position.header),
-                RelationshipClassObjectClassMapping(Position.header),
-                RelationshipMapping(1),
-                RelationshipObjectMapping(2),
-                RelationshipObjectMapping(3),
+                EntityClassMapping(0),
+                DimensionMapping(Position.header),
+                DimensionMapping(Position.header),
+                EntityMapping(1),
+                ElementMapping(2),
+                ElementMapping(3),
             ]
         )
-        expected = [["", "", "oc1", "oc2"], ["rc", "rc_o11__o21", "o11", "o21"]]
+        expected = [["", "", "oc1", "oc2"], ["rc", "o11__o21", "o11", "o21"]]
         self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_header_position_with_relationships_but_no_data(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
         db_map.commit_session("Add test data.")
         root = unflatten(
             [
-                RelationshipClassMapping(0),
-                RelationshipClassObjectClassMapping(Position.header),
-                RelationshipClassObjectClassMapping(Position.header),
-                RelationshipMapping(1),
-                RelationshipObjectMapping(2),
-                RelationshipObjectMapping(3),
+                EntityClassMapping(0),
+                DimensionMapping(Position.header),
+                DimensionMapping(Position.header),
+                EntityMapping(1),
+                ElementMapping(2),
+                ElementMapping(3),
             ]
         )
         expected = [["", "", "oc1", "oc2"]]
         self.assertEqual(list(rows(root, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_header_and_pivot(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_alternatives(db_map, ("alt",))
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p1"),))
         import_object_parameters(db_map, (("oc", "p2"),))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map,
@@ -1219,34 +1113,34 @@
                 ("oc", "o2", "p2", Map(["A", "B"], [-13.3, -14.4]), "Base"),
                 ("oc", "o2", "p2", Map(["A", "B"], [-15.5, -16.6]), "alt"),
             ),
         )
         db_map.commit_session("Add test data.")
         mapping = unflatten(
             [
-                ObjectClassMapping(0, header="class"),
+                EntityClassMapping(0, header="class"),
                 ParameterDefinitionMapping(1, header="parameter"),
-                ObjectMapping(-1, header="object"),
+                EntityMapping(-1, header="object"),
                 AlternativeMapping(-2, header="alternative"),
                 ParameterValueIndexMapping(-3, header=""),
                 ExpandedParameterValueMapping(2, header="value"),
             ]
         )
         expected = [
             [None, "object", "o1", "o1", "o1", "o1", "o2", "o2", "o2", "o2"],
             [None, "alternative", "Base", "Base", "alt", "alt", "Base", "Base", "alt", "alt"],
             ["class", "parameter", "A", "B", "A", "B", "A", "B", "A", "B"],
             ["oc", "p1", -1.1, -2.2, -3.3, -4.4, -9.9, -10.1, -11.1, -12.2],
             ["oc", "p2", -5.5, -6.6, -7.7, -8.8, -13.3, -14.4, -15.5, -16.6],
         ]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_pivot_without_left_hand_side_has_padding_column_for_headers(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_alternatives(db_map, ("alt",))
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p1"),))
         import_object_parameters(db_map, (("oc", "p2"),))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         import_object_parameter_values(
             db_map,
@@ -1260,89 +1154,89 @@
                 ("oc", "o2", "p2", Map(["A", "B"], [-13.3, -14.4]), "Base"),
                 ("oc", "o2", "p2", Map(["A", "B"], [-15.5, -16.6]), "alt"),
             ),
         )
         db_map.commit_session("Add test data.")
         mapping = unflatten(
             [
-                ObjectClassMapping(Position.header),
+                EntityClassMapping(Position.header),
                 ParameterDefinitionMapping(Position.hidden, header="parameter"),
-                ObjectMapping(-1),
+                EntityMapping(-1),
                 AlternativeMapping(-2, header="alternative"),
                 ParameterValueIndexMapping(-3, header="index"),
                 ExpandedParameterValueMapping(2, header="value"),
             ]
         )
         expected = [
             ["oc", "o1", "o1", "o1", "o1", "o2", "o2", "o2", "o2"],
             ["alternative", "Base", "Base", "alt", "alt", "Base", "Base", "alt", "alt"],
             ["index", "A", "B", "A", "B", "A", "B", "A", "B"],
             [None, -1.1, -2.2, -3.3, -4.4, -9.9, -10.1, -11.1, -12.2],
             [None, -5.5, -6.6, -7.7, -8.8, -13.3, -14.4, -15.5, -16.6],
         ]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_count_mappings(self):
-        object_class_mapping = ObjectClassMapping(2)
+        object_class_mapping = EntityClassMapping(2)
         parameter_definition_mapping = ParameterDefinitionMapping(0)
-        object_mapping = ObjectMapping(1)
+        object_mapping = EntityMapping(1)
         parameter_definition_mapping.child = object_mapping
         object_class_mapping.child = parameter_definition_mapping
         self.assertEqual(object_class_mapping.count_mappings(), 3)
 
     def test_flatten(self):
-        object_class_mapping = ObjectClassMapping(2)
+        object_class_mapping = EntityClassMapping(2)
         parameter_definition_mapping = ParameterDefinitionMapping(0)
-        object_mapping = ObjectMapping(1)
+        object_mapping = EntityMapping(1)
         parameter_definition_mapping.child = object_mapping
         object_class_mapping.child = parameter_definition_mapping
         mappings = object_class_mapping.flatten()
         self.assertEqual(mappings, [object_class_mapping, parameter_definition_mapping, object_mapping])
 
     def test_unflatten_sets_last_mappings_child_to_none(self):
-        object_class_mapping = ObjectClassMapping(2)
-        object_mapping = ObjectMapping(1)
+        object_class_mapping = EntityClassMapping(2)
+        object_mapping = EntityMapping(1)
         object_class_mapping.child = object_mapping
         mapping_list = object_class_mapping.flatten()
         root = unflatten(mapping_list[:1])
         self.assertIsNone(root.child)
 
     def test_has_titles(self):
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         parameter_definition_mapping = ParameterDefinitionMapping(Position.table_name)
-        object_mapping = ObjectMapping(1)
+        object_mapping = EntityMapping(1)
         parameter_definition_mapping.child = object_mapping
         object_class_mapping.child = parameter_definition_mapping
         self.assertTrue(object_class_mapping.has_titles())
 
     def test_drop_non_positioned_tail(self):
-        object_class_mapping = ObjectClassMapping(0)
+        object_class_mapping = EntityClassMapping(0)
         parameter_definition_mapping = ParameterDefinitionMapping(Position.hidden)
-        object_mapping = ObjectMapping(1)
+        object_mapping = EntityMapping(1)
         alternative_mapping = AlternativeMapping(Position.hidden)
         value_mapping = ParameterValueMapping(Position.hidden)
         alternative_mapping.child = value_mapping
         object_mapping.child = alternative_mapping
         parameter_definition_mapping.child = object_mapping
         object_class_mapping.child = parameter_definition_mapping
         tail_cut_mapping = drop_non_positioned_tail(object_class_mapping)
         flattened = tail_cut_mapping.flatten()
         self.assertEqual(flattened, [object_class_mapping, parameter_definition_mapping, object_mapping])
 
     def test_serialization(self):
-        highlight_dimension = 5
+        highlight_position = 5
         mappings = [
-            ObjectClassMapping(0),
-            RelationshipClassMapping(Position.table_name, highlight_dimension=highlight_dimension),
-            RelationshipClassObjectClassMapping(2),
+            EntityClassMapping(0, highlight_position=highlight_position),
+            EntityClassMapping(Position.table_name),
+            DimensionMapping(2),
             ParameterDefinitionMapping(1),
-            ObjectMapping(-1),
-            RelationshipMapping(Position.hidden),
-            RelationshipObjectMapping(-1),
+            EntityMapping(-1),
+            EntityMapping(Position.hidden),
+            ElementMapping(-1),
             AlternativeMapping(3),
             ParameterValueMapping(4),
             ParameterValueIndexMapping(5),
             ParameterValueTypeMapping(6),
             ExpandedParameterValueMapping(7),
             FixedValueMapping(8, "gaga"),
         ]
@@ -1350,282 +1244,294 @@
         expected_types = [type(m) for m in mappings]
         root = unflatten(mappings)
         serialized = to_dict(root)
         deserialized = from_dict(serialized).flatten()
         self.assertEqual([type(m) for m in deserialized], expected_types)
         self.assertEqual([m.position for m in deserialized], expected_positions)
         for m in deserialized:
+            if isinstance(m, EntityClassMapping) and m.highlight_position is not None:
+                self.assertEqual(m.highlight_position, highlight_position)
+                break
+        else:
+            self.fail("no highlight_position in deserialized mappings")
+        for m in deserialized:
             if isinstance(m, FixedValueMapping):
                 self.assertEqual(m.value, "gaga")
-            elif isinstance(m, RelationshipClassMapping):
-                self.assertEqual(m.highlight_dimension, highlight_dimension)
+                break
+        else:
+            self.fail("no fixed value in deserialized mappings")
 
     def test_setting_ignorable_flag(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         db_map.commit_session("Add test data.")
-        object_mapping = ObjectMapping(1)
-        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
+        object_mapping = EntityMapping(1)
+        root_mapping = unflatten([EntityClassMapping(0), object_mapping])
         object_mapping.set_ignorable(True)
         self.assertTrue(object_mapping.is_ignorable())
         expected = [["oc", None]]
         self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_unsetting_ignorable_flag(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"),))
         db_map.commit_session("Add test data.")
-        object_mapping = ObjectMapping(1)
-        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
+        object_mapping = EntityMapping(1)
+        root_mapping = unflatten([EntityClassMapping(0), object_mapping])
         object_mapping.set_ignorable(True)
         self.assertTrue(object_mapping.is_ignorable())
         object_mapping.set_ignorable(False)
         self.assertFalse(object_mapping.is_ignorable())
         expected = [["oc", "o1"]]
         self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_filter(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"), ("oc", "o2")))
         db_map.commit_session("Add test data.")
-        object_mapping = ObjectMapping(1)
+        object_mapping = EntityMapping(1)
         object_mapping.filter_re = "o1"
-        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
+        root_mapping = unflatten([EntityClassMapping(0), object_mapping])
         expected = [["oc", "o1"]]
         self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_hidden_tail_filter(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_objects(db_map, (("oc1", "o1"), ("oc2", "o2")))
         db_map.commit_session("Add test data.")
-        object_mapping = ObjectMapping(Position.hidden)
+        object_mapping = EntityMapping(Position.hidden)
         object_mapping.filter_re = "o1"
-        root_mapping = unflatten([ObjectClassMapping(0), object_mapping])
+        root_mapping = unflatten([EntityClassMapping(0), object_mapping])
         expected = [["oc1"]]
         self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_index_names(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o"),))
         import_object_parameter_values(db_map, (("oc", "o", "p", Map(["a"], [5.0], index_name="index")),))
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 2, Position.hidden, 1, 3, Position.hidden, 5, [Position.header], [4])
+        mapping = entity_parameter_value_export(
+            0, 2, Position.hidden, 1, None, None, 3, Position.hidden, 5, [Position.header], [4]
+        )
         expected = [["", "", "", "", "index", ""], ["oc", "o", "p", "Base", "a", 5.0]]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_default_value_index_names_with_nested_map(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(
             db_map, (("oc", "p", Map(["A"], [Map(["b"], [2.3], index_name="idx2")], index_name="idx1")),)
         )
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_default_value_export(
+        mapping = entity_parameter_default_value_export(
             0, 1, Position.hidden, 4, [Position.header, Position.header], [2, 3]
         )
         expected = [["", "", "idx1", "idx2", ""], ["oc", "p", "A", "b", 2.3]]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_multiple_index_names_with_empty_database(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        mapping = relationship_parameter_export(
+        db_map = DatabaseMapping("sqlite://", create=True)
+        mapping = entity_parameter_value_export(
             0, 4, Position.hidden, 1, [2], [3], 5, Position.hidden, 8, [Position.header, Position.header], [6, 7]
         )
         expected = [9 * [""]]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_parameter_default_value_type(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         import_object_parameters(db_map, (("oc1", "p11", 3.14), ("oc2", "p21", 14.3), ("oc2", "p22", -1.0)))
         db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_default_value_export(0, 1, 2, 3, None, None)
+        root_mapping = entity_parameter_default_value_export(0, 1, 2, 3, None, None)
         expected = [
             ["oc1", "p11", "single_value", 3.14],
             ["oc2", "p21", "single_value", 14.3],
             ["oc2", "p22", "single_value", -1.0],
         ]
         self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_map_with_more_dimensions_than_index_mappings(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o"),))
         import_object_parameter_values(db_map, (("oc", "o", "p", Map(["A"], [Map(["b"], [2.3])])),))
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(
-            0, 1, Position.hidden, 2, Position.hidden, Position.hidden, 4, [Position.hidden], [3]
+        mapping = entity_parameter_value_export(
+            0, 1, Position.hidden, 2, None, None, Position.hidden, Position.hidden, 4, [Position.hidden], [3]
         )
         expected = [["oc", "p", "o", "A", "map"]]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_default_map_value_with_more_dimensions_than_index_mappings(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p", Map(["A"], [Map(["b"], [2.3])])),))
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_default_value_export(0, 1, Position.hidden, 3, [Position.hidden], [2])
+        mapping = entity_parameter_default_value_export(0, 1, Position.hidden, 3, [Position.hidden], [2])
         expected = [["oc", "p", "A", "map"]]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_map_with_single_value_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o"),))
         import_object_parameter_values(db_map, (("oc", "o", "p", Map(["A"], [2.3])),))
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(0, 1, Position.hidden, 2, Position.hidden, Position.hidden, 3, None, None)
+        mapping = entity_parameter_value_export(
+            0, 1, Position.hidden, 2, None, None, Position.hidden, Position.hidden, 3, None, None
+        )
         expected = [["oc", "p", "o", "map"]]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_default_map_value_with_single_value_mapping(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p", Map(["A"], [2.3])),))
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_default_value_export(0, 1, Position.hidden, 2, None, None)
+        mapping = entity_parameter_default_value_export(0, 1, Position.hidden, 2, None, None)
         expected = [["oc", "p", "map"]]
         self.assertEqual(list(rows(mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_table_gets_exported_even_without_parameter_values(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         db_map.commit_session("Add test data.")
-        mapping = object_parameter_export(Position.header, Position.table_name, object_position=0, value_position=1)
+        mapping = entity_parameter_value_export(
+            Position.header, Position.table_name, entity_position=0, value_position=1
+        )
         tables = dict()
         for title, title_key in titles(mapping, db_map):
             tables[title] = list(rows(mapping, db_map, title_key))
         expected = {"p": [["oc", ""]]}
         self.assertEqual(tables, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_relationship_class_object_classes_parameters(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_relationship_classes(db_map, (("rc", ("oc",)),))
         db_map.commit_session("Add test data")
         root_mapping = unflatten(
             [
-                RelationshipClassMapping(0, highlight_dimension=0),
-                RelationshipClassObjectClassMapping(1),
+                EntityClassMapping(0, highlight_position=0),
+                DimensionMapping(1),
                 ParameterDefinitionMapping(2),
             ]
         )
         expected = [["rc", "oc", "p"]]
         self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_relationship_class_object_classes_parameters_multiple_dimensions(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21")))
         import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
         db_map.commit_session("Add test data")
         root_mapping = unflatten(
             [
-                RelationshipClassMapping(0, highlight_dimension=0),
-                RelationshipClassObjectClassMapping(1),
-                RelationshipClassObjectClassMapping(3),
+                EntityClassMapping(0, highlight_position=0),
+                DimensionMapping(1),
+                DimensionMapping(3),
                 ParameterDefinitionMapping(2),
             ]
         )
         expected = [["rc", "oc1", "p11", "oc2"], ["rc", "oc1", "p12", "oc2"]]
         self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_highlight_relationship_objects(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         import_objects(
             db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc2", "o22"), ("oc3", "o31"), ("oc3", "o32"))
         )
         import_relationship_classes(db_map, (("rc", ("oc1", "oc2")),))
         import_relationships(db_map, (("rc", ("o11", "o21")), ("rc", ("o12", "o22"))))
         db_map.commit_session("Add test data")
         root_mapping = unflatten(
             [
-                RelationshipClassMapping(0, highlight_dimension=0),
-                RelationshipClassObjectClassMapping(1),
-                RelationshipClassObjectClassMapping(2),
-                RelationshipMapping(3),
-                RelationshipObjectMapping(4),
-                RelationshipObjectMapping(5),
+                EntityClassMapping(0, highlight_position=0),
+                DimensionMapping(1),
+                DimensionMapping(2),
+                EntityMapping(3),
+                ElementMapping(4),
+                ElementMapping(5),
             ]
         )
         expected = [
-            ["rc", "oc1", "oc2", "rc_o11__o21", "o11", "o21"],
-            ["rc", "oc1", "oc2", "rc_o12__o22", "o12", "o22"],
+            ["rc", "oc1", "oc2", "o11__o21", "o11", "o21"],
+            ["rc", "oc1", "oc2", "o12__o22", "o12", "o22"],
         ]
         self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_export_object_parameters_while_exporting_relationships(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        import_object_classes(db_map, ("oc",))
-        import_object_parameters(db_map, (("oc", "p"),))
-        import_objects(db_map, (("oc", "o"),))
-        import_object_parameter_values(db_map, (("oc", "o", "p", 23.0),))
-        import_relationship_classes(db_map, (("rc", ("oc",)),))
-        import_relationships(db_map, (("rc", ("o",)),))
-        db_map.commit_session("Add test data")
-        root_mapping = unflatten(
-            [
-                RelationshipClassMapping(0, highlight_dimension=0),
-                RelationshipClassObjectClassMapping(1),
-                RelationshipMapping(2),
-                RelationshipObjectMapping(3),
-                ParameterDefinitionMapping(4),
-                AlternativeMapping(5),
-                ParameterValueMapping(6),
-            ]
-        )
-        expected = [["rc", "oc", "rc_o", "o", "p", "Base", 23.0]]
-        self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        with DatabaseMapping("sqlite://", create=True) as db_map:
+            import_object_classes(db_map, ("oc",))
+            import_object_parameters(db_map, (("oc", "p"),))
+            import_objects(db_map, (("oc", "o"),))
+            import_object_parameter_values(db_map, (("oc", "o", "p", 23.0),))
+            import_relationship_classes(db_map, (("rc", ("oc",)),))
+            import_relationships(db_map, (("rc", ("o",)),))
+            db_map.commit_session("Add test data")
+            root_mapping = unflatten(
+                [
+                    EntityClassMapping(0, highlight_position=0),
+                    DimensionMapping(1),
+                    EntityMapping(2),
+                    ElementMapping(3),
+                    ParameterDefinitionMapping(4),
+                    AlternativeMapping(5),
+                    ParameterValueMapping(6),
+                ]
+            )
+            expected = [["rc", "oc", "o__", "o", "p", "Base", 23.0]]
+            self.assertEqual(list(rows(root_mapping, db_map)), expected)
 
     def test_export_default_values_of_object_parameters_while_exporting_relationships(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p", 23.0),))
         import_objects(db_map, (("oc", "o"),))
         import_relationship_classes(db_map, (("rc", ("oc",)),))
         import_relationships(db_map, (("rc", ("o",)),))
         db_map.commit_session("Add test data")
         root_mapping = unflatten(
             [
-                RelationshipClassMapping(0, highlight_dimension=0),
-                RelationshipClassObjectClassMapping(1),
+                EntityClassMapping(0, highlight_position=0),
+                DimensionMapping(1),
                 ParameterDefinitionMapping(2),
                 ParameterDefaultValueMapping(3),
             ]
         )
         expected = [["rc", "oc", "p", 23.0]]
         self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_export_object_parameters_while_exporting_relationships_with_multiple_parameters_and_classes2(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2", "oc3"))
         import_object_parameters(db_map, (("oc1", "p11"), ("oc1", "p12"), ("oc2", "p21"), ("oc3", "p31")))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21"), ("oc2", "o22"), ("oc3", "o31")))
         import_object_parameter_values(db_map, (("oc1", "o11", "p11", 1.1),))
@@ -1639,29 +1545,69 @@
         import_relationship_classes(db_map, (("rc23", ("oc2", "oc3")),))
         import_relationships(db_map, (("rc12", ("o11", "o21")),))
         import_relationships(db_map, (("rc12", ("o12", "o21")),))
         import_relationships(db_map, (("rc23", ("o21", "o31")),))
         db_map.commit_session("Add test data")
         root_mapping = unflatten(
             [
-                RelationshipClassMapping(0, highlight_dimension=1),
-                RelationshipClassObjectClassMapping(1),
-                RelationshipClassObjectClassMapping(2),
-                RelationshipMapping(3),
-                RelationshipObjectMapping(4),
-                RelationshipObjectMapping(5),
+                EntityClassMapping(0, highlight_position=1),
+                DimensionMapping(1),
+                DimensionMapping(2),
+                EntityMapping(3),
+                ElementMapping(4),
+                ElementMapping(5),
                 ParameterDefinitionMapping(6),
                 AlternativeMapping(7),
                 ParameterValueMapping(8),
             ]
         )
         expected = [
-            ["rc12", "oc1", "oc2", "rc12_o11__o21", "o11", "o21", "p21", "Base", 5.5],
-            ["rc12", "oc1", "oc2", "rc12_o12__o21", "o12", "o21", "p21", "Base", 5.5],
-            ["rc23", "oc2", "oc3", "rc23_o21__o31", "o21", "o31", "p31", "Base", 7.7],
+            ["rc12", "oc1", "oc2", "o11__o21", "o11", "o21", "p21", "Base", 5.5],
+            ["rc12", "oc1", "oc2", "o12__o21", "o12", "o21", "p21", "Base", 5.5],
+            ["rc23", "oc2", "oc3", "o21__o31", "o21", "o31", "p31", "Base", 7.7],
         ]
         self.assertEqual(list(rows(root_mapping, db_map)), expected)
-        db_map.connection.close()
+        db_map.close()
+
+    def test_alternative_mapping_with_header_and_description(self):
+        root_mapping = AlternativeMapping(0, header="alternative")
+        root_mapping.child = AlternativeDescriptionMapping(1, header="description")
+        with DatabaseMapping("sqlite://", create=True) as db_map:
+            expected = [["alternative", "description"], ["Base", "Base alternative"]]
+            self.assertEqual(list(rows(root_mapping, db_map)), expected)
+
+    def test_fixed_value_and_alternative_mappings_with_header_and_description(self):
+        root_mapping = FixedValueMapping(Position.table_name, value="Alternative")
+        alternative_mapping = root_mapping.child = AlternativeMapping(0, header="alternative")
+        alternative_mapping.child = AlternativeDescriptionMapping(1, header="description")
+        with DatabaseMapping("sqlite://", create=True) as db_map:
+            expected = [["alternative", "description"], ["Base", "Base alternative"]]
+            self.assertEqual(list(rows(root_mapping, db_map)), expected)
+
+    def test_fixed_value_and_scenario_mappings_with_header_and_description(self):
+        root_mapping = FixedValueMapping(Position.table_name, value="Scenario")
+        scenario_mapping = root_mapping.child = ScenarioMapping(0, header="scenario")
+        scenario_mapping.child = ScenarioDescriptionMapping(1, header="description")
+        with DatabaseMapping("sqlite://", create=True) as db_map:
+            import_scenarios(db_map, (("scenario1", False, "Scenario with Base alternative"),))
+            db_map.commit_session("Add test data.")
+            expected = [["scenario", "description"], ["scenario1", "Scenario with Base alternative"]]
+            self.assertEqual(list(rows(root_mapping, db_map)), expected)
+
+    def test_rows_from_scenario_mappings_after_rows_from_alternative_mappings(self):
+        root_mapping1 = FixedValueMapping(Position.table_name, value="Alternative")
+        alternative_mapping = root_mapping1.child = AlternativeMapping(0, header="alternative")
+        alternative_mapping.child = AlternativeDescriptionMapping(1, header="description")
+        root_mapping2 = FixedValueMapping(Position.table_name, value="Scenario")
+        scenario_mapping = root_mapping2.child = ScenarioMapping(0, header="scenario")
+        scenario_mapping.child = ScenarioDescriptionMapping(1, header="description")
+        with DatabaseMapping("sqlite://", create=True) as db_map:
+            import_scenarios(db_map, (("scenario1", False, "Scenario with Base alternative"),))
+            db_map.commit_session("Add test data.")
+            expected1 = [["alternative", "description"], ["Base", "Base alternative"]]
+            self.assertEqual(list(rows(root_mapping1, db_map)), expected1)
+            expected2 = [["scenario", "description"], ["scenario1", "Scenario with Base alternative"]]
+            self.assertEqual(list(rows(root_mapping2, db_map)), expected2)
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/export_mapping/test_pivot.py` & `spinedb_api-0.31.0/tests/export_mapping/test_pivot.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/tests/filters/__init__.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/importers/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,16 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
+
+"""
+Intentionally empty.
+
+"""
```

### Comparing `spinedb_api-0.30.5/tests/filters/test_alternative_filter.py` & `spinedb_api-0.31.0/tests/filters/test_alternative_filter.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -17,20 +18,21 @@
 from tempfile import TemporaryDirectory
 import unittest
 from sqlalchemy.engine.url import URL
 from spinedb_api import (
     apply_alternative_filter_to_parameter_value_sq,
     create_new_spine_database,
     DatabaseMapping,
-    DiffDatabaseMapping,
+    from_database,
     import_alternatives,
     import_object_classes,
     import_object_parameter_values,
     import_object_parameters,
     import_objects,
+    SpineDBAPIError,
 )
 from spinedb_api.filters.alternative_filter import (
     alternative_filter_config,
     alternative_filter_from_dict,
     alternative_filter_config_to_shorthand,
     alternative_filter_shorthand_to_config,
     alternative_names_from_dict,
@@ -44,90 +46,86 @@
     @classmethod
     def setUpClass(cls):
         cls._temp_dir = TemporaryDirectory()
         cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_scenario_filter_mapping.sqlite").as_posix())
 
     def setUp(self):
         create_new_spine_database(self._db_url)
-        self._out_map = DiffDatabaseMapping(self._db_url)
+        self._out_db_map = DatabaseMapping(self._db_url)
         self._db_map = DatabaseMapping(self._db_url)
-        self._diff_db_map = DiffDatabaseMapping(self._db_url)
 
     def tearDown(self):
-        self._out_map.connection.close()
-        self._db_map.connection.close()
-        self._diff_db_map.connection.close()
+        self._out_db_map.close()
+        self._db_map.close()
 
     def test_alternative_filter_without_scenarios_or_alternatives(self):
         self._build_data_without_alternatives()
-        self._out_map.commit_session("Add test data")
-        for db_map in [self._db_map, self._diff_db_map]:
-            apply_alternative_filter_to_parameter_value_sq(db_map, [])
-            parameters = db_map.query(db_map.parameter_value_sq).all()
-            self.assertEqual(parameters, [])
+        self._out_db_map.commit_session("Add test data")
+        apply_alternative_filter_to_parameter_value_sq(self._db_map, [])
+        parameters = self._db_map.query(self._db_map.parameter_value_sq).all()
+        self.assertEqual(parameters, [])
 
     def test_alternative_filter_without_scenarios_or_alternatives_uncommitted_data(self):
         self._build_data_without_alternatives()
-        apply_alternative_filter_to_parameter_value_sq(self._out_map, alternatives=[])
-        parameters = self._out_map.query(self._out_map.parameter_value_sq).all()
+        apply_alternative_filter_to_parameter_value_sq(self._out_db_map, alternatives=[])
+        parameters = self._out_db_map.query(self._out_db_map.parameter_value_sq).all()
         self.assertEqual(parameters, [])
-        self._out_map.rollback_session()
+        self._out_db_map.rollback_session()
 
     def test_alternative_filter(self):
         self._build_data_with_single_alternative()
-        self._out_map.commit_session("Add test data")
-        for db_map in [self._db_map, self._diff_db_map]:
-            apply_alternative_filter_to_parameter_value_sq(db_map, ["alternative"])
-            parameters = db_map.query(db_map.parameter_value_sq).all()
-            self.assertEqual(len(parameters), 1)
-            self.assertEqual(parameters[0].value, b"23.0")
+        self._out_db_map.commit_session("Add test data")
+        apply_alternative_filter_to_parameter_value_sq(self._db_map, ["alternative"])
+        parameters = self._db_map.query(self._db_map.parameter_value_sq).all()
+        self.assertEqual(len(parameters), 1)
+        self.assertEqual(parameters[0].value, b"23.0")
 
     def test_alternative_filter_uncommitted_data(self):
         self._build_data_with_single_alternative()
-        apply_alternative_filter_to_parameter_value_sq(self._out_map, ["alternative"])
-        parameters = self._out_map.query(self._out_map.parameter_value_sq).all()
-        self.assertEqual(len(parameters), 1)
-        self.assertEqual(parameters[0].value, b"23.0")
-        self._out_map.rollback_session()
+        with self.assertRaises(SpineDBAPIError):
+            apply_alternative_filter_to_parameter_value_sq(self._out_db_map, ["alternative"])
+        parameters = self._out_db_map.query(self._out_db_map.parameter_value_sq).all()
+        self.assertEqual(len(parameters), 0)
+        self._out_db_map.rollback_session()
 
     def test_alternative_filter_from_dict(self):
         self._build_data_with_single_alternative()
-        self._out_map.commit_session("Add test data")
+        self._out_db_map.commit_session("Add test data")
         config = alternative_filter_config(["alternative"])
         alternative_filter_from_dict(self._db_map, config)
         parameters = self._db_map.query(self._db_map.parameter_value_sq).all()
         self.assertEqual(len(parameters), 1)
         self.assertEqual(parameters[0].value, b"23.0")
 
     def _build_data_without_alternatives(self):
-        import_object_classes(self._out_map, ["object_class"])
-        import_objects(self._out_map, [("object_class", "object")])
-        import_object_parameters(self._out_map, [("object_class", "parameter")])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 23.0)])
+        import_object_classes(self._out_db_map, ["object_class"])
+        import_objects(self._out_db_map, [("object_class", "object")])
+        import_object_parameters(self._out_db_map, [("object_class", "parameter")])
+        import_object_parameter_values(self._out_db_map, [("object_class", "object", "parameter", 23.0)])
 
     def _build_data_with_single_alternative(self):
-        import_alternatives(self._out_map, ["alternative"])
-        import_object_classes(self._out_map, ["object_class"])
-        import_objects(self._out_map, [("object_class", "object")])
-        import_object_parameters(self._out_map, [("object_class", "parameter")])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", -1.0)])
-        import_object_parameter_values(self._out_map, [("object_class", "object", "parameter", 23.0, "alternative")])
+        import_alternatives(self._out_db_map, ["alternative"])
+        import_object_classes(self._out_db_map, ["object_class"])
+        import_objects(self._out_db_map, [("object_class", "object")])
+        import_object_parameters(self._out_db_map, [("object_class", "parameter")])
+        import_object_parameter_values(self._out_db_map, [("object_class", "object", "parameter", -1.0)])
+        import_object_parameter_values(self._out_db_map, [("object_class", "object", "parameter", 23.0, "alternative")])
 
 
 class TestAlternativeFilterWithMemoryDatabase(unittest.TestCase):
     def setUp(self):
         self._db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(self._db_map, ["object_class"])
         import_objects(self._db_map, [("object_class", "object")])
         import_object_parameters(self._db_map, [("object_class", "parameter")])
         import_object_parameter_values(self._db_map, [("object_class", "object", "parameter", -1.0)])
         self._db_map.commit_session("Add initial data.")
 
     def tearDown(self):
-        self._db_map.connection.close()
+        self._db_map.close()
 
     def test_alternative_names_with_colons(self):
         self._add_value_in_alternative(23.0, "new@2023-23-23T11:12:13")
         config = alternative_filter_config(["new@2023-23-23T11:12:13"])
         alternative_filter_from_dict(self._db_map, config)
         parameters = self._db_map.query(self._db_map.parameter_value_sq).all()
         self.assertEqual(len(parameters), 1)
@@ -136,16 +134,16 @@
     def test_multiple_alternatives(self):
         self._add_value_in_alternative(23.0, "new@2023-23-23T11:12:13")
         self._add_value_in_alternative(101.1, "new@2005-05-05T22:23:24")
         config = alternative_filter_config(["new@2005-05-05T22:23:24", "new@2023-23-23T11:12:13"])
         alternative_filter_from_dict(self._db_map, config)
         parameters = self._db_map.query(self._db_map.parameter_value_sq).all()
         self.assertEqual(len(parameters), 2)
-        self.assertEqual(parameters[0].value, b"23.0")
-        self.assertEqual(parameters[1].value, b"101.1")
+        values = {from_database(p.value) for p in parameters}
+        self.assertEqual(values, {23.0, 101.1})
 
     def _add_value_in_alternative(self, value, alternative):
         import_alternatives(self._db_map, [alternative])
         import_object_parameter_values(self._db_map, [("object_class", "object", "parameter", value, alternative)])
         self._db_map.commit_session(f"Add value in {alternative}")
 
 
@@ -168,9 +166,9 @@
         self.assertEqual(config, {"type": "alternative_filter", "alternatives": ["alternative1", "alternative2"]})
 
     def test_quoted_alternative_names(self):
         config = alternative_filter_shorthand_to_config("alternatives:'alt:er:na:ti:ve':'alternative2'")
         self.assertEqual(config, {"type": "alternative_filter", "alternatives": ["alt:er:na:ti:ve", "alternative2"]})
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/filters/test_renamer.py` & `spinedb_api-0.31.0/tests/filters/test_renamer.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,32 +1,29 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
-
-"""
-Unit tests for ``renamer`` module.
-
-"""
+""" Unit tests for ``renamer`` module. """
 from pathlib import Path
 from tempfile import TemporaryDirectory
 import unittest
 from sqlalchemy.engine.url import URL
 from spinedb_api import (
     apply_renaming_to_parameter_definition_sq,
     apply_renaming_to_entity_class_sq,
     create_new_spine_database,
     DatabaseMapping,
-    DiffDatabaseMapping,
+    DatabaseMapping,
     import_object_classes,
     import_object_parameters,
     import_relationship_classes,
 )
 from spinedb_api.filters.renamer import (
     entity_class_renamer_config,
     entity_class_renamer_config_to_shorthand,
@@ -43,59 +40,59 @@
     @classmethod
     def setUpClass(cls):
         cls._temp_dir = TemporaryDirectory()
         cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_entity_class_renamer.sqlite").as_posix())
 
     def setUp(self):
         create_new_spine_database(self._db_url)
-        self._out_map = DiffDatabaseMapping(self._db_url)
+        self._out_db_map = DatabaseMapping(self._db_url)
         self._db_map = DatabaseMapping(self._db_url)
 
     def tearDown(self):
-        self._out_map.connection.close()
-        self._db_map.connection.close()
+        self._out_db_map.close()
+        self._db_map.close()
 
     def test_renaming_empty_database(self):
         apply_renaming_to_entity_class_sq(self._db_map, {"some_name": "another_name"})
         classes = list(self._db_map.query(self._db_map.entity_class_sq).all())
         self.assertEqual(classes, [])
 
     def test_renaming_singe_entity_class(self):
-        import_object_classes(self._out_map, ("old_name",))
-        self._out_map.commit_session("Add test data")
+        import_object_classes(self._out_db_map, ("old_name",))
+        self._out_db_map.commit_session("Add test data")
         apply_renaming_to_entity_class_sq(self._db_map, {"old_name": "new_name"})
         classes = list(self._db_map.query(self._db_map.entity_class_sq).all())
         self.assertEqual(len(classes), 1)
         class_row = classes[0]
         keys = tuple(class_row.keys())
-        expected_keys = ("id", "type_id", "name", "description", "display_order", "display_icon", "hidden", "commit_id")
+        expected_keys = ("id", "name", "description", "display_order", "display_icon", "hidden", "active_by_default")
         self.assertEqual(len(keys), len(expected_keys))
         for expected_key in expected_keys:
             self.assertIn(expected_key, keys)
         self.assertEqual(class_row.name, "new_name")
 
     def test_renaming_singe_relationship_class(self):
-        import_object_classes(self._out_map, ("object_class",))
-        import_relationship_classes(self._out_map, (("old_name", ("object_class",)),))
-        self._out_map.commit_session("Add test data")
+        import_object_classes(self._out_db_map, ("object_class",))
+        import_relationship_classes(self._out_db_map, (("old_name", ("object_class",)),))
+        self._out_db_map.commit_session("Add test data")
         apply_renaming_to_entity_class_sq(self._db_map, {"old_name": "new_name"})
         classes = list(self._db_map.query(self._db_map.relationship_class_sq).all())
         self.assertEqual(len(classes), 1)
         self.assertEqual(classes[0].name, "new_name")
 
     def test_renaming_multiple_entity_classes(self):
-        import_object_classes(self._out_map, ("object_class1", "object_class2"))
+        import_object_classes(self._out_db_map, ("object_class1", "object_class2"))
         import_relationship_classes(
-            self._out_map,
+            self._out_db_map,
             (
                 ("relationship_class1", ("object_class1", "object_class2")),
                 ("relationship_class2", ("object_class2", "object_class1")),
             ),
         )
-        self._out_map.commit_session("Add test data")
+        self._out_db_map.commit_session("Add test data")
         apply_renaming_to_entity_class_sq(
             self._db_map, {"object_class1": "new_object_class", "relationship_class1": "new_relationship_class"}
         )
         object_classes = list(self._db_map.query(self._db_map.object_class_sq).all())
         self.assertEqual(len(object_classes), 2)
         names = [row.name for row in object_classes]
         for expected_name in ["new_object_class", "object_class2"]:
@@ -112,23 +109,23 @@
     def test_entity_class_renamer_config(self):
         config = entity_class_renamer_config(class1="renamed1", class2="renamed2")
         self.assertEqual(
             config, {"type": "entity_class_renamer", "name_map": {"class1": "renamed1", "class2": "renamed2"}}
         )
 
     def test_entity_class_renamer_from_dict(self):
-        import_object_classes(self._out_map, ("old_name",))
-        self._out_map.commit_session("Add test data")
+        import_object_classes(self._out_db_map, ("old_name",))
+        self._out_db_map.commit_session("Add test data")
         config = entity_class_renamer_config(old_name="new_name")
         entity_class_renamer_from_dict(self._db_map, config)
         classes = list(self._db_map.query(self._db_map.entity_class_sq).all())
         self.assertEqual(len(classes), 1)
         class_row = classes[0]
         keys = tuple(class_row.keys())
-        expected_keys = ("id", "type_id", "name", "description", "display_order", "display_icon", "hidden", "commit_id")
+        expected_keys = ("id", "name", "description", "display_order", "display_icon", "hidden", "active_by_default")
         self.assertEqual(len(keys), len(expected_keys))
         for expected_key in expected_keys:
             self.assertIn(expected_key, keys)
         self.assertEqual(class_row.name, "new_name")
 
 
 class TestEntityClassRenamerWithoutDatabase(unittest.TestCase):
@@ -148,57 +145,55 @@
     @classmethod
     def setUpClass(cls):
         cls._temp_dir = TemporaryDirectory()
         cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_parameter_renamer.sqlite").as_posix())
 
     def setUp(self):
         create_new_spine_database(self._db_url)
-        self._out_map = DiffDatabaseMapping(self._db_url)
+        self._out_db_map = DatabaseMapping(self._db_url)
         self._db_map = DatabaseMapping(self._db_url)
 
     def tearDown(self):
-        self._out_map.connection.close()
-        self._db_map.connection.close()
+        self._out_db_map.close()
+        self._db_map.close()
 
     def test_renaming_empty_database(self):
         apply_renaming_to_parameter_definition_sq(self._db_map, {"some_name": "another_name"})
         classes = list(self._db_map.query(self._db_map.parameter_definition_sq).all())
         self.assertEqual(classes, [])
 
     def test_renaming_single_parameter(self):
-        import_object_classes(self._out_map, ("object_class",))
-        import_object_parameters(self._out_map, (("object_class", "old_name"),))
-        self._out_map.commit_session("Add test data")
+        import_object_classes(self._out_db_map, ("object_class",))
+        import_object_parameters(self._out_db_map, (("object_class", "old_name"),))
+        self._out_db_map.commit_session("Add test data")
         apply_renaming_to_parameter_definition_sq(self._db_map, {"object_class": {"old_name": "new_name"}})
         parameters = list(self._db_map.query(self._db_map.parameter_definition_sq).all())
         self.assertEqual(len(parameters), 1)
         parameter_row = parameters[0]
         keys = tuple(parameter_row.keys())
         expected_keys = (
             "id",
             "name",
             "description",
             "entity_class_id",
-            "object_class_id",
-            "relationship_class_id",
             "default_value",
             "default_type",
             "list_value_id",
             "commit_id",
             "parameter_value_list_id",
         )
         self.assertEqual(len(keys), len(expected_keys))
         for expected_key in expected_keys:
             self.assertIn(expected_key, keys)
         self.assertEqual(parameter_row.name, "new_name")
 
     def test_renaming_applies_to_correct_parameter(self):
-        import_object_classes(self._out_map, ("oc1", "oc2"))
-        import_object_parameters(self._out_map, (("oc1", "param"), ("oc2", "param")))
-        self._out_map.commit_session("Add test data")
+        import_object_classes(self._out_db_map, ("oc1", "oc2"))
+        import_object_parameters(self._out_db_map, (("oc1", "param"), ("oc2", "param")))
+        self._out_db_map.commit_session("Add test data")
         apply_renaming_to_parameter_definition_sq(self._db_map, {"oc2": {"param": "new_name"}})
         parameters = list(self._db_map.query(self._db_map.entity_parameter_definition_sq).all())
         self.assertEqual(len(parameters), 2)
         for parameter_row in parameters:
             if parameter_row.entity_class_name == "oc2":
                 self.assertEqual(parameter_row.parameter_name, "new_name")
             else:
@@ -208,30 +203,28 @@
         config = parameter_renamer_config({"class": {"parameter1": "renamed1", "parameter2": "renamed2"}})
         self.assertEqual(
             config,
             {"type": "parameter_renamer", "name_map": {"class": {"parameter1": "renamed1", "parameter2": "renamed2"}}},
         )
 
     def test_parameter_renamer_from_dict(self):
-        import_object_classes(self._out_map, ("object_class",))
-        import_object_parameters(self._out_map, (("object_class", "old_name"),))
-        self._out_map.commit_session("Add test data")
+        import_object_classes(self._out_db_map, ("object_class",))
+        import_object_parameters(self._out_db_map, (("object_class", "old_name"),))
+        self._out_db_map.commit_session("Add test data")
         config = parameter_renamer_config({"object_class": {"old_name": "new_name"}})
         parameter_renamer_from_dict(self._db_map, config)
         parameters = list(self._db_map.query(self._db_map.parameter_definition_sq).all())
         self.assertEqual(len(parameters), 1)
         parameter_row = parameters[0]
         keys = tuple(parameter_row.keys())
         expected_keys = (
             "id",
             "name",
             "description",
             "entity_class_id",
-            "object_class_id",
-            "relationship_class_id",
             "default_value",
             "default_type",
             "list_value_id",
             "commit_id",
             "parameter_value_list_id",
         )
         self.assertEqual(len(keys), len(expected_keys))
```

### Comparing `spinedb_api-0.30.5/tests/filters/test_tool_filter.py` & `spinedb_api-0.31.0/tests/filters/test_tool_filter.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -14,55 +15,45 @@
 
 """
 from pathlib import Path
 from tempfile import TemporaryDirectory
 import unittest
 from sqlalchemy.engine.url import URL
 from spinedb_api import (
-    apply_tool_filter_to_entity_sq,
     create_new_spine_database,
-    DiffDatabaseMapping,
+    DatabaseMapping,
     import_object_classes,
     import_relationship_classes,
     import_object_parameter_values,
     import_object_parameters,
     import_objects,
     import_relationships,
     import_relationship_parameter_values,
     import_relationship_parameters,
     import_parameter_value_lists,
-    import_tools,
-    import_features,
-    import_tool_features,
-    import_tool_feature_methods,
     SpineDBAPIError,
 )
-from spinedb_api.filters.tool_filter import (
-    tool_filter_config,
-    tool_filter_config_to_shorthand,
-    tool_filter_from_dict,
-    tool_filter_shorthand_to_config,
-)
 
 
+@unittest.skip("obsolete, but need to adapt into the scenario filter")
 class TestToolEntityFilter(unittest.TestCase):
     _db_url = None
     _temp_dir = None
 
     @classmethod
     def setUpClass(cls):
         cls._temp_dir = TemporaryDirectory()
         cls._db_url = URL("sqlite", database=Path(cls._temp_dir.name, "test_tool_filter_mapping.sqlite").as_posix())
 
     def setUp(self):
         create_new_spine_database(self._db_url)
-        self._db_map = DiffDatabaseMapping(self._db_url)
+        self._db_map = DatabaseMapping(self._db_url)
 
     def tearDown(self):
-        self._db_map.connection.close()
+        self._db_map.close()
 
     def _build_data_with_tools(self):
         import_object_classes(self._db_map, ["object_class"])
         import_objects(
             self._db_map,
             [
                 ("object_class", "object1"),
@@ -222,9 +213,9 @@
         self.assertTrue("node1" not in relationship_object_names)
         ent_pvals = self._db_map.query(self._db_map.entity_parameter_value_sq).all()
         self.assertEqual(len(ent_pvals), 1)
         pval_object_names = ent_pvals[0].object_name_list.split(",")
         self.assertTrue("node1" not in pval_object_names)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/filters/test_tools.py` & `spinedb_api-0.31.0/tests/filters/test_tools.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -16,16 +17,15 @@
 from tempfile import TemporaryDirectory
 import unittest
 from sqlalchemy.engine.url import URL
 from spinedb_api import (
     append_filter_config,
     clear_filter_configs,
     DatabaseMapping,
-    DiffDatabaseMapping,
-    export_object_classes,
+    export_entity_classes,
     import_object_classes,
     pop_filter_configs,
 )
 from spinedb_api.filters.tools import (
     apply_filter_stack,
     ensure_filtering,
     filter_configs,
@@ -87,111 +87,111 @@
     _db_url = URL("sqlite")
     _dir = None
 
     @classmethod
     def setUpClass(cls):
         cls._dir = TemporaryDirectory()
         cls._db_url.database = os.path.join(cls._dir.name, ".json")
-        db_map = DiffDatabaseMapping(cls._db_url, create=True)
+        db_map = DatabaseMapping(cls._db_url, create=True)
         import_object_classes(db_map, ("object_class",))
         db_map.commit_session("Add test data.")
-        db_map.connection.close()
+        db_map.close()
 
     def test_empty_stack(self):
         db_map = DatabaseMapping(self._db_url)
         try:
             apply_filter_stack(db_map, [])
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("object_class", None, None)])
+            object_classes = export_entity_classes(db_map)
+            self.assertEqual(object_classes, [("object_class", (), None, None, False)])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
     def test_single_renaming_filter(self):
         db_map = DatabaseMapping(self._db_url)
         try:
             stack = [entity_class_renamer_config(object_class="renamed_once")]
             apply_filter_stack(db_map, stack)
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("renamed_once", None, None)])
+            object_classes = export_entity_classes(db_map)
+            self.assertEqual(object_classes, [("renamed_once", (), None, None, False)])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
     def test_two_renaming_filters(self):
         db_map = DatabaseMapping(self._db_url)
         try:
             stack = [
                 entity_class_renamer_config(object_class="renamed_once"),
                 entity_class_renamer_config(renamed_once="renamed_twice"),
             ]
             apply_filter_stack(db_map, stack)
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("renamed_twice", None, None)])
+            object_classes = export_entity_classes(db_map)
+            self.assertEqual(object_classes, [("renamed_twice", (), None, None, False)])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
 
 class TestFilteredDatabaseMap(unittest.TestCase):
     _db_url = URL("sqlite")
     _dir = None
     _engine = None
 
     @classmethod
     def setUpClass(cls):
         cls._dir = TemporaryDirectory()
         cls._db_url.database = os.path.join(cls._dir.name, "TestFilteredDatabaseMap.json")
-        db_map = DiffDatabaseMapping(cls._db_url, create=True)
+        db_map = DatabaseMapping(cls._db_url, create=True)
         import_object_classes(db_map, ("object_class",))
         db_map.commit_session("Add test data.")
-        db_map.connection.close()
+        db_map.close()
 
     def test_without_filters(self):
         db_map = DatabaseMapping(self._db_url, self._engine)
         try:
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("object_class", None, None)])
+            object_classes = export_entity_classes(db_map)
+            self.assertEqual(object_classes, [("object_class", (), None, None, False)])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
     def test_single_renaming_filter(self):
         path = os.path.join(self._dir.name, "config.json")
         with open(path, "w") as out_file:
             store_filter(entity_class_renamer_config(object_class="renamed_once"), out_file)
         url = append_filter_config(str(self._db_url), path)
         db_map = DatabaseMapping(url, self._engine)
         try:
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("renamed_once", None, None)])
+            object_classes = export_entity_classes(db_map)
+            self.assertEqual(object_classes, [("renamed_once", (), None, None, False)])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
     def test_two_renaming_filters(self):
         path1 = os.path.join(self._dir.name, "config1.json")
         with open(path1, "w") as out_file:
             store_filter(entity_class_renamer_config(object_class="renamed_once"), out_file)
         url = append_filter_config(str(self._db_url), path1)
         path2 = os.path.join(self._dir.name, "config2.json")
         with open(path2, "w") as out_file:
             store_filter(entity_class_renamer_config(renamed_once="renamed_twice"), out_file)
         url = append_filter_config(url, path2)
         db_map = DatabaseMapping(url, self._engine)
         try:
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("renamed_twice", None, None)])
+            object_classes = export_entity_classes(db_map)
+            self.assertEqual(object_classes, [("renamed_twice", (), None, None, False)])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
     def test_config_embedded_to_url(self):
         config = entity_class_renamer_config(object_class="renamed_once")
         url = append_filter_config(str(self._db_url), config)
         db_map = DatabaseMapping(url, self._engine)
         try:
-            object_classes = export_object_classes(db_map)
-            self.assertEqual(object_classes, [("renamed_once", None, None)])
+            object_classes = export_entity_classes(db_map)
+            self.assertEqual(object_classes, [("renamed_once", (), None, None, False)])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
 
 class TestAppendFilterConfig(unittest.TestCase):
     def test_append_to_simple_url(self):
         url = append_filter_config(r"sqlite:///C:\dbs\database.sqlite", r"F:\fltr\a.json")
         self.assertEqual(url, r"sqlite:///C:\dbs\database.sqlite?spinedbfilter=F%3A%5Cfltr%5Ca.json")
 
@@ -302,16 +302,16 @@
 
 class TestNameFromDict(unittest.TestCase):
     def test_get_scenario_name(self):
         config = filter_config("scenario_filter", "scenario_name")
         self.assertEqual(name_from_dict(config), "scenario_name")
 
     def test_get_tool_name(self):
-        config = filter_config("tool_filter", "tool_name")
-        self.assertEqual(name_from_dict(config), "tool_name")
+        with self.assertRaises(KeyError):
+            _ = filter_config("tool_filter", "tool_name")
 
     def test_returns_none_if_name_not_found(self):
         config = entity_class_renamer_config(name="rename")
         self.assertIsNone(name_from_dict(config))
 
 
 if __name__ == "__main__":
```

### Comparing `spinedb_api-0.30.5/tests/filters/test_value_transformer.py` & `spinedb_api-0.31.0/tests/filters/test_value_transformer.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -13,18 +14,16 @@
 Unit tests for ``value_transformer`` module.
 
 """
 from pathlib import Path
 import unittest
 from tempfile import TemporaryDirectory
 from sqlalchemy.engine.url import URL
-
 from spinedb_api import (
     DatabaseMapping,
-    DiffDatabaseMapping,
     import_object_classes,
     import_object_parameter_values,
     import_object_parameters,
     import_objects,
     append_filter_config,
     create_new_spine_database,
     from_database,
@@ -104,115 +103,115 @@
 
     @classmethod
     def tearDownClass(cls):
         cls._temp_dir.cleanup()
 
     def setUp(self):
         create_new_spine_database(self._db_url)
-        self._out_map = DiffDatabaseMapping(self._db_url)
+        self._out_db_map = DatabaseMapping(self._db_url)
 
     def tearDown(self):
-        self._out_map.connection.close()
+        self._out_db_map.close()
 
     def test_negate_manipulator(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
-        self._out_map.commit_session("Add test data.")
+        import_object_classes(self._out_db_map, ("class",))
+        import_object_parameters(self._out_db_map, (("class", "parameter"),))
+        import_objects(self._out_db_map, (("class", "object"),))
+        import_object_parameter_values(self._out_db_map, (("class", "object", "parameter", -2.3),))
+        self._out_db_map.commit_session("Add test data.")
         instructions = {"class": {"parameter": [{"operation": "negate"}]}}
         config = value_transformer_config(instructions)
         url = append_filter_config(str(self._db_url), config)
         db_map = DatabaseMapping(url)
         try:
             values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
             self.assertEqual(values, [2.3])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
     def test_negate_manipulator_with_nested_map(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
+        import_object_classes(self._out_db_map, ("class",))
+        import_object_parameters(self._out_db_map, (("class", "parameter"),))
+        import_objects(self._out_db_map, (("class", "object"),))
         value = Map(["A"], [Map(["1"], [2.3])])
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", value),))
-        self._out_map.commit_session("Add test data.")
+        import_object_parameter_values(self._out_db_map, (("class", "object", "parameter", value),))
+        self._out_db_map.commit_session("Add test data.")
         instructions = {"class": {"parameter": [{"operation": "negate"}]}}
         config = value_transformer_config(instructions)
         url = append_filter_config(str(self._db_url), config)
         db_map = DatabaseMapping(url)
         try:
             values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
             expected = Map(["A"], [Map(["1"], [-2.3])])
             self.assertEqual(values, [expected])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
     def test_multiply_manipulator(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
-        self._out_map.commit_session("Add test data.")
+        import_object_classes(self._out_db_map, ("class",))
+        import_object_parameters(self._out_db_map, (("class", "parameter"),))
+        import_objects(self._out_db_map, (("class", "object"),))
+        import_object_parameter_values(self._out_db_map, (("class", "object", "parameter", -2.3),))
+        self._out_db_map.commit_session("Add test data.")
         instructions = {"class": {"parameter": [{"operation": "multiply", "rhs": 10.0}]}}
         config = value_transformer_config(instructions)
         url = append_filter_config(str(self._db_url), config)
         db_map = DatabaseMapping(url)
         try:
             values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
             self.assertEqual(values, [-23.0])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
     def test_invert_manipulator(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
-        self._out_map.commit_session("Add test data.")
+        import_object_classes(self._out_db_map, ("class",))
+        import_object_parameters(self._out_db_map, (("class", "parameter"),))
+        import_objects(self._out_db_map, (("class", "object"),))
+        import_object_parameter_values(self._out_db_map, (("class", "object", "parameter", -2.3),))
+        self._out_db_map.commit_session("Add test data.")
         instructions = {"class": {"parameter": [{"operation": "invert"}]}}
         config = value_transformer_config(instructions)
         url = append_filter_config(str(self._db_url), config)
         db_map = DatabaseMapping(url)
         try:
             values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
             self.assertEqual(values, [-1.0 / 2.3])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
     def test_multiple_instructions(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", -2.3),))
-        self._out_map.commit_session("Add test data.")
+        import_object_classes(self._out_db_map, ("class",))
+        import_object_parameters(self._out_db_map, (("class", "parameter"),))
+        import_objects(self._out_db_map, (("class", "object"),))
+        import_object_parameter_values(self._out_db_map, (("class", "object", "parameter", -2.3),))
+        self._out_db_map.commit_session("Add test data.")
         instructions = {"class": {"parameter": [{"operation": "invert"}, {"operation": "negate"}]}}
         config = value_transformer_config(instructions)
         url = append_filter_config(str(self._db_url), config)
         db_map = DatabaseMapping(url)
         try:
             values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
             self.assertEqual(values, [1.0 / 2.3])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
     def test_index_generator_on_time_series(self):
-        import_object_classes(self._out_map, ("class",))
-        import_object_parameters(self._out_map, (("class", "parameter"),))
-        import_objects(self._out_map, (("class", "object"),))
+        import_object_classes(self._out_db_map, ("class",))
+        import_object_parameters(self._out_db_map, (("class", "parameter"),))
+        import_objects(self._out_db_map, (("class", "object"),))
         value = TimeSeriesFixedResolution("2021-06-07T08:00", "1D", [-5.0, -2.3], False, False)
-        import_object_parameter_values(self._out_map, (("class", "object", "parameter", value),))
-        self._out_map.commit_session("Add test data.")
+        import_object_parameter_values(self._out_db_map, (("class", "object", "parameter", value),))
+        self._out_db_map.commit_session("Add test data.")
         instructions = {"class": {"parameter": [{"operation": "generate_index", "expression": "float(i)"}]}}
         config = value_transformer_config(instructions)
         url = append_filter_config(str(self._db_url), config)
         db_map = DatabaseMapping(url)
         try:
             values = [from_database(row.value, row.type) for row in db_map.query(db_map.parameter_value_sq)]
             expected = Map([1.0, 2.0], [-5.0, -2.3])
             self.assertEqual(values, [expected])
         finally:
-            db_map.connection.close()
+            db_map.close()
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/import_mapping/__init__.py` & `spinedb_api-0.31.0/tests/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,16 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
+
+"""
+Unit tests package for :mod:`spinedb_api`.
+
+"""
```

### Comparing `spinedb_api-0.30.5/tests/import_mapping/test_generator.py` & `spinedb_api-0.31.0/tests/import_mapping/test_generator.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -65,19 +66,19 @@
         convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
 
         mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
         self.assertEqual(errors, ["Could not process incomplete row 2"])
         self.assertEqual(
             mapped_data,
             {
-                'alternatives': {'Base'},
-                'object_classes': {'Object'},
-                'object_parameter_values': [['Object', 'data', 'Parameter', Map(["T1", "T2"], [5.0, 99.0]), 'Base']],
-                'object_parameters': [('Object', 'Parameter')],
-                'objects': {('Object', 'data')},
+                "alternatives": {"Base"},
+                "entity_classes": [("Object",)],
+                "parameter_values": [["Object", "data", "Parameter", Map(["T1", "T2"], [5.0, 99.0]), "Base"]],
+                "parameter_definitions": [("Object", "Parameter")],
+                "entities": [("Object", "data")],
             },
         )
 
     def test_convert_functions_get_expanded_over_last_defined_column_in_pivoted_data(self):
         data_source = iter([["", "T1", "T2"], ["Parameter", "5.0", "99.0"]])
         mappings = [
             [
@@ -97,19 +98,19 @@
         convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
 
         mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
         self.assertEqual(errors, [])
         self.assertEqual(
             mapped_data,
             {
-                'alternatives': {'Base'},
-                'object_classes': {'Object'},
-                'object_parameter_values': [['Object', 'data', 'Parameter', Map(["T1", "T2"], [5.0, 99.0]), 'Base']],
-                'object_parameters': [('Object', 'Parameter')],
-                'objects': {('Object', 'data')},
+                "alternatives": {"Base"},
+                "entity_classes": [("Object",)],
+                "parameter_values": [["Object", "data", "Parameter", Map(["T1", "T2"], [5.0, 99.0]), "Base"]],
+                "parameter_definitions": [("Object", "Parameter")],
+                "entities": [("Object", "data")],
             },
         )
 
     def test_read_start_row_skips_rows_in_pivoted_data(self):
         data_source = iter([["", "T1", "T2"], ["Parameter_1", "5.0", "99.0"], ["Parameter_2", "2.3", "23.0"]])
         mappings = [
             [
@@ -129,18 +130,18 @@
         convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
 
         mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
         self.assertEqual(errors, [])
         self.assertEqual(
             mapped_data,
             {
-                'object_classes': {'klass'},
-                'object_parameter_values': [['klass', 'kloss', 'Parameter_2', Map(["T1", "T2"], [2.3, 23.0])]],
-                'object_parameters': [('klass', 'Parameter_2')],
-                'objects': {('klass', 'kloss')},
+                "entity_classes": [("klass",)],
+                "parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], [2.3, 23.0])]],
+                "parameter_definitions": [("klass", "Parameter_2")],
+                "entities": [("klass", "kloss")],
             },
         )
 
     def test_empty_pivoted_data_is_skipped(self):
         data_header = ["period", "time"]
         data_source = iter([["p2020", "t0"], ["p2020", "t1"]])
         mappings = [
@@ -183,18 +184,18 @@
         convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
         mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
         self.assertEqual(errors, [])
         self.assertEqual(
             mapped_data,
             {
                 "alternatives": {"base"},
-                "object_classes": {"o"},
-                "object_parameters": [("o", "parameter_name")],
-                "object_parameter_values": [],
-                "objects": {("o", "o1")},
+                "entity_classes": [("o",)],
+                "parameter_definitions": [("o", "parameter_name")],
+                "parameter_values": [],
+                "entities": [("o", "o1")],
             },
         )
 
     def test_import_object_works_with_multiple_relationship_object_imports(self):
         header = ["time", "relationship 1", "relationship 2", "relationship 3"]
         data_source = iter([[None, "o1", "o2", "o1"], [None, "q1", "q2", "q2"], ["t1", 11, 33, 55], ["t2", 22, 44, 66]])
         mappings = [
@@ -219,20 +220,26 @@
         convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
         mapped_data, errors = get_mapped_data(data_source, mappings, header, column_convert_fns=convert_functions)
         self.assertEqual(errors, [])
         self.assertEqual(
             mapped_data,
             {
                 "alternatives": {"base"},
-                "object_classes": {"o", "q"},
-                "objects": {("o", "o1"), ("o", "o2"), ("q", "q1"), ("q", "q2")},
-                "relationship_classes": [("o_to_q", ["o", "q"])],
-                "relationships": {("o_to_q", ("o1", "q1")), ("o_to_q", ("o1", "q2")), ("o_to_q", ("o2", "q2"))},
-                "relationship_parameters": [("o_to_q", "param")],
-                "relationship_parameter_values": [
+                "entity_classes": [("o",), ("q",), ("o_to_q", ("o", "q"))],
+                "entities": [
+                    ("o", "o1"),
+                    ("q", "q1"),
+                    ("o_to_q", ("o1", "q1")),
+                    ("o", "o2"),
+                    ("q", "q2"),
+                    ("o_to_q", ("o2", "q2")),
+                    ("o_to_q", ("o1", "q2")),
+                ],
+                "parameter_definitions": [("o_to_q", "param")],
+                "parameter_values": [
                     ["o_to_q", ("o1", "q1"), "param", Map(["t1", "t2"], [11, 22], index_name="time"), "base"],
                     ["o_to_q", ("o2", "q2"), "param", Map(["t1", "t2"], [33, 44], index_name="time"), "base"],
                     ["o_to_q", ("o1", "q2"), "param", Map(["t1", "t2"], [55, 66], index_name="time"), "base"],
                 ],
             },
         )
 
@@ -257,18 +264,18 @@
         mapped_data, errors = get_mapped_data(
             data_source, mappings, column_convert_fns=convert_functions, default_column_convert_fn=float
         )
         self.assertEqual(errors, [])
         self.assertEqual(
             mapped_data,
             {
-                "object_classes": {"klass"},
-                "object_parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], [2.3, 23.0])]],
-                "object_parameters": [("klass", "Parameter_2")],
-                "objects": {("klass", "kloss")},
+                "entity_classes": [("klass",)],
+                "parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], [2.3, 23.0])]],
+                "parameter_definitions": [("klass", "Parameter_2")],
+                "entities": [("klass", "kloss")],
             },
         )
 
     def test_identity_function_is_used_as_convert_function_when_no_convert_functions_given(self):
         data_source = iter([["", "T1", "T2"], ["Parameter_1", "5.0", "99.0"], ["Parameter_2", "2.3", "23.0"]])
         mappings = [
             [
@@ -285,18 +292,18 @@
             ]
         ]
         mapped_data, errors = get_mapped_data(data_source, mappings)
         self.assertEqual(errors, [])
         self.assertEqual(
             mapped_data,
             {
-                "object_classes": {"klass"},
-                "object_parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], ["2.3", "23.0"])]],
-                "object_parameters": [("klass", "Parameter_2")],
-                "objects": {("klass", "kloss")},
+                "entity_classes": [("klass",)],
+                "parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], ["2.3", "23.0"])]],
+                "parameter_definitions": [("klass", "Parameter_2")],
+                "entities": [("klass", "kloss")],
             },
         )
 
     def test_last_convert_function_gets_used_as_default_convert_function_when_no_default_is_set(self):
         data_source = iter([["", "T1", "T2"], ["Parameter_1", "5.0", "99.0"], ["Parameter_2", "2.3", "23.0"]])
         mappings = [
             [
@@ -315,18 +322,18 @@
         convert_function_specs = {0: "string", 1: "float"}
         convert_functions = {column: value_to_convert_spec(spec) for column, spec in convert_function_specs.items()}
         mapped_data, errors = get_mapped_data(data_source, mappings, column_convert_fns=convert_functions)
         self.assertEqual(errors, [])
         self.assertEqual(
             mapped_data,
             {
-                "object_classes": {"klass"},
-                "object_parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], [2.3, 23.0])]],
-                "object_parameters": [("klass", "Parameter_2")],
-                "objects": {("klass", "kloss")},
+                "entity_classes": [("klass",)],
+                "parameter_values": [["klass", "kloss", "Parameter_2", Map(["T1", "T2"], [2.3, 23.0])]],
+                "parameter_definitions": [("klass", "Parameter_2")],
+                "entities": [("klass", "kloss")],
             },
         )
 
     def test_array_parameters_get_imported_correctly_when_objects_are_in_header(self):
         header = ["object_1", "object_2"]
         data_source = iter([[-1.1, 2.3], [1.1, -2.3]])
         mappings = [
@@ -348,21 +355,21 @@
 
         mapped_data, errors = get_mapped_data(data_source, mappings, header, column_convert_fns=convert_functions)
         self.assertEqual(errors, [])
         self.assertEqual(
             mapped_data,
             {
                 "alternatives": {"Base"},
-                "object_classes": {"class"},
-                "object_parameter_values": [
+                "entity_classes": [("class",)],
+                "parameter_values": [
                     ["class", "object_1", "param", Array([-1.1, 1.1]), "Base"],
                     ["class", "object_2", "param", Array([2.3, -2.3]), "Base"],
                 ],
-                "object_parameters": [("class", "param")],
-                "objects": {("class", "object_1"), ("class", "object_2")},
+                "parameter_definitions": [("class", "param")],
+                "entities": [("class", "object_1"), ("class", "object_2")],
             },
         )
 
     def test_arrays_get_imported_correctly_when_objects_are_in_header_and_alternatives_in_first_row(self):
         header = ["object_1", "object_2"]
         data_source = iter([["Base", "Base"], [-1.1, 2.3], [1.1, -2.3]])
         mappings = [
@@ -384,21 +391,21 @@
 
         mapped_data, errors = get_mapped_data(data_source, mappings, header, column_convert_fns=convert_functions)
         self.assertEqual(errors, [])
         self.assertEqual(
             mapped_data,
             {
                 "alternatives": {"Base"},
-                "object_classes": {"Gadget"},
-                "object_parameter_values": [
+                "entity_classes": [("Gadget",)],
+                "parameter_values": [
                     ["Gadget", "object_1", "data", Array([-1.1, 1.1]), "Base"],
                     ["Gadget", "object_2", "data", Array([2.3, -2.3]), "Base"],
                 ],
-                "object_parameters": [("Gadget", "data")],
-                "objects": {("Gadget", "object_1"), ("Gadget", "object_2")},
+                "parameter_definitions": [("Gadget", "data")],
+                "entities": [("Gadget", "object_1"), ("Gadget", "object_2")],
             },
         )
 
     def test_header_position_is_ignored_in_last_mapping_if_other_mappings_are_in_header(self):
         header = ["Dimension", "parameter1", "parameter2"]
         data_source = iter([["d1", 1.1, -2.3], ["d2", -1.1, 2.3]])
         mappings = [
@@ -419,22 +426,22 @@
             data_source, mappings, header, table_name="Data", column_convert_fns=convert_functions
         )
         self.assertEqual(errors, [])
         self.assertEqual(
             mapped_data,
             {
                 "alternatives": {"Base"},
-                "object_classes": {"Data"},
-                "object_parameter_values": [
+                "entity_classes": [("Data",)],
+                "parameter_values": [
                     ["Data", "d1", "parameter1", 1.1, "Base"],
                     ["Data", "d1", "parameter2", -2.3, "Base"],
                     ["Data", "d2", "parameter1", -1.1, "Base"],
                     ["Data", "d2", "parameter2", 2.3, "Base"],
                 ],
-                "object_parameters": [("Data", "parameter1"), ("Data", "parameter2")],
-                "objects": {("Data", "d1"), ("Data", "d2")},
+                "parameter_definitions": [("Data", "parameter1"), ("Data", "parameter2")],
+                "entities": [("Data", "d1"), ("Data", "d2")],
             },
         )
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/import_mapping/test_import_mapping.py` & `spinedb_api-0.31.0/tests/import_mapping/test_import_mapping.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,32 +1,31 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
-"""
-Unit tests for import Mappings.
-
-"""
+""" Unit tests for import Mappings. """
 import unittest
 from unittest.mock import Mock
 from spinedb_api.exception import InvalidMapping
 from spinedb_api.mapping import Position, to_dict as mapping_to_dict, unflatten
 from spinedb_api.import_mapping.import_mapping import (
+    default_import_mapping,
     ImportMapping,
+    EntityClassMapping,
+    EntityMapping,
     check_validity,
     ParameterDefinitionMapping,
-    ObjectClassMapping,
-    ObjectMapping,
     IndexNameMapping,
     ParameterValueIndexMapping,
     ExpandedParameterValueMapping,
     ParameterValueMapping,
     ParameterValueTypeMapping,
     ParameterDefaultValueTypeMapping,
     DefaultValueIndexNameMapping,
@@ -53,15 +52,19 @@
         mapping.child.value = "obj"
         mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
             {"map_type": "ParameterDefinition"}
         )
         param_def_mapping.value = "param"
         param_def_mapping.flatten()[-1].position = 1
         mapped_data, _ = get_mapped_data(data, [mapping], column_convert_fns=column_convert_fns)
-        expected = {'object_classes': {'a'}, 'objects': {('a', 'obj')}, 'object_parameters': [('a', 'param', 1.2)]}
+        expected = {
+            "entity_classes": [("a",)],
+            "entities": [("a", "obj")],
+            "parameter_definitions": [("a", "param", 1.2)],
+        }
         self.assertEqual(mapped_data, expected)
 
     def test_convert_functions_str(self):
         data = [["a", '"1111.2222"']]
         column_convert_fns = {0: str, 1: StringConvertSpec()}
         mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
         mapping.position = 0
@@ -69,17 +72,17 @@
         mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
             {"map_type": "ParameterDefinition"}
         )
         param_def_mapping.value = "param"
         param_def_mapping.flatten()[-1].position = 1
         mapped_data, _ = get_mapped_data(data, [mapping], column_convert_fns=column_convert_fns)
         expected = {
-            'object_classes': {'a'},
-            'objects': {('a', 'obj')},
-            'object_parameters': [('a', 'param', '1111.2222')],
+            "entity_classes": [("a",)],
+            "entities": [("a", "obj")],
+            "parameter_definitions": [("a", "param", "1111.2222")],
         }
         self.assertEqual(mapped_data, expected)
 
     def test_convert_functions_bool(self):
         data = [["a", "false"]]
         column_convert_fns = {0: str, 1: BooleanConvertSpec()}
         mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
@@ -87,15 +90,19 @@
         mapping.child.value = "obj"
         mapping.flatten()[-1].child = param_def_mapping = parameter_mapping_from_dict(
             {"map_type": "ParameterDefinition"}
         )
         param_def_mapping.value = "param"
         param_def_mapping.flatten()[-1].position = 1
         mapped_data, _ = get_mapped_data(data, [mapping], column_convert_fns=column_convert_fns)
-        expected = {'object_classes': {'a'}, 'objects': {('a', 'obj')}, 'object_parameters': [('a', 'param', False)]}
+        expected = {
+            "entity_classes": [("a",)],
+            "entities": [("a", "obj")],
+            "parameter_definitions": [("a", "param", False)],
+        }
         self.assertEqual(mapped_data, expected)
 
     def test_convert_functions_with_error(self):
         data = [["a", "not a float"]]
         column_convert_fns = {0: str, 1: FloatConvertSpec()}
         mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
         mapping.position = 0
@@ -182,177 +189,157 @@
 
 
 class TestImportMappingIO(unittest.TestCase):
     def test_object_class_mapping(self):
         mapping = import_mapping_from_dict({"map_type": "ObjectClass"})
         d = mapping_to_dict(mapping)
         types = [m["map_type"] for m in d]
-        expected = ['ObjectClass', 'Object', 'ObjectMetadata']
+        expected = ["EntityClass", "Entity", "EntityMetadata"]
         self.assertEqual(types, expected)
 
     def test_relationship_class_mapping(self):
         mapping = import_mapping_from_dict({"map_type": "RelationshipClass"})
         d = mapping_to_dict(mapping)
         types = [m["map_type"] for m in d]
-        expected = [
-            'RelationshipClass',
-            'RelationshipClassObjectClass',
-            'Relationship',
-            'RelationshipObject',
-            'RelationshipMetadata',
-        ]
+        expected = ["EntityClass", "Dimension", "Entity", "Element", "EntityMetadata"]
         self.assertEqual(types, expected)
 
     def test_object_group_mapping(self):
         mapping = import_mapping_from_dict({"map_type": "ObjectGroup"})
         d = mapping_to_dict(mapping)
         types = [m["map_type"] for m in d]
-        expected = ['ObjectClass', 'Object', 'ObjectGroup']
+        expected = ["EntityClass", "Entity", "EntityGroup"]
         self.assertEqual(types, expected)
 
     def test_alternative_mapping(self):
         mapping = import_mapping_from_dict({"map_type": "Alternative"})
         d = mapping_to_dict(mapping)
         types = [m["map_type"] for m in d]
-        expected = ['Alternative']
+        expected = ["Alternative"]
         self.assertEqual(types, expected)
 
     def test_scenario_mapping(self):
         mapping = import_mapping_from_dict({"map_type": "Scenario"})
         d = mapping_to_dict(mapping)
         types = [m["map_type"] for m in d]
-        expected = ['Scenario', 'ScenarioActiveFlag']
+        expected = ["Scenario", "ScenarioActiveFlag"]
         self.assertEqual(types, expected)
 
     def test_scenario_alternative_mapping(self):
         mapping = import_mapping_from_dict({"map_type": "ScenarioAlternative"})
         d = mapping_to_dict(mapping)
         types = [m["map_type"] for m in d]
-        expected = ['Scenario', 'ScenarioAlternative', 'ScenarioBeforeAlternative']
+        expected = ["Scenario", "ScenarioAlternative", "ScenarioBeforeAlternative"]
         self.assertEqual(types, expected)
 
     def test_tool_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "Tool"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = ['Tool']
-        self.assertEqual(types, expected)
+        with self.assertRaises(ValueError):
+            import_mapping_from_dict({"map_type": "Tool"})
 
     def test_tool_feature_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "ToolFeature"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = ['Tool', 'ToolFeatureEntityClass', 'ToolFeatureParameterDefinition', 'ToolFeatureRequiredFlag']
-        self.assertEqual(types, expected)
+        with self.assertRaises(ValueError):
+            import_mapping_from_dict({"map_type": "ToolFeature"})
 
     def test_tool_feature_method_mapping(self):
-        mapping = import_mapping_from_dict({"map_type": "ToolFeatureMethod"})
-        d = mapping_to_dict(mapping)
-        types = [m["map_type"] for m in d]
-        expected = [
-            'Tool',
-            'ToolFeatureMethodEntityClass',
-            'ToolFeatureMethodParameterDefinition',
-            'ToolFeatureMethodMethod',
-        ]
-        self.assertEqual(types, expected)
+        with self.assertRaises(ValueError):
+            import_mapping_from_dict({"map_type": "ToolFeatureMethod"})
 
     def test_parameter_value_list_mapping(self):
         mapping = import_mapping_from_dict({"map_type": "ParameterValueList"})
         d = mapping_to_dict(mapping)
         types = [m["map_type"] for m in d]
-        expected = ['ParameterValueList', 'ParameterValueListValue']
+        expected = ["ParameterValueList", "ParameterValueListValue"]
         self.assertEqual(types, expected)
 
 
 class TestImportMappingLegacy(unittest.TestCase):
     def test_ObjectClass_to_dict_from_dict(self):
         mapping = {
             "map_type": "ObjectClass",
             "name": 0,
             "objects": 1,
             "parameters": {"map_type": "parameter", "name": 2, "value": 3, "parameter_type": "single value"},
         }
         mapping = import_mapping_from_dict(mapping)
         out = mapping_to_dict(mapping)
         expected = [
-            {'map_type': 'ObjectClass', 'position': 0},
-            {'map_type': 'Object', 'position': 1},
-            {'map_type': 'ObjectMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterDefinition', 'position': 2},
-            {'map_type': 'Alternative', 'position': 'hidden'},
-            {'map_type': 'ParameterValueMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterValue', 'position': 3},
+            {"map_type": "EntityClass", "position": 0},
+            {"map_type": "Entity", "position": 1},
+            {"map_type": "EntityMetadata", "position": "hidden"},
+            {"map_type": "ParameterDefinition", "position": 2},
+            {"map_type": "Alternative", "position": "hidden"},
+            {"map_type": "ParameterValueMetadata", "position": "hidden"},
+            {"map_type": "ParameterValue", "position": 3},
         ]
         self.assertEqual(out, expected)
 
     def test_ObjectClass_object_from_dict_to_dict(self):
         mapping = {"map_type": "ObjectClass", "name": 0, "objects": 1}
         mapping = import_mapping_from_dict(mapping)
         out = mapping_to_dict(mapping)
         expected = [
-            {'map_type': 'ObjectClass', 'position': 0},
-            {'map_type': 'Object', 'position': 1},
-            {'map_type': 'ObjectMetadata', 'position': 'hidden'},
+            {"map_type": "EntityClass", "position": 0},
+            {"map_type": "Entity", "position": 1},
+            {"map_type": "EntityMetadata", "position": "hidden"},
         ]
         self.assertEqual(out, expected)
 
     def test_ObjectClass_object_from_dict_to_dict2(self):
         mapping = {"map_type": "ObjectClass", "name": "cls", "objects": "obj"}
         mapping = import_mapping_from_dict(mapping)
         out = mapping_to_dict(mapping)
         expected = [
-            {'map_type': 'ObjectClass', 'position': 'hidden', 'value': 'cls'},
-            {'map_type': 'Object', 'position': 'hidden', 'value': 'obj'},
-            {'map_type': 'ObjectMetadata', 'position': 'hidden'},
+            {"map_type": "EntityClass", "position": "hidden", "value": "cls"},
+            {"map_type": "Entity", "position": "hidden", "value": "obj"},
+            {"map_type": "EntityMetadata", "position": "hidden"},
         ]
         self.assertEqual(out, expected)
 
     def test_RelationshipClassMapping_from_dict_to_dict(self):
         mapping = {
             "map_type": "RelationshipClass",
             "name": "unit__node",
             "object_classes": [0, 1],
             "objects": [0, 1],
             "parameters": {"map_type": "parameter", "name": "pname", "value": 2},
         }
         mapping = import_mapping_from_dict(mapping)
         out = mapping_to_dict(mapping)
         expected = [
-            {'map_type': 'RelationshipClass', 'position': 'hidden', 'value': 'unit__node'},
-            {'map_type': 'RelationshipClassObjectClass', 'position': 0},
-            {'map_type': 'RelationshipClassObjectClass', 'position': 1},
-            {'map_type': 'Relationship', 'position': 'hidden', 'value': 'relationship'},
-            {'map_type': 'RelationshipObject', 'position': 0},
-            {'map_type': 'RelationshipObject', 'position': 1},
-            {'map_type': 'RelationshipMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterDefinition', 'position': 'hidden', 'value': 'pname'},
-            {'map_type': 'Alternative', 'position': 'hidden'},
-            {'map_type': 'ParameterValueMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterValue', 'position': 2},
+            {"map_type": "EntityClass", "position": "hidden", "value": "unit__node"},
+            {"map_type": "Dimension", "position": 0},
+            {"map_type": "Dimension", "position": 1},
+            {"map_type": "Entity", "position": "hidden"},
+            {"map_type": "Element", "position": 0},
+            {"map_type": "Element", "position": 1},
+            {"map_type": "EntityMetadata", "position": "hidden"},
+            {"map_type": "ParameterDefinition", "position": "hidden", "value": "pname"},
+            {"map_type": "Alternative", "position": "hidden"},
+            {"map_type": "ParameterValueMetadata", "position": "hidden"},
+            {"map_type": "ParameterValue", "position": 2},
         ]
         self.assertEqual(out, expected)
 
     def test_RelationshipClassMapping_from_dict_to_dict2(self):
         mapping = {
             "map_type": "RelationshipClass",
             "name": "unit__node",
             "object_classes": ["cls", 0],
             "objects": ["obj", 0],
         }
         mapping = import_mapping_from_dict(mapping)
         out = mapping_to_dict(mapping)
         expected = [
-            {'map_type': 'RelationshipClass', 'position': 'hidden', 'value': 'unit__node'},
-            {'map_type': 'RelationshipClassObjectClass', 'position': 'hidden', 'value': 'cls'},
-            {'map_type': 'RelationshipClassObjectClass', 'position': 0},
-            {'map_type': 'Relationship', 'position': 'hidden', 'value': 'relationship'},
-            {'map_type': 'RelationshipObject', 'position': 'hidden', 'value': 'obj'},
-            {'map_type': 'RelationshipObject', 'position': 0},
-            {'map_type': 'RelationshipMetadata', 'position': 'hidden'},
+            {"map_type": "EntityClass", "position": "hidden", "value": "unit__node"},
+            {"map_type": "Dimension", "position": "hidden", "value": "cls"},
+            {"map_type": "Dimension", "position": 0},
+            {"map_type": "Entity", "position": "hidden"},
+            {"map_type": "Element", "position": "hidden", "value": "obj"},
+            {"map_type": "Element", "position": 0},
+            {"map_type": "EntityMetadata", "position": "hidden"},
         ]
         self.assertEqual(out, expected)
 
     def test_RelationshipClassMapping_from_dict_to_dict3(self):
         mapping = {
             "map_type": "RelationshipClass",
             "name": "unit__node",
@@ -363,26 +350,26 @@
                 "parameter_type": "array",
                 "extra_dimensions": ["dim"],
             },
         }
         mapping = import_mapping_from_dict(mapping)
         out = mapping_to_dict(mapping)
         expected = [
-            {'map_type': 'RelationshipClass', 'position': 'hidden', 'value': 'unit__node'},
-            {'map_type': 'RelationshipClassObjectClass', 'position': 'hidden'},
-            {'map_type': 'Relationship', 'position': 'hidden', 'value': 'relationship'},
-            {'map_type': 'RelationshipObject', 'position': 'hidden'},
-            {'map_type': 'RelationshipMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterDefinition', 'position': 'hidden', 'value': 'pname'},
-            {'map_type': 'Alternative', 'position': 'hidden'},
-            {'map_type': 'ParameterValueMetadata', 'position': 'hidden'},
-            {'map_type': 'ParameterValueType', 'position': 'hidden', 'value': 'array'},
-            {'map_type': 'IndexName', 'position': 'hidden'},
-            {'map_type': 'ParameterValueIndex', 'position': 'hidden', 'value': 'dim'},
-            {'map_type': 'ExpandedValue', 'position': 2},
+            {"map_type": "EntityClass", "position": "hidden", "value": "unit__node"},
+            {"map_type": "Dimension", "position": "hidden"},
+            {"map_type": "Entity", "position": "hidden"},
+            {"map_type": "Element", "position": "hidden"},
+            {"map_type": "EntityMetadata", "position": "hidden"},
+            {"map_type": "ParameterDefinition", "position": "hidden", "value": "pname"},
+            {"map_type": "Alternative", "position": "hidden"},
+            {"map_type": "ParameterValueMetadata", "position": "hidden"},
+            {"map_type": "ParameterValueType", "position": "hidden", "value": "array"},
+            {"map_type": "IndexName", "position": "hidden"},
+            {"map_type": "ParameterValueIndex", "position": "hidden", "value": "dim"},
+            {"map_type": "ExpandedValue", "position": 2},
         ]
         self.assertEqual(out, expected)
 
     def test_ObjectGroupMapping_to_dict_from_dict(self):
         mapping = {
             "map_type": "ObjectGroup",
             "name": 0,
@@ -395,136 +382,115 @@
                 "parameter_type": "single value",
                 "value": {"reference": 2, "map_type": "column"},
             },
         }
         mapping = import_mapping_from_dict(mapping)
         out = mapping_to_dict(mapping)
         expected = [
-            {'map_type': 'ObjectClass', 'position': 0},
-            {'map_type': 'Object', 'position': 1},
-            {'map_type': 'ObjectGroup', 'position': 2},
+            {"map_type": "EntityClass", "position": 0},
+            {"map_type": "Entity", "position": 1},
+            {"map_type": "EntityGroup", "position": 2},
         ]
         self.assertEqual(out, expected)
 
     def test_Alternative_to_dict_from_dict(self):
         mapping = {"map_type": "Alternative", "name": 0}
         mapping = import_mapping_from_dict(mapping)
         out = mapping_to_dict(mapping)
-        expected = [{'map_type': 'Alternative', 'position': 0}]
+        expected = [{"map_type": "Alternative", "position": 0}]
         self.assertEqual(out, expected)
 
     def test_Scenario_to_dict_from_dict(self):
         mapping = {"map_type": "Scenario", "name": 0}
         mapping = import_mapping_from_dict(mapping)
         out = mapping_to_dict(mapping)
         expected = [
-            {'map_type': 'Scenario', 'position': 0},
-            {'map_type': 'ScenarioActiveFlag', 'position': 'hidden', 'value': 'false'},
+            {"map_type": "Scenario", "position": 0},
+            {"map_type": "ScenarioActiveFlag", "position": "hidden", "value": "false"},
         ]
         self.assertEqual(out, expected)
 
     def test_ScenarioAlternative_to_dict_from_dict(self):
         mapping = {
             "map_type": "ScenarioAlternative",
             "scenario_name": 0,
             "alternative_name": 1,
             "before_alternative_name": 2,
         }
         mapping = import_mapping_from_dict(mapping)
         out = mapping_to_dict(mapping)
         expected = [
-            {'map_type': 'Scenario', 'position': 0},
-            {'map_type': 'ScenarioAlternative', 'position': 1},
-            {'map_type': 'ScenarioBeforeAlternative', 'position': 2},
+            {"map_type": "Scenario", "position": 0},
+            {"map_type": "ScenarioAlternative", "position": 1},
+            {"map_type": "ScenarioBeforeAlternative", "position": 2},
         ]
         self.assertEqual(out, expected)
 
     def test_Tool_to_dict_from_dict(self):
         mapping = {"map_type": "Tool", "name": 0}
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [{'map_type': 'Tool', 'position': 0}]
-        self.assertEqual(out, expected)
+        with self.assertRaises(ValueError):
+            import_mapping_from_dict(mapping)
 
     def test_Feature_to_dict_from_dict(self):
         mapping = {"map_type": "Feature", "entity_class_name": 0, "parameter_definition_name": 1}
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'FeatureEntityClass', 'position': 0},
-            {'map_type': 'FeatureParameterDefinition', 'position': 1},
-        ]
-        self.assertEqual(out, expected)
+        with self.assertRaises(ValueError):
+            import_mapping_from_dict(mapping)
 
     def test_ToolFeature_to_dict_from_dict(self):
         mapping = {"map_type": "ToolFeature", "name": 0, "entity_class_name": 1, "parameter_definition_name": 2}
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'Tool', 'position': 0},
-            {'map_type': 'ToolFeatureEntityClass', 'position': 1},
-            {'map_type': 'ToolFeatureParameterDefinition', 'position': 2},
-            {'map_type': 'ToolFeatureRequiredFlag', 'position': 'hidden', 'value': 'false'},
-        ]
-        self.assertEqual(out, expected)
+        with self.assertRaises(ValueError):
+            import_mapping_from_dict(mapping)
 
     def test_ToolFeatureMethod_to_dict_from_dict(self):
         mapping = {
             "map_type": "ToolFeatureMethod",
             "name": 0,
             "entity_class_name": 1,
             "parameter_definition_name": 2,
             "method": 3,
         }
-        mapping = import_mapping_from_dict(mapping)
-        out = mapping_to_dict(mapping)
-        expected = [
-            {'map_type': 'Tool', 'position': 0},
-            {'map_type': 'ToolFeatureMethodEntityClass', 'position': 1},
-            {'map_type': 'ToolFeatureMethodParameterDefinition', 'position': 2},
-            {'map_type': 'ToolFeatureMethodMethod', 'position': 3},
-        ]
-        self.assertEqual(out, expected)
+        with self.assertRaises(ValueError):
+            import_mapping_from_dict(mapping)
 
     def test_MapValueMapping_from_dict_to_dict(self):
         mapping_dict = {
             "value_type": "map",
             "main_value": {"reference": 23, "map_type": "row"},
             "extra_dimensions": [{"reference": "fifth column", "map_type": "column"}],
             "compress": True,
         }
         parameter_mapping = parameter_value_mapping_from_dict(mapping_dict)
         out = mapping_to_dict(parameter_mapping)
         expected = [
-            {'map_type': 'ParameterValueType', 'position': 'hidden', 'value': 'map', 'compress': True},
-            {'map_type': 'IndexName', 'position': 'hidden'},
-            {'map_type': 'ParameterValueIndex', 'position': 'fifth column'},
-            {'map_type': 'ExpandedValue', 'position': -24},
+            {"map_type": "ParameterValueType", "position": "hidden", "value": "map", "compress": True},
+            {"map_type": "IndexName", "position": "hidden"},
+            {"map_type": "ParameterValueIndex", "position": "fifth column"},
+            {"map_type": "ExpandedValue", "position": -24},
         ]
         self.assertEqual(out, expected)
 
     def test_TimeSeriesValueMapping_from_dict_to_dict(self):
         mapping_dict = {
             "value_type": "time series",
             "main_value": {"reference": 23, "map_type": "row"},
             "extra_dimensions": [{"reference": "fifth column", "map_type": "column"}],
             "options": {"repeat": True, "ignore_year": False, "fixed_resolution": False},
         }
         parameter_mapping = parameter_value_mapping_from_dict(mapping_dict)
         out = mapping_to_dict(parameter_mapping)
         expected = [
             {
-                'map_type': 'ParameterValueType',
-                'position': 'hidden',
-                'value': 'time_series',
-                'options': {'repeat': True, 'ignore_year': False, 'fixed_resolution': False},
+                "map_type": "ParameterValueType",
+                "position": "hidden",
+                "value": "time_series",
+                "options": {"repeat": True, "ignore_year": False, "fixed_resolution": False},
             },
-            {'map_type': 'IndexName', 'position': 'hidden'},
-            {'map_type': 'ParameterValueIndex', 'position': 'fifth column'},
-            {'map_type': 'ExpandedValue', 'position': -24},
+            {"map_type": "IndexName", "position": "hidden"},
+            {"map_type": "ParameterValueIndex", "position": "fifth column"},
+            {"map_type": "ExpandedValue", "position": -24},
         ]
         self.assertEqual(out, expected)
 
 
 def _parent_with_pivot(is_pivoted):
     parent = Mock()
     parent.is_pivoted.return_value = is_pivoted
@@ -793,28 +759,28 @@
 
     def test_read_iterator_with_row_with_all_Nones(self):
         input_data = [
             ["object_class", "object", "parameter", "value"],
             [None, None, None, None],
             ["oc2", "obj2", "parameter_name2", 2],
         ]
-        expected = {"object_classes": {"oc2"}}
+        expected = {"entity_classes": [("oc2",)]}
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {"map_type": "ObjectClass", "name": 0}
 
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_iterator_with_None(self):
         input_data = [["object_class", "object", "parameter", "value"], None, ["oc2", "obj2", "parameter_name2", 2]]
-        expected = {"object_classes": {"oc2"}}
+        expected = {"entity_classes": [("oc2",)]}
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {"map_type": "ObjectClass", "name": 0}
 
         out, errors = get_mapped_data(data, [mapping], data_header)
@@ -824,18 +790,18 @@
     def test_read_flat_file(self):
         input_data = [
             ["object_class", "object", "parameter", "value"],
             ["oc1", "obj1", "parameter_name1", 1],
             ["oc2", "obj2", "parameter_name2", 2],
         ]
         expected = {
-            "object_classes": {"oc1", "oc2"},
-            "objects": {("oc1", "obj1"), ("oc2", "obj2")},
-            "object_parameters": [("oc1", "parameter_name1"), ("oc2", "parameter_name2")],
-            "object_parameter_values": [["oc1", "obj1", "parameter_name1", 1], ["oc2", "obj2", "parameter_name2", 2]],
+            "entity_classes": [("oc1",), ("oc2",)],
+            "entities": [("oc1", "obj1"), ("oc2", "obj2")],
+            "parameter_definitions": [("oc1", "parameter_name1"), ("oc2", "parameter_name2")],
+            "parameter_values": [["oc1", "obj1", "parameter_name1", 1], ["oc2", "obj2", "parameter_name2", 2]],
         }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {
             "map_type": "ObjectClass",
@@ -851,18 +817,18 @@
     def test_read_flat_file_array(self):
         input_data = [
             ["object_class", "object", "parameter", "value"],
             ["oc1", "obj1", "parameter_name1", 1],
             ["oc1", "obj1", "parameter_name1", 2],
         ]
         expected = {
-            "object_classes": {"oc1"},
-            "objects": {("oc1", "obj1")},
-            "object_parameters": [("oc1", "parameter_name1")],
-            "object_parameter_values": [["oc1", "obj1", "parameter_name1", Array([1, 2])]],
+            "entity_classes": [("oc1",)],
+            "entities": [("oc1", "obj1")],
+            "parameter_definitions": [("oc1", "parameter_name1")],
+            "parameter_values": [["oc1", "obj1", "parameter_name1", Array([1, 2])]],
         }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {
             "map_type": "ObjectClass",
@@ -878,18 +844,18 @@
     def test_read_flat_file_array_with_ed(self):
         input_data = [
             ["object_class", "object", "parameter", "value", "value_order"],
             ["oc1", "obj1", "parameter_name1", 1, 0],
             ["oc1", "obj1", "parameter_name1", 2, 1],
         ]
         expected = {
-            "object_classes": {"oc1"},
-            "objects": {("oc1", "obj1")},
-            "object_parameters": [("oc1", "parameter_name1")],
-            "object_parameter_values": [["oc1", "obj1", "parameter_name1", Array([1, 2])]],
+            "entity_classes": [("oc1",)],
+            "entities": [("oc1", "obj1")],
+            "parameter_definitions": [("oc1", "parameter_name1")],
+            "parameter_values": [["oc1", "obj1", "parameter_name1", Array([1, 2])]],
         }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {
             "map_type": "ObjectClass",
@@ -906,41 +872,47 @@
 
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_flat_file_with_column_name_reference(self):
         input_data = [["object", "parameter", "value"], ["obj1", "parameter_name1", 1], ["obj2", "parameter_name2", 2]]
-        expected = {"object_classes": {"object"}, "objects": {("object", "obj1"), ("object", "obj2")}}
+        expected = {"entity_classes": [("object",)], "entities": [("object", "obj1"), ("object", "obj2")]}
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {"map_type": "ObjectClass", "name": {"map_type": "column_name", "reference": 0}, "object": 0}
 
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_object_class_from_header_using_string_as_integral_index(self):
         input_data = [["object_class"], ["obj1"], ["obj2"]]
-        expected = {"object_classes": {"object_class"}, "objects": {("object_class", "obj1"), ("object_class", "obj2")}}
+        expected = {
+            "entity_classes": [("object_class",)],
+            "entities": [("object_class", "obj1"), ("object_class", "obj2")],
+        }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {"map_type": "ObjectClass", "name": {"map_type": "column_header", "reference": "0"}, "object": 0}
 
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_object_class_from_header_using_string_as_column_header_name(self):
         input_data = [["object_class"], ["obj1"], ["obj2"]]
-        expected = {"object_classes": {"object_class"}, "objects": {("object_class", "obj1"), ("object_class", "obj2")}}
+        expected = {
+            "entity_classes": [("object_class",)],
+            "entities": [("object_class", "obj1"), ("object_class", "obj2")],
+        }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {
             "map_type": "ObjectClass",
             "name": {"map_type": "column_header", "reference": "object_class"},
@@ -949,32 +921,32 @@
 
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_with_list_of_mappings(self):
         input_data = [["object", "parameter", "value"], ["obj1", "parameter_name1", 1], ["obj2", "parameter_name2", 2]]
-        expected = {"object_classes": {"object"}, "objects": {("object", "obj1"), ("object", "obj2")}}
+        expected = {"entity_classes": [("object",)], "entities": [("object", "obj1"), ("object", "obj2")]}
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {"map_type": "ObjectClass", "name": {"map_type": "column_header", "reference": 0}, "object": 0}
 
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_pivoted_parameters_from_header(self):
         input_data = [["object", "parameter_name1", "parameter_name2"], ["obj1", 0, 1], ["obj2", 2, 3]]
         expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1"), ("object", "obj2")},
-            "object_parameters": [("object", "parameter_name1"), ("object", "parameter_name2")],
-            "object_parameter_values": [
+            "entity_classes": [("object",)],
+            "entities": [("object", "obj1"), ("object", "obj2")],
+            "parameter_definitions": [("object", "parameter_name1"), ("object", "parameter_name2")],
+            "parameter_values": [
                 ["object", "obj1", "parameter_name1", 0],
                 ["object", "obj1", "parameter_name2", 1],
                 ["object", "obj2", "parameter_name1", 2],
                 ["object", "obj2", "parameter_name2", 3],
             ],
         }
 
@@ -1009,18 +981,18 @@
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_pivoted_parameters_from_data(self):
         input_data = [["object", "parameter_name1", "parameter_name2"], ["obj1", 0, 1], ["obj2", 2, 3]]
         expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1"), ("object", "obj2")},
-            "object_parameters": [("object", "parameter_name1"), ("object", "parameter_name2")],
-            "object_parameter_values": [
+            "entity_classes": [("object",)],
+            "entities": [("object", "obj1"), ("object", "obj2")],
+            "parameter_definitions": [("object", "parameter_name1"), ("object", "parameter_name2")],
+            "parameter_values": [
                 ["object", "obj1", "parameter_name1", 0],
                 ["object", "obj1", "parameter_name2", 1],
                 ["object", "obj2", "parameter_name1", 2],
                 ["object", "obj2", "parameter_name2", 3],
             ],
         }
 
@@ -1043,19 +1015,19 @@
             ["object", "timestep", "value"],
             ["obj1", "T1", 11.0],
             ["obj1", "T2", 12.0],
             ["obj2", "T1", 21.0],
             ["obj2", "T2", 22.0],
         ]
         expected = {
-            "object_classes": {"timeline"},
-            "objects": {("timeline", "obj1"), ("timeline", "obj2")},
-            "object_parameters": [("timeline", "value")],
+            "entity_classes": [("timeline",)],
+            "entities": [("timeline", "obj1"), ("timeline", "obj2")],
+            "parameter_definitions": [("timeline", "value")],
             "alternatives": {"Base"},
-            "object_parameter_values": [
+            "parameter_values": [
                 ["timeline", "obj1", "value", Map(["T1", "T2"], [11.0, 12.0], index_name="timestep"), "Base"],
                 ["timeline", "obj2", "value", Map(["T1", "T2"], [21.0, 22.0], index_name="timestep"), "Base"],
             ],
         }
         data = iter(input_data)
         mapping_dicts = [
             {"map_type": "ObjectClass", "position": "hidden", "value": "timeline"},
@@ -1073,19 +1045,19 @@
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_import_objects_from_pivoted_data_when_they_lack_parameter_values(self):
         """Pivoted mapping works even when last mapping has valid position in columns."""
         input_data = [["object", "is_skilled", "has_powers"], ["obj1", "yes", "no"], ["obj2", None, None]]
         expected = {
-            "object_classes": {"node"},
-            "objects": {("node", "obj1"), ("node", "obj2")},
-            "object_parameters": [("node", "is_skilled"), ("node", "has_powers")],
+            "entity_classes": [("node",)],
+            "entities": [("node", "obj1"), ("node", "obj2")],
+            "parameter_definitions": [("node", "is_skilled"), ("node", "has_powers")],
             "alternatives": {"Base"},
-            "object_parameter_values": [
+            "parameter_values": [
                 ["node", "obj1", "is_skilled", "yes", "Base"],
                 ["node", "obj1", "has_powers", "no", "Base"],
             ],
         }
         data = iter(input_data)
         mapping_dicts = [
             {"map_type": "ObjectClass", "position": "hidden", "value": "node"},
@@ -1104,19 +1076,19 @@
         """Pivoted mapping works even when last mapping has valid position in columns."""
         input_data = [
             ["object", "my_index", "is_skilled", "has_powers"],
             ["obj1", "yesterday", None, "no"],
             ["obj1", "today", None, "yes"],
         ]
         expected = {
-            "object_classes": {"node"},
-            "objects": {("node", "obj1")},
-            "object_parameters": [("node", "is_skilled"), ("node", "has_powers")],
+            "entity_classes": [("node",)],
+            "entities": [("node", "obj1")],
+            "parameter_definitions": [("node", "is_skilled"), ("node", "has_powers")],
             "alternatives": {"Base"},
-            "object_parameter_values": [
+            "parameter_values": [
                 ["node", "obj1", "has_powers", Map(["yesterday", "today"], ["no", "yes"], index_name="period"), "Base"]
             ],
         }
         data = iter(input_data)
         mapping_dicts = [
             {"map_type": "ObjectClass", "position": "hidden", "value": "node"},
             {"map_type": "Object", "position": 0},
@@ -1133,18 +1105,18 @@
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_flat_file_with_extra_value_dimensions(self):
         input_data = [["object", "time", "parameter_name1"], ["obj1", "2018-01-01", 1], ["obj1", "2018-01-02", 2]]
 
         expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1")},
-            "object_parameters": [("object", "parameter_name1")],
-            "object_parameter_values": [
+            "entity_classes": [("object",)],
+            "entities": [("object", "obj1")],
+            "parameter_definitions": [("object", "parameter_name1")],
+            "parameter_values": [
                 [
                     "object",
                     "obj1",
                     "parameter_name1",
                     TimeSeriesVariableResolution(["2018-01-01", "2018-01-02"], [1, 2], False, False),
                 ]
             ],
@@ -1170,17 +1142,17 @@
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_flat_file_with_parameter_definition(self):
         input_data = [["object", "time", "parameter_name1"], ["obj1", "2018-01-01", 1], ["obj1", "2018-01-02", 2]]
 
         expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1")},
-            "object_parameters": [("object", "parameter_name1")],
+            "entity_classes": [("object",)],
+            "entities": [("object", "obj1")],
+            "parameter_definitions": [("object", "parameter_name1")],
         }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {
             "map_type": "ObjectClass",
@@ -1197,16 +1169,16 @@
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_1dim_relationships(self):
         input_data = [["unit", "node"], ["u1", "n1"], ["u1", "n2"]]
         expected = {
-            "relationship_classes": [("node_group", ["node"])],
-            "relationships": {("node_group", ("n1",)), ("node_group", ("n2",))},
+            "entity_classes": [("node_group", ("node",))],
+            "entities": [("node_group", ("n1",)), ("node_group", ("n2",))],
         }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {
             "map_type": "RelationshipClass",
@@ -1218,16 +1190,16 @@
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_relationships(self):
         input_data = [["unit", "node"], ["u1", "n1"], ["u1", "n2"]]
         expected = {
-            "relationship_classes": [("unit__node", ["unit", "node"])],
-            "relationships": {("unit__node", ("u1", "n1")), ("unit__node", ("u1", "n2"))},
+            "entity_classes": [("unit__node", ("unit", "node"))],
+            "entities": [("unit__node", ("u1", "n1")), ("unit__node", ("u1", "n2"))],
         }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {
             "map_type": "RelationshipClass",
@@ -1242,20 +1214,20 @@
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_relationships_with_parameters(self):
         input_data = [["unit", "node", "rel_parameter"], ["u1", "n1", 0], ["u1", "n2", 1]]
         expected = {
-            "relationship_classes": [("unit__node", ["unit", "node"])],
-            "relationships": {("unit__node", ("u1", "n1")), ("unit__node", ("u1", "n2"))},
-            "relationship_parameters": [("unit__node", "rel_parameter")],
-            "relationship_parameter_values": [
-                ["unit__node", ["u1", "n1"], "rel_parameter", 0],
-                ["unit__node", ["u1", "n2"], "rel_parameter", 1],
+            "entity_classes": [("unit__node", ("unit", "node"))],
+            "entities": [("unit__node", ("u1", "n1")), ("unit__node", ("u1", "n2"))],
+            "parameter_definitions": [("unit__node", "rel_parameter")],
+            "parameter_values": [
+                ["unit__node", ("u1", "n1"), "rel_parameter", 0],
+                ["unit__node", ("u1", "n2"), "rel_parameter", 1],
             ],
         }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {
@@ -1272,22 +1244,26 @@
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_relationships_with_parameters2(self):
         input_data = [["nuts2", "Capacity", "Fueltype"], ["BE23", 268.0, "Bioenergy"], ["DE11", 14.0, "Bioenergy"]]
         expected = {
-            "object_classes": {"nuts2", "fueltype"},
-            "objects": {("nuts2", "BE23"), ("fueltype", "Bioenergy"), ("nuts2", "DE11"), ("fueltype", "Bioenergy")},
-            "relationship_classes": [("nuts2__fueltype", ["nuts2", "fueltype"])],
-            "relationships": {("nuts2__fueltype", ("BE23", "Bioenergy")), ("nuts2__fueltype", ("DE11", "Bioenergy"))},
-            "relationship_parameters": [("nuts2__fueltype", "capacity")],
-            "relationship_parameter_values": [
-                ["nuts2__fueltype", ["BE23", "Bioenergy"], "capacity", 268.0],
-                ["nuts2__fueltype", ["DE11", "Bioenergy"], "capacity", 14.0],
+            "entity_classes": [("nuts2",), ("fueltype",), ("nuts2__fueltype", ("nuts2", "fueltype"))],
+            "entities": [
+                ("nuts2", "BE23"),
+                ("fueltype", "Bioenergy"),
+                ("nuts2__fueltype", ("BE23", "Bioenergy")),
+                ("nuts2", "DE11"),
+                ("nuts2__fueltype", ("DE11", "Bioenergy")),
+            ],
+            "parameter_definitions": [("nuts2__fueltype", "capacity")],
+            "parameter_values": [
+                ["nuts2__fueltype", ("BE23", "Bioenergy"), "capacity", 268.0],
+                ["nuts2__fueltype", ("DE11", "Bioenergy"), "capacity", 14.0],
             ],
         }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {
@@ -1312,18 +1288,18 @@
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_parameter_header_with_only_one_parameter(self):
         input_data = [["object", "parameter_name1"], ["obj1", 0], ["obj2", 2]]
         expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1"), ("object", "obj2")},
-            "object_parameters": [("object", "parameter_name1")],
-            "object_parameter_values": [
+            "entity_classes": [("object",)],
+            "entities": [("object", "obj1"), ("object", "obj2")],
+            "parameter_definitions": [("object", "parameter_name1")],
+            "parameter_values": [
                 ["object", "obj1", "parameter_name1", 0],
                 ["object", "obj2", "parameter_name1", 2],
             ],
         }
 
         data = iter(input_data)
         data_header = next(data)
@@ -1338,18 +1314,18 @@
         out, errors = get_mapped_data(data, [mapping], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_pivoted_parameters_from_data_with_skipped_column(self):
         input_data = [["object", "parameter_name1", "parameter_name2"], ["obj1", 0, 1], ["obj2", 2, 3]]
         expected = {
-            "object_classes": {"object"},
-            "objects": {("object", "obj1"), ("object", "obj2")},
-            "object_parameters": [("object", "parameter_name1")],
-            "object_parameter_values": [
+            "entity_classes": [("object",)],
+            "entities": [("object", "obj1"), ("object", "obj2")],
+            "parameter_definitions": [("object", "parameter_name1")],
+            "parameter_values": [
                 ["object", "obj1", "parameter_name1", 0],
                 ["object", "obj2", "parameter_name1", 2],
             ],
         }
 
         data = iter(input_data)
 
@@ -1364,18 +1340,23 @@
         out, errors = get_mapped_data(data, [mapping])
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_relationships_and_import_objects(self):
         input_data = [["unit", "node"], ["u1", "n1"], ["u2", "n2"]]
         expected = {
-            "relationship_classes": [("unit__node", ["unit", "node"])],
-            "relationships": {("unit__node", ("u1", "n1")), ("unit__node", ("u2", "n2"))},
-            "object_classes": {"unit", "node"},
-            "objects": {("unit", "u1"), ("node", "n1"), ("unit", "u2"), ("node", "n2")},
+            "entity_classes": [("unit",), ("node",), ("unit__node", ("unit", "node"))],
+            "entities": [
+                ("unit", "u1"),
+                ("node", "n1"),
+                ("unit__node", ("u1", "n1")),
+                ("unit", "u2"),
+                ("node", "n2"),
+                ("unit__node", ("u2", "n2")),
+            ],
         }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {
             "map_type": "RelationshipClass",
@@ -1392,18 +1373,18 @@
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_relationships_parameter_values_with_extra_dimensions(self):
         input_data = [["", "a", "b"], ["", "c", "d"], ["", "e", "f"], ["a", 2, 3], ["b", 4, 5]]
 
         expected = {
-            "relationship_classes": [("unit__node", ["unit", "node"])],
-            "relationship_parameters": [("unit__node", "e"), ("unit__node", "f")],
-            "relationships": {("unit__node", ("a", "c")), ("unit__node", ("b", "d"))},
-            "relationship_parameter_values": [
+            "entity_classes": [("unit__node", ("unit", "node"))],
+            "parameter_definitions": [("unit__node", "e"), ("unit__node", "f")],
+            "entities": [("unit__node", ("a", "c")), ("unit__node", ("b", "d"))],
+            "parameter_values": [
                 ["unit__node", ("a", "c"), "e", Map(["a", "b"], [2, 4])],
                 ["unit__node", ("b", "d"), "f", Map(["a", "b"], [3, 5])],
             ],
         }
 
         data = iter(input_data)
         data_header = []
@@ -1429,18 +1410,18 @@
         input_data = [
             ["object_class", "object", "parameter", "value"],
             [" ", " ", " ", " "],
             ["oc1", "obj1", "parameter_name1", 1],
             ["oc2", "obj2", "parameter_name2", 2],
         ]
         expected = {
-            "object_classes": {"oc1", "oc2"},
-            "objects": {("oc1", "obj1"), ("oc2", "obj2")},
-            "object_parameters": [("oc1", "parameter_name1"), ("oc2", "parameter_name2")],
-            "object_parameter_values": [["oc1", "obj1", "parameter_name1", 1], ["oc2", "obj2", "parameter_name2", 2]],
+            "entity_classes": [("oc1",), ("oc2",)],
+            "entities": [("oc1", "obj1"), ("oc2", "obj2")],
+            "parameter_definitions": [("oc1", "parameter_name1"), ("oc2", "parameter_name2")],
+            "parameter_values": [["oc1", "obj1", "parameter_name1", 1], ["oc2", "obj2", "parameter_name2", 2]],
         }
 
         data = iter(input_data)
         data_header = next(data)
 
         mapping = {
             "map_type": "ObjectClass",
@@ -1458,18 +1439,18 @@
         input_data = [
             ["oc1", "oc2", "parameter_class1", "parameter_class2"],
             [" ", " ", " ", " "],
             ["oc1_obj1", "oc2_obj1", 1, 3],
             ["oc1_obj2", "oc2_obj2", 2, 4],
         ]
         expected = {
-            "object_classes": {"oc1", "oc2"},
-            "objects": {("oc1", "oc1_obj1"), ("oc1", "oc1_obj2"), ("oc2", "oc2_obj2")},
-            "object_parameters": [("oc1", "parameter_class1"), ("oc2", "parameter_class2")],
-            "object_parameter_values": [
+            "entity_classes": [("oc1",), ("oc2",)],
+            "entities": [("oc1", "oc1_obj1"), ("oc1", "oc1_obj2"), ("oc2", "oc2_obj2")],
+            "parameter_definitions": [("oc1", "parameter_class1"), ("oc2", "parameter_class2")],
+            "parameter_values": [
                 ["oc1", "oc1_obj1", "parameter_class1", 1],
                 ["oc1", "oc1_obj2", "parameter_class1", 2],
                 ["oc2", "oc2_obj2", "parameter_class2", 4],
             ],
         }
 
         data = iter(input_data)
@@ -1491,26 +1472,26 @@
         }
 
         out, errors = get_mapped_data(data, [mapping1, mapping2], data_header)
         self.assertEqual(errors, [])
         self.assertEqual(out, expected)
 
     def test_read_object_class_with_table_name_as_class_name(self):
-        input_data = [["Object names"], ["object 1"], ["object 2"]]
+        input_data = [["Entity names"], ["object 1"], ["object 2"]]
         data = iter(input_data)
         data_header = next(data)
         mapping = {
             "map_type": "ObjectClass",
             "name": {"map_type": "table_name", "reference": "class name"},
             "object": 0,
         }
         out, errors = get_mapped_data(data, [mapping], data_header, "class name")
         expected = {
-            "object_classes": {"class name"},
-            "objects": {("class name", "object 1"), ("class name", "object 2")},
+            "entity_classes": [("class name",)],
+            "entities": [("class name", "object 1"), ("class name", "object 2")],
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_read_flat_map_from_columns(self):
         input_data = [["Index", "Value"], ["key1", -2], ["key2", -1]]
         data = iter(input_data)
@@ -1526,18 +1507,18 @@
                 "compress": False,
                 "extra_dimensions": [0],
             },
         }
         out, errors = get_mapped_data(data, [mapping], data_header)
         expected_map = Map(["key1", "key2"], [-2, -1])
         expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
+            "entity_classes": [("object_class",)],
+            "entities": [("object_class", "object")],
+            "parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "parameter_definitions": [("object_class", "parameter")],
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_read_nested_map_from_columns(self):
         input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
         data = iter(input_data)
@@ -1553,18 +1534,18 @@
                 "compress": False,
                 "extra_dimensions": [0, 1],
             },
         }
         out, errors = get_mapped_data(data, [mapping], data_header)
         expected_map = Map(["key11", "key21"], [Map(["key12"], [-2]), Map(["key22"], [-1])])
         expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
+            "entity_classes": [("object_class",)],
+            "entities": [("object_class", "object")],
+            "parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "parameter_definitions": [("object_class", "parameter")],
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_read_uneven_nested_map_from_columns(self):
         input_data = [
             ["Index", "A", "B", "C"],
@@ -1597,18 +1578,18 @@
                 Map(["key11", "key12"], [-2, -1]),
                 -23,
                 -33,
                 Map(["key31", "key32"], [Map(["key311", "key312"], [50, 51]), 66]),
             ],
         )
         expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
+            "entity_classes": [("object_class",)],
+            "entities": [("object_class", "object")],
+            "parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "parameter_definitions": [("object_class", "parameter")],
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_read_nested_map_with_compression(self):
         input_data = [
             ["Index 1", "Time stamp", "Value"],
@@ -1639,18 +1620,18 @@
                     False,
                     False,
                     index_name=Map.DEFAULT_INDEX_NAME,
                 )
             ],
         )
         expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
+            "entity_classes": [("object_class",)],
+            "entities": [("object_class", "object")],
+            "parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "parameter_definitions": [("object_class", "parameter")],
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_read_alternative(self):
         input_data = [["Alternatives"], ["alternative1"], ["second_alternative"], ["last_one"]]
         data = iter(input_data)
@@ -1721,38 +1702,32 @@
         self.assertEqual(out, expected)
 
     def test_read_tool(self):
         input_data = [["Tools"], ["tool1"], ["second_tool"], ["last_one"]]
         data = iter(input_data)
         data_header = next(data)
         mapping = {"map_type": "Tool", "name": 0}
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = {"tools": {"tool1", "second_tool", "last_one"}}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
+        with self.assertRaises(ValueError):
+            get_mapped_data(data, [mapping], data_header)
 
     def test_read_feature(self):
         input_data = [["Class", "Parameter"], ["class1", "param1"], ["class2", "param2"]]
         data = iter(input_data)
         data_header = next(data)
         mapping = {"map_type": "Feature", "entity_class_name": 0, "parameter_definition_name": 1}
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = {"features": {("class1", "param1"), ("class2", "param2")}}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
+        with self.assertRaises(ValueError):
+            get_mapped_data(data, [mapping], data_header)
 
     def test_read_tool_feature(self):
         input_data = [["Tool", "Class", "Parameter"], ["tool1", "class1", "param1"], ["tool2", "class2", "param2"]]
         data = iter(input_data)
         data_header = next(data)
         mapping = {"map_type": "ToolFeature", "name": 0, "entity_class_name": 1, "parameter_definition_name": 2}
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = {"tool_features": [["tool1", "class1", "param1", False], ["tool2", "class2", "param2", False]]}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
+        with self.assertRaises(ValueError):
+            get_mapped_data(data, [mapping], data_header)
 
     def test_read_tool_feature_with_required_flag(self):
         input_data = [
             ["Tool", "Class", "Parameter", "Required"],
             ["tool1", "class1", "param1", "f"],
             ["tool2", "class2", "param2", "true"],
         ]
@@ -1761,18 +1736,16 @@
         mapping = {
             "map_type": "ToolFeature",
             "name": 0,
             "entity_class_name": 1,
             "parameter_definition_name": 2,
             "required": 3,
         }
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = {"tool_features": [["tool1", "class1", "param1", False], ["tool2", "class2", "param2", True]]}
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
+        with self.assertRaises(ValueError):
+            get_mapped_data(data, [mapping], data_header)
 
     def test_read_tool_feature_method(self):
         input_data = [
             ["Tool", "Class", "Parameter", "Method"],
             ["tool1", "class1", "param1", "meth1"],
             ["tool2", "class2", "param2", "meth2"],
         ]
@@ -1781,37 +1754,31 @@
         mapping = {
             "map_type": "ToolFeatureMethod",
             "name": 0,
             "entity_class_name": 1,
             "parameter_definition_name": 2,
             "method": 3,
         }
-        out, errors = get_mapped_data(data, [mapping], data_header)
-        expected = dict()
-        expected["tool_feature_methods"] = [
-            ["tool1", "class1", "param1", "meth1"],
-            ["tool2", "class2", "param2", "meth2"],
-        ]
-        self.assertFalse(errors)
-        self.assertEqual(out, expected)
+        with self.assertRaises(ValueError):
+            get_mapped_data(data, [mapping], data_header)
 
     def test_read_object_group_without_parameters(self):
         input_data = [
             ["Object Class", "Group", "Object"],
             ["class_A", "group1", "object1"],
             ["class_A", "group1", "object2"],
             ["class_A", "group2", "object3"],
         ]
         data = iter(input_data)
         data_header = next(data)
         mapping = {"map_type": "ObjectGroup", "name": 0, "groups": 1, "members": 2}
         out, errors = get_mapped_data(data, [mapping], data_header)
         expected = dict()
-        expected["object_classes"] = {"class_A"}
-        expected["object_groups"] = {
+        expected["entity_classes"] = [("class_A",)]
+        expected["entity_groups"] = {
             ("class_A", "group1", "object1"),
             ("class_A", "group1", "object2"),
             ("class_A", "group2", "object3"),
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
@@ -1823,28 +1790,27 @@
             ["class_A", "group2", "object3"],
         ]
         data = iter(input_data)
         data_header = next(data)
         mapping = {"map_type": "ObjectGroup", "name": 0, "groups": 1, "members": 2, "import_objects": True}
         out, errors = get_mapped_data(data, [mapping], data_header)
         expected = dict()
-        expected["object_groups"] = {
+        expected["entity_groups"] = {
             ("class_A", "group1", "object1"),
             ("class_A", "group1", "object2"),
             ("class_A", "group2", "object3"),
         }
-        expected["object_classes"] = {"class_A"}
-        expected["objects"] = {
+        expected["entity_classes"] = [("class_A",)]
+        expected["entities"] = [
             ("class_A", "group1"),
             ("class_A", "object1"),
-            ("class_A", "group1"),
             ("class_A", "object2"),
             ("class_A", "group2"),
             ("class_A", "object3"),
-        }
+        ]
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_read_parameter_definition_with_default_values_and_value_lists(self):
         input_data = [
             ["Class", "Parameter", "Default", "Value list"],
             ["class_A", "param1", 23.0, "listA"],
@@ -1861,16 +1827,16 @@
                 "map_type": "ParameterDefinition",
                 "default_value": {"value_type": "single value", "main_value": 2},
                 "parameter_value_list_name": 3,
             },
         }
         out, errors = get_mapped_data(data, [mapping], data_header)
         expected = dict()
-        expected["object_classes"] = {"class_A", "class_A", "class_B"}
-        expected["object_parameters"] = [
+        expected["entity_classes"] = [("class_A",), ("class_B",)]
+        expected["parameter_definitions"] = [
             ("class_A", "param1", 23.0, "listA"),
             ("class_A", "param2", 42.0, "listB"),
             ("class_B", "param3", 5.0, "listA"),
         ]
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
@@ -1885,16 +1851,16 @@
                 "map_type": "ParameterDefinition",
                 "default_value": {"value_type": "map", "main_value": 1, "compress": False, "extra_dimensions": [0]},
             },
         }
         out, errors = get_mapped_data(data, [mapping])
         expected_map = Map(["key1", "key2", "key3"], [-2.3, 5.5, 3.2])
         expected = {
-            "object_classes": {"object_class"},
-            "object_parameters": [("object_class", "parameter", expected_map)],
+            "entity_classes": [("object_class",)],
+            "parameter_definitions": [("object_class", "parameter", expected_map)],
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_read_parameter_definition_with_nested_map_as_default_value(self):
         input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
         data = iter(input_data)
@@ -1907,29 +1873,29 @@
                 "map_type": "ParameterDefinition",
                 "default_value": {"value_type": "map", "main_value": 2, "compress": False, "extra_dimensions": [0, 1]},
             },
         }
         out, errors = get_mapped_data(data, [mapping], data_header)
         expected_map = Map(["key11", "key21"], [Map(["key12"], [-2]), Map(["key22"], [-1])])
         expected = {
-            "object_classes": {"object_class"},
-            "object_parameters": [("object_class", "parameter", expected_map)],
+            "entity_classes": [("object_class",)],
+            "parameter_definitions": [("object_class", "parameter", expected_map)],
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_read_map_index_names_from_columns(self):
         input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
         data = iter(input_data)
         data_header = next(data)
         mapping_root = unflatten(
             [
-                ObjectClassMapping(Position.hidden, value="object_class"),
+                EntityClassMapping(Position.hidden, value="object_class"),
                 ParameterDefinitionMapping(Position.hidden, value="parameter"),
-                ObjectMapping(Position.hidden, value="object"),
+                EntityMapping(Position.hidden, value="object"),
                 ParameterValueTypeMapping(Position.hidden, value="map"),
                 IndexNameMapping(Position.header, value=0),
                 ParameterValueIndexMapping(0),
                 IndexNameMapping(Position.header, value=1),
                 ParameterValueIndexMapping(1),
                 ExpandedParameterValueMapping(2),
             ]
@@ -1937,31 +1903,31 @@
         out, errors = get_mapped_data(data, [mapping_root], data_header)
         expected_map = Map(
             ["key11", "key21"],
             [Map(["key12"], [-2], index_name="Index 2"), Map(["key22"], [-1], index_name="Index 2")],
             index_name="Index 1",
         )
         expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
+            "entity_classes": [("object_class",)],
+            "entities": [("object_class", "object")],
+            "parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "parameter_definitions": [("object_class", "parameter")],
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_missing_map_index_name(self):
         input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
         data = iter(input_data)
         data_header = next(data)
         mapping_root = unflatten(
             [
-                ObjectClassMapping(Position.hidden, value="object_class"),
+                EntityClassMapping(Position.hidden, value="object_class"),
                 ParameterDefinitionMapping(Position.hidden, value="parameter"),
-                ObjectMapping(Position.hidden, value="object"),
+                EntityMapping(Position.hidden, value="object"),
                 ParameterValueTypeMapping(Position.hidden, value="map"),
                 IndexNameMapping(Position.hidden, value=None),
                 ParameterValueIndexMapping(0),
                 IndexNameMapping(Position.header, value=1),
                 ParameterValueIndexMapping(1),
                 ExpandedParameterValueMapping(2),
             ]
@@ -1969,29 +1935,29 @@
         out, errors = get_mapped_data(data, [mapping_root], data_header)
         expected_map = Map(
             ["key11", "key21"],
             [Map(["key12"], [-2], index_name="Index 2"), Map(["key22"], [-1], index_name="Index 2")],
             index_name="",
         )
         expected = {
-            "object_classes": {"object_class"},
-            "objects": {("object_class", "object")},
-            "object_parameter_values": [["object_class", "object", "parameter", expected_map]],
-            "object_parameters": [("object_class", "parameter")],
+            "entity_classes": [("object_class",)],
+            "entities": [("object_class", "object")],
+            "parameter_values": [["object_class", "object", "parameter", expected_map]],
+            "parameter_definitions": [("object_class", "parameter")],
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_read_default_value_index_names_from_columns(self):
         input_data = [["Index 1", "Index 2", "Value"], ["key11", "key12", -2], ["key21", "key22", -1]]
         data = iter(input_data)
         data_header = next(data)
         mapping_root = unflatten(
             [
-                ObjectClassMapping(Position.hidden, value="object_class"),
+                EntityClassMapping(Position.hidden, value="object_class"),
                 ParameterDefinitionMapping(Position.hidden, value="parameter"),
                 ParameterDefaultValueTypeMapping(Position.hidden, value="map"),
                 DefaultValueIndexNameMapping(Position.header, value=0),
                 ParameterDefaultValueIndexMapping(0),
                 DefaultValueIndexNameMapping(Position.header, value=1),
                 ParameterDefaultValueIndexMapping(1),
                 ExpandedParameterDefaultValueMapping(2),
@@ -2000,104 +1966,104 @@
         out, errors = get_mapped_data(data, [mapping_root], data_header)
         expected_map = Map(
             ["key11", "key21"],
             [Map(["key12"], [-2], index_name="Index 2"), Map(["key22"], [-1], index_name="Index 2")],
             index_name="Index 1",
         )
         expected = {
-            "object_classes": {"object_class"},
-            "object_parameters": [("object_class", "parameter", expected_map)],
+            "entity_classes": [("object_class",)],
+            "parameter_definitions": [("object_class", "parameter", expected_map)],
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_filter_regular_expression_in_root_mapping(self):
         input_data = [["A", "p"], ["A", "q"], ["B", "r"]]
         data = iter(input_data)
-        mapping_root = unflatten([ObjectClassMapping(0, filter_re="B"), ObjectMapping(1)])
+        mapping_root = unflatten([EntityClassMapping(0, filter_re="B"), EntityMapping(1)])
         out, errors = get_mapped_data(data, [mapping_root])
-        expected = {"object_classes": {"B"}, "objects": {("B", "r")}}
+        expected = {"entity_classes": [("B",)], "entities": [("B", "r")]}
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_filter_regular_expression_in_child_mapping(self):
         input_data = [["A", "p"], ["A", "q"], ["B", "r"]]
         data = iter(input_data)
-        mapping_root = unflatten([ObjectClassMapping(0), ObjectMapping(1, filter_re="q|r")])
+        mapping_root = unflatten([EntityClassMapping(0), EntityMapping(1, filter_re="q|r")])
         out, errors = get_mapped_data(data, [mapping_root])
-        expected = {"object_classes": {"A", "B"}, "objects": {("A", "q"), ("B", "r")}}
+        expected = {"entity_classes": [("A",), ("B",)], "entities": [("A", "q"), ("B", "r")]}
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_filter_regular_expression_in_child_mapping_filters_parent_mappings_too(self):
         input_data = [["A", "p"], ["A", "q"], ["B", "r"]]
         data = iter(input_data)
-        mapping_root = unflatten([ObjectClassMapping(0), ObjectMapping(1, filter_re="q")])
+        mapping_root = unflatten([EntityClassMapping(0), EntityMapping(1, filter_re="q")])
         out, errors = get_mapped_data(data, [mapping_root])
-        expected = {"object_classes": {"A"}, "objects": {("A", "q")}}
+        expected = {"entity_classes": [("A",)], "entities": [("A", "q")]}
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
     def test_arrays_get_imported_to_correct_alternatives(self):
         input_data = [["Base", "y", "p1"], ["alternative", "y", "p1"]]
         data = iter(input_data)
         mapping_root = unflatten(
             [
-                ObjectClassMapping(Position.hidden, value="class"),
-                ObjectMapping(1),
+                EntityClassMapping(Position.hidden, value="class"),
+                EntityMapping(1),
                 ParameterDefinitionMapping(Position.hidden, value="parameter"),
                 AlternativeMapping(0),
                 ParameterValueTypeMapping(Position.hidden, value="array"),
                 ExpandedParameterValueMapping(2),
             ]
         )
         out, errors = get_mapped_data(data, [mapping_root])
         expected = {
-            "object_classes": {"class"},
-            "objects": {("class", "y")},
-            "object_parameters": [("class", "parameter")],
+            "entity_classes": [("class",)],
+            "entities": [("class", "y")],
+            "parameter_definitions": [("class", "parameter")],
             "alternatives": {"Base", "alternative"},
-            "object_parameter_values": [
+            "parameter_values": [
                 ["class", "y", "parameter", Array(["p1"]), "Base"],
                 ["class", "y", "parameter", Array(["p1"]), "alternative"],
             ],
         }
         self.assertFalse(errors)
         self.assertEqual(out, expected)
 
 
 class TestHasFilter(unittest.TestCase):
     def test_mapping_without_filter_doesnt_have_filter(self):
-        mapping = ObjectClassMapping(0)
+        mapping = EntityClassMapping(0)
         self.assertFalse(mapping.has_filter())
 
     def test_hidden_mapping_without_value_doesnt_have_filter(self):
-        mapping = ObjectClassMapping(Position.hidden, filter_re="a")
+        mapping = EntityClassMapping(Position.hidden, filter_re="a")
         self.assertFalse(mapping.has_filter())
 
     def test_hidden_mapping_with_value_has_filter(self):
-        mapping = ObjectClassMapping(0, value="a", filter_re="b")
+        mapping = EntityClassMapping(0, value="a", filter_re="b")
         self.assertTrue(mapping.has_filter())
 
     def test_mapping_without_value_has_filter(self):
-        mapping = ObjectClassMapping(Position.hidden, value="a", filter_re="b")
+        mapping = EntityClassMapping(Position.hidden, value="a", filter_re="b")
         self.assertTrue(mapping.has_filter())
 
     def test_mapping_with_value_but_without_filter_doesnt_have_filter(self):
-        mapping = ObjectClassMapping(0, value="a")
+        mapping = EntityClassMapping(0, value="a")
         self.assertFalse(mapping.has_filter())
 
     def test_child_mapping_with_filter_has_filter(self):
-        mapping = ObjectClassMapping(0)
-        mapping.child = ObjectMapping(1, filter_re="a")
+        mapping = EntityClassMapping(0)
+        mapping.child = EntityMapping(1, filter_re="a")
         self.assertTrue(mapping.has_filter())
 
     def test_child_mapping_without_filter_doesnt_have_filter(self):
-        mapping = ObjectClassMapping(0)
-        mapping.child = ObjectMapping(1)
+        mapping = EntityClassMapping(0)
+        mapping.child = EntityMapping(1)
         self.assertFalse(mapping.has_filter())
 
 
 class TestIsPivoted(unittest.TestCase):
     def test_pivoted_position_returns_true(self):
         mapping = AlternativeMapping(-1)
         self.assertTrue(mapping.is_pivoted())
@@ -2111,9 +2077,25 @@
         self.assertTrue(mapping.is_pivoted())
 
     def test_returns_false_when_position_is_header_and_is_leaf(self):
         mapping = unflatten([AlternativeMapping(0), ParameterValueMapping(Position.header)])
         self.assertFalse(mapping.is_pivoted())
 
 
+class TestDefaultMappings(unittest.TestCase):
+    def test_mappings_are_hidden(self):
+        map_types = (
+            "EntityClass",
+            "Alternative",
+            "Scenario",
+            "ScenarioAlternative",
+            "EntityGroup",
+            "ParameterValueList",
+        )
+        for map_type in map_types:
+            root = default_import_mapping(map_type)
+            flattened = root.flatten()
+            self.assertTrue(all(m.position == Position.hidden for m in flattened))
+
+
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/import_mapping/test_type_conversion.py` & `spinedb_api-0.31.0/tests/import_mapping/test_type_conversion.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/__init__.py` & `spinedb_api-0.31.0/tests/spine_io/exporters/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
 """
-Init file for tests.spine_io package. Intentionally empty.
+Init file for tests.spine_io.exporters package. Intentionally empty.
 
 """
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/exporters/__init__.py` & `spinedb_api-0.31.0/spinedb_api/import_mapping/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
-
 """
-Init file for tests.spine_io.exporters package. Intentionally empty.
+This package contains facilities to map tables into a Spine database.
 
 """
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/exporters/test_csv_writer.py` & `spinedb_api-0.31.0/tests/spine_io/exporters/test_csv_writer.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -11,59 +12,59 @@
 """
 Unit tests for csv writer.
 
 """
 from pathlib import Path
 from tempfile import TemporaryDirectory
 import unittest
-from spinedb_api import DiffDatabaseMapping, import_object_classes, import_objects
+from spinedb_api import DatabaseMapping, import_object_classes, import_objects
 from spinedb_api.mapping import Position
-from spinedb_api.export_mapping import object_export
+from spinedb_api.export_mapping import entity_export
 from spinedb_api.spine_io.exporters.writer import write
 from spinedb_api.spine_io.exporters.csv_writer import CsvWriter
 
 
 class TestCsvWriter(unittest.TestCase):
     def setUp(self):
         self._temp_dir = TemporaryDirectory()
 
     def tearDown(self):
         self._temp_dir.cleanup()
 
     def test_write_empty_database(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root_mapping = object_export(0, 1)
+        db_map = DatabaseMapping("sqlite://", create=True)
+        root_mapping = entity_export(0, 1)
         out_path = Path(self._temp_dir.name, "out.csv")
         writer = CsvWriter(out_path.parent, out_path.name)
         write(db_map, writer, root_mapping)
         self.assertTrue(out_path.exists())
         with open(out_path) as out_file:
             self.assertEqual(out_file.readlines(), [])
-        db_map.connection.close()
+        db_map.close()
 
     def test_write_single_object_class_and_object(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"),))
         db_map.commit_session("Add test data.")
-        root_mapping = object_export(0, 1)
+        root_mapping = entity_export(0, 1)
         out_path = Path(self._temp_dir.name, "out.csv")
         writer = CsvWriter(out_path.parent, out_path.name)
         write(db_map, writer, root_mapping)
         self.assertTrue(out_path.exists())
         with open(out_path) as out_file:
             self.assertEqual(out_file.readlines(), ["oc,o1\n"])
-        db_map.connection.close()
+        db_map.close()
 
     def test_tables_are_written_to_separate_files(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_objects(db_map, (("oc1", "o1"), ("oc2", "o2")))
         db_map.commit_session("Add test data.")
-        root_mapping = object_export(Position.table_name, 0)
+        root_mapping = entity_export(Position.table_name, 0)
         out_path = Path(self._temp_dir.name, "out.csv")
         writer = CsvWriter(out_path.parent, out_path.name)
         write(db_map, writer, root_mapping)
         self.assertFalse(out_path.exists())
         out_files = list()
         for real_out_path in Path(self._temp_dir.name).iterdir():
             out_files.append(real_out_path.name)
@@ -72,27 +73,27 @@
                 expected = ["o1\n"]
             elif real_out_path.name == "oc2.csv":
                 expected = ["o2\n"]
             with open(real_out_path) as out_file:
                 self.assertEqual(out_file.readlines(), expected)
         self.assertEqual(len(out_files), 2)
         self.assertEqual(set(out_files), {"oc1.csv", "oc2.csv"})
-        db_map.connection.close()
+        db_map.close()
 
     def test_append_to_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"),))
         db_map.commit_session("Add test data.")
-        root_mapping1 = object_export(0, 1)
-        root_mapping2 = object_export(0, 1)
+        root_mapping1 = entity_export(0, 1)
+        root_mapping2 = entity_export(0, 1)
         out_path = Path(self._temp_dir.name, "out.csv")
         writer = CsvWriter(out_path.parent, out_path.name)
         write(db_map, writer, root_mapping1, root_mapping2)
         self.assertTrue(out_path.exists())
         with open(out_path) as out_file:
             self.assertEqual(out_file.readlines(), ["oc,o1\n", "oc,o1\n"])
-        db_map.connection.close()
+        db_map.close()
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/exporters/test_excel_writer.py` & `spinedb_api-0.31.0/tests/spine_io/exporters/test_excel_writer.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -12,124 +13,124 @@
 Unit tests for Excel writer.
 
 """
 import os.path
 from tempfile import TemporaryDirectory
 import unittest
 from openpyxl import load_workbook
-from spinedb_api import DiffDatabaseMapping, import_object_classes, import_objects
+from spinedb_api import DatabaseMapping, import_object_classes, import_objects
 from spinedb_api.mapping import Position
-from spinedb_api.export_mapping import object_export
+from spinedb_api.export_mapping import entity_export
 from spinedb_api.spine_io.exporters.writer import write
 from spinedb_api.spine_io.exporters.excel_writer import ExcelWriter
 
 
 class TestExcelWriter(unittest.TestCase):
     def setUp(self):
         self._temp_dir = TemporaryDirectory()
 
     def tearDown(self):
         self._temp_dir.cleanup()
 
     def test_write_empty_database(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
-        root_mapping = object_export(0, 1)
+        db_map = DatabaseMapping("sqlite://", create=True)
+        root_mapping = entity_export(0, 1)
         path = os.path.join(self._temp_dir.name, "test.xlsx")
         writer = ExcelWriter(path)
         write(db_map, writer, root_mapping)
         workbook = load_workbook(path, read_only=True)
         self.assertEqual(workbook.sheetnames, ["Sheet1"])
         sheet = workbook["Sheet1"]
         self.assertEqual(sheet.calculate_dimension(), "A1:A1")
         workbook.close()
-        db_map.connection.close()
+        db_map.close()
 
     def test_write_single_object_class_and_object(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"),))
         db_map.commit_session("Add test data.")
-        root_mapping = object_export(0, 1)
+        root_mapping = entity_export(0, 1)
         path = os.path.join(self._temp_dir.name, "test.xlsx")
         writer = ExcelWriter(path)
         write(db_map, writer, root_mapping)
         workbook = load_workbook(path, read_only=True)
         self.assertEqual(workbook.sheetnames, ["Sheet1"])
         expected = [["oc", "o1"]]
         self.check_sheet(workbook, "Sheet1", expected)
         workbook.close()
-        db_map.connection.close()
+        db_map.close()
 
     def test_write_to_existing_sheet(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("Sheet1",))
         import_objects(db_map, (("Sheet1", "o1"), ("Sheet1", "o2")))
         db_map.commit_session("Add test data.")
-        root_mapping = object_export(Position.table_name, 0)
+        root_mapping = entity_export(Position.table_name, 0)
         path = os.path.join(self._temp_dir.name, "test.xlsx")
         writer = ExcelWriter(path)
         write(db_map, writer, root_mapping)
         workbook = load_workbook(path, read_only=True)
         self.assertEqual(workbook.sheetnames, ["Sheet1"])
         expected = [["o1"], ["o2"]]
         self.check_sheet(workbook, "Sheet1", expected)
         workbook.close()
-        db_map.connection.close()
+        db_map.close()
 
     def test_write_to_named_sheets(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", ("oc2")))
         import_objects(db_map, (("oc1", "o11"), ("oc1", "o12"), ("oc2", "o21")))
         db_map.commit_session("Add test data.")
-        root_mapping = object_export(Position.table_name, 1)
+        root_mapping = entity_export(Position.table_name, 1)
         path = os.path.join(self._temp_dir.name, "test.xlsx")
         writer = ExcelWriter(path)
         write(db_map, writer, root_mapping)
         workbook = load_workbook(path, read_only=True)
         self.assertEqual(workbook.sheetnames, ["oc1", "oc2"])
         expected = [[None, "o11"], [None, "o12"]]
         self.check_sheet(workbook, "oc1", expected)
         expected = [[None, "o21"]]
         self.check_sheet(workbook, "oc2", expected)
         workbook.close()
-        db_map.connection.close()
+        db_map.close()
 
     def test_append_to_anonymous_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"),))
         db_map.commit_session("Add test data.")
-        root_mapping1 = object_export(0, 1)
-        root_mapping2 = object_export(0, 1)
+        root_mapping1 = entity_export(0, 1)
+        root_mapping2 = entity_export(0, 1)
         path = os.path.join(self._temp_dir.name, "test.xlsx")
         writer = ExcelWriter(path)
         write(db_map, writer, root_mapping1, root_mapping2)
         workbook = load_workbook(path, read_only=True)
         self.assertEqual(workbook.sheetnames, ["Sheet1"])
         expected = [["oc", "o1"], ["oc", "o1"]]
         self.check_sheet(workbook, "Sheet1", expected)
         workbook.close()
-        db_map.connection.close()
+        db_map.close()
 
     def test_append_to_named_table(self):
-        db_map = DiffDatabaseMapping("sqlite://", create=True)
+        db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"),))
         db_map.commit_session("Add test data.")
-        root_mapping1 = object_export(Position.table_name, 0)
-        root_mapping2 = object_export(Position.table_name, 0)
+        root_mapping1 = entity_export(Position.table_name, 0)
+        root_mapping2 = entity_export(Position.table_name, 0)
         path = os.path.join(self._temp_dir.name, "test.xlsx")
         writer = ExcelWriter(path)
         write(db_map, writer, root_mapping1, root_mapping2)
         workbook = load_workbook(path, read_only=True)
         self.assertEqual(workbook.sheetnames, ["oc"])
         expected = [["o1"], ["o1"]]
         self.check_sheet(workbook, "oc", expected)
         workbook.close()
-        db_map.connection.close()
+        db_map.close()
 
     def check_sheet(self, workbook, sheet_name, expected):
         """
         Args:
             workbook (Workbook): a workbook to check
             sheet_name (str): sheet name
             expected (list): expected rows
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/exporters/test_gdx_writer.py` & `spinedb_api-0.31.0/tests/spine_io/exporters/test_gdx_writer.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -29,240 +30,250 @@
     import_object_parameter_values,
     import_objects,
     import_relationship_classes,
     import_relationships,
     Map,
 )
 from spinedb_api.mapping import Position, unflatten
-from spinedb_api.export_mapping import object_export, object_parameter_export, relationship_export
+from spinedb_api.export_mapping import entity_export, entity_parameter_value_export, entity_export
 from spinedb_api.export_mapping.export_mapping import FixedValueMapping
 
 
 class TestGdxWriter(unittest.TestCase):
     _gams_dir = find_gams_directory()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_write_empty_database(self):
         db_map = DatabaseMapping("sqlite://", create=True)
-        root_mapping = object_export(class_position=Position.table_name, object_position=0)
+        root_mapping = entity_export(entity_class_position=Position.table_name, entity_position=0)
         root_mapping.child.header = "*"
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_write_empty_database.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             write(db_map, writer, root_mapping)
             with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
                 self.assertEqual(len(gdx_file), 0)
-        db_map.connection.close()
+        db_map.close()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_write_single_object_class_and_object(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"),))
         db_map.commit_session("Add test data.")
-        root_mapping = object_export(Position.table_name, 0)
+        root_mapping = entity_export(Position.table_name, 0)
         root_mapping.child.header = "*"
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_write_single_object_class_and_object.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             write(db_map, writer, root_mapping)
             with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
                 self.assertEqual(len(gdx_file), 1)
                 gams_set = gdx_file["oc"]
                 self.assertIsNone(gams_set.domain)
                 self.assertEqual(gams_set.elements, ["o1"])
-        db_map.connection.close()
+        db_map.close()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_write_2D_relationship(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_objects(db_map, (("oc1", "o1"), ("oc2", "o2")))
         import_relationship_classes(db_map, (("rel", ("oc1", "oc2")),))
         import_relationships(db_map, (("rel", ("o1", "o2")),))
         db_map.commit_session("Add test data.")
-        root_mapping = relationship_export(
-            Position.table_name, Position.hidden, [Position.header, Position.header], [0, 1]
-        )
+        root_mapping = entity_export(Position.table_name, Position.hidden, [Position.header, Position.header], [0, 1])
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_write_2D_relationship.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             write(db_map, writer, root_mapping)
             with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
                 self.assertEqual(len(gdx_file), 1)
                 gams_set = gdx_file["rel"]
                 self.assertEqual(gams_set.domain, ["oc1", "oc2"])
                 self.assertEqual(gams_set.elements, [("o1", "o2")])
-        db_map.connection.close()
+        db_map.close()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_write_parameters(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o1"),))
         import_object_parameter_values(db_map, (("oc", "o1", "p", 2.3),))
         db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(class_position=Position.table_name, object_position=0, value_position=1)
+        root_mapping = entity_parameter_value_export(
+            entity_class_position=Position.table_name, entity_position=0, value_position=1
+        )
         mappings = root_mapping.flatten()
         mappings[3].header = "*"
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_write_parameters.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             write(db_map, writer, root_mapping)
             with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
                 self.assertEqual(len(gdx_file), 1)
                 gams_parameter = gdx_file["oc"]
                 self.assertEqual(len(gams_parameter), 1)
                 self.assertEqual(gams_parameter["o1"], 2.3)
-        db_map.connection.close()
+        db_map.close()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_non_numerical_parameter_value_raises_writer_expection(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o1"),))
         import_object_parameter_values(db_map, (("oc", "o1", "p", "text"),))
         db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(class_position=Position.table_name, object_position=0, value_position=1)
+        root_mapping = entity_parameter_value_export(
+            entity_class_position=Position.table_name, entity_position=0, value_position=1
+        )
         mappings = root_mapping.flatten()
         mappings[3].header = "*"
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_write_parameters.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             self.assertRaises(WriterException, write, db_map, writer, root_mapping)
-        db_map.connection.close()
+        db_map.close()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_empty_parameter(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o1"),))
         import_object_parameter_values(db_map, (("oc", "o1", "p", Map([], [], str)),))
         db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(class_position=Position.table_name, object_position=0, value_position=1)
+        root_mapping = entity_parameter_value_export(
+            entity_class_position=Position.table_name, entity_position=0, value_position=1
+        )
         mappings = root_mapping.flatten()
         mappings[3].header = "*"
         mappings[-1].filter_re = "single_value"
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_write_parameters.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             write(db_map, writer, root_mapping)
             with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
                 self.assertEqual(len(gdx_file), 1)
                 gams_parameter = gdx_file["oc"]
                 self.assertIsInstance(gams_parameter, GAMSParameter)
                 self.assertEqual(len(gams_parameter), 0)
-        db_map.connection.close()
+        db_map.close()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_write_scalars(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o1"),))
         import_object_parameter_values(db_map, (("oc", "o1", "p", 2.3),))
         db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(class_position=Position.table_name, value_position=0)
+        root_mapping = entity_parameter_value_export(entity_class_position=Position.table_name, value_position=0)
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_write_scalars.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             write(db_map, writer, root_mapping)
             with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
                 self.assertEqual(len(gdx_file), 1)
                 gams_scalar = gdx_file["oc"]
                 self.assertEqual(float(gams_scalar), 2.3)
-        db_map.connection.close()
+        db_map.close()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_two_tables(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_objects(db_map, (("oc1", "o"), ("oc2", "p")))
         db_map.commit_session("Add test data.")
-        root_mapping = object_export(class_position=Position.table_name, object_position=0)
+        root_mapping = entity_export(entity_class_position=Position.table_name, entity_position=0)
         root_mapping.child.header = "*"
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_two_tables.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             write(db_map, writer, root_mapping)
             with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
                 self.assertEqual(len(gdx_file), 2)
                 gams_set = gdx_file["oc1"]
                 self.assertIsNone(gams_set.domain)
                 self.assertEqual(gams_set.elements, ["o"])
                 gams_set = gdx_file["oc2"]
                 self.assertIsNone(gams_set.domain)
                 self.assertEqual(gams_set.elements, ["p"])
-        db_map.connection.close()
+        db_map.close()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_append_to_table(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc1", "oc2"))
         import_objects(db_map, (("oc1", "o"), ("oc2", "p")))
         db_map.commit_session("Add test data.")
         root_mapping1 = unflatten(
-            [FixedValueMapping(Position.table_name, value="set_X")] + object_export(object_position=0).flatten()
+            [FixedValueMapping(Position.table_name, value="set_X")] + entity_export(entity_position=0).flatten()
         )
         root_mapping1.child.filter_re = "oc1"
         root_mapping1.child.child.header = "*"
         root_mapping2 = unflatten(
-            [FixedValueMapping(Position.table_name, value="set_X")] + object_export(object_position=0).flatten()
+            [FixedValueMapping(Position.table_name, value="set_X")] + entity_export(entity_position=0).flatten()
         )
         root_mapping2.child.filter_re = "oc2"
         root_mapping2.child.child.header = "*"
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_two_tables.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             write(db_map, writer, root_mapping1, root_mapping2)
             with GdxFile(str(file_path), "r", self._gams_dir) as gdx_file:
                 self.assertEqual(len(gdx_file), 1)
                 gams_set = gdx_file["set_X"]
                 self.assertIsNone(gams_set.domain)
                 self.assertEqual(gams_set.elements, ["o", "p"])
-        db_map.connection.close()
+        db_map.close()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_parameter_value_non_convertible_to_float_raises_WriterException(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "param"),))
         import_objects(db_map, (("oc", "o"), ("oc", "p")))
         import_object_parameter_values(db_map, (("oc", "o", "param", "text"), ("oc", "p", "param", 2.3)))
         db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(
-            class_position=Position.hidden, definition_position=Position.table_name, object_position=0, value_position=1
+        root_mapping = entity_parameter_value_export(
+            entity_class_position=Position.hidden,
+            definition_position=Position.table_name,
+            entity_position=0,
+            value_position=1,
         )
         root_mapping.child.child.child.header = "*"
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_two_tables.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             self.assertRaises(WriterException, write, db_map, writer, root_mapping)
-        db_map.connection.close()
+        db_map.close()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_non_string_set_element_raises_WriterException(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(db_map, (("oc", "param"),))
         import_objects(db_map, (("oc", "o"), ("oc", "p")))
         import_object_parameter_values(db_map, (("oc", "o", "param", 2.3), ("oc", "p", "param", "text")))
         db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(
-            class_position=Position.hidden, definition_position=Position.table_name, object_position=0, value_position=1
+        root_mapping = entity_parameter_value_export(
+            entity_class_position=Position.hidden,
+            definition_position=Position.table_name,
+            entity_position=0,
+            value_position=1,
         )
         root_mapping.child.child.child.header = "*"
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_two_tables.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             self.assertRaises(WriterException, write, db_map, writer, root_mapping)
-        db_map.connection.close()
+        db_map.close()
 
     @unittest.skipIf(_gams_dir is None, "No working GAMS installation found.")
     def test_special_value_conversions(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_object_parameters(
             db_map, (("oc", "epsilon"), ("oc", "infinity"), ("oc", "negative_infinity"), ("oc", "nan"))
@@ -274,16 +285,16 @@
                 ("oc", "o1", "epsilon", sys.float_info.min),
                 ("oc", "o1", "infinity", math.inf),
                 ("oc", "o1", "negative_infinity", -math.inf),
                 ("oc", "o1", "nan", math.nan),
             ),
         )
         db_map.commit_session("Add test data.")
-        root_mapping = object_parameter_export(
-            class_position=Position.table_name, object_position=0, definition_position=1, value_position=2
+        root_mapping = entity_parameter_value_export(
+            entity_class_position=Position.table_name, entity_position=0, definition_position=1, value_position=2
         )
         mappings = root_mapping.flatten()
         mappings[1].header = mappings[3].header = "*"
         with TemporaryDirectory() as temp_dir:
             file_path = Path(temp_dir, "test_special_value_conversions.gdx")
             writer = GdxWriter(str(file_path), self._gams_dir)
             write(db_map, writer, root_mapping)
@@ -291,12 +302,12 @@
                 self.assertEqual(len(gdx_file), 1)
                 gams_parameter = gdx_file["oc"]
                 self.assertEqual(len(gams_parameter), 4)
                 self.assertEqual(gams_parameter[("o1", "epsilon")], sys.float_info.min)
                 self.assertEqual(gams_parameter[("o1", "infinity")], math.inf)
                 self.assertEqual(gams_parameter[("o1", "negative_infinity")], -math.inf)
                 self.assertTrue(math.isnan(gams_parameter[("o1", "nan")]))
-        db_map.connection.close()
+        db_map.close()
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/exporters/test_sql_writer.py` & `spinedb_api-0.31.0/tests/spine_io/exporters/test_sql_writer.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -23,20 +24,20 @@
     Duration,
     import_object_classes,
     import_objects,
     import_object_parameters,
     import_object_parameter_values,
 )
 from spinedb_api.mapping import Position, unflatten
-from spinedb_api.export_mapping import object_export
+from spinedb_api.export_mapping import entity_export
 from spinedb_api.export_mapping.export_mapping import (
     AlternativeMapping,
     FixedValueMapping,
-    ObjectClassMapping,
-    ObjectMapping,
+    EntityClassMapping,
+    EntityMapping,
     ParameterDefinitionMapping,
     ParameterValueMapping,
 )
 from spinedb_api.spine_io.exporters.writer import write
 from spinedb_api.spine_io.exporters.sql_writer import SqlWriter
 
 
@@ -49,32 +50,32 @@
 
     def test_write_empty_database(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         settings = FixedValueMapping(Position.table_name, "table 1")
         out_path = Path(self._temp_dir.name, "out.sqlite")
         writer = SqlWriter(str(out_path), overwrite_existing=True)
         write(db_map, writer, settings)
-        db_map.connection.close()
+        db_map.close()
         self.assertTrue(out_path.exists())
 
     def test_write_header_only(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         db_map.commit_session("Add test data.")
         root_mapping = unflatten(
             [
                 FixedValueMapping(Position.table_name, "table 1"),
-                ObjectClassMapping(0, header="classes"),
-                ObjectMapping(1, header="objects"),
+                EntityClassMapping(0, header="classes"),
+                EntityMapping(1, header="objects"),
             ]
         )
         out_path = Path(self._temp_dir.name, "out.sqlite")
         writer = SqlWriter(str(out_path), overwrite_existing=True)
         write(db_map, writer, root_mapping)
-        db_map.connection.close()
+        db_map.close()
         self.assertTrue(out_path.exists())
         engine = create_engine("sqlite:///" + str(out_path))
         connection = engine.connect()
         try:
             metadata = MetaData()
             metadata.reflect(bind=engine)
             session = Session(engine)
@@ -91,22 +92,22 @@
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"),))
         db_map.commit_session("Add test data.")
         root_mapping = unflatten(
             [
                 FixedValueMapping(Position.table_name, "table 1"),
-                ObjectClassMapping(0, header="classes"),
-                ObjectMapping(1, header="objects"),
+                EntityClassMapping(0, header="classes"),
+                EntityMapping(1, header="objects"),
             ]
         )
         out_path = Path(self._temp_dir.name, "out.sqlite")
         writer = SqlWriter(str(out_path), overwrite_existing=True)
         write(db_map, writer, root_mapping)
-        db_map.connection.close()
+        db_map.close()
         self.assertTrue(out_path.exists())
         engine = create_engine("sqlite:///" + str(out_path))
         connection = engine.connect()
         try:
             metadata = MetaData()
             metadata.reflect(bind=engine)
             session = Session(engine)
@@ -127,25 +128,25 @@
         import_objects(db_map, (("oc", "o1"),))
         dt = DateTime("2021-04-08T08:00")
         import_object_parameter_values(db_map, (("oc", "o1", "p", DateTime("2021-04-08T08:00")),))
         db_map.commit_session("Add test data.")
         root_mapping = unflatten(
             [
                 FixedValueMapping(Position.table_name, "table 1"),
-                ObjectClassMapping(0, header="classes"),
-                ObjectMapping(1, header="objects"),
+                EntityClassMapping(0, header="classes"),
+                EntityMapping(1, header="objects"),
                 ParameterDefinitionMapping(2, header="parameters"),
                 AlternativeMapping(Position.hidden),
                 ParameterValueMapping(3, header="values"),
             ]
         )
         out_path = Path(self._temp_dir.name, "out.sqlite")
         writer = SqlWriter(str(out_path), overwrite_existing=True)
         write(db_map, writer, root_mapping)
-        db_map.connection.close()
+        db_map.close()
         self.assertTrue(out_path.exists())
         engine = create_engine("sqlite:///" + str(out_path))
         connection = engine.connect()
         try:
             metadata = MetaData()
             metadata.reflect(bind=engine)
             session = Session(engine)
@@ -167,25 +168,25 @@
         import_object_parameters(db_map, (("oc", "p"),))
         import_objects(db_map, (("oc", "o1"),))
         import_object_parameter_values(db_map, (("oc", "o1", "p", Duration("3h")),))
         db_map.commit_session("Add test data.")
         root_mapping = unflatten(
             [
                 FixedValueMapping(Position.table_name, "table 1"),
-                ObjectClassMapping(0, header="classes"),
-                ObjectMapping(1, header="objects"),
+                EntityClassMapping(0, header="classes"),
+                EntityMapping(1, header="objects"),
                 ParameterDefinitionMapping(2, header="parameters"),
                 AlternativeMapping(Position.hidden),
                 ParameterValueMapping(3, header="values"),
             ]
         )
         out_path = Path(self._temp_dir.name, "out.sqlite")
         writer = SqlWriter(str(out_path), overwrite_existing=True)
         write(db_map, writer, root_mapping)
-        db_map.connection.close()
+        db_map.close()
         self.assertTrue(out_path.exists())
         engine = create_engine("sqlite:///" + str(out_path))
         connection = engine.connect()
         try:
             metadata = MetaData()
             metadata.reflect(bind=engine)
             session = Session(engine)
@@ -202,25 +203,25 @@
             connection.close()
 
     def test_append_to_table(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("oc",))
         import_objects(db_map, (("oc", "o1"), ("oc", "q1")))
         db_map.commit_session("Add test data.")
-        root_mapping1 = object_export(Position.table_name, 0)
+        root_mapping1 = entity_export(Position.table_name, 0)
         root_mapping1.child.header = "objects"
         root_mapping1.child.filter_re = "o1"
-        root_mapping2 = object_export(Position.table_name, 0)
+        root_mapping2 = entity_export(Position.table_name, 0)
         root_mapping2.child.header = "objects"
         root_mapping2.child.filter_re = "q1"
         out_path = Path(self._temp_dir.name, "out.sqlite")
         writer = SqlWriter(str(out_path), overwrite_existing=True)
         write(db_map, writer, root_mapping1)
         write(db_map, writer, root_mapping2)
-        db_map.connection.close()
+        db_map.close()
         self.assertTrue(out_path.exists())
         engine = create_engine("sqlite:///" + str(out_path))
         connection = engine.connect()
         try:
             metadata = MetaData()
             metadata.reflect(bind=engine)
             session = Session(engine)
@@ -248,19 +249,19 @@
         try:
             metadata = MetaData()
             object_table = Table("oc", metadata, Column("objects", String))
             metadata.create_all(out_engine)
             out_connection.execute(object_table.insert(), objects="initial_object")
         finally:
             out_connection.close()
-        root_mapping = object_export(Position.table_name, 0)
+        root_mapping = entity_export(Position.table_name, 0)
         root_mapping.child.header = "objects"
         writer = SqlWriter(str(out_path), overwrite_existing=False)
         write(db_map, writer, root_mapping)
-        db_map.connection.close()
+        db_map.close()
         self.assertTrue(out_path.exists())
         engine = create_engine("sqlite:///" + str(out_path))
         connection = engine.connect()
         try:
             metadata = MetaData()
             metadata.reflect(bind=engine)
             session = Session(engine)
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/exporters/test_writer.py` & `spinedb_api-0.31.0/tests/spine_io/exporters/test_writer.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -11,15 +12,15 @@
 """
 Unit tests for ``writer`` module.
 
 """
 import unittest
 from spinedb_api import DatabaseMapping, import_object_classes, import_objects
 from spinedb_api.spine_io.exporters.writer import Writer, write
-from spinedb_api.export_mapping.settings import object_export
+from spinedb_api.export_mapping.settings import entity_export
 
 
 class _TableWriter(Writer):
     def __init__(self):
         self._tables = dict()
         self._current_table = None
 
@@ -40,15 +41,15 @@
 
 
 class TestWrite(unittest.TestCase):
     def setUp(self):
         self._db_map = DatabaseMapping("sqlite://", create=True)
 
     def tearDown(self):
-        self._db_map.connection.close()
+        self._db_map.close()
 
     def test_max_rows(self):
         import_object_classes(self._db_map, ("class1", "class2"))
         import_objects(
             self._db_map,
             (
                 ("class1", "obj1"),
@@ -57,15 +58,15 @@
                 ("class2", "obj4"),
                 ("class2", "obj5"),
                 ("class2", "obj6"),
             ),
         )
         self._db_map.commit_session("Add test data.")
         writer = _TableWriter()
-        root_mapping = object_export(0, 1)
+        root_mapping = entity_export(0, 1)
         write(self._db_map, writer, root_mapping, max_rows=2)
         self.assertEqual(writer.tables, {None: [["class1", "obj1"], ["class1", "obj2"]]})
 
     def test_max_rows_with_filter(self):
         import_object_classes(self._db_map, ("class1", "class2"))
         import_objects(
             self._db_map,
@@ -76,15 +77,15 @@
                 ("class2", "obj4"),
                 ("class2", "obj5"),
                 ("class2", "obj6"),
             ),
         )
         self._db_map.commit_session("Add test data.")
         writer = _TableWriter()
-        root_mapping = object_export(0, 1)
+        root_mapping = entity_export(0, 1)
         root_mapping.child.filter_re = "obj6"
         write(self._db_map, writer, root_mapping, max_rows=1)
         self.assertEqual(writer.tables, {None: [["class2", "obj6"]]})
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/importers/__init__.py` & `spinedb_api-0.31.0/spinedb_api/spine_io/exporters/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
 
 """
-Init file for tests.spine_io.importers package. Intentionally empty.
+Init file for spine_io.exporters package. Intentionally empty.
 
 """
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/importers/test_CSVConnector.py` & `spinedb_api-0.31.0/tests/spine_io/importers/test_CSVConnector.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/importers/test_GdxConnector.py` & `spinedb_api-0.31.0/tests/spine_io/importers/test_GdxConnector.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/importers/test_datapackage_reader.py` & `spinedb_api-0.31.0/tests/spine_io/importers/test_datapackage_reader.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -16,14 +17,16 @@
 from contextlib import contextmanager
 import csv
 import unittest
 from pathlib import Path
 import pickle
 from tempfile import TemporaryDirectory
 from datapackage import Package
+
+from spinedb_api.exception import ConnectorError
 from spinedb_api.spine_io.importers.datapackage_reader import DataPackageConnector
 
 
 class TestDatapackageConnector(unittest.TestCase):
     def test_connector_is_picklable(self):
         reader = DataPackageConnector(None)
         pickled = pickle.dumps(reader)
@@ -52,14 +55,33 @@
         with test_datapackage(data) as package_path:
             reader = DataPackageConnector(None)
             reader.connect_to_source(str(package_path))
             data_iterator, header = reader.get_data_iterator("test_data", {"has_header": False})
             self.assertIsNone(header)
             self.assertEqual(list(data_iterator), data)
 
+    def test_wrong_datapackage_encoding_raises_connector_error(self):
+        broken_text = b"Slagn\xe4s"
+        # Fool the datapackage sniffing algorithm by hiding the broken line behind a large number of UTF-8 lines.
+        data = 1000 * [b"normal_text\n"] + [broken_text]
+        with TemporaryDirectory() as temp_dir:
+            csv_file_path = Path(temp_dir, "test_data.csv")
+            with open(csv_file_path, "wb") as csv_file:
+                for row in data:
+                    csv_file.write(row)
+            package = Package(base_path=temp_dir)
+            package.add_resource({"path": str(csv_file_path.relative_to(temp_dir))})
+            package_path = Path(temp_dir, "datapackage.json")
+            package.save(package_path)
+            reader = DataPackageConnector(None)
+            reader.connect_to_source(str(package_path))
+            data_iterator, header = reader.get_data_iterator("test_data", {"has_header": False})
+            self.assertIsNone(header)
+            self.assertRaises(ConnectorError, list, data_iterator)
+
 
 @contextmanager
 def test_datapackage(rows):
     with TemporaryDirectory() as temp_dir:
         csv_file_path = Path(temp_dir, "test_data.csv")
         with open(csv_file_path, "w", newline="") as csv_file:
             csv_writer = csv.writer(csv_file)
@@ -67,9 +89,9 @@
         package = Package(base_path=temp_dir)
         package.add_resource({"path": str(csv_file_path.relative_to(temp_dir))})
         package_path = Path(temp_dir, "datapackage.json")
         package.save(package_path)
         yield package_path
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/importers/test_excel_reader.py` & `spinedb_api-0.31.0/tests/spine_io/importers/test_excel_reader.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/importers/test_json_reader.py` & `spinedb_api-0.31.0/tests/spine_io/importers/test_json_reader.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -36,9 +37,9 @@
                 json.dump(data, out_file)
             reader.connect_to_source(str(file_path))
             rows = list(reader.file_iterator("data", {}))
             reader.disconnect()
         self.assertEqual(rows, [["a", 1], ["b", "c", 2]])
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/importers/test_sqlalchemy_connector.py` & `spinedb_api-0.31.0/tests/spine_io/importers/test_sqlalchemy_connector.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -21,9 +22,9 @@
 class TestSqlAlchemyConnector(unittest.TestCase):
     def test_connector_is_picklable(self):
         reader = SqlAlchemyConnector(None)
         pickled = pickle.dumps(reader)
         self.assertTrue(pickled)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/spine_io/test_excel_integration.py` & `spinedb_api-0.31.0/tests/spine_io/test_excel_integration.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -17,28 +18,28 @@
 from pathlib import PurePath
 from tempfile import TemporaryDirectory
 import unittest
 import json
 from spinedb_api import DatabaseMapping, import_data, from_database
 from spinedb_api.spine_io.exporters.excel import export_spine_database_to_xlsx
 from spinedb_api.spine_io.importers.excel_reader import get_mapped_data_from_xlsx
-from ..test_import_functions import assert_import_equivalent
+from tests.test_import_functions import assert_import_equivalent
 
 _TEMP_EXCEL_FILENAME = "excel.xlsx"
 
 
 class TestExcelIntegration(unittest.TestCase):
     def test_array(self):
         array = b'{"type": "array", "data": [1, 2, 3]}'
-        array = from_database(array, value_type="array")
+        array = from_database(array, type_="array")
         self._check_parameter_value(array)
 
     def test_time_series(self):
         ts = b'{"type": "time_series", "index": {"start": "1999-12-31 23:00:00", "resolution": "1h"}, "data": [0.1, 0.2]}'
-        ts = from_database(ts, value_type="time_series")
+        ts = from_database(ts, type_="time_series")
         self._check_parameter_value(ts)
 
     def test_map(self):
         map_ = json.dumps(
             {
                 "type": "map",
                 "index_type": "str",
@@ -93,38 +94,38 @@
                                 ]
                             ],
                         },
                     ],
                 ],
             }
         ).encode("UTF8")
-        map_ = from_database(map_, value_type="map")
+        map_ = from_database(map_, type_="map")
         self._check_parameter_value(map_)
 
     def _check_parameter_value(self, val):
         input_data = {
-            "object_classes": ["dog"],
-            "objects": [("dog", "pluto")],
-            "object_parameters": [("dog", "bone")],
-            "object_parameter_values": [("dog", "pluto", "bone", val)],
+            "entity_classes": {("dog",)},
+            "entities": {("dog", "pluto")},
+            "parameter_definitions": [("dog", "bone")],
+            "parameter_values": [("dog", "pluto", "bone", val)],
         }
         db_map = DatabaseMapping("sqlite://", create=True)
         import_data(db_map, **input_data)
         db_map.commit_session("yeah")
         with TemporaryDirectory() as directory:
             path = str(PurePath(directory, _TEMP_EXCEL_FILENAME))
             export_spine_database_to_xlsx(db_map, path)
             output_data, errors = get_mapped_data_from_xlsx(path)
-        db_map.connection.close()
+        db_map.close()
         self.assertEqual([], errors)
-        input_obj_param_vals = input_data.pop("object_parameter_values")
-        output_obj_param_vals = output_data.pop("object_parameter_values")
-        self.assertEqual(1, len(output_obj_param_vals))
-        input_obj_param_val = input_obj_param_vals[0]
-        output_obj_param_val = output_obj_param_vals[0]
+        input_param_vals = input_data.pop("parameter_values")
+        output_param_vals = output_data.pop("parameter_values")
+        self.assertEqual(1, len(output_param_vals))
+        input_obj_param_val = input_param_vals[0]
+        output_obj_param_val = output_param_vals[0]
         for input_, output in zip(input_obj_param_val[:3], output_obj_param_val[:3]):
             self.assertEqual(input_, output)
         input_val = input_obj_param_val[3]
         output_val = output_obj_param_val[3]
         self.assertEqual(set(indexed_values(output_val)), set(indexed_values(input_val)))
         assert_import_equivalent(self, input_data, output_data, strict=False)
```

### Comparing `spinedb_api-0.30.5/tests/test_import_functions.py` & `spinedb_api-0.31.0/tests/test_import_functions.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,53 +1,46 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
 ######################################################################################################################
-
-"""
-Unit tests for import_functions.py.
-
-"""
+""" Unit tests for import_functions.py. """
 
 import unittest
 
 from spinedb_api.spine_db_server import _unparse_value
-from spinedb_api.diff_db_mapping import DiffDatabaseMapping
 from spinedb_api.db_mapping import DatabaseMapping
 from spinedb_api.import_functions import (
     import_alternatives,
+    import_entity_classes,
     import_object_classes,
     import_object_parameter_values,
     import_object_parameters,
     import_objects,
     import_relationship_classes,
     import_relationship_parameter_values,
     import_relationship_parameters,
     import_relationships,
     import_scenario_alternatives,
     import_scenarios,
     import_parameter_value_lists,
-    import_tools,
-    import_features,
-    import_tool_features,
-    import_tool_feature_methods,
     import_metadata,
     import_object_metadata,
     import_relationship_metadata,
     import_object_parameter_value_metadata,
     import_relationship_parameter_value_metadata,
     import_data,
 )
-from spinedb_api.parameter_value import from_database
+from spinedb_api.parameter_value import from_database, dump_db_value, TimeSeriesFixedResolution
 
 
 def assert_import_equivalent(test, obs, exp, strict=True):
     """Helper function to assert that two dictionaries will have the same effect if passed to ``import_data()``"""
     if strict:
         test.assertEqual(obs.keys(), exp.keys())
     for key in obs:
@@ -71,35 +64,35 @@
             except IndexError:
                 obs_val = None
             _assert_same_elements(test, obs_val, exp_val)
         return
     test.assertEqual(obs_vals, exp_vals)
 
 
-def create_diff_db_map():
+def create_db_map():
     db_url = "sqlite://"
-    return DiffDatabaseMapping(db_url, username="UnitTest", create=True)
+    return DatabaseMapping(db_url, username="UnitTest", create=True)
 
 
 class TestIntegrationImportData(unittest.TestCase):
     def test_import_data_integration(self):
         database_url = "sqlite://"
-        db_map = DiffDatabaseMapping(database_url, username="IntegrationTest", create=True)
+        db_map = DatabaseMapping(database_url, username="IntegrationTest", create=True)
 
         object_c = ["example_class", "other_class"]  # 2 items
         objects = [["example_class", "example_object"], ["other_class", "other_object"]]  # 2 items
         relationship_c = [["example_rel_class", ["example_class", "other_class"]]]  # 1 item
         relationships = [["example_rel_class", ["example_object", "other_object"]]]  # 1 item
         obj_parameters = [["example_class", "example_parameter"]]  # 1 item
         rel_parameters = [["example_rel_class", "rel_parameter"]]  # 1 item
         object_p_values = [["example_class", "example_object", "example_parameter", 3.14]]  # 1 item
         rel_p_values = [["example_rel_class", ["example_object", "other_object"], "rel_parameter", 2.718]]  # 1
-        alternatives = [['example_alternative', 'An example']]
-        scenarios = [['example_scenario', True, 'An example']]
-        scenario_alternatives = [['example_scenario', 'example_alternative']]
+        alternatives = [["example_alternative", "An example"]]
+        scenarios = [["example_scenario", True, "An example"]]
+        scenario_alternatives = [["example_scenario", "example_alternative"]]
 
         num_imports, errors = import_data(
             db_map,
             object_classes=object_c,
             relationship_classes=relationship_c,
             object_parameters=obj_parameters,
             relationship_parameters=rel_parameters,
@@ -107,233 +100,303 @@
             relationships=relationships,
             object_parameter_values=object_p_values,
             relationship_parameter_values=rel_p_values,
             alternatives=alternatives,
             scenarios=scenarios,
             scenario_alternatives=scenario_alternatives,
         )
-        db_map.connection.close()
+        db_map.close()
         self.assertEqual(num_imports, 13)
         self.assertFalse(errors)
 
 
 class TestImportObjectClass(unittest.TestCase):
     def test_import_object_class(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         _, errors = import_object_classes(db_map, ["new_class"])
         self.assertFalse(errors)
+        db_map.commit_session("test")
         self.assertIn("new_class", [oc.name for oc in db_map.query(db_map.object_class_sq)])
-        db_map.connection.close()
+        db_map.close()
 
 
 class TestImportObject(unittest.TestCase):
     def test_import_valid_objects(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class"])
         _, errors = import_objects(db_map, [["object_class", "new_object"]])
         self.assertFalse(errors)
+        db_map.commit_session("test")
         self.assertIn("new_object", [o.name for o in db_map.query(db_map.object_sq)])
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_object_with_invalid_object_class_name(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         _, errors = import_objects(db_map, [["nonexistent_class", "new_object"]])
         self.assertTrue(errors)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_two_objects_with_same_name(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class1", "object_class2"])
         _, errors = import_objects(db_map, [["object_class1", "object"], ["object_class2", "object"]])
         self.assertFalse(errors)
+        db_map.commit_session("test")
         objects = {
             o.class_name: o.name
             for o in db_map.query(
                 db_map.object_sq.c.name.label("name"), db_map.object_class_sq.c.name.label("class_name")
             )
         }
         expected = {"object_class1": "object", "object_class2": "object"}
         self.assertEqual(objects, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_existing_object(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class"])
         import_objects(db_map, [["object_class", "object"]])
+        db_map.commit_session("test")
         self.assertIn("object", [o.name for o in db_map.query(db_map.object_sq)])
         _, errors = import_objects(db_map, [["object_class", "object"]])
         self.assertFalse(errors)
         self.assertIn("object", [o.name for o in db_map.query(db_map.object_sq)])
-        db_map.connection.close()
+        db_map.close()
 
 
 class TestImportRelationshipClass(unittest.TestCase):
     def test_import_valid_relationship_class(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class1", "object_class2"])
         _, errors = import_relationship_classes(db_map, [["relationship_class", ["object_class1", "object_class2"]]])
         self.assertFalse(errors)
+        db_map.commit_session("test")
         relationship_classes = {
             rc.name: rc.object_class_name_list for rc in db_map.query(db_map.wide_relationship_class_sq)
         }
         expected = {"relationship_class": "object_class1,object_class2"}
         self.assertEqual(relationship_classes, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_relationship_class_with_invalid_object_class_name(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class"])
         _, errors = import_relationship_classes(db_map, [["relationship_class", ["object_class", "nonexistent"]]])
         self.assertTrue(errors)
-        self.assertFalse([rc for rc in db_map.query(db_map.wide_relationship_class_sq)])
-        db_map.connection.close()
+        db_map.commit_session("test")
+        self.assertFalse(db_map.query(db_map.wide_relationship_class_sq).all())
+        db_map.close()
 
     def test_import_relationship_class_name_twice(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class1", "object_class2"])
         _, errors = import_relationship_classes(
             db_map, [["new_rc", ["object_class1", "object_class2"]], ["new_rc", ["object_class1", "object_class2"]]]
         )
         self.assertFalse(errors)
+        db_map.commit_session("test")
         relationship_classes = {
             rc.name: rc.object_class_name_list for rc in db_map.query(db_map.wide_relationship_class_sq)
         }
         expected = {"new_rc": "object_class1,object_class2"}
         self.assertEqual(relationship_classes, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_existing_relationship_class(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class1", "object_class2"])
         import_relationship_classes(db_map, [["rc", ["object_class1", "object_class2"]]])
         _, errors = import_relationship_classes(db_map, [["rc", ["object_class1", "object_class2"]]])
         self.assertFalse(errors)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_relationship_class_with_one_object_class_as_None(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class1"])
         _, errors = import_relationship_classes(db_map, [["new_rc", ["object_class", None]]])
         self.assertTrue(errors)
+        db_map.commit_session("test")
         self.assertFalse([rc for rc in db_map.query(db_map.wide_relationship_class_sq)])
-        db_map.connection.close()
+        db_map.close()
 
 
 class TestImportObjectClassParameter(unittest.TestCase):
     def test_import_valid_object_class_parameter(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class"])
         _, errors = import_object_parameters(db_map, [["object_class", "new_parameter"]])
         self.assertFalse(errors)
+        db_map.commit_session("test")
         self.assertIn("new_parameter", [p.name for p in db_map.query(db_map.parameter_definition_sq)])
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_parameter_with_invalid_object_class_name(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         _, errors = import_object_parameters(db_map, [["nonexistent_object_class", "new_parameter"]])
         self.assertTrue(errors)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_object_class_parameter_name_twice(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class1", "object_class2"])
         _, errors = import_object_parameters(
             db_map, [["object_class1", "new_parameter"], ["object_class2", "new_parameter"]]
         )
         self.assertFalse(errors)
+        db_map.commit_session("test")
         definitions = {
             definition.object_class_name: definition.parameter_name
             for definition in db_map.query(db_map.object_parameter_definition_sq)
         }
         expected = {"object_class1": "new_parameter", "object_class2": "new_parameter"}
         self.assertEqual(definitions, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_existing_object_class_parameter(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class"])
         import_object_parameters(db_map, [["object_class", "parameter"]])
+        db_map.commit_session("test")
         self.assertIn("parameter", [p.name for p in db_map.query(db_map.parameter_definition_sq)])
         _, errors = import_object_parameters(db_map, [["object_class", "parameter"]])
         self.assertIn("parameter", [p.name for p in db_map.query(db_map.parameter_definition_sq)])
         self.assertFalse(errors)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_object_class_parameter_with_null_default_value_and_db_server_unparsing(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ["object_class"])
         _, errors = import_object_parameters(
             db_map, [["object_class", "parameter", [None, None]]], unparse_value=_unparse_value
         )
         self.assertEqual(errors, [])
         db_map.commit_session("Add test data.")
         parameters = db_map.query(db_map.object_parameter_definition_sq).all()
         self.assertEqual(len(parameters), 1)
         self.assertIsNone(parameters[0].default_value)
         self.assertIsNone(parameters[0].default_type)
-        db_map.connection.close()
+        db_map.close()
 
 
 class TestImportRelationshipClassParameter(unittest.TestCase):
     def test_import_valid_relationship_class_parameter(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class1", "object_class2"])
         import_relationship_classes(db_map, [["relationship_class", ["object_class1", "object_class2"]]])
         _, errors = import_relationship_parameters(db_map, [["relationship_class", "new_parameter"]])
         self.assertFalse(errors)
+        db_map.commit_session("test")
         definitions = {
             d.class_name: d.name
             for d in db_map.query(
                 db_map.relationship_parameter_definition_sq.c.parameter_name.label("name"),
                 db_map.relationship_class_sq.c.name.label("class_name"),
             )
         }
         expected = {"relationship_class": "new_parameter"}
         self.assertEqual(definitions, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_parameter_with_invalid_relationship_class_name(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         _, errors = import_relationship_parameters(db_map, [["nonexistent_relationship_class", "new_parameter"]])
         self.assertTrue(errors)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_relationship_class_parameter_name_twice(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class1", "object_class2"])
         import_relationship_classes(
             db_map,
             [
                 ["relationship_class1", ["object_class1", "object_class2"]],
                 ["relationship_class2", ["object_class2", "object_class1"]],
             ],
         )
         _, errors = import_relationship_parameters(
             db_map, [["relationship_class1", "new_parameter"], ["relationship_class2", "new_parameter"]]
         )
         self.assertFalse(errors)
+        db_map.commit_session("test")
         definitions = {
             d.class_name: d.name
             for d in db_map.query(
                 db_map.relationship_parameter_definition_sq.c.parameter_name.label("name"),
                 db_map.relationship_class_sq.c.name.label("class_name"),
             )
         }
         expected = {"relationship_class1": "new_parameter", "relationship_class2": "new_parameter"}
         self.assertEqual(definitions, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_existing_relationship_class_parameter(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class1", "object_class2"])
         import_relationship_classes(db_map, [["relationship_class", ["object_class1", "object_class2"]]])
         import_relationship_parameters(db_map, [["relationship_class", "new_parameter"]])
         _, errors = import_relationship_parameters(db_map, [["relationship_class", "new_parameter"]])
         self.assertFalse(errors)
-        db_map.connection.close()
+        db_map.close()
+
+
+class TestImportEntityClasses(unittest.TestCase):
+    def _assert_success(self, result):
+        items, errors = result
+        self.assertEqual(errors, [])
+        return items
+
+    def test_import_object_class_with_all_optional_data(self):
+        with DatabaseMapping("sqlite://", create=True) as db_map:
+            self._assert_success(
+                import_entity_classes(
+                    db_map,
+                    (
+                        ("Object", (), "The test class.", 23, True),
+                        ("Relation", ("Object",), "The test relationship.", 5, False),
+                    ),
+                )
+            )
+            entity_classes = db_map.get_entity_class_items()
+            self.assertEqual(len(entity_classes), 2)
+            data = (
+                (
+                    row["name"],
+                    row["dimension_name_list"],
+                    row["description"],
+                    row["display_icon"],
+                    row["active_by_default"],
+                )
+                for row in entity_classes
+            )
+            expected = (
+                ("Object", (), "The test class.", 23, True),
+                ("Relation", ("Object",), "The test relationship.", 5, False),
+            )
+            self.assertCountEqual(data, expected)
+
+
+class TestImportEntity(unittest.TestCase):
+    def test_import_multi_d_entity_twice(self):
+        db_map = DatabaseMapping("sqlite://", create=True)
+        import_data(
+            db_map,
+            entity_classes=(
+                ("object_class1",),
+                ("object_class2",),
+                ("relationship_class", ("object_class1", "object_class2")),
+            ),
+            entities=(
+                ("object_class1", "object1"),
+                ("object_class2", "object2"),
+                ("relationship_class", ("object1", "object2")),
+            ),
+        )
+        count, errors = import_data(db_map, entities=(("relationship_class", ("object1", "object2")),))
+        self.assertEqual(count, 0)
+        self.assertEqual(errors, [])
 
 
 class TestImportRelationship(unittest.TestCase):
     @staticmethod
     def populate(db_map):
         import_object_classes(db_map, ["object_class1", "object_class2"])
         import_objects(db_map, [["object_class1", "object1"], ["object_class2", "object2"]])
@@ -341,90 +404,227 @@
     def test_import_relationships(self):
         db_map = DatabaseMapping("sqlite://", create=True)
         import_object_classes(db_map, ("object_class",))
         import_objects(db_map, (("object_class", "object"),))
         import_relationship_classes(db_map, (("relationship_class", ("object_class",)),))
         _, errors = import_relationships(db_map, (("relationship_class", ("object",)),))
         self.assertFalse(errors)
-        self.assertIn("relationship_class_object", [r.name for r in db_map.query(db_map.relationship_sq)])
-        db_map.connection.close()
+        db_map.commit_session("test")
+        self.assertIn("object__", [r.name for r in db_map.query(db_map.relationship_sq)])
+        db_map.close()
 
     def test_import_valid_relationship(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         import_relationship_classes(db_map, [["relationship_class", ["object_class1", "object_class2"]]])
         _, errors = import_relationships(db_map, [["relationship_class", ["object1", "object2"]]])
         self.assertFalse(errors)
-        self.assertIn("relationship_class_object1__object2", [r.name for r in db_map.query(db_map.relationship_sq)])
-        db_map.connection.close()
+        db_map.commit_session("test")
+        self.assertIn("object1__object2", [r.name for r in db_map.query(db_map.relationship_sq)])
+        db_map.close()
 
     def test_import_valid_relationship_with_object_name_in_multiple_classes(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         import_objects(db_map, [["object_class1", "duplicate"], ["object_class2", "duplicate"]])
         import_relationship_classes(db_map, [["relationship_class", ["object_class1", "object_class2"]]])
         _, errors = import_relationships(db_map, [["relationship_class", ["duplicate", "object2"]]])
         self.assertFalse(errors)
-        self.assertIn("relationship_class_duplicate__object2", [r.name for r in db_map.query(db_map.relationship_sq)])
-        db_map.connection.close()
+        db_map.commit_session("test")
+        self.assertIn("duplicate__object2", [r.name for r in db_map.query(db_map.relationship_sq)])
+        db_map.close()
 
     def test_import_relationship_with_invalid_class_name(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         _, errors = import_relationships(db_map, [["nonexistent_relationship_class", ["object1", "object2"]]])
         self.assertTrue(errors)
+        db_map.commit_session("test")
         self.assertFalse([r.name for r in db_map.query(db_map.relationship_sq)])
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_relationship_with_invalid_object_name(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         import_relationship_classes(db_map, [["relationship_class", ["object_class1", "object_class2"]]])
         _, errors = import_relationships(db_map, [["relationship_class", ["nonexistent_object", "object2"]]])
         self.assertTrue(errors)
+        db_map.commit_session("test")
         self.assertFalse([r.name for r in db_map.query(db_map.relationship_sq)])
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_existing_relationship(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         import_relationship_classes(db_map, [["relationship_class", ["object_class1", "object_class2"]]])
         import_relationships(db_map, [["relationship_class", ["object1", "object2"]]])
-        self.assertIn("relationship_class_object1__object2", [r.name for r in db_map.query(db_map.relationship_sq)])
+        db_map.commit_session("test")
+        self.assertIn("object1__object2", [r.name for r in db_map.query(db_map.relationship_sq)])
         _, errors = import_relationships(db_map, [["relationship_class", ["object1", "object2"]]])
         self.assertFalse(errors)
-        self.assertIn("relationship_class_object1__object2", [r.name for r in db_map.query(db_map.relationship_sq)])
-        db_map.connection.close()
+        self.assertIn("object1__object2", [r.name for r in db_map.query(db_map.relationship_sq)])
+        db_map.close()
 
     def test_import_relationship_with_one_None_object(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         import_relationship_classes(db_map, [["relationship_class", ["object_class1", "object_class2"]]])
         _, errors = import_relationships(db_map, [["relationship_class", [None, "object2"]]])
         self.assertTrue(errors)
+        db_map.commit_session("test")
         self.assertFalse([r.name for r in db_map.query(db_map.relationship_sq)])
-        db_map.connection.close()
+        db_map.close()
+
+    def test_import_multi_d_entity_with_elements_from_superclass(self):
+        db_map = create_db_map()
+        import_data(
+            db_map,
+            entity_classes=[
+                ["object_class1", []],
+                ["object_class2", []],
+                ["superclass", []],
+                ["relationship_class1", ["superclass", "superclass"]],
+            ],
+            superclass_subclasses=[["superclass", "object_class1"], ["superclass", "object_class2"]],
+            entities=[["object_class1", "object1"], ["object_class2", "object2"]],
+        )
+        _, errors = import_data(db_map, entities=[["relationship_class1", ["object1", "object2"]]])
+        self.assertFalse(errors)
+        db_map.commit_session("test")
+        entities = {
+            tuple(r.element_name_list.split(",")) if r.element_name_list else r.name: r.name
+            for r in db_map.query(db_map.wide_entity_sq)
+        }
+        self.assertTrue("object1" in entities)
+        self.assertTrue("object2" in entities)
+        self.assertTrue(("object1", "object2") in entities)
+        self.assertEqual(len(entities), 3)
+
+    def test_import_multi_d_entity_with_elements_from_superclass_fails_with_wrong_dimension_count(self):
+        db_map = create_db_map()
+        import_data(
+            db_map,
+            entity_classes=[
+                ["object_class1", []],
+                ["object_class2", []],
+                ["superclass", []],
+                ["relationship_class1", ["superclass", "superclass"]],
+            ],
+            superclass_subclasses=[["superclass", "object_class1"], ["superclass", "object_class2"]],
+            entities=[["object_class1", "object1"], ["object_class2", "object2"]],
+        )
+        _, errors = import_data(db_map, entities=[["relationship_class1", ["object1"]]])
+        self.assertEqual(len(errors), 1)
+        self.assertIn("too few elements", errors[0])
+        _, errors = import_data(db_map, entities=[["relationship_class1", ["object1", "object2", "object1"]]])
+        self.assertEqual(len(errors), 1)
+        self.assertIn("too many elements", errors[0])
+
+    def test_import_multi_d_entity_with_multi_d_elements(self):
+        db_map = create_db_map()
+        self.populate(db_map)
+        import_data(
+            db_map,
+            entity_classes=[
+                ["relationship_class1", ["object_class1", "object_class2"]],
+                ["relationship_class2", ["object_class2", "object_class1"]],
+                ["meta_relationship_class", ["relationship_class1", "relationship_class2"]],
+            ],
+            entities=[["relationship_class1", ["object1", "object2"]], ["relationship_class2", ["object2", "object1"]]],
+        )
+        _, errors = import_data(
+            db_map, entities=[["meta_relationship_class", ["object1", "object2", "object2", "object1"]]]
+        )
+        self.assertFalse(errors)
+        db_map.commit_session("test")
+        entities = {
+            tuple(r.element_name_list.split(",")) if r.element_name_list else r.name: r.name
+            for r in db_map.query(db_map.wide_entity_sq)
+        }
+        self.assertTrue("object1" in entities)
+        self.assertTrue("object2" in entities)
+        self.assertTrue(("object1", "object2") in entities)
+        self.assertTrue(("object2", "object1") in entities)
+        self.assertTrue((entities["object1", "object2"], entities["object2", "object1"]) in entities)
+        self.assertEqual(len(entities), 5)
+
+    def test_import_multi_d_entity_with_multi_d_elements_from_superclass(self):
+        db_map = create_db_map()
+        self.populate(db_map)
+        import_data(
+            db_map,
+            entity_classes=[
+                ["relationship_class1", ["object_class1", "object_class2"]],
+                ["relationship_class2", ["object_class2", "object_class1"]],
+                ["superclass", []],
+            ],
+            superclass_subclasses=[["superclass", "relationship_class1"], ["superclass", "relationship_class2"]],
+        )
+        import_data(
+            db_map,
+            entity_classes=[["meta_relationship_class", ["superclass", "superclass"]]],
+            entities=[["relationship_class1", ["object1", "object2"]], ["relationship_class2", ["object2", "object1"]]],
+        )
+        _, errors = import_data(
+            db_map, entities=[["meta_relationship_class", ["object1", "object2", "object2", "object1"]]]
+        )
+        self.assertFalse(errors)
+        db_map.commit_session("test")
+        entities = {
+            tuple(r.element_name_list.split(",")) if r.element_name_list else r.name: r.name
+            for r in db_map.query(db_map.wide_entity_sq)
+        }
+        self.assertTrue("object1" in entities)
+        self.assertTrue("object2" in entities)
+        self.assertTrue(("object1", "object2") in entities)
+        self.assertTrue(("object2", "object1") in entities)
+        self.assertTrue((entities["object1", "object2"], entities["object2", "object1"]) in entities)
+        self.assertEqual(len(entities), 5)
+
+    def test_import_multi_d_entity_with_multi_d_elements_from_superclass_fails_with_wrong_dimension_count(self):
+        db_map = create_db_map()
+        self.populate(db_map)
+        import_data(
+            db_map,
+            entity_classes=[
+                ["relationship_class1", ["object_class1", "object_class2"]],
+                ["relationship_class2", ["object_class2", "object_class1"]],
+                ["superclass", []],
+            ],
+            superclass_subclasses=[["superclass", "relationship_class1"], ["superclass", "relationship_class2"]],
+        )
+        import_data(
+            db_map,
+            entity_classes=[["meta_relationship_class", ["superclass", "superclass"]]],
+            entities=[["relationship_class1", ["object1", "object2"]], ["relationship_class2", ["object2", "object1"]]],
+        )
+        _, errors = import_data(db_map, entities=[["meta_relationship_class", ["object1", "object2", "object2"]]])
+        self.assertEqual(len(errors), 1)
+        self.assertIn("too few elements", errors[0])
+        _, errors = import_data(
+            db_map, entities=[["meta_relationship_class", ["object1", "object2", "object2", "object1", "object1"]]]
+        )
+        self.assertEqual(len(errors), 1)
+        self.assertIn("too many elements", errors[0])
 
 
 class TestImportParameterDefinition(unittest.TestCase):
     def setUp(self):
         self._db_map = DatabaseMapping("sqlite://", create=True)
 
     def tearDown(self):
-        self._db_map.connection.close()
+        self._db_map.close()
 
     def test_import_object_parameter_definition(self):
         import_object_classes(self._db_map, ["my_object_class"])
         count, errors = import_object_parameters(self._db_map, (("my_object_class", "my_parameter"),))
         self.assertEqual(errors, [])
         self.assertEqual(count, 1)
         self._db_map.commit_session("Add test data.")
-        parameter_definitions = [
-            row._asdict() for row in self._db_map.query(self._db_map.object_parameter_definition_sq)
-        ]
+        parameter_definitions = [dict(row) for row in self._db_map.query(self._db_map.object_parameter_definition_sq)]
         self.assertEqual(
             parameter_definitions,
             [
                 {
                     "default_type": None,
                     "default_value": None,
                     "description": None,
@@ -443,17 +643,15 @@
     def test_import_object_parameter_definition_with_value_list(self):
         import_object_classes(self._db_map, ["my_object_class"])
         import_parameter_value_lists(self._db_map, (("my_list", 99.0),))
         count, errors = import_object_parameters(self._db_map, (("my_object_class", "my_parameter", None, "my_list"),))
         self.assertEqual(errors, [])
         self.assertEqual(count, 1)
         self._db_map.commit_session("Add test data.")
-        parameter_definitions = [
-            row._asdict() for row in self._db_map.query(self._db_map.object_parameter_definition_sq)
-        ]
+        parameter_definitions = [dict(row) for row in self._db_map.query(self._db_map.object_parameter_definition_sq)]
         self.assertEqual(
             parameter_definitions,
             [
                 {
                     "default_type": None,
                     "default_value": b"null",
                     "description": None,
@@ -472,17 +670,15 @@
     def test_import_object_parameter_definition_with_default_value_from_value_list(self):
         import_object_classes(self._db_map, ["my_object_class"])
         import_parameter_value_lists(self._db_map, (("my_list", 99.0),))
         count, errors = import_object_parameters(self._db_map, (("my_object_class", "my_parameter", 99.0, "my_list"),))
         self.assertEqual(errors, [])
         self.assertEqual(count, 1)
         self._db_map.commit_session("Add test data.")
-        parameter_definitions = [
-            row._asdict() for row in self._db_map.query(self._db_map.object_parameter_definition_sq)
-        ]
+        parameter_definitions = [dict(row) for row in self._db_map.query(self._db_map.object_parameter_definition_sq)]
         self.assertEqual(
             parameter_definitions,
             [
                 {
                     "default_type": None,
                     "default_value": b"99.0",
                     "description": None,
@@ -498,21 +694,15 @@
             ],
         )
 
     def test_import_object_parameter_definition_with_default_value_from_value_list_fails_gracefully(self):
         import_object_classes(self._db_map, ["my_object_class"])
         import_parameter_value_lists(self._db_map, (("my_list", 99.0),))
         count, errors = import_object_parameters(self._db_map, (("my_object_class", "my_parameter", 23.0, "my_list"),))
-        self.assertEqual(
-            [error.msg for error in errors],
-            [
-                "Could not import parameter 'my_parameter' with class 'my_object_class': "
-                "Invalid default_value '23.0' - it should be one from the parameter value list: '99.0'."
-            ],
-        )
+        self.assertEqual(errors, ["default value 23.0 of my_parameter is not in my_list"])
         self.assertEqual(count, 0)
 
 
 class TestImportParameterValue(unittest.TestCase):
     @staticmethod
     def populate(db_map):
         import_object_classes(db_map, ["object_class1", "object_class2"])
@@ -523,136 +713,146 @@
     def populate_with_relationship(db_map):
         TestImportParameterValue.populate(db_map)
         import_relationship_classes(db_map, [["relationship_class", ["object_class1", "object_class2"]]])
         import_relationship_parameters(db_map, [["relationship_class", "parameter"]])
         import_relationships(db_map, [["relationship_class", ["object1", "object2"]]])
 
     def test_import_valid_object_parameter_value(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         _, errors = import_object_parameter_values(db_map, [["object_class1", "object1", "parameter", 1]])
         self.assertFalse(errors)
+        db_map.commit_session("test")
         values = {v.object_name: v.value for v in db_map.query(db_map.object_parameter_value_sq)}
         expected = {"object1": b"1"}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_valid_object_parameter_value_string(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         _, errors = import_object_parameter_values(db_map, [["object_class1", "object1", "parameter", "value_string"]])
         self.assertFalse(errors)
+        db_map.commit_session("test")
         values = {v.object_name: v.value for v in db_map.query(db_map.object_parameter_value_sq)}
         expected = {"object1": b'"value_string"'}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_valid_object_parameter_value_with_duplicate_object_name(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         import_objects(db_map, [["object_class1", "duplicate_object"], ["object_class2", "duplicate_object"]])
         _, errors = import_object_parameter_values(db_map, [["object_class1", "duplicate_object", "parameter", 1]])
         self.assertFalse(errors)
+        db_map.commit_session("test")
         values = {v.object_class_name: {v.object_name: v.value} for v in db_map.query(db_map.object_parameter_value_sq)}
         expected = {"object_class1": {"duplicate_object": b"1"}}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_valid_object_parameter_value_with_duplicate_parameter_name(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         import_object_parameters(db_map, [["object_class2", "parameter"]])
         _, errors = import_object_parameter_values(db_map, [["object_class1", "object1", "parameter", 1]])
         self.assertFalse(errors)
+        db_map.commit_session("test")
         values = {v.object_class_name: {v.object_name: v.value} for v in db_map.query(db_map.object_parameter_value_sq)}
         expected = {"object_class1": {"object1": b"1"}}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_object_parameter_value_with_invalid_object(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class"])
         import_object_parameters(db_map, [["object_class", "parameter"]])
         _, errors = import_object_parameter_values(db_map, [["object_class", "nonexistent_object", "parameter", 1]])
         self.assertTrue(errors)
+        db_map.commit_session("test")
         self.assertFalse(db_map.query(db_map.object_parameter_value_sq).all())
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_object_parameter_value_with_invalid_parameter(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_object_classes(db_map, ["object_class"])
         import_objects(db_map, ["object_class", "object"])
         _, errors = import_object_parameter_values(db_map, [["object_class", "object", "nonexistent_parameter", 1]])
         self.assertTrue(errors)
+        db_map.commit_session("test")
         self.assertFalse(db_map.query(db_map.object_parameter_value_sq).all())
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_existing_object_parameter_value_update_the_value(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         import_object_parameter_values(db_map, [["object_class1", "object1", "parameter", "initial_value"]])
         _, errors = import_object_parameter_values(db_map, [["object_class1", "object1", "parameter", "new_value"]])
         self.assertFalse(errors)
+        db_map.commit_session("test")
         values = {v.object_name: v.value for v in db_map.query(db_map.object_parameter_value_sq)}
         expected = {"object1": b'"new_value"'}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_existing_object_parameter_value_on_conflict_keep(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         initial_value = {"type": "time_series", "data": [("2000-01-01T01:00", "1"), ("2000-01-01T02:00", "2")]}
         new_value = {"type": "time_series", "data": [("2000-01-01T02:00", "3"), ("2000-01-01T03:00", "4")]}
         import_object_parameter_values(db_map, [["object_class1", "object1", "parameter", initial_value]])
         _, errors = import_object_parameter_values(
             db_map, [["object_class1", "object1", "parameter", new_value]], on_conflict="keep"
         )
         self.assertFalse(errors)
+        db_map.commit_session("test")
         pv = db_map.query(db_map.object_parameter_value_sq).filter_by(object_name="object1").first()
         value = from_database(pv.value, pv.type)
-        self.assertEqual(['2000-01-01T01:00:00', '2000-01-01T02:00:00'], [str(x) for x in value.indexes])
+        self.assertEqual(["2000-01-01T01:00:00", "2000-01-01T02:00:00"], [str(x) for x in value.indexes])
         self.assertEqual([1.0, 2.0], list(value.values))
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_existing_object_parameter_value_on_conflict_replace(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         initial_value = {"type": "time_series", "data": [("2000-01-01T01:00", "1"), ("2000-01-01T02:00", "2")]}
         new_value = {"type": "time_series", "data": [("2000-01-01T02:00", "3"), ("2000-01-01T03:00", "4")]}
         import_object_parameter_values(db_map, [["object_class1", "object1", "parameter", initial_value]])
         _, errors = import_object_parameter_values(
             db_map, [["object_class1", "object1", "parameter", new_value]], on_conflict="replace"
         )
         self.assertFalse(errors)
+        db_map.commit_session("test")
         pv = db_map.query(db_map.object_parameter_value_sq).filter_by(object_name="object1").first()
         value = from_database(pv.value, pv.type)
-        self.assertEqual(['2000-01-01T02:00:00', '2000-01-01T03:00:00'], [str(x) for x in value.indexes])
+        self.assertEqual(["2000-01-01T02:00:00", "2000-01-01T03:00:00"], [str(x) for x in value.indexes])
         self.assertEqual([3.0, 4.0], list(value.values))
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_existing_object_parameter_value_on_conflict_merge(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         initial_value = {"type": "time_series", "data": [("2000-01-01T01:00", "1"), ("2000-01-01T02:00", "2")]}
         new_value = {"type": "time_series", "data": [("2000-01-01T02:00", "3"), ("2000-01-01T03:00", "4")]}
         import_object_parameter_values(db_map, [["object_class1", "object1", "parameter", initial_value]])
         _, errors = import_object_parameter_values(
             db_map, [["object_class1", "object1", "parameter", new_value]], on_conflict="merge"
         )
         self.assertFalse(errors)
+        db_map.commit_session("test")
         pv = db_map.query(db_map.object_parameter_value_sq).filter_by(object_name="object1").first()
         value = from_database(pv.value, pv.type)
         self.assertEqual(
-            ['2000-01-01T01:00:00', '2000-01-01T02:00:00', '2000-01-01T03:00:00'], [str(x) for x in value.indexes]
+            ["2000-01-01T01:00:00", "2000-01-01T02:00:00", "2000-01-01T03:00:00"], [str(x) for x in value.indexes]
         )
         self.assertEqual([1.0, 3.0, 4.0], list(value.values))
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_existing_object_parameter_value_on_conflict_merge_map(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         initial_value = {
             "type": "map",
             "index_type": "str",
             "data": {"xxx": {"type": "time_series", "data": [("2000-01-01T01:00", "1"), ("2000-01-01T02:00", "2")]}},
         }
         new_value = {
@@ -661,272 +861,341 @@
             "data": {"xxx": {"type": "time_series", "data": [("2000-01-01T02:00", "3"), ("2000-01-01T03:00", "4")]}},
         }
         import_object_parameter_values(db_map, [["object_class1", "object1", "parameter", initial_value]])
         _, errors = import_object_parameter_values(
             db_map, [["object_class1", "object1", "parameter", new_value]], on_conflict="merge"
         )
         self.assertFalse(errors)
+        db_map.commit_session("test")
         pv = db_map.query(db_map.object_parameter_value_sq).filter_by(object_name="object1").first()
         map_ = from_database(pv.value, pv.type)
-        self.assertEqual(['xxx'], [str(x) for x in map_.indexes])
-        ts = map_.get_value('xxx')
+        self.assertEqual(["xxx"], [str(x) for x in map_.indexes])
+        ts = map_.get_value("xxx")
         self.assertEqual(
-            ['2000-01-01T01:00:00', '2000-01-01T02:00:00', '2000-01-01T03:00:00'], [str(x) for x in ts.indexes]
+            ["2000-01-01T01:00:00", "2000-01-01T02:00:00", "2000-01-01T03:00:00"], [str(x) for x in ts.indexes]
         )
         self.assertEqual([1.0, 3.0, 4.0], list(ts.values))
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_duplicate_object_parameter_value(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         _, errors = import_object_parameter_values(
             db_map,
             [["object_class1", "object1", "parameter", "first"], ["object_class1", "object1", "parameter", "second"]],
         )
         self.assertTrue(errors)
+        db_map.commit_session("test")
         values = {v.object_name: v.value for v in db_map.query(db_map.object_parameter_value_sq)}
         expected = {"object1": b'"first"'}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_object_parameter_value_with_alternative(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         import_alternatives(db_map, ["alternative"])
         count, errors = import_object_parameter_values(
             db_map, [["object_class1", "object1", "parameter", 1, "alternative"]]
         )
         self.assertFalse(errors)
         self.assertEqual(count, 1)
+        db_map.commit_session("test")
         values = {
-            v.object_name: (v.value, v.alternative_name)
-            for v in db_map.query(
-                db_map.object_parameter_value_sq, db_map.alternative_sq.c.name.label("alternative_name")
-            )
-            .filter(db_map.object_parameter_value_sq.c.alternative_id == db_map.alternative_sq.c.id)
-            .all()
+            v.object_name: (v.value, v.alternative_name) for v in db_map.query(db_map.object_parameter_value_sq).all()
         }
         expected = {"object1": (b"1", "alternative")}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_object_parameter_value_fails_with_nonexistent_alternative(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         count, errors = import_object_parameter_values(
             db_map, [["object_class1", "object1", "parameter", 1, "nonexistent_alternative"]]
         )
         self.assertTrue(errors)
         self.assertEqual(count, 0)
-        db_map.connection.close()
+        db_map.close()
+
+    def test_import_parameter_values_from_committed_value_list(self):
+        db_map = create_db_map()
+        import_data(db_map, parameter_value_lists=(("values_1", 5.0),))
+        db_map.commit_session("test")
+        count, errors = import_data(
+            db_map,
+            object_classes=("object_class",),
+            object_parameters=(("object_class", "parameter", None, "values_1"),),
+            objects=(("object_class", "my_object"),),
+            object_parameter_values=(("object_class", "my_object", "parameter", 5.0),),
+        )
+        self.assertEqual(count, 4)
+        self.assertEqual(errors, [])
+        db_map.commit_session("test")
+        values = db_map.query(db_map.object_parameter_value_sq).all()
+        value = values[0]
+        self.assertEqual(from_database(value.value), 5.0)
+        db_map.close()
 
     def test_valid_object_parameter_value_from_value_list(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_parameter_value_lists(db_map, (("values_1", 5.0),))
         import_object_classes(db_map, ("object_class",))
         import_object_parameters(db_map, (("object_class", "parameter", None, "values_1"),))
         import_objects(db_map, (("object_class", "my_object"),))
         count, errors = import_object_parameter_values(db_map, (("object_class", "my_object", "parameter", 5.0),))
         self.assertEqual(count, 1)
         self.assertEqual(errors, [])
+        db_map.commit_session("test")
         values = db_map.query(db_map.object_parameter_value_sq).all()
         self.assertEqual(len(values), 1)
         value = values[0]
         self.assertEqual(from_database(value.value), 5.0)
-        db_map.connection.close()
+        db_map.close()
 
     def test_non_existent_object_parameter_value_from_value_list_fails_gracefully(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_parameter_value_lists(db_map, (("values_1", 5.0),))
         import_object_classes(db_map, ("object_class",))
         import_object_parameters(db_map, (("object_class", "parameter", None, "values_1"),))
         import_objects(db_map, (("object_class", "my_object"),))
         count, errors = import_object_parameter_values(db_map, (("object_class", "my_object", "parameter", 2.3),))
         self.assertEqual(count, 0)
         self.assertEqual(len(errors), 1)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_valid_relationship_parameter_value(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate_with_relationship(db_map)
         _, errors = import_relationship_parameter_values(
             db_map, [["relationship_class", ["object1", "object2"], "parameter", 1]]
         )
         self.assertFalse(errors)
+        db_map.commit_session("test")
         values = {v.object_name_list: v.value for v in db_map.query(db_map.relationship_parameter_value_sq)}
         expected = {"object1,object2": b"1"}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_valid_relationship_parameter_value_with_duplicate_parameter_name(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate_with_relationship(db_map)
         import_relationship_classes(db_map, [["relationship_class2", ["object_class2", "object_class1"]]])
         import_relationship_parameters(db_map, [["relationship_class2", "parameter"]])
         _, errors = import_relationship_parameter_values(
             db_map, [["relationship_class", ["object1", "object2"], "parameter", 1]]
         )
         self.assertFalse(errors)
+        db_map.commit_session("test")
         values = {v.object_name_list: v.value for v in db_map.query(db_map.relationship_parameter_value_sq)}
         expected = {"object1,object2": b"1"}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_valid_relationship_parameter_value_with_duplicate_object_name(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate_with_relationship(db_map)
         import_objects(db_map, [["object_class1", "duplicate_object"], ["object_class2", "duplicate_object"]])
         import_relationships(db_map, [["relationship_class", ["duplicate_object", "duplicate_object"]]])
         _, errors = import_relationship_parameter_values(
             db_map, [["relationship_class", ["duplicate_object", "duplicate_object"], "parameter", 1]]
         )
         self.assertFalse(errors)
+        db_map.commit_session("test")
         values = {v.object_name_list: v.value for v in db_map.query(db_map.relationship_parameter_value_sq)}
         expected = {"duplicate_object,duplicate_object": b"1"}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_relationship_parameter_value_with_invalid_object(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate_with_relationship(db_map)
         _, errors = import_relationship_parameter_values(
             db_map, [["relationship_class", ["nonexistent_object", "object2"], "parameter", 1]]
         )
         self.assertTrue(errors)
+        db_map.commit_session("test")
         self.assertFalse(db_map.query(db_map.relationship_parameter_value_sq).all())
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_relationship_parameter_value_with_invalid_relationship_class(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate_with_relationship(db_map)
         _, errors = import_relationship_parameter_values(
             db_map, [["nonexistent_class", ["object1", "object2"], "parameter", 1]]
         )
         self.assertTrue(errors)
+        db_map.commit_session("test")
         self.assertFalse(db_map.query(db_map.relationship_parameter_value_sq).all())
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_relationship_parameter_value_with_invalid_parameter(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate_with_relationship(db_map)
         _, errors = import_relationship_parameter_values(
             db_map, [["relationship_class", ["object1", "object2"], "nonexistent_parameter", 1]]
         )
         self.assertTrue(errors)
+        db_map.commit_session("test")
         self.assertFalse(db_map.query(db_map.relationship_parameter_value_sq).all())
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_existing_relationship_parameter_value(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate_with_relationship(db_map)
         import_relationship_parameter_values(
             db_map, [["relationship_class", ["object1", "object2"], "parameter", "initial_value"]]
         )
         _, errors = import_relationship_parameter_values(
             db_map, [["relationship_class", ["object1", "object2"], "parameter", "new_value"]]
         )
         self.assertFalse(errors)
+        db_map.commit_session("test")
         values = {v.object_name_list: v.value for v in db_map.query(db_map.relationship_parameter_value_sq)}
         expected = {"object1,object2": b'"new_value"'}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_duplicate_relationship_parameter_value(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate_with_relationship(db_map)
         _, errors = import_relationship_parameter_values(
             db_map,
             [
                 ["relationship_class", ["object1", "object2"], "parameter", "first"],
                 ["relationship_class", ["object1", "object2"], "parameter", "second"],
             ],
         )
         self.assertTrue(errors)
+        db_map.commit_session("test")
         values = {v.object_name_list: v.value for v in db_map.query(db_map.relationship_parameter_value_sq)}
         expected = {"object1,object2": b'"first"'}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_relationship_parameter_value_with_alternative(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate_with_relationship(db_map)
         import_alternatives(db_map, ["alternative"])
         count, errors = import_relationship_parameter_values(
             db_map, [["relationship_class", ["object1", "object2"], "parameter", 1, "alternative"]]
         )
         self.assertFalse(errors)
         self.assertEqual(count, 1)
+        db_map.commit_session("test")
         values = {
             v.object_name_list: (v.value, v.alternative_name)
-            for v in db_map.query(
-                db_map.relationship_parameter_value_sq, db_map.alternative_sq.c.name.label("alternative_name")
-            )
-            .filter(db_map.relationship_parameter_value_sq.c.alternative_id == db_map.alternative_sq.c.id)
-            .all()
+            for v in db_map.query(db_map.relationship_parameter_value_sq).all()
         }
         expected = {"object1,object2": (b"1", "alternative")}
         self.assertEqual(values, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_relationship_parameter_value_fails_with_nonexistent_alternative(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         count, errors = import_relationship_parameter_values(
             db_map, [["relationship_class", ["object1", "object2"], "parameter", 1, "alternative"]]
         )
         self.assertTrue(errors)
         self.assertEqual(count, 0)
-        db_map.connection.close()
+        db_map.close()
 
     def test_valid_relationship_parameter_value_from_value_list(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_parameter_value_lists(db_map, (("values_1", 5.0),))
         import_object_classes(db_map, ("object_class",))
         import_objects(db_map, (("object_class", "my_object"),))
         import_relationship_classes(db_map, (("relationship_class", ("object_class",)),))
         import_relationship_parameters(db_map, (("relationship_class", "parameter", None, "values_1"),))
         import_relationships(db_map, (("relationship_class", ("my_object",)),))
         count, errors = import_relationship_parameter_values(
             db_map, (("relationship_class", ("my_object",), "parameter", 5.0),)
         )
         self.assertEqual(count, 1)
         self.assertEqual(errors, [])
+        db_map.commit_session("test")
         values = db_map.query(db_map.relationship_parameter_value_sq).all()
         self.assertEqual(len(values), 1)
         value = values[0]
         self.assertEqual(from_database(value.value), 5.0)
-        db_map.connection.close()
+        db_map.close()
 
     def test_non_existent_relationship_parameter_value_from_value_list_fails_gracefully(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_parameter_value_lists(db_map, (("values_1", 5.0),))
         import_object_classes(db_map, ("object_class",))
         import_objects(db_map, (("object_class", "my_object"),))
         import_relationship_classes(db_map, (("relationship_class", ("object_class",)),))
         import_relationship_parameters(db_map, (("relationship_class", "parameter", None, "values_1"),))
         import_relationships(db_map, (("relationship_class", ("my_object",)),))
         count, errors = import_relationship_parameter_values(
             db_map, (("relationship_class", ("my_object",), "parameter", 2.3),)
         )
         self.assertEqual(count, 0)
         self.assertEqual(len(errors), 1)
-        db_map.connection.close()
+        db_map.close()
+
+    def test_unparse_value_imports_fields_correctly(self):
+        with DatabaseMapping("sqlite:///", create=True) as db_map:
+            data = {
+                "entity_classes": [("A", (), None, None, False)],
+                "entities": [("A", "aa", None)],
+                "parameter_definitions": [("A", "test1", None, None, None)],
+                "parameter_values": [
+                    (
+                        "A",
+                        "aa",
+                        "test1",
+                        {
+                            "type": "time_series",
+                            "index": {
+                                "start": "2000-01-01 00:00:00",
+                                "resolution": "1h",
+                                "ignore_year": False,
+                                "repeat": False,
+                            },
+                            "data": [0.0, 1.0, 2.0, 4.0, 8.0, 0.0],
+                        },
+                        "Base",
+                    )
+                ],
+                "alternatives": [("Base", "Base alternative")],
+            }
+
+            count, errors = import_data(db_map, **data, unparse_value=dump_db_value)
+            self.assertEqual(errors, [])
+            self.assertEqual(count, 4)
+            db_map.commit_session("add test data")
+            value = db_map.query(db_map.entity_parameter_value_sq).one()
+            self.assertEqual(value.type, "time_series")
+            self.assertEqual(value.parameter_name, "test1")
+            self.assertEqual(value.alternative_name, "Base")
+            self.assertEqual(value.entity_class_name, "A")
+            self.assertEqual(value.entity_name, "aa")
+
+            time_series = from_database(value.value, value.type)
+            expected_result = TimeSeriesFixedResolution(
+                "2000-01-01 00:00:00", "1h", [0.0, 1.0, 2.0, 4.0, 8.0, 0.0], False, False
+            )
+            self.assertEqual(time_series, expected_result)
 
 
 class TestImportParameterValueList(unittest.TestCase):
     def setUp(self):
         self._db_map = DatabaseMapping("sqlite://", create=True)
 
     def tearDown(self):
-        self._db_map.connection.close()
+        self._db_map.close()
 
     def test_list_with_single_value(self):
         count, errors = import_parameter_value_lists(self._db_map, (("list_1", 23.0),))
         self.assertEqual(errors, [])
         self.assertEqual(count, 2)
+        self._db_map.commit_session("test")
         value_lists = self._db_map.query(self._db_map.parameter_value_list_sq).all()
         list_values = self._db_map.query(self._db_map.list_value_sq).all()
         self.assertEqual(len(value_lists), 1)
         self.assertEqual(len(list_values), 1)
         self.assertEqual(value_lists[0].name, "list_1")
         self.assertEqual(from_database(list_values[0].value, list_values[0].type), 23.0)
         self.assertEqual(list_values[0].index, 0)
@@ -936,492 +1205,300 @@
         initial_list = tuple(("list_1", 1.1 * i) for i in range(1, n_values + 1))
         count, errors = import_parameter_value_lists(self._db_map, initial_list)
         self.assertEqual(errors, [])
         self.assertEqual(count, n_values + 1)
         count, errors = import_parameter_value_lists(self._db_map, (("list_1", 23.0),))
         self.assertEqual(errors, [])
         self.assertEqual(count, 1)
+        self._db_map.commit_session("test")
         value_lists = self._db_map.query(self._db_map.parameter_value_list_sq).all()
         self.assertEqual(len(value_lists), 1)
         self.assertEqual(value_lists[0].name, "list_1")
         list_values = self._db_map.query(self._db_map.list_value_sq).all()
         self.assertEqual(len(list_values), n_values + 1)
         expected = {i: 1.1 * (i + 1) for i in range(n_values)}
         expected[len(expected)] = 23.0
         for row in list_values:
             self.assertEqual(from_database(row.value, row.type), expected[row.index])
 
 
 class TestImportAlternative(unittest.TestCase):
     def test_single_alternative(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         count, errors = import_alternatives(db_map, ["alternative"])
         self.assertEqual(count, 1)
         self.assertFalse(errors)
+        db_map.commit_session("test")
         alternatives = [a.name for a in db_map.query(db_map.alternative_sq)]
         self.assertEqual(len(alternatives), 2)
         self.assertIn("Base", alternatives)
         self.assertIn("alternative", alternatives)
-        db_map.connection.close()
+        db_map.close()
 
     def test_alternative_description(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         count, errors = import_alternatives(db_map, [["alternative", "description"]])
         self.assertEqual(count, 1)
         self.assertFalse(errors)
+        db_map.commit_session("test")
         alternatives = {a.name: a.description for a in db_map.query(db_map.alternative_sq)}
         expected = {"Base": "Base alternative", "alternative": "description"}
         self.assertEqual(alternatives, expected)
-        db_map.connection.close()
+        db_map.close()
 
     def test_update_alternative_description(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         count, errors = import_alternatives(db_map, [["Base", "new description"]])
         self.assertEqual(count, 1)
         self.assertFalse(errors)
+        db_map.commit_session("test")
         alternatives = {a.name: a.description for a in db_map.query(db_map.alternative_sq)}
         expected = {"Base": "new description"}
         self.assertEqual(alternatives, expected)
-        db_map.connection.close()
+        db_map.close()
 
 
 class TestImportScenario(unittest.TestCase):
     def test_single_scenario(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         count, errors = import_scenarios(db_map, ["scenario"])
         self.assertEqual(count, 1)
         self.assertFalse(errors)
+        db_map.commit_session("test")
         scenarios = {s.name: s.description for s in db_map.query(db_map.scenario_sq)}
         self.assertEqual(scenarios, {"scenario": None})
-        db_map.connection.close()
+        db_map.close()
 
     def test_scenario_with_description(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         count, errors = import_scenarios(db_map, [["scenario", False, "description"]])
         self.assertEqual(count, 1)
         self.assertFalse(errors)
+        db_map.commit_session("test")
         scenarios = {s.name: s.description for s in db_map.query(db_map.scenario_sq)}
         self.assertEqual(scenarios, {"scenario": "description"})
-        db_map.connection.close()
+        db_map.close()
 
     def test_update_scenario_description(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         import_scenarios(db_map, [["scenario", False, "initial description"]])
         count, errors = import_scenarios(db_map, [["scenario", False, "new description"]])
         self.assertEqual(count, 1)
         self.assertFalse(errors)
+        db_map.commit_session("test")
         scenarios = {s.name: s.description for s in db_map.query(db_map.scenario_sq)}
         self.assertEqual(scenarios, {"scenario": "new description"})
-        db_map.connection.close()
+        db_map.close()
 
 
 class TestImportScenarioAlternative(unittest.TestCase):
     def setUp(self):
-        self._db_map = create_diff_db_map()
+        self._db_map = create_db_map()
 
     def tearDown(self):
-        self._db_map.connection.close()
+        self._db_map.close()
 
     def test_single_scenario_alternative_import(self):
+        import_data(self._db_map, scenarios=["scenario"], alternatives=["alternative"])
         count, errors = import_scenario_alternatives(self._db_map, [["scenario", "alternative"]])
         self.assertFalse(errors)
-        self.assertEqual(count, 3)
-        scenario_alternatives = self.scenario_alternatives()
-        self.assertEqual(scenario_alternatives, {"scenario": {"alternative": 1}})
-
-    def test_scenario_alternative_import_imports_missing_scenarios_and_alternatives(self):
-        count, errors = import_scenario_alternatives(self._db_map, [["scenario", "alternative"]])
-        self.assertFalse(errors)
-        self.assertEqual(count, 3)
+        self.assertEqual(count, 1)
         scenario_alternatives = self.scenario_alternatives()
         self.assertEqual(scenario_alternatives, {"scenario": {"alternative": 1}})
 
     def test_scenario_alternative_import_multiple_without_before_alternatives(self):
+        import_data(self._db_map, scenarios=["scenario"], alternatives=["alternative1", "alternative2"])
         count, errors = import_scenario_alternatives(
             self._db_map, [["scenario", "alternative1"], ["scenario", "alternative2"]]
         )
         self.assertFalse(errors)
-        self.assertEqual(count, 5)
+        self.assertEqual(count, 2)
         scenario_alternatives = self.scenario_alternatives()
         self.assertEqual(scenario_alternatives, {"scenario": {"alternative1": 1, "alternative2": 2}})
 
     def test_scenario_alternative_import_multiple_with_before_alternatives(self):
+        import_data(self._db_map, scenarios=["scenario"], alternatives=["alternative1", "alternative2", "alternative3"])
         count, errors = import_scenario_alternatives(
             self._db_map,
             [["scenario", "alternative1"], ["scenario", "alternative3"], ["scenario", "alternative2", "alternative3"]],
         )
         self.assertFalse(errors)
-        self.assertEqual(count, 7)
+        self.assertEqual(count, 3)
         scenario_alternatives = self.scenario_alternatives()
         self.assertEqual(scenario_alternatives, {"scenario": {"alternative1": 1, "alternative2": 2, "alternative3": 3}})
 
     def test_fails_with_nonexistent_before_alternative(self):
+        import_data(self._db_map, scenarios=["scenario"], alternatives=["alternative"])
         count, errors = import_scenario_alternatives(
             self._db_map, [["scenario", "alternative", "nonexistent_alternative"]]
         )
-        self.assertTrue(errors)
-        self.assertEqual(count, 2)
+        self.assertEqual(
+            errors,
+            [
+                "can't insert alternative 'alternative' before 'nonexistent_alternative' "
+                "because the latter is not in scenario 'scenario'"
+            ],
+        )
+        self.assertEqual(count, 0)
+        scenario_alternatives = self.scenario_alternatives()
+        self.assertEqual(scenario_alternatives, {})
 
     def test_importing_existing_scenario_alternative_does_not_alter_scenario_alternatives(self):
+        import_data(self._db_map, scenarios=["scenario"], alternatives=["alternative1", "alternative2"])
         count, errors = import_scenario_alternatives(
             self._db_map,
             [["scenario", "alternative2", "alternative1"], ["scenario", "alternative1"]],
         )
         self.assertFalse(errors)
-        self.assertEqual(count, 5)
+        self.assertEqual(count, 2)
         scenario_alternatives = self.scenario_alternatives()
         self.assertEqual(scenario_alternatives, {"scenario": {"alternative1": 2, "alternative2": 1}})
         count, errors = import_scenario_alternatives(
             self._db_map,
             [["scenario", "alternative1"]],
         )
         self.assertFalse(errors)
-        self.assertEqual(count, 3)
-        scenario_alternatives = self.scenario_alternatives()
-        self.assertEqual(scenario_alternatives, {"scenario": {"alternative1": 2, "alternative2": 1}})
+        self.assertEqual(count, 0)
 
     def test_import_scenario_alternatives_in_arbitrary_order(self):
-        count, errors = import_scenarios(self._db_map, [('A (1)', False, '')])
+        count, errors = import_scenarios(self._db_map, [("A (1)", False, "")])
         self.assertEqual(errors, [])
         self.assertEqual(count, 1)
         count, errors = import_alternatives(
-            self._db_map, [('Base', 'Base alternative'), ('b', ''), ('c', ''), ('d', '')]
+            self._db_map, [("Base", "Base alternative"), ("b", ""), ("c", ""), ("d", "")]
         )
         self.assertEqual(errors, [])
-        self.assertEqual(count, 4)
+        self.assertEqual(count, 3)
         count, errors = import_scenario_alternatives(
-            self._db_map, [('A (1)', 'c', 'd'), ('A (1)', 'd', None), ('A (1)', 'Base', 'b'), ('A (1)', 'b', 'c')]
+            self._db_map, [("A (1)", "c", "d"), ("A (1)", "d", None), ("A (1)", "Base", "b"), ("A (1)", "b", "c")]
         )
         self.assertEqual(errors, [])
-        self.assertEqual(count, 9)
+        self.assertEqual(count, 4)
         scenario_alternatives = self.scenario_alternatives()
         self.assertEqual(scenario_alternatives, {"A (1)": {"Base": 1, "b": 2, "c": 3, "d": 4}})
 
+    def test_insert_scenario_alternative_in_the_middle_of_other_alternatives(self):
+        import_data(self._db_map, scenarios=["scenario"], alternatives=["alternative1", "alternative2", "alternative3"])
+        count, errors = import_scenario_alternatives(
+            self._db_map,
+            [["scenario", "alternative2", "alternative1"], ["scenario", "alternative1"]],
+        )
+        self.assertFalse(errors)
+        self.assertEqual(count, 2)
+        scenario_alternatives = self.scenario_alternatives()
+        self.assertEqual(scenario_alternatives, {"scenario": {"alternative1": 2, "alternative2": 1}})
+        count, errors = import_scenario_alternatives(self._db_map, [["scenario", "alternative3", "alternative1"]])
+        self.assertFalse(errors)
+        self.assertEqual(count, 2)
+        scenario_alternatives = self.scenario_alternatives()
+        self.assertEqual(scenario_alternatives, {"scenario": {"alternative1": 3, "alternative2": 1, "alternative3": 2}})
+
+    def test_import_inconsistent_scenario_alternatives(self):
+        import_data(self._db_map, scenarios=["scenario"], alternatives=["alternative1", "alternative2", "alternative3"])
+        count, errors = import_scenario_alternatives(
+            self._db_map,
+            [["scenario", "alternative3", "alternative1"], ["scenario", "alternative1"]],
+        )
+        self.assertFalse(errors)
+        self.assertEqual(count, 2)
+        scenario_alternatives = self.scenario_alternatives()
+        self.assertEqual(scenario_alternatives, {"scenario": {"alternative1": 2, "alternative3": 1}})
+        count, errors = import_scenario_alternatives(
+            self._db_map,
+            [
+                ["scenario", "alternative3", "alternative2"],
+                ["scenario", "alternative2", "alternative1"],
+                ["scenario", "alternative1"],
+            ],
+        )
+        self.assertFalse(errors)
+        self.assertEqual(count, 2)
+        scenario_alternatives = self.scenario_alternatives()
+        self.assertEqual(scenario_alternatives, {"scenario": {"alternative1": 3, "alternative2": 2, "alternative3": 1}})
+
     def scenario_alternatives(self):
+        self._db_map.commit_session("test")
         scenario_alternative_qry = (
             self._db_map.query(
                 self._db_map.scenario_sq.c.name.label("scenario_name"),
                 self._db_map.alternative_sq.c.name.label("alternative_name"),
                 self._db_map.scenario_alternative_sq.c.rank,
             )
             .filter(self._db_map.scenario_alternative_sq.c.scenario_id == self._db_map.scenario_sq.c.id)
             .filter(self._db_map.scenario_alternative_sq.c.alternative_id == self._db_map.alternative_sq.c.id)
         )
-        scenario_alternatives = dict()
+        scenario_alternatives = {}
         for scenario_alternative in scenario_alternative_qry:
-            alternative_rank = scenario_alternatives.setdefault(scenario_alternative.scenario_name, dict())
+            alternative_rank = scenario_alternatives.setdefault(scenario_alternative.scenario_name, {})
             alternative_rank[scenario_alternative.alternative_name] = scenario_alternative.rank
         return scenario_alternatives
 
 
-class TestImportTool(unittest.TestCase):
-    def test_single_tool(self):
-        db_map = create_diff_db_map()
-        count, errors = import_tools(db_map, ["tool"])
-        self.assertEqual(count, 1)
-        self.assertFalse(errors)
-        tools = [x.name for x in db_map.query(db_map.tool_sq)]
-        self.assertEqual(len(tools), 1)
-        self.assertIn("tool", tools)
-        db_map.connection.close()
-
-    def test_tool_description(self):
-        db_map = create_diff_db_map()
-        count, errors = import_tools(db_map, [["tool", "description"]])
-        self.assertEqual(count, 1)
-        self.assertFalse(errors)
-        tools = {x.name: x.description for x in db_map.query(db_map.tool_sq)}
-        expected = {"tool": "description"}
-        self.assertEqual(tools, expected)
-        db_map.connection.close()
-
-    def test_update_tool_description(self):
-        db_map = create_diff_db_map()
-        count, errors = import_tools(db_map, [["tool", "description"]])
-        count, errors = import_tools(db_map, [["tool", "new description"]])
-        self.assertEqual(count, 1)
-        self.assertFalse(errors)
-        tools = {x.name: x.description for x in db_map.query(db_map.tool_sq)}
-        expected = {"tool": "new description"}
-        self.assertEqual(tools, expected)
-        db_map.connection.close()
-
-
-class TestImportFeature(unittest.TestCase):
-    @staticmethod
-    def populate(db_map):
-        import_object_classes(db_map, ["object_class1", "object_class2"])
-        import_parameter_value_lists(
-            db_map, [['value_list', 'value1'], ['value_list', 'value2'], ['value_list', 'value3']]
-        )
-        import_object_parameters(
-            db_map, [["object_class1", "parameter1", "value1", "value_list"], ["object_class1", "parameter2"]]
-        )
-
-    def test_single_feature(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_features(db_map, [["object_class1", "parameter1"]])
-        self.assertEqual(count, 1)
-        self.assertFalse(errors)
-        features = [
-            (x.entity_class_name, x.parameter_definition_name, x.parameter_value_list_name)
-            for x in db_map.query(db_map.ext_feature_sq)
-        ]
-        self.assertEqual(len(features), 1)
-        self.assertIn(("object_class1", "parameter1", "value_list"), features)
-        db_map.connection.close()
-
-    def test_feature_for_parameter_without_value_list(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_features(db_map, [["object_class1", "parameter2"]])
-        self.assertEqual(count, 0)
-        self.assertTrue(errors)
-        db_map.connection.close()
-
-    def test_feature_description(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_features(db_map, [["object_class1", "parameter1", "description"]])
-        self.assertEqual(count, 1)
-        self.assertFalse(errors)
-        features = {
-            (x.entity_class_name, x.parameter_definition_name, x.parameter_value_list_name): x.description
-            for x in db_map.query(db_map.ext_feature_sq)
-        }
-        expected = {("object_class1", "parameter1", "value_list"): "description"}
-        self.assertEqual(features, expected)
-        db_map.connection.close()
-
-    def test_update_feature_description(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_features(db_map, [["object_class1", "parameter1", "description"]])
-        count, errors = import_features(db_map, [["object_class1", "parameter1", "new description"]])
-        self.assertEqual(count, 1)
-        self.assertFalse(errors)
-        features = {
-            (x.entity_class_name, x.parameter_definition_name, x.parameter_value_list_name): x.description
-            for x in db_map.query(db_map.ext_feature_sq)
-        }
-        expected = {("object_class1", "parameter1", "value_list"): "new description"}
-        self.assertEqual(features, expected)
-        db_map.connection.close()
-
-
-class TestImportToolFeature(unittest.TestCase):
-    @staticmethod
-    def populate(db_map):
-        import_object_classes(db_map, ["object_class1", "object_class2"])
-        import_parameter_value_lists(
-            db_map, [['value_list', 'value1'], ['value_list', 'value2'], ['value_list', 'value3']]
-        )
-        import_object_parameters(
-            db_map, [["object_class1", "parameter1", "value1", "value_list"], ["object_class1", "parameter2"]]
-        )
-        import_features(db_map, [["object_class1", "parameter1"]])
-        import_tools(db_map, ["tool1"])
-
-    def test_single_tool_feature(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_tool_features(db_map, [["tool1", "object_class1", "parameter1"]])
-        self.assertEqual(count, 1)
-        self.assertFalse(errors)
-        tool_features = [
-            (x.tool_name, x.entity_class_name, x.parameter_definition_name, x.required)
-            for x in db_map.query(db_map.ext_tool_feature_sq)
-        ]
-        self.assertEqual(len(tool_features), 1)
-        self.assertIn(("tool1", "object_class1", "parameter1", False), tool_features)
-        db_map.connection.close()
-
-    def test_tool_feature_with_non_feature_parameter(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_tool_features(db_map, [["tool1", "object_class1", "parameter2"]])
-        self.assertEqual(count, 0)
-        self.assertTrue(errors)
-        db_map.connection.close()
-
-    def test_tool_feature_with_non_existing_tool(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_tool_features(db_map, [["non_existing_tool", "object_class1", "parameter1"]])
-        self.assertEqual(count, 0)
-        self.assertTrue(errors)
-        db_map.connection.close()
-
-    def test_tool_feature_required(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_tool_features(db_map, [["tool1", "object_class1", "parameter1", True]])
-        self.assertEqual(count, 1)
-        self.assertFalse(errors)
-        tool_features = [
-            (x.tool_name, x.entity_class_name, x.parameter_definition_name, x.required)
-            for x in db_map.query(db_map.ext_tool_feature_sq)
-        ]
-        self.assertEqual(len(tool_features), 1)
-        self.assertIn(("tool1", "object_class1", "parameter1", True), tool_features)
-        db_map.connection.close()
-
-    def test_update_tool_feature_required(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        import_tool_features(db_map, [["tool1", "object_class1", "parameter1"]])
-        count, errors = import_tool_features(db_map, [["tool1", "object_class1", "parameter1", True]])
-        self.assertEqual(count, 1)
-        self.assertFalse(errors)
-        tool_features = [
-            (x.tool_name, x.entity_class_name, x.parameter_definition_name, x.required)
-            for x in db_map.query(db_map.ext_tool_feature_sq)
-        ]
-        self.assertEqual(len(tool_features), 1)
-        self.assertIn(("tool1", "object_class1", "parameter1", True), tool_features)
-        db_map.connection.close()
-
-
-class TestImportToolFeatureMethod(unittest.TestCase):
-    @staticmethod
-    def populate(db_map):
-        import_object_classes(db_map, ["object_class1", "object_class2"])
-        import_parameter_value_lists(
-            db_map, [['value_list', 'value1'], ['value_list', 'value2'], ['value_list', 'value3']]
-        )
-        import_object_parameters(
-            db_map, [["object_class1", "parameter1", "value1", "value_list"], ["object_class1", "parameter2"]]
-        )
-        import_features(db_map, [["object_class1", "parameter1"]])
-        import_tools(db_map, ["tool1"])
-        import_tool_features(db_map, [["tool1", "object_class1", "parameter1"]])
-
-    def test_import_a_couple_of_tool_feature_methods(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_tool_feature_methods(
-            db_map,
-            [["tool1", "object_class1", "parameter1", "value2"], ["tool1", "object_class1", "parameter1", "value3"]],
-        )
-        self.assertEqual(count, 2)
-        self.assertFalse(errors)
-        tool_feature_methods = [
-            (x.tool_name, x.entity_class_name, x.parameter_definition_name, from_database(x.method))
-            for x in db_map.query(db_map.ext_tool_feature_method_sq)
-        ]
-        self.assertEqual(len(tool_feature_methods), 2)
-        self.assertIn(("tool1", "object_class1", "parameter1", "value2"), tool_feature_methods)
-        self.assertIn(("tool1", "object_class1", "parameter1", "value3"), tool_feature_methods)
-        db_map.connection.close()
-
-    def test_tool_feature_method_with_non_feature_parameter(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_tool_feature_methods(db_map, [["tool1", "object_class1", "parameter2", "method"]])
-        self.assertEqual(count, 0)
-        self.assertTrue(errors)
-        db_map.connection.close()
-
-    def test_tool_feature_method_with_non_existing_tool(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_tool_feature_methods(
-            db_map, [["non_existing_tool", "object_class1", "parameter1", "value2"]]
-        )
-        self.assertEqual(count, 0)
-        self.assertTrue(errors)
-        db_map.connection.close()
-
-    def test_tool_feature_method_with_invalid_method(self):
-        db_map = create_diff_db_map()
-        self.populate(db_map)
-        count, errors = import_tool_feature_methods(
-            db_map, [["tool1", "object_class1", "parameter1", "invalid_method"]]
-        )
-        self.assertEqual(count, 0)
-        self.assertTrue(errors)
-        db_map.connection.close()
-
-    def test_tool_feature_method_with_db_server_style_method(self):
-        db_map = DatabaseMapping("sqlite://", create=True)
-        self.populate(db_map)
-        db_map.commit_session("Add test data.")
-        count, errors = import_tool_feature_methods(
-            db_map, [["tool1", "object_class1", "parameter1", [b'"value1"', None]]], unparse_value=_unparse_value
-        )
-        self.assertEqual(errors, [])
-        self.assertEqual(count, 1)
-        tool_feature_methods = db_map.query(db_map.ext_tool_feature_method_sq).all()
-        self.assertEqual(len(tool_feature_methods), 1)
-        self.assertEqual(tool_feature_methods[0].entity_class_name, "object_class1")
-        self.assertEqual(from_database(tool_feature_methods[0].method), "value1")
-        self.assertEqual(tool_feature_methods[0].parameter_definition_name, "parameter1")
-        self.assertEqual(tool_feature_methods[0].parameter_value_list_name, "value_list")
-        self.assertEqual(tool_feature_methods[0].tool_name, "tool1")
-        db_map.connection.close()
-
-
 class TestImportMetadata(unittest.TestCase):
     def test_import_metadata(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         count, errors = import_metadata(db_map, ['{"name": "John", "age": 17}', '{"name": "Charly", "age": 90}'])
         self.assertEqual(count, 4)
         self.assertFalse(errors)
+        db_map.commit_session("test")
         metadata = [(x.name, x.value) for x in db_map.query(db_map.metadata_sq)]
         self.assertEqual(len(metadata), 4)
         self.assertIn(("name", "John"), metadata)
         self.assertIn(("name", "Charly"), metadata)
         self.assertIn(("age", "17"), metadata)
         self.assertIn(("age", "90"), metadata)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_metadata_with_duplicate_entry(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         count, errors = import_metadata(db_map, ['{"name": "John", "age": 17}', '{"name": "Charly", "age": 17}'])
         self.assertEqual(count, 3)
         self.assertFalse(errors)
+        db_map.commit_session("test")
         metadata = [(x.name, x.value) for x in db_map.query(db_map.metadata_sq)]
         self.assertEqual(len(metadata), 3)
         self.assertIn(("name", "John"), metadata)
         self.assertIn(("name", "Charly"), metadata)
         self.assertIn(("age", "17"), metadata)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_metadata_with_nested_dict(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         count, errors = import_metadata(db_map, ['{"name": "John", "info": {"age": 17, "city": "LA"}}'])
+        db_map.commit_session("test")
         metadata = [(x.name, x.value) for x in db_map.query(db_map.metadata_sq)]
         self.assertEqual(count, 2)
         self.assertFalse(errors)
         self.assertEqual(len(metadata), 2)
         self.assertIn(("name", "John"), metadata)
         self.assertIn(("info", "{'age': 17, 'city': 'LA'}"), metadata)
-        db_map.connection.close()
+        db_map.close()
 
     def test_import_metadata_with_nested_list(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         count, errors = import_metadata(db_map, ['{"contributors": [{"name": "John"}, {"name": "Charly"}]}'])
+        db_map.commit_session("test")
         metadata = [(x.name, x.value) for x in db_map.query(db_map.metadata_sq)]
         self.assertEqual(count, 2)
         self.assertFalse(errors)
         self.assertEqual(len(metadata), 2)
-        self.assertIn(('contributors', "{'name': 'John'}"), metadata)
-        self.assertIn(('contributors', "{'name': 'Charly'}"), metadata)
-        db_map.connection.close()
+        self.assertIn(("contributors", "{'name': 'John'}"), metadata)
+        self.assertIn(("contributors", "{'name': 'Charly'}"), metadata)
+        db_map.close()
 
     def test_import_unformatted_metadata(self):
-        db_map = create_diff_db_map()
-        count, errors = import_metadata(db_map, ['not a JSON object'])
+        db_map = create_db_map()
+        count, errors = import_metadata(db_map, ["not a JSON object"])
+        db_map.commit_session("test")
         metadata = [(x.name, x.value) for x in db_map.query(db_map.metadata_sq)]
         self.assertEqual(count, 1)
         self.assertFalse(errors)
         self.assertEqual(len(metadata), 1)
         self.assertIn(("unnamed", "not a JSON object"), metadata)
-        db_map.connection.close()
+        db_map.close()
 
 
 class TestImportEntityMetadata(unittest.TestCase):
     @staticmethod
     def populate(db_map):
         import_object_classes(db_map, ["object_class1", "object_class2"])
         import_relationship_classes(db_map, [("rel_cls1", ("object_class1", "object_class2"))])
@@ -1430,102 +1507,105 @@
         import_object_parameters(db_map, [("object_class1", "param1")])
         import_relationship_parameters(db_map, [("rel_cls1", "param2")])
         import_object_parameter_values(db_map, [("object_class1", "object1", "param1", "value1")])
         import_relationship_parameter_values(db_map, [("rel_cls1", ("object1", "object2"), "param2", "value2")])
         import_metadata(db_map, ['{"co-author": "John", "age": 17}', '{"co-author": "Charly", "age": 90}'])
 
     def test_import_object_metadata(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         count, errors = import_object_metadata(
             db_map,
             [
                 ("object_class1", "object1", '{"co-author": "John", "age": 90}'),
                 ("object_class1", "object1", '{"co-author": "Charly", "age": 17}'),
             ],
         )
         self.assertEqual(count, 4)
         self.assertFalse(errors)
+        db_map.commit_session("test")
         metadata = [
             (x.entity_name, x.metadata_name, x.metadata_value) for x in db_map.query(db_map.ext_entity_metadata_sq)
         ]
         self.assertEqual(len(metadata), 4)
-        self.assertIn(('object1', 'co-author', 'John'), metadata)
-        self.assertIn(('object1', 'age', '90'), metadata)
-        self.assertIn(('object1', 'co-author', 'Charly'), metadata)
-        self.assertIn(('object1', 'age', '17'), metadata)
-        db_map.connection.close()
+        self.assertIn(("object1", "co-author", "John"), metadata)
+        self.assertIn(("object1", "age", "90"), metadata)
+        self.assertIn(("object1", "co-author", "Charly"), metadata)
+        self.assertIn(("object1", "age", "17"), metadata)
+        db_map.close()
 
     def test_import_relationship_metadata(self):
-        db_map = create_diff_db_map()
+        db_map = create_db_map()
         self.populate(db_map)
         count, errors = import_relationship_metadata(
             db_map,
             [
                 ("rel_cls1", ("object1", "object2"), '{"co-author": "John", "age": 90}'),
                 ("rel_cls1", ("object1", "object2"), '{"co-author": "Charly", "age": 17}'),
             ],
         )
         self.assertEqual(count, 4)
         self.assertFalse(errors)
+        db_map.commit_session("test")
         metadata = [(x.metadata_name, x.metadata_value) for x in db_map.query(db_map.ext_entity_metadata_sq)]
         self.assertEqual(len(metadata), 4)
-        self.assertIn(('co-author', 'John'), metadata)
-        self.assertIn(('age', '90'), metadata)
-        self.assertIn(('co-author', 'Charly'), metadata)
-        self.assertIn(('age', '17'), metadata)
-        db_map.connection.close()
+        self.assertIn(("co-author", "John"), metadata)
+        self.assertIn(("age", "90"), metadata)
+        self.assertIn(("co-author", "Charly"), metadata)
+        self.assertIn(("age", "17"), metadata)
+        db_map.close()
 
 
 class TestImportParameterValueMetadata(unittest.TestCase):
     def setUp(self):
-        self._db_map = create_diff_db_map()
+        self._db_map = create_db_map()
         import_metadata(self._db_map, ['{"co-author": "John", "age": 17}'])
 
     def tearDown(self):
-        self._db_map.connection.close()
+        self._db_map.close()
 
     def test_import_object_parameter_value_metadata(self):
         import_object_classes(self._db_map, ["object_class"])
         import_object_parameters(self._db_map, [("object_class", "param")])
         import_objects(self._db_map, [("object_class", "object")])
         import_object_parameter_values(self._db_map, [("object_class", "object", "param", "value")])
         count, errors = import_object_parameter_value_metadata(
             self._db_map, [("object_class", "object", "param", '{"co-author": "John", "age": 17}')]
         )
         self.assertEqual(errors, [])
         self.assertEqual(count, 2)
+        self._db_map.commit_session("test")
         metadata = self._db_map.query(self._db_map.ext_parameter_value_metadata_sq).all()
         self.assertEqual(len(metadata), 2)
         self.assertEqual(
-            metadata[0]._asdict(),
+            dict(metadata[0]),
             {
                 "alternative_name": "Base",
                 "entity_name": "object",
                 "id": 1,
                 "metadata_id": 1,
                 "metadata_name": "co-author",
                 "metadata_value": "John",
                 "parameter_name": "param",
                 "parameter_value_id": 1,
-                "commit_id": None,
+                "commit_id": 2,
             },
         )
         self.assertEqual(
-            metadata[1]._asdict(),
+            dict(metadata[1]),
             {
                 "alternative_name": "Base",
                 "entity_name": "object",
                 "id": 2,
                 "metadata_id": 2,
                 "metadata_name": "age",
                 "metadata_value": "17",
                 "parameter_name": "param",
                 "parameter_value_id": 1,
-                "commit_id": None,
+                "commit_id": 2,
             },
         )
 
     def test_import_relationship_parameter_value_metadata(self):
         import_object_classes(self._db_map, ["object_class"])
         import_objects(self._db_map, [("object_class", "object")])
         import_relationship_classes(self._db_map, (("relationship_class", ("object_class",)),))
@@ -1533,41 +1613,42 @@
         import_relationship_parameters(self._db_map, (("relationship_class", "param"),))
         import_relationship_parameter_values(self._db_map, (("relationship_class", ("object",), "param", "value"),))
         count, errors = import_relationship_parameter_value_metadata(
             self._db_map, (("relationship_class", ("object",), "param", '{"co-author": "John", "age": 17}'),)
         )
         self.assertEqual(errors, [])
         self.assertEqual(count, 2)
+        self._db_map.commit_session("test")
         metadata = self._db_map.query(self._db_map.ext_parameter_value_metadata_sq).all()
         self.assertEqual(len(metadata), 2)
         self.assertEqual(
-            metadata[0]._asdict(),
+            dict(metadata[0]),
             {
                 "alternative_name": "Base",
-                "entity_name": "relationship_class_object",
+                "entity_name": "object__",
                 "id": 1,
                 "metadata_id": 1,
                 "metadata_name": "co-author",
                 "metadata_value": "John",
                 "parameter_name": "param",
                 "parameter_value_id": 1,
-                "commit_id": None,
+                "commit_id": 2,
             },
         )
         self.assertEqual(
-            metadata[1]._asdict(),
+            dict(metadata[1]),
             {
                 "alternative_name": "Base",
-                "entity_name": "relationship_class_object",
+                "entity_name": "object__",
                 "id": 2,
                 "metadata_id": 2,
                 "metadata_name": "age",
                 "metadata_value": "17",
                 "parameter_name": "param",
                 "parameter_value_id": 1,
-                "commit_id": None,
+                "commit_id": 2,
             },
         )
 
 
 if __name__ == "__main__":
     unittest.main()
```

### Comparing `spinedb_api-0.30.5/tests/test_mapping.py` & `spinedb_api-0.31.0/tests/test_mapping.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
```

### Comparing `spinedb_api-0.30.5/tests/test_migration.py` & `spinedb_api-0.31.0/tests/test_migration.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -15,20 +16,21 @@
 """
 import os.path
 from tempfile import TemporaryDirectory
 import unittest
 from sqlalchemy import inspect
 from sqlalchemy.engine.url import URL
 from spinedb_api.helpers import create_new_spine_database, _create_first_spine_database, is_head_engine, schema_dict
-from spinedb_api import DiffDatabaseMapping
+from spinedb_api import DatabaseMapping
 
 
 class TestMigration(unittest.TestCase):
     @unittest.skip(
-        "default_values's server_default has been changed from 0 to NULL in the create scrip, but there's no associated upgrade script yet."
+        "default_values's server_default has been changed from 0 to NULL in the create scrip, "
+        "but there's no associated upgrade script yet."
     )
     def test_upgrade_schema(self):
         """Tests that the upgrade scripts produce the same schema as the function to create
         a Spine db anew.
         """
         left_engine = _create_first_spine_database("sqlite://")
         is_head_engine(left_engine, upgrade=True)
@@ -88,54 +90,57 @@
             engine.execute("INSERT INTO parameter (id, object_class_id, name) VALUES (2, 2, 'water')")
             engine.execute("INSERT INTO parameter (id, relationship_class_id, name) VALUES (3, 1, 'relative_speed')")
             engine.execute("INSERT INTO parameter_value (parameter_id, object_id, value) VALUES (1, 1, '\"labrador\"')")
             engine.execute("INSERT INTO parameter_value (parameter_id, object_id, value) VALUES (1, 2, '\"big dane\"')")
             engine.execute("INSERT INTO parameter_value (parameter_id, relationship_id, value) VALUES (3, 1, '100')")
             engine.execute("INSERT INTO parameter_value (parameter_id, relationship_id, value) VALUES (3, 2, '-1')")
             # Upgrade the db and check that our stuff is still there
-            db_map = DiffDatabaseMapping(db_url, upgrade=True)
-            object_classes = {x.id: x.name for x in db_map.object_class_list()}
-            objects = {x.id: (object_classes[x.class_id], x.name) for x in db_map.object_list()}
-            rel_clss = {x.id: (x.name, x.object_class_name_list) for x in db_map.wide_relationship_class_list()}
+            db_map = DatabaseMapping(db_url, upgrade=True)
+            object_classes = {x.id: x.name for x in db_map.query(db_map.object_class_sq)}
+            objects = {x.id: (object_classes[x.class_id], x.name) for x in db_map.query(db_map.object_sq)}
+            rel_clss = {
+                x.id: (x.name, x.object_class_name_list) for x in db_map.query(db_map.wide_relationship_class_sq)
+            }
             rels = {
-                x.id: (rel_clss[x.class_id][0], x.name, x.object_name_list) for x in db_map.wide_relationship_list()
+                x.id: (rel_clss[x.class_id][0], x.name, x.object_name_list)
+                for x in db_map.query(db_map.wide_relationship_sq)
             }
             obj_par_defs = {
                 x.id: (object_classes[x.object_class_id], x.parameter_name)
-                for x in db_map.object_parameter_definition_list()
+                for x in db_map.query(db_map.object_parameter_definition_sq)
             }
             rel_par_defs = {
                 x.id: (rel_clss[x.relationship_class_id][0], x.parameter_name)
-                for x in db_map.relationship_parameter_definition_list()
+                for x in db_map.query(db_map.relationship_parameter_definition_sq)
             }
             obj_par_vals = {
                 (obj_par_defs[x.parameter_id][1], objects[x.object_id][1], x.value)
-                for x in db_map.object_parameter_value_list()
+                for x in db_map.query(db_map.object_parameter_value_sq)
             }
             rel_par_vals = {
                 (rel_par_defs[x.parameter_id][1], rels[x.relationship_id][1], x.value)
-                for x in db_map.relationship_parameter_value_list()
+                for x in db_map.query(db_map.relationship_parameter_value_sq)
             }
             self.assertTrue(len(object_classes), 2)
             self.assertTrue(len(objects), 3)
             self.assertTrue(len(rel_clss), 1)
             self.assertTrue(len(rels), 2)
             self.assertTrue(len(obj_par_defs), 2)
             self.assertTrue(len(rel_par_defs), 1)
             self.assertTrue(len(obj_par_vals), 2)
             self.assertTrue(len(rel_par_vals), 2)
-            self.assertTrue('dog' in object_classes.values())
-            self.assertTrue('fish' in object_classes.values())
-            self.assertTrue(('dog', 'pluto') in objects.values())
-            self.assertTrue(('dog', 'scooby') in objects.values())
-            self.assertTrue(('fish', 'nemo') in objects.values())
-            self.assertTrue(('dog__fish', 'dog,fish') in rel_clss.values())
-            self.assertTrue(('dog__fish', 'pluto__nemo', 'pluto,nemo') in rels.values())
-            self.assertTrue(('dog__fish', 'scooby__nemo', 'scooby,nemo') in rels.values())
-            self.assertTrue(('dog', 'breed') in obj_par_defs.values())
-            self.assertTrue(('fish', 'water') in obj_par_defs.values())
-            self.assertTrue(('dog__fish', 'relative_speed') in rel_par_defs.values())
-            self.assertTrue(('breed', 'scooby', b'"big dane"') in obj_par_vals)
-            self.assertTrue(('breed', 'pluto', b'"labrador"') in obj_par_vals)
-            self.assertTrue(('relative_speed', 'pluto__nemo', b'100') in rel_par_vals)
-            self.assertTrue(('relative_speed', 'scooby__nemo', b'-1') in rel_par_vals)
-            db_map.connection.close()
+            self.assertTrue("dog" in object_classes.values())
+            self.assertTrue("fish" in object_classes.values())
+            self.assertTrue(("dog", "pluto") in objects.values())
+            self.assertTrue(("dog", "scooby") in objects.values())
+            self.assertTrue(("fish", "nemo") in objects.values())
+            self.assertTrue(("dog__fish", "dog,fish") in rel_clss.values())
+            self.assertTrue(("dog__fish", "pluto__nemo", "pluto,nemo") in rels.values())
+            self.assertTrue(("dog__fish", "scooby__nemo", "scooby,nemo") in rels.values())
+            self.assertTrue(("dog", "breed") in obj_par_defs.values())
+            self.assertTrue(("fish", "water") in obj_par_defs.values())
+            self.assertTrue(("dog__fish", "relative_speed") in rel_par_defs.values())
+            self.assertTrue(("breed", "scooby", b'"big dane"') in obj_par_vals)
+            self.assertTrue(("breed", "pluto", b'"labrador"') in obj_par_vals)
+            self.assertTrue(("relative_speed", "pluto__nemo", b"100") in rel_par_vals)
+            self.assertTrue(("relative_speed", "scooby__nemo", b"-1") in rel_par_vals)
+            db_map.close()
```

### Comparing `spinedb_api-0.30.5/tests/test_parameter_value.py` & `spinedb_api-0.31.0/tests/test_parameter_value.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 ######################################################################################################################
 # Copyright (C) 2017-2022 Spine project consortium
+# Copyright Spine Database API contributors
 # This file is part of Spine Database API.
 # Spine Database API is free software: you can redistribute it and/or modify it under the terms of the GNU Lesser
 # General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 # option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
 # without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General
 # Public License for more details. You should have received a copy of the GNU Lesser General Public License along with
 # this program. If not, see <http://www.gnu.org/licenses/>.
@@ -21,22 +22,22 @@
 from dateutil.relativedelta import relativedelta
 import numpy as np
 import numpy.testing
 from spinedb_api.parameter_value import (
     convert_containers_to_maps,
     convert_leaf_maps_to_specialized_containers,
     convert_map_to_table,
+    deep_copy_value,
     duration_to_relativedelta,
     relativedelta_to_duration,
     from_database,
     to_database,
     Array,
     DateTime,
     Duration,
-    IndexedNumberArray,
     Map,
     TimePattern,
     TimeSeriesFixedResolution,
     TimeSeriesVariableResolution,
     TimeSeries,
 )
 
@@ -139,21 +140,21 @@
     def test_relativedelta_to_duration_years(self):
         delta = duration_to_relativedelta("7Y")
         duration = relativedelta_to_duration(delta)
         self.assertEqual(duration, "7Y")
 
     def test_from_database_plain_number(self):
         database_value = b"23.0"
-        value = from_database(database_value, value_type=None)
+        value = from_database(database_value, type_=None)
         self.assertTrue(isinstance(value, float))
         self.assertEqual(value, 23.0)
 
     def test_from_database_boolean(self):
         database_value = b"true"
-        value = from_database(database_value, value_type=None)
+        value = from_database(database_value, type_=None)
         self.assertTrue(isinstance(value, bool))
         self.assertEqual(value, True)
 
     def test_to_database_plain_number(self):
         value = 23.0
         database_value, value_type = to_database(value)
         value_as_float = json.loads(database_value)
@@ -165,37 +166,37 @@
         database_value, value_type = to_database(value)
         value_as_dict = json.loads(database_value)
         self.assertEqual(value_as_dict, {"data": "2019-06-26T12:50:13"})
         self.assertEqual(value_type, "date_time")
 
     def test_from_database_DateTime(self):
         database_value = b'{"data": "2019-06-01T22:15:00+01:00"}'
-        value = from_database(database_value, value_type="date_time")
+        value = from_database(database_value, type_="date_time")
         self.assertEqual(value.value, dateutil.parser.parse("2019-06-01T22:15:00+01:00"))
 
     def test_DateTime_to_database(self):
         value = DateTime(datetime(year=2019, month=6, day=26, hour=10, minute=50, second=34))
         database_value, value_type = value.to_database()
         value_dict = json.loads(database_value)
         self.assertEqual(value_dict, {"data": "2019-06-26T10:50:34"})
         self.assertEqual(value_type, "date_time")
 
     def test_from_database_Duration(self):
         database_value = b'{"data": "4 seconds"}'
-        value = from_database(database_value, value_type="duration")
+        value = from_database(database_value, type_="duration")
         self.assertEqual(value.value, relativedelta(seconds=4))
 
     def test_from_database_Duration_default_units(self):
         database_value = b'{"data": 23}'
-        value = from_database(database_value, value_type="duration")
+        value = from_database(database_value, type_="duration")
         self.assertEqual(value.value, relativedelta(minutes=23))
 
     def test_from_database_Duration_legacy_list_format_converted_to_Array(self):
         database_value = b'{"data": ["1 hour", "1h", 60, "2 hours"]}'
-        value = from_database(database_value, value_type="duration")
+        value = from_database(database_value, type_="duration")
         expected = Array([Duration("1h"), Duration("1h"), Duration("1h"), Duration("2h")])
         self.assertEqual(value, expected)
 
     def test_Duration_to_database(self):
         value = Duration(duration_to_relativedelta("8 years"))
         database_value, value_type = value.to_database()
         value_as_dict = json.loads(database_value)
@@ -207,30 +208,30 @@
         {
           "data": {
             "m1-4,m9-12": 300,
             "m5-8": 221.5
           }
         }
         """
-        value = from_database(database_value, value_type="time_pattern")
+        value = from_database(database_value, type_="time_pattern")
         self.assertEqual(len(value), 2)
         self.assertEqual(value.indexes, ["m1-4,m9-12", "m5-8"])
         numpy.testing.assert_equal(value.values, numpy.array([300.0, 221.5]))
         self.assertEqual(value.index_name, "p")
 
     def test_from_database_TimePattern_with_index_name(self):
         database_value = b"""
         {
           "index_name": "index",
           "data": {
             "M1-12": 300
           }
         }
         """
-        value = from_database(database_value, value_type="time_pattern")
+        value = from_database(database_value, type_="time_pattern")
         self.assertEqual(value.indexes, ["M1-12"])
         numpy.testing.assert_equal(value.values, numpy.array([300.0]))
         self.assertEqual(value.index_name, "index")
 
     def test_TimePattern_to_database(self):
         value = TimePattern(["M1-4,M9-12", "M5-8"], numpy.array([300.0, 221.5]))
         database_value, value_type = value.to_database()
@@ -263,15 +264,15 @@
         releases = b"""{
                           "data": {
                               "1977-05-25": 4,
                               "1980-05-21": 5,
                               "1983-05-25": 6
                           }
                       }"""
-        time_series = from_database(releases, value_type="time_series")
+        time_series = from_database(releases, type_="time_series")
         self.assertEqual(
             time_series.indexes,
             numpy.array(
                 [numpy.datetime64("1977-05-25"), numpy.datetime64("1980-05-21"), numpy.datetime64("1983-05-25")],
                 dtype="datetime64[D]",
             ),
         )
@@ -284,26 +285,26 @@
         releases = b"""{
                           "data": {
                               "1977-05-25": 4,
                               "1980-05-21": 5
                           },
                           "index_name": "index"
                       }"""
-        time_series = from_database(releases, value_type="time_series")
+        time_series = from_database(releases, type_="time_series")
         self.assertEqual(time_series.index_name, "index")
 
     def test_from_database_TimeSeriesVariableResolution_as_two_column_array(self):
         releases = b"""{
                           "data": [
                               ["1977-05-25", 4],
                               ["1980-05-21", 5],
                               ["1983-05-25", 6]
                           ]
                       }"""
-        time_series = from_database(releases, value_type="time_series")
+        time_series = from_database(releases, type_="time_series")
         self.assertEqual(
             time_series.indexes,
             numpy.array(
                 [numpy.datetime64("1977-05-25"), numpy.datetime64("1980-05-21"), numpy.datetime64("1983-05-25")],
                 dtype="datetime64[D]",
             ),
         )
@@ -316,26 +317,26 @@
         releases = b"""{
                           "data": [
                               ["1977-05-25", 4],
                               ["1980-05-21", 5]
                           ],
                           "index_name": "index"
                       }"""
-        time_series = from_database(releases, value_type="time_series")
+        time_series = from_database(releases, type_="time_series")
         self.assertEqual(time_series.index_name, "index")
 
     def test_from_database_TimeSeriesFixedResolution_default_repeat(self):
         database_value = b"""{
                                    "index": {
                                        "ignore_year": true
                                    },
                                    "data": [["2019-07-02T10:00:00", 7.0],
                                             ["2019-07-02T10:00:01", 4.0]]
                                }"""
-        time_series = from_database(database_value, value_type="time_series")
+        time_series = from_database(database_value, type_="time_series")
         self.assertTrue(time_series.ignore_year)
         self.assertFalse(time_series.repeat)
 
     def test_TimeSeriesVariableResolution_to_database(self):
         dates = numpy.array(["1999-05-19", "2002-05-16", "2005-05-19"], dtype="datetime64[D]")
         episodes = numpy.array([1, 2, 3], dtype=float)
         value = TimeSeriesVariableResolution(dates, episodes, False, False)
@@ -374,15 +375,15 @@
                                        "start": "2019-03-23",
                                        "resolution": "1 day",
                                        "ignore_year": false,
                                        "repeat": false
                                    },
                                    "data": [7.0, 5.0, 8.1]
                                }"""
-        time_series = from_database(days_of_our_lives, value_type="time_series")
+        time_series = from_database(days_of_our_lives, type_="time_series")
         self.assertEqual(len(time_series), 3)
         self.assertEqual(
             time_series.indexes,
             numpy.array(
                 [numpy.datetime64("2019-03-23"), numpy.datetime64("2019-03-24"), numpy.datetime64("2019-03-25")],
                 dtype="datetime64[s]",
             ),
@@ -397,15 +398,15 @@
         self.assertEqual(time_series.index_name, "t")
 
     def test_from_database_TimeSeriesFixedResolution_no_index(self):
         database_value = b"""{
                                 "data": [1, 2, 3, 4, 5, 8]
                             }
         """
-        time_series = from_database(database_value, value_type="time_series")
+        time_series = from_database(database_value, type_="time_series")
         self.assertEqual(len(time_series), 6)
         self.assertEqual(
             time_series.indexes,
             numpy.array(
                 [
                     numpy.datetime64("0001-01-01T00:00:00"),
                     numpy.datetime64("0001-01-01T01:00:00"),
@@ -426,28 +427,28 @@
 
     def test_from_database_TimeSeriesFixedResolution_index_name(self):
         database_value = b"""{
                                 "data": [1],
                                 "index_name": "index"
                             }
         """
-        time_series = from_database(database_value, value_type="time_series")
+        time_series = from_database(database_value, type_="time_series")
         self.assertEqual(time_series.index_name, "index")
 
     def test_from_database_TimeSeriesFixedResolution_resolution_list(self):
         database_value = b"""{
                                 "index": {
                                     "start": "2019-01-31",
                                     "resolution": ["1 day", "1M"],
                                     "ignore_year": false,
                                     "repeat": false
                                 },
                                 "data": [7.0, 5.0, 8.1, -4.1]
                             }"""
-        time_series = from_database(database_value, value_type="time_series")
+        time_series = from_database(database_value, type_="time_series")
         self.assertEqual(len(time_series), 4)
         self.assertEqual(
             time_series.indexes,
             numpy.array(
                 [
                     numpy.datetime64("2019-01-31"),
                     numpy.datetime64("2019-02-01"),
@@ -469,39 +470,39 @@
                                    "index": {
                                        "start": "2019-03-23",
                                        "ignore_year": false,
                                        "repeat": false
                                    },
                                    "data": [7.0, 5.0, 8.1]
                                }"""
-        time_series = from_database(database_value, value_type="time_series")
+        time_series = from_database(database_value, type_="time_series")
         self.assertEqual(len(time_series), 3)
         self.assertEqual(len(time_series.resolution), 1)
         self.assertEqual(time_series.resolution[0], relativedelta(hours=1))
 
     def test_from_database_TimeSeriesFixedResolution_default_resolution_unit_is_minutes(self):
         database_value = b"""{
                                    "index": {
                                        "start": "2019-03-23",
                                        "resolution": 30
                                    },
                                    "data": [7.0, 5.0, 8.1]
                                }"""
-        time_series = from_database(database_value, value_type="time_series")
+        time_series = from_database(database_value, type_="time_series")
         self.assertEqual(len(time_series), 3)
         self.assertEqual(len(time_series.resolution), 1)
         self.assertEqual(time_series.resolution[0], relativedelta(minutes=30))
         database_value = b"""{
                                    "index": {
                                        "start": "2019-03-23",
                                        "resolution": [30, 45]
                                    },
                                    "data": [7.0, 5.0, 8.1]
                                }"""
-        time_series = from_database(database_value, value_type="time_series")
+        time_series = from_database(database_value, type_="time_series")
         self.assertEqual(len(time_series), 3)
         self.assertEqual(len(time_series.resolution), 2)
         self.assertEqual(time_series.resolution[0], relativedelta(minutes=30))
         self.assertEqual(time_series.resolution[1], relativedelta(minutes=45))
 
     def test_from_database_TimeSeriesFixedResolution_default_ignore_year(self):
         # Should be false if start is given
@@ -509,25 +510,25 @@
                                    "index": {
                                        "start": "2019-03-23",
                                        "resolution": "1 day",
                                        "repeat": false
                                    },
                                    "data": [7.0, 5.0, 8.1]
                                }"""
-        time_series = from_database(database_value, value_type="time_series")
+        time_series = from_database(database_value, type_="time_series")
         self.assertFalse(time_series.ignore_year)
         # Should be true if start is omitted
         database_value = b"""{
                                    "index": {
                                        "resolution": "1 day",
                                        "repeat": false
                                    },
                                    "data": [7.0, 5.0, 8.1]
                                }"""
-        time_series = from_database(database_value, value_type="time_series")
+        time_series = from_database(database_value, type_="time_series")
         self.assertTrue(time_series.ignore_year)
 
     def test_TimeSeriesFixedResolution_to_database(self):
         values = numpy.array([3, 2, 4], dtype=float)
         resolution = [duration_to_relativedelta("1 months")]
         start = datetime(year=2007, month=6, day=1)
         value = TimeSeriesFixedResolution(start, resolution, values, True, True)
@@ -598,84 +599,84 @@
         self.assertTrue(isinstance(series.indexes, np.ndarray))
         for index in series.indexes:
             self.assertTrue(isinstance(index, np.datetime64))
         self.assertTrue(isinstance(series.values, np.ndarray))
 
     def test_from_database_Map_with_index_name(self):
         database_value = b'{"index_type":"str", "index_name": "index", "data":[["a", 1.1]]}'
-        value = from_database(database_value, value_type="map")
+        value = from_database(database_value, type_="map")
         self.assertIsInstance(value, Map)
         self.assertEqual(value.indexes, ["a"])
         self.assertEqual(value.values, [1.1])
         self.assertEqual(value.index_name, "index")
 
     def test_from_database_Map_dictionary_format(self):
         database_value = b'{"index_type":"str", "data":{"a": 1.1, "b": 2.2}}'
-        value = from_database(database_value, value_type="map")
+        value = from_database(database_value, type_="map")
         self.assertIsInstance(value, Map)
         self.assertEqual(value.indexes, ["a", "b"])
         self.assertEqual(value.values, [1.1, 2.2])
         self.assertEqual(value.index_name, "x")
 
     def test_from_database_Map_two_column_array_format(self):
         database_value = b'{"index_type":"float", "data":[[1.1, "a"], [2.2, "b"]]}'
-        value = from_database(database_value, value_type="map")
+        value = from_database(database_value, type_="map")
         self.assertIsInstance(value, Map)
         self.assertEqual(value.indexes, [1.1, 2.2])
         self.assertEqual(value.values, ["a", "b"])
         self.assertEqual(value.index_name, "x")
 
     def test_from_database_Map_nested_maps(self):
-        database_value = b'''
+        database_value = b"""
         {
              "index_type": "duration",
               "data":[["1 hour", {"type": "map",
                                   "index_type": "date_time",
                                   "data": {"2020-01-01T12:00": {"type":"duration", "data":"3 hours"}}}]]
-        }'''
-        value = from_database(database_value, value_type="map")
+        }"""
+        value = from_database(database_value, type_="map")
         self.assertEqual(value.indexes, [Duration("1 hour")])
         nested_map = value.values[0]
         self.assertIsInstance(nested_map, Map)
         self.assertEqual(nested_map.indexes, [DateTime("2020-01-01T12:00")])
         self.assertEqual(nested_map.values, [Duration("3 hours")])
 
     def test_from_database_Map_with_TimeSeries_values(self):
-        database_value = b'''
+        database_value = b"""
         {
              "index_type": "duration",
               "data":[["1 hour", {"type": "time_series",
                                   "data": [["2020-01-01T12:00", -3.0], ["2020-01-02T12:00", -9.3]]
                                  }
                      ]]
-        }'''
-        value = from_database(database_value, value_type="map")
+        }"""
+        value = from_database(database_value, type_="map")
         self.assertEqual(value.indexes, [Duration("1 hour")])
         self.assertEqual(
             value.values,
             [TimeSeriesVariableResolution(["2020-01-01T12:00", "2020-01-02T12:00"], [-3.0, -9.3], False, False)],
         )
 
     def test_from_database_Map_with_Array_values(self):
-        database_value = b'''
+        database_value = b"""
         {
              "index_type": "duration",
               "data":[["1 hour", {"type": "array", "data": [-3.0, -9.3]}]]
-        }'''
-        value = from_database(database_value, value_type="map")
+        }"""
+        value = from_database(database_value, type_="map")
         self.assertEqual(value.indexes, [Duration("1 hour")])
         self.assertEqual(value.values, [Array([-3.0, -9.3])])
 
     def test_from_database_Map_with_TimePattern_values(self):
-        database_value = b'''
+        database_value = b"""
         {
              "index_type": "float",
               "data":[["2.3", {"type": "time_pattern", "data": {"M1-2": -9.3, "M3-12": -3.9}}]]
-        }'''
-        value = from_database(database_value, value_type="map")
+        }"""
+        value = from_database(database_value, type_="map")
         self.assertEqual(value.indexes, [2.3])
         self.assertEqual(value.values, [TimePattern(["M1-2", "M3-12"], [-9.3, -3.9])])
 
     def test_Map_to_database(self):
         map_value = Map(["a", "b"], [1.1, 2.2])
         db_value, value_type = to_database(map_value)
         raw = json.loads(db_value)
@@ -767,65 +768,65 @@
         self.assertEqual(value_type, "array")
 
     def test_Array_of_floats_from_database(self):
         database_value = b"""{
             "value_type": "float",
             "data": [1.2, 2.3]
         }"""
-        array = from_database(database_value, value_type="array")
+        array = from_database(database_value, type_="array")
         self.assertEqual(array.values, [1.2, 2.3])
         self.assertEqual(array.indexes, [0, 1])
         self.assertEqual(array.index_name, "i")
 
     def test_Array_of_default_value_type_from_database(self):
         database_value = b"""{
             "data": [1.2, 2.3]
         }"""
-        array = from_database(database_value, value_type="array")
+        array = from_database(database_value, type_="array")
         self.assertEqual(array.values, [1.2, 2.3])
         self.assertEqual(array.indexes, [0, 1])
         self.assertEqual(array.index_name, "i")
 
     def test_Array_of_strings_from_database(self):
         database_value = b"""{
             "value_type": "str",
             "data": ["A", "B"]
         }"""
-        array = from_database(database_value, value_type="array")
+        array = from_database(database_value, type_="array")
         self.assertEqual(array.values, ["A", "B"])
         self.assertEqual(array.indexes, [0, 1])
         self.assertEqual(array.index_name, "i")
 
     def test_Array_of_DateTimes_from_database(self):
         database_value = b"""{
             "value_type": "date_time",
             "data": ["2020-03-25T10:34:00"]
         }"""
-        array = from_database(database_value, value_type="array")
+        array = from_database(database_value, type_="array")
         self.assertEqual(array.values, [DateTime("2020-03-25T10:34:00")])
         self.assertEqual(array.indexes, [0])
         self.assertEqual(array.index_name, "i")
 
     def test_Array_of_Durations_from_database(self):
         database_value = b"""{
             "value_type": "duration",
             "data": ["2 years", "7 seconds"]
         }"""
-        array = from_database(database_value, value_type="array")
+        array = from_database(database_value, type_="array")
         self.assertEqual(array.values, [Duration("2 years"), Duration("7s")])
         self.assertEqual(array.indexes, [0, 1])
         self.assertEqual(array.index_name, "i")
 
     def test_Array_from_database_with_index_name(self):
         database_value = b"""{
             "value_type": "float",
             "index_name": "index",
             "data": [1.2]
         }"""
-        array = from_database(database_value, value_type="array")
+        array = from_database(database_value, type_="array")
         self.assertEqual(array.values, [1.2])
         self.assertEqual(array.indexes, [0])
         self.assertEqual(array.index_name, "index")
 
     def test_Array_constructor_converts_ints_to_floats(self):
         array = Array([9, 5])
         self.assertIs(array.value_type, float)
@@ -860,14 +861,18 @@
     def test_Map_equality(self):
         map_value = Map(["a"], [-2.3])
         self.assertEqual(map_value, Map(["a"], [-2.3]))
         nested_map = Map(["a"], [-2.3])
         map_value = Map(["A"], [nested_map])
         self.assertEqual(map_value, Map(["A"], [Map(["a"], [-2.3])]))
 
+    def test_Map_inequality(self):
+        map_value = Map(["1", "2", "3", "4", "5"], [-2.3, 2.3, 2.3, 2.3, 2.3])
+        self.assertNotEqual(map_value, Map(["a", "b"], [2.3, -2.3]))
+
     def test_TimePattern_equality(self):
         pattern = TimePattern(["D1-2", "D3-7"], np.array([-2.3, -5.0]))
         self.assertEqual(pattern, pattern)
         equal_pattern = TimePattern(["D1-2", "D3-7"], np.array([-2.3, -5.0]))
         self.assertEqual(pattern, equal_pattern)
         inequal_pattern = TimePattern(["M1-3", "M4-12"], np.array([-5.0, 23.0]))
         self.assertNotEqual(pattern, inequal_pattern)
@@ -884,19 +889,19 @@
         series = TimeSeriesVariableResolution(["2000-01-01T00:00", "2001-01-01T00:00"], [4.2, 2.4], True, True)
         self.assertEqual(series, series)
         equal_series = TimeSeriesVariableResolution(["2000-01-01T00:00", "2001-01-01T00:00"], [4.2, 2.4], True, True)
         self.assertEqual(series, equal_series)
         inequal_series = TimeSeriesVariableResolution(["2000-01-01T00:00", "2002-01-01T00:00"], [4.2, 2.4], False, True)
         self.assertNotEqual(series, inequal_series)
 
-    def test_IndexedValue_constructor_converts_values_to_floats(self):
-        value = IndexedNumberArray("", [4, -9, 11])
+    def test_TimeSeries_constructor_converts_values_to_floats(self):
+        value = TimeSeries([4, -9, 11], False, False)
         self.assertEqual(value.values.dtype, np.dtype(float))
         numpy.testing.assert_equal(value.values, numpy.array([4.0, -9.0, 11.0]))
-        value = IndexedNumberArray("", numpy.array([16, -251, 99]))
+        value = TimeSeries(numpy.array([16, -251, 99]), False, False)
         self.assertEqual(value.values.dtype, np.dtype(float))
         numpy.testing.assert_equal(value.values, numpy.array([16.0, -251.0, 99.0]))
 
     def test_Map_is_nested(self):
         map_value = Map(["a"], [-2.3])
         self.assertFalse(map_value.is_nested())
         nested_map = Map(["a"], [-2.3])
@@ -995,10 +1000,81 @@
 
     def convert_map_to_dict(self):
         map1 = Map(["a", "b"], [-3.2, -2.3])
         map2 = Map(["c", "d"], [3.2, 2.3])
         nested_map = Map(["A", "B"], [map1, map2])
         self.assertEqual(nested_map, {"A": {"a": -3.2, "b": -2.3}, "B": {"c": 3.2, "d": 2.3}})
 
+    def test_deep_copy_value_for_scalars(self):
+        x = None
+        copy_of_x = deep_copy_value(x)
+        self.assertIsNone(copy_of_x)
+        x = 1.0
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        x = "y"
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        x = Duration("3h")
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        self.assertIsNot(x, copy_of_x)
+        x = DateTime("2024-03-25T15:58:33")
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        self.assertIsNot(x, copy_of_x)
+
+    def test_deep_copy_for_arrays(self):
+        x = Array([], value_type=float, index_name="floaters")
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        self.assertIsNot(x, copy_of_x)
+        x = Array(["1", "2", "3"], index_name="number likes")
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        self.assertIsNot(x, copy_of_x)
+
+    def test_deep_copy_time_pattern(self):
+        x = TimePattern(["M1-12"], [2.3], index_name="moments")
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        self.assertIsNot(x, copy_of_x)
+
+    def test_deep_copy_time_series_fixed_resolution(self):
+        x = TimeSeriesFixedResolution(
+            "2024-03-25T16:08:23", "4h", [2.3, 23.0, 5.0], ignore_year=True, repeat=True, index_name="my times"
+        )
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        self.assertIsNot(x, copy_of_x)
+
+    def test_deep_copy_time_series_variable_resolution(self):
+        x = TimeSeriesVariableResolution(
+            ["2024-03-25T16:10:23", "2024-04-26T16:11:23"],
+            [2.3, 23.0],
+            ignore_year=True,
+            repeat=True,
+            index_name="your times",
+        )
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        self.assertIsNot(x, copy_of_x)
+
+    def test_deep_copy_map(self):
+        x = Map([], [], index_type=str, index_name="first i")
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        self.assertIsNot(x, copy_of_x)
+        x = Map(["T1", "T2"], [2.3, 23.0], index_name="our times")
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        self.assertIsNot(x, copy_of_x)
+        leaf = Map(["t1"], [2.3], index_name="inner")
+        x = Map(["T1"], [leaf], index_name="outer")
+        copy_of_x = deep_copy_value(x)
+        self.assertEqual(x, copy_of_x)
+        self.assertIsNot(x, copy_of_x)
+        self.assertIsNot(x.get_value("T1"), copy_of_x.get_value("T1"))
+
 
 if __name__ == "__main__":
     unittest.main()
```

