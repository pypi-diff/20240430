# Comparing `tmp/qai_hub_models-0.5.0-py3-none-any.whl.zip` & `tmp/qai_hub_models-0.5.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,926 +1,944 @@
-Zip file size: 1012491 bytes, number of entries: 924
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/__init__.py
--rw-r--r--  2.0 unx      281 b- defN 24-Apr-16 23:38 qai_hub_models/_version.py
--rw-r--r--  2.0 unx      620 b- defN 24-Apr-16 23:40 qai_hub_models/asset_bases.yaml
--rw-r--r--  2.0 unx      734 b- defN 24-Apr-16 23:38 qai_hub_models/conftest.py
--rw-r--r--  2.0 unx      913 b- defN 24-Apr-16 23:38 qai_hub_models/global_requirements.txt
--rw-r--r--  2.0 unx      395 b- defN 24-Apr-16 23:38 qai_hub_models/requirements-dev.txt
--rw-r--r--  2.0 unx      427 b- defN 24-Apr-16 23:38 qai_hub_models/requirements.txt
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/datasets/__init__.py
--rw-r--r--  2.0 unx     4757 b- defN 24-Apr-16 23:38 qai_hub_models/datasets/bsd300.py
--rw-r--r--  2.0 unx     4109 b- defN 24-Apr-16 23:38 qai_hub_models/datasets/coco.py
--rw-r--r--  2.0 unx     1614 b- defN 24-Apr-16 23:38 qai_hub_models/datasets/common.py
--rw-r--r--  2.0 unx     3203 b- defN 24-Apr-16 23:38 qai_hub_models/datasets/imagenette.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/evaluators/__init__.py
--rw-r--r--  2.0 unx     6212 b- defN 24-Apr-16 23:38 qai_hub_models/evaluators/base_evaluators.py
--rw-r--r--  2.0 unx     1326 b- defN 24-Apr-16 23:38 qai_hub_models/evaluators/classification_evaluator.py
--rw-r--r--  2.0 unx     3699 b- defN 24-Apr-16 23:38 qai_hub_models/evaluators/detection_evaluator.py
--rw-r--r--  2.0 unx     2434 b- defN 24-Apr-16 23:38 qai_hub_models/evaluators/image_evaluator.py
--rw-r--r--  2.0 unx     2181 b- defN 24-Apr-16 23:38 qai_hub_models/evaluators/superres_evaluator.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/__init__.py
--rw-r--r--  2.0 unx      666 b- defN 24-Apr-16 23:38 qai_hub_models/models/common.py
--rw-r--r--  2.0 unx     7882 b- defN 24-Apr-16 23:38 qai_hub_models/models/protocols.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/__init__.py
--rw-r--r--  2.0 unx     1655 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/common.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/cityscapes_segmentation/__init__.py
--rw-r--r--  2.0 unx     4346 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/cityscapes_segmentation/app.py
--rw-r--r--  2.0 unx     2904 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/cityscapes_segmentation/demo.py
--rw-r--r--  2.0 unx      890 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/cityscapes_segmentation/evaluator.py
--rw-r--r--  2.0 unx     2888 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/cityscapes_segmentation/model.py
--rw-r--r--  2.0 unx     8565 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/cityscapes_segmentation/patches/move_datasets.diff
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/deeplab/__init__.py
--rw-r--r--  2.0 unx     2651 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/deeplab/app.py
--rw-r--r--  2.0 unx     2252 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/deeplab/demo.py
--rw-r--r--  2.0 unx      915 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/deeplab/evaluator.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/detr/__init__.py
--rw-r--r--  2.0 unx     4604 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/detr/app.py
--rw-r--r--  2.0 unx     3565 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/detr/coco_label_map.py
--rw-r--r--  2.0 unx     2091 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/detr/demo.py
--rw-r--r--  2.0 unx     2050 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/detr/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/fastsam/__init__.py
--rw-r--r--  2.0 unx     4883 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/fastsam/app.py
--rw-r--r--  2.0 unx     2022 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/fastsam/demo.py
--rw-r--r--  2.0 unx     1935 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/fastsam/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/ffnet/__init__.py
--rw-r--r--  2.0 unx     4548 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/ffnet/model.py
--rw-r--r--  2.0 unx     1569 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/ffnet/test_utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/ffnet_quantized/__init__.py
--rw-r--r--  2.0 unx     1165 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/ffnet_quantized/aimet_config.json
--rw-r--r--  2.0 unx     2475 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/ffnet_quantized/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/imagenet_classifier/__init__.py
--rw-r--r--  2.0 unx     3041 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/imagenet_classifier/app.py
--rw-r--r--  2.0 unx     2432 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/imagenet_classifier/demo.py
--rw-r--r--  2.0 unx     4661 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/imagenet_classifier/model.py
--rw-r--r--  2.0 unx     3781 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/imagenet_classifier/test_utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/mediapipe/__init__.py
--rw-r--r--  2.0 unx    30293 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/mediapipe/app.py
--rw-r--r--  2.0 unx     4394 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/mediapipe/utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/quicksrnet/__init__.py
--rw-r--r--  2.0 unx      985 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/quicksrnet/common.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/repaint/__init__.py
--rw-r--r--  2.0 unx     3366 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/repaint/app.py
--rw-r--r--  2.0 unx     2229 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/repaint/demo.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/sesr/__init__.py
--rw-r--r--  2.0 unx     1029 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/sesr/common.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/super_resolution/__init__.py
--rw-r--r--  2.0 unx     2143 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/super_resolution/app.py
--rw-r--r--  2.0 unx     2775 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/super_resolution/demo.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/swin/__init__.py
--rw-r--r--  2.0 unx     9175 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/swin/swin_transformer.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/video_classifier/__init__.py
--rw-r--r--  2.0 unx    11183 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/video_classifier/app.py
--rw-r--r--  2.0 unx     1581 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/video_classifier/demo.py
--rw-r--r--  2.0 unx     1969 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/video_classifier/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/whisper/__init__.py
--rw-r--r--  2.0 unx    10188 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/whisper/app.py
--rw-r--r--  2.0 unx     1302 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/whisper/demo.py
--rw-r--r--  2.0 unx    14119 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/whisper/model.py
--rw-r--r--  2.0 unx     2848 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/whisper/test_utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/yolo/__init__.py
--rw-r--r--  2.0 unx     7354 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/yolo/app.py
--rw-r--r--  2.0 unx     2432 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/yolo/demo.py
--rw-r--r--  2.0 unx     5643 b- defN 24-Apr-16 23:38 qai_hub_models/models/_shared/yolo/utils.py
--rw-r--r--  2.0 unx      450 b- defN 24-Apr-16 23:38 qai_hub_models/models/aotgan/__init__.py
--rw-r--r--  2.0 unx      980 b- defN 24-Apr-16 23:38 qai_hub_models/models/aotgan/conftest.py
--rw-r--r--  2.0 unx      597 b- defN 24-Apr-16 23:38 qai_hub_models/models/aotgan/demo.py
--rw-r--r--  2.0 unx     8156 b- defN 24-Apr-16 23:38 qai_hub_models/models/aotgan/export.py
--rw-r--r--  2.0 unx     1023 b- defN 24-Apr-16 23:38 qai_hub_models/models/aotgan/info.yaml
--rw-r--r--  2.0 unx     4838 b- defN 24-Apr-16 23:38 qai_hub_models/models/aotgan/model.py
--rw-r--r--  2.0 unx     4412 b- defN 24-Apr-16 23:38 qai_hub_models/models/aotgan/perf.yaml
--rw-r--r--  2.0 unx     2006 b- defN 24-Apr-16 23:38 qai_hub_models/models/aotgan/test.py
--rw-r--r--  2.0 unx      532 b- defN 24-Apr-16 23:38 qai_hub_models/models/aotgan/patches/layer_norm.diff
--rw-r--r--  2.0 unx     1983 b- defN 24-Apr-16 23:38 qai_hub_models/models/baichuan_7b_quantized/info.yaml
--rw-r--r--  2.0 unx     2036 b- defN 24-Apr-16 23:38 qai_hub_models/models/baichuan_7b_quantized/perf.yaml
--rw-r--r--  2.0 unx      559 b- defN 24-Apr-16 23:38 qai_hub_models/models/controlnet_quantized/__init__.py
--rw-r--r--  2.0 unx     9705 b- defN 24-Apr-16 23:38 qai_hub_models/models/controlnet_quantized/app.py
--rw-r--r--  2.0 unx     6397 b- defN 24-Apr-16 23:38 qai_hub_models/models/controlnet_quantized/demo.py
--rw-r--r--  2.0 unx     7622 b- defN 24-Apr-16 23:38 qai_hub_models/models/controlnet_quantized/export.py
--rw-r--r--  2.0 unx     1329 b- defN 24-Apr-16 23:38 qai_hub_models/models/controlnet_quantized/info.yaml
--rw-r--r--  2.0 unx     5426 b- defN 24-Apr-16 23:38 qai_hub_models/models/controlnet_quantized/model.py
--rw-r--r--  2.0 unx     8427 b- defN 24-Apr-16 23:38 qai_hub_models/models/controlnet_quantized/perf.yaml
--rw-r--r--  2.0 unx       46 b- defN 24-Apr-16 23:38 qai_hub_models/models/controlnet_quantized/requirements.txt
--rw-r--r--  2.0 unx     1556 b- defN 24-Apr-16 23:38 qai_hub_models/models/controlnet_quantized/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Apr-16 23:38 qai_hub_models/models/convnext_tiny/__init__.py
--rw-r--r--  2.0 unx      908 b- defN 24-Apr-16 23:38 qai_hub_models/models/convnext_tiny/conftest.py
--rw-r--r--  2.0 unx      543 b- defN 24-Apr-16 23:38 qai_hub_models/models/convnext_tiny/demo.py
--rw-r--r--  2.0 unx     7915 b- defN 24-Apr-16 23:38 qai_hub_models/models/convnext_tiny/export.py
--rw-r--r--  2.0 unx     1287 b- defN 24-Apr-16 23:38 qai_hub_models/models/convnext_tiny/info.yaml
--rw-r--r--  2.0 unx      708 b- defN 24-Apr-16 23:38 qai_hub_models/models/convnext_tiny/model.py
--rw-r--r--  2.0 unx     2707 b- defN 24-Apr-16 23:38 qai_hub_models/models/convnext_tiny/perf.yaml
--rw-r--r--  2.0 unx      857 b- defN 24-Apr-16 23:38 qai_hub_models/models/convnext_tiny/test.py
--rw-r--r--  2.0 unx      398 b- defN 24-Apr-16 23:38 qai_hub_models/models/ddrnet23_slim/__init__.py
--rw-r--r--  2.0 unx     3750 b- defN 24-Apr-16 23:38 qai_hub_models/models/ddrnet23_slim/app.py
--rw-r--r--  2.0 unx      994 b- defN 24-Apr-16 23:38 qai_hub_models/models/ddrnet23_slim/conftest.py
--rw-r--r--  2.0 unx     2128 b- defN 24-Apr-16 23:38 qai_hub_models/models/ddrnet23_slim/demo.py
--rw-r--r--  2.0 unx     8193 b- defN 24-Apr-16 23:38 qai_hub_models/models/ddrnet23_slim/export.py
--rw-r--r--  2.0 unx     1334 b- defN 24-Apr-16 23:38 qai_hub_models/models/ddrnet23_slim/info.yaml
--rw-r--r--  2.0 unx     3831 b- defN 24-Apr-16 23:38 qai_hub_models/models/ddrnet23_slim/model.py
--rw-r--r--  2.0 unx     3617 b- defN 24-Apr-16 23:38 qai_hub_models/models/ddrnet23_slim/perf.yaml
--rw-r--r--  2.0 unx     1790 b- defN 24-Apr-16 23:38 qai_hub_models/models/ddrnet23_slim/test.py
--rw-r--r--  2.0 unx      451 b- defN 24-Apr-16 23:38 qai_hub_models/models/deeplabv3_resnet50/__init__.py
--rw-r--r--  2.0 unx     1004 b- defN 24-Apr-16 23:38 qai_hub_models/models/deeplabv3_resnet50/conftest.py
--rw-r--r--  2.0 unx     1026 b- defN 24-Apr-16 23:38 qai_hub_models/models/deeplabv3_resnet50/demo.py
--rw-r--r--  2.0 unx     8068 b- defN 24-Apr-16 23:38 qai_hub_models/models/deeplabv3_resnet50/export.py
--rw-r--r--  2.0 unx     1278 b- defN 24-Apr-16 23:38 qai_hub_models/models/deeplabv3_resnet50/info.yaml
--rw-r--r--  2.0 unx     2886 b- defN 24-Apr-16 23:38 qai_hub_models/models/deeplabv3_resnet50/model.py
--rw-r--r--  2.0 unx     4374 b- defN 24-Apr-16 23:38 qai_hub_models/models/deeplabv3_resnet50/perf.yaml
--rw-r--r--  2.0 unx     2086 b- defN 24-Apr-16 23:38 qai_hub_models/models/deeplabv3_resnet50/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Apr-16 23:38 qai_hub_models/models/densenet121/__init__.py
--rw-r--r--  2.0 unx      904 b- defN 24-Apr-16 23:38 qai_hub_models/models/densenet121/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Apr-16 23:38 qai_hub_models/models/densenet121/demo.py
--rw-r--r--  2.0 unx     7888 b- defN 24-Apr-16 23:38 qai_hub_models/models/densenet121/export.py
--rw-r--r--  2.0 unx     1310 b- defN 24-Apr-16 23:38 qai_hub_models/models/densenet121/info.yaml
--rw-r--r--  2.0 unx      698 b- defN 24-Apr-16 23:38 qai_hub_models/models/densenet121/model.py
--rw-r--r--  2.0 unx     4399 b- defN 24-Apr-16 23:38 qai_hub_models/models/densenet121/perf.yaml
--rw-r--r--  2.0 unx      841 b- defN 24-Apr-16 23:38 qai_hub_models/models/densenet121/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101/__init__.py
--rw-r--r--  2.0 unx      910 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101/conftest.py
--rw-r--r--  2.0 unx      896 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101/demo.py
--rw-r--r--  2.0 unx     7905 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101/export.py
--rw-r--r--  2.0 unx     1188 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101/info.yaml
--rw-r--r--  2.0 unx      661 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101/model.py
--rw-r--r--  2.0 unx     3658 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101/requirements.txt
--rw-r--r--  2.0 unx     1316 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101_dc5/__init__.py
--rw-r--r--  2.0 unx      918 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101_dc5/conftest.py
--rw-r--r--  2.0 unx      906 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101_dc5/demo.py
--rw-r--r--  2.0 unx     7921 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101_dc5/export.py
--rw-r--r--  2.0 unx     1215 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101_dc5/info.yaml
--rw-r--r--  2.0 unx      668 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101_dc5/model.py
--rw-r--r--  2.0 unx     3677 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101_dc5/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101_dc5/requirements.txt
--rw-r--r--  2.0 unx     1373 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet101_dc5/test.py
--rw-r--r--  2.0 unx      481 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50/__init__.py
--rw-r--r--  2.0 unx      908 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50/conftest.py
--rw-r--r--  2.0 unx      893 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50/demo.py
--rw-r--r--  2.0 unx     7901 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50/export.py
--rw-r--r--  2.0 unx     1185 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50/info.yaml
--rw-r--r--  2.0 unx      659 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50/model.py
--rw-r--r--  2.0 unx     3662 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50/requirements.txt
--rw-r--r--  2.0 unx     1636 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50/test.py
--rw-r--r--  2.0 unx      488 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50_dc5/__init__.py
--rw-r--r--  2.0 unx      916 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50_dc5/conftest.py
--rw-r--r--  2.0 unx      903 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50_dc5/demo.py
--rw-r--r--  2.0 unx     7917 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50_dc5/export.py
--rw-r--r--  2.0 unx     1212 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50_dc5/info.yaml
--rw-r--r--  2.0 unx      666 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50_dc5/model.py
--rw-r--r--  2.0 unx     3670 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50_dc5/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50_dc5/requirements.txt
--rw-r--r--  2.0 unx     1344 b- defN 24-Apr-16 23:38 qai_hub_models/models/detr_resnet50_dc5/test.py
--rw-r--r--  2.0 unx      477 b- defN 24-Apr-16 23:38 qai_hub_models/models/efficientnet_b0/__init__.py
--rw-r--r--  2.0 unx      912 b- defN 24-Apr-16 23:38 qai_hub_models/models/efficientnet_b0/conftest.py
--rw-r--r--  2.0 unx      549 b- defN 24-Apr-16 23:38 qai_hub_models/models/efficientnet_b0/demo.py
--rw-r--r--  2.0 unx     7903 b- defN 24-Apr-16 23:38 qai_hub_models/models/efficientnet_b0/export.py
--rw-r--r--  2.0 unx     1361 b- defN 24-Apr-16 23:38 qai_hub_models/models/efficientnet_b0/info.yaml
--rw-r--r--  2.0 unx      714 b- defN 24-Apr-16 23:38 qai_hub_models/models/efficientnet_b0/model.py
--rw-r--r--  2.0 unx     4428 b- defN 24-Apr-16 23:38 qai_hub_models/models/efficientnet_b0/perf.yaml
--rw-r--r--  2.0 unx      867 b- defN 24-Apr-16 23:38 qai_hub_models/models/efficientnet_b0/test.py
--rw-r--r--  2.0 unx      463 b- defN 24-Apr-16 23:38 qai_hub_models/models/esrgan/__init__.py
--rw-r--r--  2.0 unx      980 b- defN 24-Apr-16 23:38 qai_hub_models/models/esrgan/conftest.py
--rw-r--r--  2.0 unx      939 b- defN 24-Apr-16 23:38 qai_hub_models/models/esrgan/demo.py
--rw-r--r--  2.0 unx     8002 b- defN 24-Apr-16 23:38 qai_hub_models/models/esrgan/export.py
--rw-r--r--  2.0 unx     1117 b- defN 24-Apr-16 23:38 qai_hub_models/models/esrgan/info.yaml
--rw-r--r--  2.0 unx     3473 b- defN 24-Apr-16 23:38 qai_hub_models/models/esrgan/model.py
--rw-r--r--  2.0 unx     4460 b- defN 24-Apr-16 23:38 qai_hub_models/models/esrgan/perf.yaml
--rw-r--r--  2.0 unx     1831 b- defN 24-Apr-16 23:38 qai_hub_models/models/esrgan/test.py
--rw-r--r--  2.0 unx      418 b- defN 24-Apr-16 23:38 qai_hub_models/models/facebook_denoiser/__init__.py
--rw-r--r--  2.0 unx     3207 b- defN 24-Apr-16 23:38 qai_hub_models/models/facebook_denoiser/app.py
--rw-r--r--  2.0 unx     1002 b- defN 24-Apr-16 23:38 qai_hub_models/models/facebook_denoiser/conftest.py
--rw-r--r--  2.0 unx     3172 b- defN 24-Apr-16 23:38 qai_hub_models/models/facebook_denoiser/demo.py
--rw-r--r--  2.0 unx     7670 b- defN 24-Apr-16 23:38 qai_hub_models/models/facebook_denoiser/export.py
--rw-r--r--  2.0 unx     1070 b- defN 24-Apr-16 23:38 qai_hub_models/models/facebook_denoiser/info.yaml
--rw-r--r--  2.0 unx     2414 b- defN 24-Apr-16 23:38 qai_hub_models/models/facebook_denoiser/model.py
--rw-r--r--  2.0 unx     3685 b- defN 24-Apr-16 23:38 qai_hub_models/models/facebook_denoiser/perf.yaml
--rw-r--r--  2.0 unx       74 b- defN 24-Apr-16 23:38 qai_hub_models/models/facebook_denoiser/requirements.txt
--rw-r--r--  2.0 unx     2492 b- defN 24-Apr-16 23:38 qai_hub_models/models/facebook_denoiser/test.py
--rw-r--r--  2.0 unx      440 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_s/__init__.py
--rw-r--r--  2.0 unx      900 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_s/conftest.py
--rw-r--r--  2.0 unx      762 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_s/demo.py
--rw-r--r--  2.0 unx     8264 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_s/export.py
--rw-r--r--  2.0 unx     1301 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_s/info.yaml
--rw-r--r--  2.0 unx      683 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_s/model.py
--rw-r--r--  2.0 unx     3660 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_s/perf.yaml
--rw-r--r--  2.0 unx       64 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_s/requirements.txt
--rw-r--r--  2.0 unx     1332 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_s/test.py
--rw-r--r--  2.0 unx      440 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_x/__init__.py
--rw-r--r--  2.0 unx      900 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_x/conftest.py
--rw-r--r--  2.0 unx      762 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_x/demo.py
--rw-r--r--  2.0 unx     8264 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_x/export.py
--rw-r--r--  2.0 unx     1300 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_x/info.yaml
--rw-r--r--  2.0 unx      683 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_x/model.py
--rw-r--r--  2.0 unx     3668 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_x/perf.yaml
--rw-r--r--  2.0 unx       64 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_x/requirements.txt
--rw-r--r--  2.0 unx     1332 b- defN 24-Apr-16 23:38 qai_hub_models/models/fastsam_x/test.py
--rw-r--r--  2.0 unx      410 b- defN 24-Apr-16 23:38 qai_hub_models/models/fcn_resnet50/__init__.py
--rw-r--r--  2.0 unx     2683 b- defN 24-Apr-16 23:38 qai_hub_models/models/fcn_resnet50/app.py
--rw-r--r--  2.0 unx      992 b- defN 24-Apr-16 23:38 qai_hub_models/models/fcn_resnet50/conftest.py
--rw-r--r--  2.0 unx     2315 b- defN 24-Apr-16 23:38 qai_hub_models/models/fcn_resnet50/demo.py
--rw-r--r--  2.0 unx     8169 b- defN 24-Apr-16 23:38 qai_hub_models/models/fcn_resnet50/export.py
--rw-r--r--  2.0 unx     1241 b- defN 24-Apr-16 23:38 qai_hub_models/models/fcn_resnet50/info.yaml
--rw-r--r--  2.0 unx     1989 b- defN 24-Apr-16 23:38 qai_hub_models/models/fcn_resnet50/model.py
--rw-r--r--  2.0 unx     4442 b- defN 24-Apr-16 23:38 qai_hub_models/models/fcn_resnet50/perf.yaml
--rw-r--r--  2.0 unx     1637 b- defN 24-Apr-16 23:38 qai_hub_models/models/fcn_resnet50/test.py
--rw-r--r--  2.0 unx      487 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_122ns_lowres/__init__.py
--rw-r--r--  2.0 unx     1004 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_122ns_lowres/conftest.py
--rw-r--r--  2.0 unx      607 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_122ns_lowres/demo.py
--rw-r--r--  2.0 unx     8050 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_122ns_lowres/export.py
--rw-r--r--  2.0 unx     1322 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_122ns_lowres/info.yaml
--rw-r--r--  2.0 unx      648 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_122ns_lowres/model.py
--rw-r--r--  2.0 unx     4455 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_122ns_lowres/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_122ns_lowres/requirements.txt
--rw-r--r--  2.0 unx      804 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_122ns_lowres/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s/__init__.py
--rw-r--r--  2.0 unx      986 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s/conftest.py
--rw-r--r--  2.0 unx      582 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s/demo.py
--rw-r--r--  2.0 unx     8014 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s/export.py
--rw-r--r--  2.0 unx     1298 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s/info.yaml
--rw-r--r--  2.0 unx      580 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s/model.py
--rw-r--r--  2.0 unx     4437 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s/requirements.txt
--rw-r--r--  2.0 unx      746 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s_quantized/__init__.py
--rw-r--r--  2.0 unx     1006 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s_quantized/conftest.py
--rw-r--r--  2.0 unx      627 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s_quantized/demo.py
--rw-r--r--  2.0 unx     8467 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s_quantized/export.py
--rw-r--r--  2.0 unx     1347 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s_quantized/info.yaml
--rw-r--r--  2.0 unx     1169 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s_quantized/model.py
--rw-r--r--  2.0 unx     3661 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s_quantized/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s_quantized/requirements.txt
--rw-r--r--  2.0 unx      840 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_40s_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s/__init__.py
--rw-r--r--  2.0 unx      986 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s/conftest.py
--rw-r--r--  2.0 unx      582 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s/demo.py
--rw-r--r--  2.0 unx     8014 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s/export.py
--rw-r--r--  2.0 unx     1275 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s/info.yaml
--rw-r--r--  2.0 unx      580 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s/model.py
--rw-r--r--  2.0 unx     4454 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s/requirements.txt
--rw-r--r--  2.0 unx      746 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s_quantized/__init__.py
--rw-r--r--  2.0 unx     1006 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s_quantized/conftest.py
--rw-r--r--  2.0 unx      627 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s_quantized/demo.py
--rw-r--r--  2.0 unx     8467 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s_quantized/export.py
--rw-r--r--  2.0 unx     1347 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s_quantized/info.yaml
--rw-r--r--  2.0 unx     1156 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s_quantized/model.py
--rw-r--r--  2.0 unx     3668 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s_quantized/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s_quantized/requirements.txt
--rw-r--r--  2.0 unx      840 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_54s_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s/__init__.py
--rw-r--r--  2.0 unx      986 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s/conftest.py
--rw-r--r--  2.0 unx      582 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s/demo.py
--rw-r--r--  2.0 unx     8014 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s/export.py
--rw-r--r--  2.0 unx     1279 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s/info.yaml
--rw-r--r--  2.0 unx      580 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s/model.py
--rw-r--r--  2.0 unx     4450 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s/requirements.txt
--rw-r--r--  2.0 unx      746 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s/test.py
--rw-r--r--  2.0 unx      485 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_lowres/__init__.py
--rw-r--r--  2.0 unx     1000 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_lowres/conftest.py
--rw-r--r--  2.0 unx      601 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_lowres/demo.py
--rw-r--r--  2.0 unx     8042 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_lowres/export.py
--rw-r--r--  2.0 unx     1327 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_lowres/info.yaml
--rw-r--r--  2.0 unx      640 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_lowres/model.py
--rw-r--r--  2.0 unx     4446 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_lowres/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_lowres/requirements.txt
--rw-r--r--  2.0 unx      794 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_lowres/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_quantized/__init__.py
--rw-r--r--  2.0 unx     1006 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_quantized/conftest.py
--rw-r--r--  2.0 unx      627 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_quantized/demo.py
--rw-r--r--  2.0 unx     8467 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_quantized/export.py
--rw-r--r--  2.0 unx     1347 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_quantized/info.yaml
--rw-r--r--  2.0 unx     1156 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_quantized/model.py
--rw-r--r--  2.0 unx     3666 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_quantized/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_quantized/requirements.txt
--rw-r--r--  2.0 unx      840 b- defN 24-Apr-16 23:38 qai_hub_models/models/ffnet_78s_quantized/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet/__init__.py
--rw-r--r--  2.0 unx      900 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet/demo.py
--rw-r--r--  2.0 unx     7879 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet/export.py
--rw-r--r--  2.0 unx     1295 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet/info.yaml
--rw-r--r--  2.0 unx      743 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet/model.py
--rw-r--r--  2.0 unx     4409 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet/perf.yaml
--rw-r--r--  2.0 unx      840 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet/test.py
--rw-r--r--  2.0 unx      573 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet_quantized/__init__.py
--rw-r--r--  2.0 unx      920 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet_quantized/conftest.py
--rw-r--r--  2.0 unx      578 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet_quantized/demo.py
--rw-r--r--  2.0 unx     8359 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet_quantized/export.py
--rw-r--r--  2.0 unx     1325 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet_quantized/info.yaml
--rw-r--r--  2.0 unx     4298 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet_quantized/model.py
--rw-r--r--  2.0 unx     4423 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet_quantized/perf.yaml
--rw-r--r--  2.0 unx      885 b- defN 24-Apr-16 23:38 qai_hub_models/models/googlenet_quantized/test.py
--rw-r--r--  2.0 unx      430 b- defN 24-Apr-16 23:38 qai_hub_models/models/hrnet_pose/__init__.py
--rw-r--r--  2.0 unx     8134 b- defN 24-Apr-16 23:38 qai_hub_models/models/hrnet_pose/app.py
--rw-r--r--  2.0 unx      988 b- defN 24-Apr-16 23:38 qai_hub_models/models/hrnet_pose/conftest.py
--rw-r--r--  2.0 unx     1739 b- defN 24-Apr-16 23:38 qai_hub_models/models/hrnet_pose/demo.py
--rw-r--r--  2.0 unx     8160 b- defN 24-Apr-16 23:38 qai_hub_models/models/hrnet_pose/export.py
--rw-r--r--  2.0 unx     1195 b- defN 24-Apr-16 23:38 qai_hub_models/models/hrnet_pose/info.yaml
--rw-r--r--  2.0 unx     2801 b- defN 24-Apr-16 23:38 qai_hub_models/models/hrnet_pose/model.py
--rw-r--r--  2.0 unx     4426 b- defN 24-Apr-16 23:38 qai_hub_models/models/hrnet_pose/perf.yaml
--rw-r--r--  2.0 unx       51 b- defN 24-Apr-16 23:38 qai_hub_models/models/hrnet_pose/requirements.txt
--rw-r--r--  2.0 unx     1420 b- defN 24-Apr-16 23:38 qai_hub_models/models/hrnet_pose/test.py
--rw-r--r--  2.0 unx      434 b- defN 24-Apr-16 23:38 qai_hub_models/models/huggingface_wavlm_base_plus/__init__.py
--rw-r--r--  2.0 unx     2133 b- defN 24-Apr-16 23:38 qai_hub_models/models/huggingface_wavlm_base_plus/app.py
--rw-r--r--  2.0 unx     1022 b- defN 24-Apr-16 23:38 qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py
--rw-r--r--  2.0 unx     1517 b- defN 24-Apr-16 23:38 qai_hub_models/models/huggingface_wavlm_base_plus/demo.py
--rw-r--r--  2.0 unx     7567 b- defN 24-Apr-16 23:38 qai_hub_models/models/huggingface_wavlm_base_plus/export.py
--rw-r--r--  2.0 unx     1248 b- defN 24-Apr-16 23:38 qai_hub_models/models/huggingface_wavlm_base_plus/info.yaml
--rw-r--r--  2.0 unx     7587 b- defN 24-Apr-16 23:38 qai_hub_models/models/huggingface_wavlm_base_plus/model.py
--rw-r--r--  2.0 unx     3698 b- defN 24-Apr-16 23:38 qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml
--rw-r--r--  2.0 unx       72 b- defN 24-Apr-16 23:38 qai_hub_models/models/huggingface_wavlm_base_plus/requirements.txt
--rw-r--r--  2.0 unx     2560 b- defN 24-Apr-16 23:38 qai_hub_models/models/huggingface_wavlm_base_plus/test.py
--rw-r--r--  2.0 unx      477 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3/__init__.py
--rw-r--r--  2.0 unx      906 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3/conftest.py
--rw-r--r--  2.0 unx      546 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3/demo.py
--rw-r--r--  2.0 unx     7891 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3/export.py
--rw-r--r--  2.0 unx     1359 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3/info.yaml
--rw-r--r--  2.0 unx      756 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3/model.py
--rw-r--r--  2.0 unx     4432 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3/perf.yaml
--rw-r--r--  2.0 unx      861 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3/test.py
--rw-r--r--  2.0 unx      584 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3_quantized/__init__.py
--rw-r--r--  2.0 unx      926 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3_quantized/conftest.py
--rw-r--r--  2.0 unx      591 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3_quantized/demo.py
--rw-r--r--  2.0 unx     8392 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3_quantized/export.py
--rw-r--r--  2.0 unx     1556 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3_quantized/info.yaml
--rw-r--r--  2.0 unx     7762 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3_quantized/model.py
--rw-r--r--  2.0 unx     3656 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3_quantized/perf.yaml
--rw-r--r--  2.0 unx      901 b- defN 24-Apr-16 23:38 qai_hub_models/models/inception_v3_quantized/test.py
--rw-r--r--  2.0 unx      455 b- defN 24-Apr-16 23:38 qai_hub_models/models/lama_dilated/__init__.py
--rw-r--r--  2.0 unx      992 b- defN 24-Apr-16 23:38 qai_hub_models/models/lama_dilated/conftest.py
--rw-r--r--  2.0 unx      911 b- defN 24-Apr-16 23:38 qai_hub_models/models/lama_dilated/demo.py
--rw-r--r--  2.0 unx     8179 b- defN 24-Apr-16 23:38 qai_hub_models/models/lama_dilated/export.py
--rw-r--r--  2.0 unx     1082 b- defN 24-Apr-16 23:38 qai_hub_models/models/lama_dilated/info.yaml
--rw-r--r--  2.0 unx     4977 b- defN 24-Apr-16 23:38 qai_hub_models/models/lama_dilated/model.py
--rw-r--r--  2.0 unx     4407 b- defN 24-Apr-16 23:38 qai_hub_models/models/lama_dilated/perf.yaml
--rw-r--r--  2.0 unx      171 b- defN 24-Apr-16 23:38 qai_hub_models/models/lama_dilated/requirements.txt
--rw-r--r--  2.0 unx     2074 b- defN 24-Apr-16 23:38 qai_hub_models/models/lama_dilated/test.py
--rw-r--r--  2.0 unx      404 b- defN 24-Apr-16 23:38 qai_hub_models/models/litehrnet/__init__.py
--rw-r--r--  2.0 unx     3986 b- defN 24-Apr-16 23:38 qai_hub_models/models/litehrnet/app.py
--rw-r--r--  2.0 unx      900 b- defN 24-Apr-16 23:38 qai_hub_models/models/litehrnet/conftest.py
--rw-r--r--  2.0 unx     2072 b- defN 24-Apr-16 23:38 qai_hub_models/models/litehrnet/demo.py
--rw-r--r--  2.0 unx     7638 b- defN 24-Apr-16 23:38 qai_hub_models/models/litehrnet/export.py
--rw-r--r--  2.0 unx     1112 b- defN 24-Apr-16 23:38 qai_hub_models/models/litehrnet/info.yaml
--rw-r--r--  2.0 unx     3788 b- defN 24-Apr-16 23:38 qai_hub_models/models/litehrnet/model.py
--rw-r--r--  2.0 unx     3620 b- defN 24-Apr-16 23:38 qai_hub_models/models/litehrnet/perf.yaml
--rw-r--r--  2.0 unx       39 b- defN 24-Apr-16 23:38 qai_hub_models/models/litehrnet/requirements.txt
--rw-r--r--  2.0 unx     1714 b- defN 24-Apr-16 23:38 qai_hub_models/models/litehrnet/test.py
--rw-r--r--  2.0 unx     1993 b- defN 24-Apr-16 23:38 qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml
--rw-r--r--  2.0 unx     2037 b- defN 24-Apr-16 23:38 qai_hub_models/models/llama_v2_7b_chat_quantized/perf.yaml
--rw-r--r--  2.0 unx      412 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_face/__init__.py
--rw-r--r--  2.0 unx     2099 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_face/app.py
--rw-r--r--  2.0 unx      996 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_face/conftest.py
--rw-r--r--  2.0 unx     2862 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_face/demo.py
--rw-r--r--  2.0 unx     9755 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_face/export.py
--rw-r--r--  2.0 unx     1469 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_face/info.yaml
--rw-r--r--  2.0 unx     7669 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_face/model.py
--rw-r--r--  2.0 unx     8096 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_face/perf.yaml
--rw-r--r--  2.0 unx     1441 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_face/test.py
--rw-r--r--  2.0 unx      412 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_hand/__init__.py
--rw-r--r--  2.0 unx     9819 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_hand/app.py
--rw-r--r--  2.0 unx      996 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_hand/conftest.py
--rw-r--r--  2.0 unx     2832 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_hand/demo.py
--rw-r--r--  2.0 unx     9755 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_hand/export.py
--rw-r--r--  2.0 unx     1374 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_hand/info.yaml
--rw-r--r--  2.0 unx     6017 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_hand/model.py
--rw-r--r--  2.0 unx     8118 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_hand/perf.yaml
--rw-r--r--  2.0 unx     1442 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_hand/test.py
--rw-r--r--  2.0 unx      412 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_pose/__init__.py
--rw-r--r--  2.0 unx     4430 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_pose/app.py
--rw-r--r--  2.0 unx      996 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_pose/conftest.py
--rw-r--r--  2.0 unx     2889 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_pose/demo.py
--rw-r--r--  2.0 unx     9756 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_pose/export.py
--rw-r--r--  2.0 unx     1387 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_pose/info.yaml
--rw-r--r--  2.0 unx     5801 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_pose/model.py
--rw-r--r--  2.0 unx     8103 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_pose/perf.yaml
--rw-r--r--  2.0 unx     1443 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_pose/test.py
--rw-r--r--  2.0 unx      362 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_selfie/__init__.py
--rw-r--r--  2.0 unx     1411 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_selfie/app.py
--rw-r--r--  2.0 unx      914 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_selfie/conftest.py
--rw-r--r--  2.0 unx     2674 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_selfie/demo.py
--rw-r--r--  2.0 unx     8198 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_selfie/export.py
--rw-r--r--  2.0 unx     1455 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_selfie/info.yaml
--rw-r--r--  2.0 unx    12352 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_selfie/model.py
--rw-r--r--  2.0 unx     4449 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_selfie/perf.yaml
--rw-r--r--  2.0 unx       15 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_selfie/requirements.txt
--rw-r--r--  2.0 unx     1396 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_selfie/test.py
--rw-r--r--  2.0 unx     2492 b- defN 24-Apr-16 23:38 qai_hub_models/models/mediapipe_selfie/utils.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-16 23:38 qai_hub_models/models/mnasnet05/__init__.py
--rw-r--r--  2.0 unx      900 b- defN 24-Apr-16 23:38 qai_hub_models/models/mnasnet05/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Apr-16 23:38 qai_hub_models/models/mnasnet05/demo.py
--rw-r--r--  2.0 unx     7879 b- defN 24-Apr-16 23:38 qai_hub_models/models/mnasnet05/export.py
--rw-r--r--  2.0 unx     1333 b- defN 24-Apr-16 23:38 qai_hub_models/models/mnasnet05/info.yaml
--rw-r--r--  2.0 unx      699 b- defN 24-Apr-16 23:38 qai_hub_models/models/mnasnet05/model.py
--rw-r--r--  2.0 unx     4393 b- defN 24-Apr-16 23:38 qai_hub_models/models/mnasnet05/perf.yaml
--rw-r--r--  2.0 unx      882 b- defN 24-Apr-16 23:38 qai_hub_models/models/mnasnet05/test.py
--rw-r--r--  2.0 unx      474 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2/__init__.py
--rw-r--r--  2.0 unx      992 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2/conftest.py
--rw-r--r--  2.0 unx      540 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2/demo.py
--rw-r--r--  2.0 unx     7891 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2/export.py
--rw-r--r--  2.0 unx     1380 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2/info.yaml
--rw-r--r--  2.0 unx     2457 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2/model.py
--rw-r--r--  2.0 unx     4419 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2/perf.yaml
--rw-r--r--  2.0 unx     1091 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2/test.py
--rw-r--r--  2.0 unx      485 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2_quantized/__init__.py
--rw-r--r--  2.0 unx     1012 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2_quantized/conftest.py
--rw-r--r--  2.0 unx      585 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2_quantized/demo.py
--rw-r--r--  2.0 unx     8372 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2_quantized/export.py
--rw-r--r--  2.0 unx     1362 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2_quantized/info.yaml
--rw-r--r--  2.0 unx     3429 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2_quantized/model.py
--rw-r--r--  2.0 unx     4423 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2_quantized/perf.yaml
--rw-r--r--  2.0 unx     1002 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v2_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large/__init__.py
--rw-r--r--  2.0 unx      918 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large/conftest.py
--rw-r--r--  2.0 unx      556 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large/demo.py
--rw-r--r--  2.0 unx     7935 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large/export.py
--rw-r--r--  2.0 unx     1340 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large/info.yaml
--rw-r--r--  2.0 unx      721 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large/model.py
--rw-r--r--  2.0 unx     3644 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large/perf.yaml
--rw-r--r--  2.0 unx      879 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large/test.py
--rw-r--r--  2.0 unx      607 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large_quantized/__init__.py
--rw-r--r--  2.0 unx      938 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large_quantized/conftest.py
--rw-r--r--  2.0 unx      748 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large_quantized/demo.py
--rw-r--r--  2.0 unx     8104 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large_quantized/export.py
--rw-r--r--  2.0 unx     1374 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large_quantized/info.yaml
--rw-r--r--  2.0 unx     3075 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large_quantized/model.py
--rw-r--r--  2.0 unx     3673 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml
--rw-r--r--  2.0 unx      917 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_large_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_small/__init__.py
--rw-r--r--  2.0 unx      918 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_small/conftest.py
--rw-r--r--  2.0 unx      556 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_small/demo.py
--rw-r--r--  2.0 unx     7935 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_small/export.py
--rw-r--r--  2.0 unx     1338 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_small/info.yaml
--rw-r--r--  2.0 unx      721 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_small/model.py
--rw-r--r--  2.0 unx     3648 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_small/perf.yaml
--rw-r--r--  2.0 unx      879 b- defN 24-Apr-16 23:38 qai_hub_models/models/mobilenet_v3_small/test.py
--rw-r--r--  2.0 unx      394 b- defN 24-Apr-16 23:38 qai_hub_models/models/openai_clip/__init__.py
--rw-r--r--  2.0 unx     3958 b- defN 24-Apr-16 23:38 qai_hub_models/models/openai_clip/app.py
--rw-r--r--  2.0 unx      990 b- defN 24-Apr-16 23:38 qai_hub_models/models/openai_clip/conftest.py
--rw-r--r--  2.0 unx     3404 b- defN 24-Apr-16 23:38 qai_hub_models/models/openai_clip/demo.py
--rw-r--r--  2.0 unx     9711 b- defN 24-Apr-16 23:38 qai_hub_models/models/openai_clip/export.py
--rw-r--r--  2.0 unx     1494 b- defN 24-Apr-16 23:38 qai_hub_models/models/openai_clip/info.yaml
--rw-r--r--  2.0 unx     5374 b- defN 24-Apr-16 23:38 qai_hub_models/models/openai_clip/model.py
--rw-r--r--  2.0 unx     6529 b- defN 24-Apr-16 23:38 qai_hub_models/models/openai_clip/perf.yaml
--rw-r--r--  2.0 unx       29 b- defN 24-Apr-16 23:38 qai_hub_models/models/openai_clip/requirements.txt
--rw-r--r--  2.0 unx     2118 b- defN 24-Apr-16 23:38 qai_hub_models/models/openai_clip/test.py
--rw-r--r--  2.0 unx      402 b- defN 24-Apr-16 23:38 qai_hub_models/models/openpose/__init__.py
--rw-r--r--  2.0 unx    12008 b- defN 24-Apr-16 23:38 qai_hub_models/models/openpose/app.py
--rw-r--r--  2.0 unx      984 b- defN 24-Apr-16 23:38 qai_hub_models/models/openpose/conftest.py
--rw-r--r--  2.0 unx     2053 b- defN 24-Apr-16 23:38 qai_hub_models/models/openpose/demo.py
--rw-r--r--  2.0 unx     8171 b- defN 24-Apr-16 23:38 qai_hub_models/models/openpose/export.py
--rw-r--r--  2.0 unx     1246 b- defN 24-Apr-16 23:38 qai_hub_models/models/openpose/info.yaml
--rw-r--r--  2.0 unx     5084 b- defN 24-Apr-16 23:38 qai_hub_models/models/openpose/model.py
--rw-r--r--  2.0 unx     4438 b- defN 24-Apr-16 23:38 qai_hub_models/models/openpose/perf.yaml
--rw-r--r--  2.0 unx       31 b- defN 24-Apr-16 23:38 qai_hub_models/models/openpose/requirements.txt
--rw-r--r--  2.0 unx     1321 b- defN 24-Apr-16 23:38 qai_hub_models/models/openpose/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge/__init__.py
--rw-r--r--  2.0 unx      998 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge/conftest.py
--rw-r--r--  2.0 unx      972 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge/demo.py
--rw-r--r--  2.0 unx     8181 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge/export.py
--rw-r--r--  2.0 unx     1248 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge/info.yaml
--rw-r--r--  2.0 unx     3170 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge/model.py
--rw-r--r--  2.0 unx     4422 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge/perf.yaml
--rw-r--r--  2.0 unx     1422 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge_quantized/__init__.py
--rw-r--r--  2.0 unx     1018 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge_quantized/conftest.py
--rw-r--r--  2.0 unx      891 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge_quantized/demo.py
--rw-r--r--  2.0 unx     8634 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge_quantized/export.py
--rw-r--r--  2.0 unx     1274 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge_quantized/info.yaml
--rw-r--r--  2.0 unx     4587 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge_quantized/model.py
--rw-r--r--  2.0 unx     2485 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml
--rw-r--r--  2.0 unx     2921 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetlarge_quantized/test.py
--rw-r--r--  2.0 unx      473 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium/__init__.py
--rw-r--r--  2.0 unx     1000 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium/conftest.py
--rw-r--r--  2.0 unx      976 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium/demo.py
--rw-r--r--  2.0 unx     8185 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium/export.py
--rw-r--r--  2.0 unx     1242 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium/info.yaml
--rw-r--r--  2.0 unx     3177 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium/model.py
--rw-r--r--  2.0 unx     4419 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium/perf.yaml
--rw-r--r--  2.0 unx     1428 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium/test.py
--rw-r--r--  2.0 unx      484 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium_quantized/__init__.py
--rw-r--r--  2.0 unx     1020 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium_quantized/conftest.py
--rw-r--r--  2.0 unx      900 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium_quantized/demo.py
--rw-r--r--  2.0 unx     8638 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium_quantized/export.py
--rw-r--r--  2.0 unx     1282 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium_quantized/info.yaml
--rw-r--r--  2.0 unx     4597 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium_quantized/model.py
--rw-r--r--  2.0 unx     2488 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml
--rw-r--r--  2.0 unx     2912 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetmedium_quantized/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall/__init__.py
--rw-r--r--  2.0 unx      998 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall/conftest.py
--rw-r--r--  2.0 unx      972 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall/demo.py
--rw-r--r--  2.0 unx     8181 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall/export.py
--rw-r--r--  2.0 unx     1238 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall/info.yaml
--rw-r--r--  2.0 unx     3170 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall/model.py
--rw-r--r--  2.0 unx     4413 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall/perf.yaml
--rw-r--r--  2.0 unx     1422 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall_quantized/__init__.py
--rw-r--r--  2.0 unx     1018 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall_quantized/conftest.py
--rw-r--r--  2.0 unx      891 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall_quantized/demo.py
--rw-r--r--  2.0 unx     8634 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall_quantized/export.py
--rw-r--r--  2.0 unx     1278 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall_quantized/info.yaml
--rw-r--r--  2.0 unx     4577 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall_quantized/model.py
--rw-r--r--  2.0 unx     2484 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml
--rw-r--r--  2.0 unx     2859 b- defN 24-Apr-16 23:38 qai_hub_models/models/quicksrnetsmall_quantized/test.py
--rw-r--r--  2.0 unx      481 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_general_x4v3/__init__.py
--rw-r--r--  2.0 unx     1016 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_general_x4v3/conftest.py
--rw-r--r--  2.0 unx     1280 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_general_x4v3/demo.py
--rw-r--r--  2.0 unx     8217 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_general_x4v3/export.py
--rw-r--r--  2.0 unx     1206 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_general_x4v3/info.yaml
--rw-r--r--  2.0 unx     5226 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_general_x4v3/model.py
--rw-r--r--  2.0 unx     4443 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml
--rw-r--r--  2.0 unx       44 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_general_x4v3/requirements.txt
--rw-r--r--  2.0 unx     1480 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_general_x4v3/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_x4plus/__init__.py
--rw-r--r--  2.0 unx     1004 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_x4plus/conftest.py
--rw-r--r--  2.0 unx     1256 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_x4plus/demo.py
--rw-r--r--  2.0 unx     7654 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_x4plus/export.py
--rw-r--r--  2.0 unx     1330 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_x4plus/info.yaml
--rw-r--r--  2.0 unx     4443 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_x4plus/model.py
--rw-r--r--  2.0 unx     3673 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_x4plus/perf.yaml
--rw-r--r--  2.0 unx       44 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_x4plus/requirements.txt
--rw-r--r--  2.0 unx     1440 b- defN 24-Apr-16 23:38 qai_hub_models/models/real_esrgan_x4plus/test.py
--rw-r--r--  2.0 unx      469 b- defN 24-Apr-16 23:38 qai_hub_models/models/regnet/__init__.py
--rw-r--r--  2.0 unx      894 b- defN 24-Apr-16 23:38 qai_hub_models/models/regnet/conftest.py
--rw-r--r--  2.0 unx      524 b- defN 24-Apr-16 23:38 qai_hub_models/models/regnet/demo.py
--rw-r--r--  2.0 unx     7867 b- defN 24-Apr-16 23:38 qai_hub_models/models/regnet/export.py
--rw-r--r--  2.0 unx     1291 b- defN 24-Apr-16 23:38 qai_hub_models/models/regnet/info.yaml
--rw-r--r--  2.0 unx      635 b- defN 24-Apr-16 23:38 qai_hub_models/models/regnet/model.py
--rw-r--r--  2.0 unx     4424 b- defN 24-Apr-16 23:38 qai_hub_models/models/regnet/perf.yaml
--rw-r--r--  2.0 unx      984 b- defN 24-Apr-16 23:38 qai_hub_models/models/regnet/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101/__init__.py
--rw-r--r--  2.0 unx      900 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101/demo.py
--rw-r--r--  2.0 unx     7879 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101/export.py
--rw-r--r--  2.0 unx     1312 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101/info.yaml
--rw-r--r--  2.0 unx      609 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101/model.py
--rw-r--r--  2.0 unx     4436 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101/perf.yaml
--rw-r--r--  2.0 unx      961 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101_quantized/__init__.py
--rw-r--r--  2.0 unx      920 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101_quantized/conftest.py
--rw-r--r--  2.0 unx      578 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101_quantized/demo.py
--rw-r--r--  2.0 unx     8359 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101_quantized/export.py
--rw-r--r--  2.0 unx     1346 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101_quantized/info.yaml
--rw-r--r--  2.0 unx     3210 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101_quantized/model.py
--rw-r--r--  2.0 unx     4437 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101_quantized/perf.yaml
--rw-r--r--  2.0 unx      921 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet101_quantized/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18/__init__.py
--rw-r--r--  2.0 unx      898 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18/conftest.py
--rw-r--r--  2.0 unx      530 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18/demo.py
--rw-r--r--  2.0 unx     7875 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18/export.py
--rw-r--r--  2.0 unx     1310 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18/info.yaml
--rw-r--r--  2.0 unx      607 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18/model.py
--rw-r--r--  2.0 unx     4408 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18/perf.yaml
--rw-r--r--  2.0 unx      955 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18/test.py
--rw-r--r--  2.0 unx      482 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18_quantized/__init__.py
--rw-r--r--  2.0 unx      918 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18_quantized/conftest.py
--rw-r--r--  2.0 unx      562 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18_quantized/demo.py
--rw-r--r--  2.0 unx     8355 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18_quantized/export.py
--rw-r--r--  2.0 unx     1343 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18_quantized/info.yaml
--rw-r--r--  2.0 unx     3012 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18_quantized/model.py
--rw-r--r--  2.0 unx     4411 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18_quantized/perf.yaml
--rw-r--r--  2.0 unx      917 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet18_quantized/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet50/__init__.py
--rw-r--r--  2.0 unx      898 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet50/conftest.py
--rw-r--r--  2.0 unx      530 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet50/demo.py
--rw-r--r--  2.0 unx     7875 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet50/export.py
--rw-r--r--  2.0 unx     1303 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet50/info.yaml
--rw-r--r--  2.0 unx      607 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet50/model.py
--rw-r--r--  2.0 unx     4418 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet50/perf.yaml
--rw-r--r--  2.0 unx      955 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnet50/test.py
--rw-r--r--  2.0 unx      473 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101/__init__.py
--rw-r--r--  2.0 unx      902 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101/conftest.py
--rw-r--r--  2.0 unx      536 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101/demo.py
--rw-r--r--  2.0 unx     7883 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101/export.py
--rw-r--r--  2.0 unx     1324 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101/info.yaml
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101/model.py
--rw-r--r--  2.0 unx     4436 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101/perf.yaml
--rw-r--r--  2.0 unx      897 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101/test.py
--rw-r--r--  2.0 unx      484 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101_quantized/__init__.py
--rw-r--r--  2.0 unx      922 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101_quantized/conftest.py
--rw-r--r--  2.0 unx      581 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101_quantized/demo.py
--rw-r--r--  2.0 unx     8383 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101_quantized/export.py
--rw-r--r--  2.0 unx     1365 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101_quantized/info.yaml
--rw-r--r--  2.0 unx     3017 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101_quantized/model.py
--rw-r--r--  2.0 unx     3658 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101_quantized/perf.yaml
--rw-r--r--  2.0 unx      925 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext101_quantized/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50/__init__.py
--rw-r--r--  2.0 unx      900 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50/demo.py
--rw-r--r--  2.0 unx     7879 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50/export.py
--rw-r--r--  2.0 unx     1322 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50/info.yaml
--rw-r--r--  2.0 unx      704 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50/model.py
--rw-r--r--  2.0 unx     4421 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50/perf.yaml
--rw-r--r--  2.0 unx      840 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50_quantized/__init__.py
--rw-r--r--  2.0 unx      920 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50_quantized/conftest.py
--rw-r--r--  2.0 unx      578 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50_quantized/demo.py
--rw-r--r--  2.0 unx     8379 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50_quantized/export.py
--rw-r--r--  2.0 unx     1362 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50_quantized/info.yaml
--rw-r--r--  2.0 unx     3008 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50_quantized/model.py
--rw-r--r--  2.0 unx     3642 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50_quantized/perf.yaml
--rw-r--r--  2.0 unx      921 b- defN 24-Apr-16 23:38 qai_hub_models/models/resnext50_quantized/test.py
--rw-r--r--  2.0 unx      404 b- defN 24-Apr-16 23:38 qai_hub_models/models/sam/__init__.py
--rw-r--r--  2.0 unx     5101 b- defN 24-Apr-16 23:38 qai_hub_models/models/sam/app.py
--rw-r--r--  2.0 unx     1015 b- defN 24-Apr-16 23:38 qai_hub_models/models/sam/conftest.py
--rw-r--r--  2.0 unx     3088 b- defN 24-Apr-16 23:38 qai_hub_models/models/sam/demo.py
--rw-r--r--  2.0 unx    10152 b- defN 24-Apr-16 23:38 qai_hub_models/models/sam/export.py
--rw-r--r--  2.0 unx     1391 b- defN 24-Apr-16 23:38 qai_hub_models/models/sam/info.yaml
--rw-r--r--  2.0 unx    12000 b- defN 24-Apr-16 23:38 qai_hub_models/models/sam/model.py
--rw-r--r--  2.0 unx     3667 b- defN 24-Apr-16 23:38 qai_hub_models/models/sam/perf.yaml
--rw-r--r--  2.0 unx       37 b- defN 24-Apr-16 23:38 qai_hub_models/models/sam/requirements.txt
--rw-r--r--  2.0 unx     3062 b- defN 24-Apr-16 23:38 qai_hub_models/models/sam/test.py
--rw-r--r--  2.0 unx      826 b- defN 24-Apr-16 23:38 qai_hub_models/models/sam/utils.py
--rw-r--r--  2.0 unx      464 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5/__init__.py
--rw-r--r--  2.0 unx      982 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5/conftest.py
--rw-r--r--  2.0 unx      923 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5/demo.py
--rw-r--r--  2.0 unx     8006 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5/export.py
--rw-r--r--  2.0 unx     1104 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5/info.yaml
--rw-r--r--  2.0 unx     2984 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5/model.py
--rw-r--r--  2.0 unx     4418 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5/perf.yaml
--rw-r--r--  2.0 unx     1471 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5_quantized/__init__.py
--rw-r--r--  2.0 unx     1002 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5_quantized/conftest.py
--rw-r--r--  2.0 unx      990 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5_quantized/demo.py
--rw-r--r--  2.0 unx     8167 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5_quantized/export.py
--rw-r--r--  2.0 unx     1151 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5_quantized/info.yaml
--rw-r--r--  2.0 unx     4269 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5_quantized/model.py
--rw-r--r--  2.0 unx     2477 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5_quantized/perf.yaml
--rw-r--r--  2.0 unx     2927 b- defN 24-Apr-16 23:38 qai_hub_models/models/sesr_m5_quantized/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2/__init__.py
--rw-r--r--  2.0 unx      908 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2/conftest.py
--rw-r--r--  2.0 unx      543 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2/demo.py
--rw-r--r--  2.0 unx     7895 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2/export.py
--rw-r--r--  2.0 unx     1353 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2/info.yaml
--rw-r--r--  2.0 unx      713 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2/model.py
--rw-r--r--  2.0 unx     4427 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2/perf.yaml
--rw-r--r--  2.0 unx      857 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2/test.py
--rw-r--r--  2.0 unx      584 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2_quantized/__init__.py
--rw-r--r--  2.0 unx      928 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2_quantized/conftest.py
--rw-r--r--  2.0 unx      588 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2_quantized/demo.py
--rw-r--r--  2.0 unx     8375 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2_quantized/export.py
--rw-r--r--  2.0 unx     1383 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2_quantized/info.yaml
--rw-r--r--  2.0 unx     5989 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2_quantized/model.py
--rw-r--r--  2.0 unx     3269 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2_quantized/perf.yaml
--rw-r--r--  2.0 unx      899 b- defN 24-Apr-16 23:38 qai_hub_models/models/shufflenet_v2_quantized/test.py
--rw-r--r--  2.0 unx      396 b- defN 24-Apr-16 23:38 qai_hub_models/models/sinet/__init__.py
--rw-r--r--  2.0 unx     3793 b- defN 24-Apr-16 23:38 qai_hub_models/models/sinet/app.py
--rw-r--r--  2.0 unx      978 b- defN 24-Apr-16 23:38 qai_hub_models/models/sinet/conftest.py
--rw-r--r--  2.0 unx     1657 b- defN 24-Apr-16 23:38 qai_hub_models/models/sinet/demo.py
--rw-r--r--  2.0 unx     8141 b- defN 24-Apr-16 23:38 qai_hub_models/models/sinet/export.py
--rw-r--r--  2.0 unx     1260 b- defN 24-Apr-16 23:38 qai_hub_models/models/sinet/info.yaml
--rw-r--r--  2.0 unx     4770 b- defN 24-Apr-16 23:38 qai_hub_models/models/sinet/model.py
--rw-r--r--  2.0 unx     4384 b- defN 24-Apr-16 23:38 qai_hub_models/models/sinet/perf.yaml
--rw-r--r--  2.0 unx     1355 b- defN 24-Apr-16 23:38 qai_hub_models/models/sinet/test.py
--rw-r--r--  2.0 unx      473 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1/__init__.py
--rw-r--r--  2.0 unx      908 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1/conftest.py
--rw-r--r--  2.0 unx      539 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1/demo.py
--rw-r--r--  2.0 unx     7896 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1/export.py
--rw-r--r--  2.0 unx     1325 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1/info.yaml
--rw-r--r--  2.0 unx      696 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1/model.py
--rw-r--r--  2.0 unx     4420 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1/perf.yaml
--rw-r--r--  2.0 unx      851 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1/test.py
--rw-r--r--  2.0 unx      582 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1_quantized/__init__.py
--rw-r--r--  2.0 unx      928 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1_quantized/conftest.py
--rw-r--r--  2.0 unx      584 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1_quantized/demo.py
--rw-r--r--  2.0 unx     8328 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1_quantized/export.py
--rw-r--r--  2.0 unx     1358 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1_quantized/info.yaml
--rw-r--r--  2.0 unx     3023 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1_quantized/model.py
--rw-r--r--  2.0 unx     4420 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1_quantized/perf.yaml
--rw-r--r--  2.0 unx      895 b- defN 24-Apr-16 23:38 qai_hub_models/models/squeezenet1_1_quantized/test.py
--rw-r--r--  2.0 unx      540 b- defN 24-Apr-16 23:38 qai_hub_models/models/stable_diffusion_quantized/__init__.py
--rw-r--r--  2.0 unx     7966 b- defN 24-Apr-16 23:38 qai_hub_models/models/stable_diffusion_quantized/app.py
--rw-r--r--  2.0 unx     5765 b- defN 24-Apr-16 23:38 qai_hub_models/models/stable_diffusion_quantized/demo.py
--rw-r--r--  2.0 unx     7432 b- defN 24-Apr-16 23:38 qai_hub_models/models/stable_diffusion_quantized/export.py
--rw-r--r--  2.0 unx     1355 b- defN 24-Apr-16 23:38 qai_hub_models/models/stable_diffusion_quantized/info.yaml
--rw-r--r--  2.0 unx     3604 b- defN 24-Apr-16 23:38 qai_hub_models/models/stable_diffusion_quantized/model.py
--rw-r--r--  2.0 unx     6384 b- defN 24-Apr-16 23:38 qai_hub_models/models/stable_diffusion_quantized/perf.yaml
--rw-r--r--  2.0 unx       46 b- defN 24-Apr-16 23:38 qai_hub_models/models/stable_diffusion_quantized/requirements.txt
--rw-r--r--  2.0 unx     1599 b- defN 24-Apr-16 23:38 qai_hub_models/models/stable_diffusion_quantized/test.py
--rw-r--r--  2.0 unx      404 b- defN 24-Apr-16 23:38 qai_hub_models/models/stylegan2/__init__.py
--rw-r--r--  2.0 unx     4155 b- defN 24-Apr-16 23:38 qai_hub_models/models/stylegan2/app.py
--rw-r--r--  2.0 unx      986 b- defN 24-Apr-16 23:38 qai_hub_models/models/stylegan2/conftest.py
--rw-r--r--  2.0 unx     2847 b- defN 24-Apr-16 23:38 qai_hub_models/models/stylegan2/demo.py
--rw-r--r--  2.0 unx     7762 b- defN 24-Apr-16 23:38 qai_hub_models/models/stylegan2/export.py
--rw-r--r--  2.0 unx     1084 b- defN 24-Apr-16 23:38 qai_hub_models/models/stylegan2/info.yaml
--rw-r--r--  2.0 unx     8406 b- defN 24-Apr-16 23:38 qai_hub_models/models/stylegan2/model.py
--rw-r--r--  2.0 unx     3607 b- defN 24-Apr-16 23:38 qai_hub_models/models/stylegan2/perf.yaml
--rw-r--r--  2.0 unx       11 b- defN 24-Apr-16 23:38 qai_hub_models/models/stylegan2/requirements.txt
--rw-r--r--  2.0 unx     2497 b- defN 24-Apr-16 23:38 qai_hub_models/models/stylegan2/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_base/__init__.py
--rw-r--r--  2.0 unx      900 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_base/conftest.py
--rw-r--r--  2.0 unx      531 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_base/demo.py
--rw-r--r--  2.0 unx     7899 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_base/export.py
--rw-r--r--  2.0 unx     1383 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_base/info.yaml
--rw-r--r--  2.0 unx     1241 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_base/model.py
--rw-r--r--  2.0 unx     3665 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_base/perf.yaml
--rw-r--r--  2.0 unx     1358 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_base/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_small/__init__.py
--rw-r--r--  2.0 unx      902 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_small/conftest.py
--rw-r--r--  2.0 unx      534 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_small/demo.py
--rw-r--r--  2.0 unx     7903 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_small/export.py
--rw-r--r--  2.0 unx     1378 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_small/info.yaml
--rw-r--r--  2.0 unx     1242 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_small/model.py
--rw-r--r--  2.0 unx     3663 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_small/perf.yaml
--rw-r--r--  2.0 unx     1364 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_small/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_tiny/__init__.py
--rw-r--r--  2.0 unx      900 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_tiny/conftest.py
--rw-r--r--  2.0 unx      531 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_tiny/demo.py
--rw-r--r--  2.0 unx     7899 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_tiny/export.py
--rw-r--r--  2.0 unx     1376 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_tiny/info.yaml
--rw-r--r--  2.0 unx     1241 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_tiny/model.py
--rw-r--r--  2.0 unx     3657 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_tiny/perf.yaml
--rw-r--r--  2.0 unx     1476 b- defN 24-Apr-16 23:38 qai_hub_models/models/swin_tiny/test.py
--rw-r--r--  2.0 unx      396 b- defN 24-Apr-16 23:38 qai_hub_models/models/trocr/__init__.py
--rw-r--r--  2.0 unx    10207 b- defN 24-Apr-16 23:38 qai_hub_models/models/trocr/app.py
--rw-r--r--  2.0 unx      892 b- defN 24-Apr-16 23:38 qai_hub_models/models/trocr/conftest.py
--rw-r--r--  2.0 unx     1779 b- defN 24-Apr-16 23:38 qai_hub_models/models/trocr/demo.py
--rw-r--r--  2.0 unx     9655 b- defN 24-Apr-16 23:38 qai_hub_models/models/trocr/export.py
--rw-r--r--  2.0 unx     1370 b- defN 24-Apr-16 23:38 qai_hub_models/models/trocr/info.yaml
--rw-r--r--  2.0 unx    10482 b- defN 24-Apr-16 23:38 qai_hub_models/models/trocr/model.py
--rw-r--r--  2.0 unx     6534 b- defN 24-Apr-16 23:38 qai_hub_models/models/trocr/perf.yaml
--rw-r--r--  2.0 unx       42 b- defN 24-Apr-16 23:38 qai_hub_models/models/trocr/requirements.txt
--rw-r--r--  2.0 unx     2357 b- defN 24-Apr-16 23:38 qai_hub_models/models/trocr/test.py
--rw-r--r--  2.0 unx      348 b- defN 24-Apr-16 23:38 qai_hub_models/models/unet_segmentation/__init__.py
--rw-r--r--  2.0 unx     1305 b- defN 24-Apr-16 23:38 qai_hub_models/models/unet_segmentation/app.py
--rw-r--r--  2.0 unx      916 b- defN 24-Apr-16 23:38 qai_hub_models/models/unet_segmentation/conftest.py
--rw-r--r--  2.0 unx     2509 b- defN 24-Apr-16 23:38 qai_hub_models/models/unet_segmentation/demo.py
--rw-r--r--  2.0 unx     8189 b- defN 24-Apr-16 23:38 qai_hub_models/models/unet_segmentation/export.py
--rw-r--r--  2.0 unx     1310 b- defN 24-Apr-16 23:38 qai_hub_models/models/unet_segmentation/info.yaml
--rw-r--r--  2.0 unx     2666 b- defN 24-Apr-16 23:38 qai_hub_models/models/unet_segmentation/model.py
--rw-r--r--  2.0 unx     4464 b- defN 24-Apr-16 23:38 qai_hub_models/models/unet_segmentation/perf.yaml
--rw-r--r--  2.0 unx     1215 b- defN 24-Apr-16 23:38 qai_hub_models/models/unet_segmentation/test.py
--rw-r--r--  2.0 unx      466 b- defN 24-Apr-16 23:38 qai_hub_models/models/vit/__init__.py
--rw-r--r--  2.0 unx      888 b- defN 24-Apr-16 23:38 qai_hub_models/models/vit/conftest.py
--rw-r--r--  2.0 unx      515 b- defN 24-Apr-16 23:38 qai_hub_models/models/vit/demo.py
--rw-r--r--  2.0 unx     7908 b- defN 24-Apr-16 23:38 qai_hub_models/models/vit/export.py
--rw-r--r--  2.0 unx     1342 b- defN 24-Apr-16 23:38 qai_hub_models/models/vit/info.yaml
--rw-r--r--  2.0 unx      685 b- defN 24-Apr-16 23:38 qai_hub_models/models/vit/model.py
--rw-r--r--  2.0 unx     3652 b- defN 24-Apr-16 23:38 qai_hub_models/models/vit/perf.yaml
--rw-r--r--  2.0 unx      807 b- defN 24-Apr-16 23:38 qai_hub_models/models/vit/test.py
--rw-r--r--  2.0 unx      444 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_base_en/__init__.py
--rw-r--r--  2.0 unx      912 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_base_en/conftest.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_base_en/demo.py
--rw-r--r--  2.0 unx     9707 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_base_en/export.py
--rw-r--r--  2.0 unx     1849 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_base_en/info.yaml
--rw-r--r--  2.0 unx      558 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_base_en/model.py
--rw-r--r--  2.0 unx     6521 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_base_en/perf.yaml
--rw-r--r--  2.0 unx       31 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_base_en/requirements.txt
--rw-r--r--  2.0 unx      696 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_base_en/test.py
--rw-r--r--  2.0 unx      445 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_small_en/__init__.py
--rw-r--r--  2.0 unx      914 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_small_en/conftest.py
--rw-r--r--  2.0 unx      486 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_small_en/demo.py
--rw-r--r--  2.0 unx     9711 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_small_en/export.py
--rw-r--r--  2.0 unx     1848 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_small_en/info.yaml
--rw-r--r--  2.0 unx      560 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_small_en/model.py
--rw-r--r--  2.0 unx     6509 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_small_en/perf.yaml
--rw-r--r--  2.0 unx       38 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_small_en/requirements.txt
--rw-r--r--  2.0 unx      696 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_small_en/test.py
--rw-r--r--  2.0 unx      444 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_tiny_en/__init__.py
--rw-r--r--  2.0 unx      912 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_tiny_en/conftest.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_tiny_en/demo.py
--rw-r--r--  2.0 unx     9707 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_tiny_en/export.py
--rw-r--r--  2.0 unx     1849 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_tiny_en/info.yaml
--rw-r--r--  2.0 unx      558 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_tiny_en/model.py
--rw-r--r--  2.0 unx     6514 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_tiny_en/perf.yaml
--rw-r--r--  2.0 unx       31 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_tiny_en/requirements.txt
--rw-r--r--  2.0 unx      696 b- defN 24-Apr-16 23:38 qai_hub_models/models/whisper_tiny_en/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50/__init__.py
--rw-r--r--  2.0 unx      906 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50/conftest.py
--rw-r--r--  2.0 unx      542 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50/demo.py
--rw-r--r--  2.0 unx     7891 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50/export.py
--rw-r--r--  2.0 unx     1298 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50/info.yaml
--rw-r--r--  2.0 unx      710 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50/model.py
--rw-r--r--  2.0 unx     4431 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50/perf.yaml
--rw-r--r--  2.0 unx      855 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50/test.py
--rw-r--r--  2.0 unx      582 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50_quantized/__init__.py
--rw-r--r--  2.0 unx      926 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50_quantized/conftest.py
--rw-r--r--  2.0 unx      587 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50_quantized/demo.py
--rw-r--r--  2.0 unx     8324 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50_quantized/export.py
--rw-r--r--  2.0 unx     1333 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50_quantized/info.yaml
--rw-r--r--  2.0 unx     3214 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50_quantized/model.py
--rw-r--r--  2.0 unx     4425 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50_quantized/perf.yaml
--rw-r--r--  2.0 unx      932 b- defN 24-Apr-16 23:38 qai_hub_models/models/wideresnet50_quantized/test.py
--rw-r--r--  2.0 unx      461 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr/__init__.py
--rw-r--r--  2.0 unx      976 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr/conftest.py
--rw-r--r--  2.0 unx      742 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr/demo.py
--rw-r--r--  2.0 unx     7994 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr/export.py
--rw-r--r--  2.0 unx     1156 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr/info.yaml
--rw-r--r--  2.0 unx     3403 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr/model.py
--rw-r--r--  2.0 unx     4411 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr/perf.yaml
--rw-r--r--  2.0 unx     1402 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr_quantized/__init__.py
--rw-r--r--  2.0 unx      996 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr_quantized/conftest.py
--rw-r--r--  2.0 unx      956 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr_quantized/demo.py
--rw-r--r--  2.0 unx     8447 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr_quantized/export.py
--rw-r--r--  2.0 unx     1191 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr_quantized/info.yaml
--rw-r--r--  2.0 unx     3915 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr_quantized/model.py
--rw-r--r--  2.0 unx     2474 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr_quantized/perf.yaml
--rw-r--r--  2.0 unx     1607 b- defN 24-Apr-16 23:38 qai_hub_models/models/xlsr_quantized/test.py
--rw-r--r--  2.0 unx      436 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov6/__init__.py
--rw-r--r--  2.0 unx     1071 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov6/app.py
--rw-r--r--  2.0 unx      980 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov6/conftest.py
--rw-r--r--  2.0 unx     1027 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov6/demo.py
--rw-r--r--  2.0 unx     7897 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov6/export.py
--rw-r--r--  2.0 unx     1168 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov6/info.yaml
--rw-r--r--  2.0 unx     4686 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov6/model.py
--rw-r--r--  2.0 unx     4431 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov6/perf.yaml
--rw-r--r--  2.0 unx     1845 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov6/test.py
--rw-r--r--  2.0 unx      436 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7/__init__.py
--rw-r--r--  2.0 unx     2033 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7/app.py
--rw-r--r--  2.0 unx      980 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7/conftest.py
--rw-r--r--  2.0 unx      909 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7/demo.py
--rw-r--r--  2.0 unx     7917 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7/export.py
--rw-r--r--  2.0 unx     1131 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7/info.yaml
--rw-r--r--  2.0 unx    11831 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7/model.py
--rw-r--r--  2.0 unx     3664 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7/perf.yaml
--rw-r--r--  2.0 unx       83 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7/requirements.txt
--rw-r--r--  2.0 unx     2322 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7/test.py
--rw-r--r--  2.0 unx      447 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7_quantized/__init__.py
--rw-r--r--  2.0 unx     1000 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7_quantized/conftest.py
--rw-r--r--  2.0 unx      853 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7_quantized/demo.py
--rw-r--r--  2.0 unx     8330 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7_quantized/export.py
--rw-r--r--  2.0 unx     1318 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7_quantized/info.yaml
--rw-r--r--  2.0 unx     4728 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7_quantized/model.py
--rw-r--r--  2.0 unx     3270 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7_quantized/perf.yaml
--rw-r--r--  2.0 unx       83 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7_quantized/requirements.txt
--rw-r--r--  2.0 unx     1558 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov7_quantized/test.py
--rw-r--r--  2.0 unx      415 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det/__init__.py
--rw-r--r--  2.0 unx      892 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det/app.py
--rw-r--r--  2.0 unx      988 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det/conftest.py
--rw-r--r--  2.0 unx      926 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det/demo.py
--rw-r--r--  2.0 unx     7951 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det/export.py
--rw-r--r--  2.0 unx     1171 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det/info.yaml
--rw-r--r--  2.0 unx     7721 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det/model.py
--rw-r--r--  2.0 unx     2768 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det/perf.yaml
--rw-r--r--  2.0 unx      100 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det/requirements.txt
--rw-r--r--  2.0 unx     2270 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det/test.py
--rw-r--r--  2.0 unx      459 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det_quantized/__init__.py
--rw-r--r--  2.0 unx     1008 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det_quantized/conftest.py
--rw-r--r--  2.0 unx      804 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det_quantized/demo.py
--rw-r--r--  2.0 unx     8351 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det_quantized/export.py
--rw-r--r--  2.0 unx     1354 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det_quantized/info.yaml
--rw-r--r--  2.0 unx     3540 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det_quantized/model.py
--rw-r--r--  2.0 unx     3279 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det_quantized/perf.yaml
--rw-r--r--  2.0 unx      100 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det_quantized/requirements.txt
--rw-r--r--  2.0 unx     1542 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_det_quantized/test.py
--rw-r--r--  2.0 unx      419 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_seg/__init__.py
--rw-r--r--  2.0 unx     7698 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_seg/app.py
--rw-r--r--  2.0 unx      902 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_seg/conftest.py
--rw-r--r--  2.0 unx     3155 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_seg/demo.py
--rw-r--r--  2.0 unx     7974 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_seg/export.py
--rw-r--r--  2.0 unx     1287 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_seg/info.yaml
--rw-r--r--  2.0 unx     4663 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_seg/model.py
--rw-r--r--  2.0 unx     3665 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_seg/perf.yaml
--rw-r--r--  2.0 unx       64 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_seg/requirements.txt
--rw-r--r--  2.0 unx     2536 b- defN 24-Apr-16 23:38 qai_hub_models/models/yolov8_seg/test.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/test/__init__.py
--rw-r--r--  2.0 unx     1043 b- defN 24-Apr-16 23:38 qai_hub_models/test/test_async_compile_jobs.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/test/e2e/__init__.py
--rw-r--r--  2.0 unx     1661 b- defN 24-Apr-16 23:38 qai_hub_models/test/e2e/test_aimet_compile.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/test/test_utils/__init__.py
--rw-r--r--  2.0 unx     1493 b- defN 24-Apr-16 23:38 qai_hub_models/test/test_utils/perf.yaml
--rw-r--r--  2.0 unx     3229 b- defN 24-Apr-16 23:38 qai_hub_models/test/test_utils/test_info_specs.py
--rw-r--r--  2.0 unx     6525 b- defN 24-Apr-16 23:38 qai_hub_models/test/test_utils/test_perf_summary.py
--rw-r--r--  2.0 unx     3295 b- defN 24-Apr-16 23:38 qai_hub_models/test/test_utils/test_qai_hub_helpers.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/utils/__init__.py
--rw-r--r--  2.0 unx    16452 b- defN 24-Apr-16 23:38 qai_hub_models/utils/args.py
--rw-r--r--  2.0 unx    34094 b- defN 24-Apr-16 23:38 qai_hub_models/utils/asset_loaders.py
--rw-r--r--  2.0 unx     6052 b- defN 24-Apr-16 23:38 qai_hub_models/utils/base_model.py
--rw-r--r--  2.0 unx     9261 b- defN 24-Apr-16 23:38 qai_hub_models/utils/bounding_box_processing.py
--rw-r--r--  2.0 unx     1771 b- defN 24-Apr-16 23:38 qai_hub_models/utils/camera_capture.py
--rw-r--r--  2.0 unx     5222 b- defN 24-Apr-16 23:38 qai_hub_models/utils/compare.py
--rw-r--r--  2.0 unx    32743 b- defN 24-Apr-16 23:38 qai_hub_models/utils/config_loaders.py
--rw-r--r--  2.0 unx     3066 b- defN 24-Apr-16 23:38 qai_hub_models/utils/display.py
--rw-r--r--  2.0 unx     6403 b- defN 24-Apr-16 23:38 qai_hub_models/utils/draw.py
--rw-r--r--  2.0 unx     1549 b- defN 24-Apr-16 23:38 qai_hub_models/utils/huggingface.py
--rw-r--r--  2.0 unx    13246 b- defN 24-Apr-16 23:38 qai_hub_models/utils/image_processing.py
--rw-r--r--  2.0 unx    12482 b- defN 24-Apr-16 23:38 qai_hub_models/utils/inference.py
--rw-r--r--  2.0 unx     1308 b- defN 24-Apr-16 23:38 qai_hub_models/utils/input_spec.py
--rw-r--r--  2.0 unx     4559 b- defN 24-Apr-16 23:38 qai_hub_models/utils/measurement.py
--rw-r--r--  2.0 unx     1577 b- defN 24-Apr-16 23:38 qai_hub_models/utils/model_adapters.py
--rw-r--r--  2.0 unx     1406 b- defN 24-Apr-16 23:38 qai_hub_models/utils/path_helpers.py
--rw-r--r--  2.0 unx     5007 b- defN 24-Apr-16 23:38 qai_hub_models/utils/printing.py
--rw-r--r--  2.0 unx     5365 b- defN 24-Apr-16 23:38 qai_hub_models/utils/qai_hub_helpers.py
--rw-r--r--  2.0 unx     1463 b- defN 24-Apr-16 23:38 qai_hub_models/utils/qnn_helpers.py
--rw-r--r--  2.0 unx     2170 b- defN 24-Apr-16 23:38 qai_hub_models/utils/quantization.py
--rw-r--r--  2.0 unx    17774 b- defN 24-Apr-16 23:38 qai_hub_models/utils/quantization_aimet.py
--rw-r--r--  2.0 unx      754 b- defN 24-Apr-16 23:38 qai_hub_models/utils/test_compare.py
--rw-r--r--  2.0 unx     3173 b- defN 24-Apr-16 23:38 qai_hub_models/utils/testing.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/utils/aimet/__init__.py
--rw-r--r--  2.0 unx      876 b- defN 24-Apr-16 23:38 qai_hub_models/utils/aimet/config_loader.py
--rw-r--r--  2.0 unx     1233 b- defN 24-Apr-16 23:38 qai_hub_models/utils/aimet/default_config.json
--rw-r--r--  2.0 unx      946 b- defN 24-Apr-16 23:38 qai_hub_models/utils/aimet/default_config_legacy_v1.json
--rw-r--r--  2.0 unx      955 b- defN 24-Apr-16 23:38 qai_hub_models/utils/aimet/default_config_legacy_v2.json
--rw-r--r--  2.0 unx      919 b- defN 24-Apr-16 23:38 qai_hub_models/utils/aimet/default_config_per_channel_qnn.json
--rw-r--r--  2.0 unx     1187 b- defN 24-Apr-16 23:38 qai_hub_models/utils/aimet/repo.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-16 23:38 qai_hub_models/utils/scorecard/__init__.py
--rw-r--r--  2.0 unx     1468 b- defN 24-Apr-16 23:38 qai_hub_models/utils/scorecard/common.py
--rw-r--r--  2.0 unx    12269 b- defN 24-Apr-16 23:38 qai_hub_models/utils/scorecard/job_summary.py
--rw-r--r--  2.0 unx    13385 b- defN 24-Apr-16 23:38 qai_hub_models/utils/scorecard/model_card.py
--rw-r--r--  2.0 unx    11415 b- defN 24-Apr-16 23:38 qai_hub_models/utils/scorecard/perf_summary.py
--rw-r--r--  2.0 unx     1481 b- defN 24-Apr-16 23:40 qai_hub_models-0.5.0.dist-info/LICENSE
--rw-r--r--  2.0 unx    42594 b- defN 24-Apr-16 23:40 qai_hub_models-0.5.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-16 23:40 qai_hub_models-0.5.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       15 b- defN 24-Apr-16 23:40 qai_hub_models-0.5.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    94607 b- defN 24-Apr-16 23:40 qai_hub_models-0.5.0.dist-info/RECORD
-924 files, 2655325 bytes uncompressed, 857463 bytes compressed:  67.7%
+Zip file size: 1056690 bytes, number of entries: 942
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/__init__.py
+-rw-r--r--  2.0 unx      281 b- defN 24-Apr-30 21:08 qai_hub_models/_version.py
+-rw-r--r--  2.0 unx      620 b- defN 24-Apr-30 21:10 qai_hub_models/asset_bases.yaml
+-rw-r--r--  2.0 unx      734 b- defN 24-Apr-30 21:08 qai_hub_models/conftest.py
+-rw-r--r--  2.0 unx      913 b- defN 24-Apr-30 21:08 qai_hub_models/global_requirements.txt
+-rw-r--r--  2.0 unx      395 b- defN 24-Apr-30 21:08 qai_hub_models/requirements-dev.txt
+-rw-r--r--  2.0 unx      427 b- defN 24-Apr-30 21:08 qai_hub_models/requirements.txt
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/__init__.py
+-rw-r--r--  2.0 unx     4757 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/bsd300.py
+-rw-r--r--  2.0 unx     4109 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/coco.py
+-rw-r--r--  2.0 unx     1614 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/common.py
+-rw-r--r--  2.0 unx     3173 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/imagenette.py
+-rw-r--r--  2.0 unx     2616 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/pascal_voc.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/__init__.py
+-rw-r--r--  2.0 unx     6212 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/base_evaluators.py
+-rw-r--r--  2.0 unx     1326 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/classification_evaluator.py
+-rw-r--r--  2.0 unx     3699 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/detection_evaluator.py
+-rw-r--r--  2.0 unx     2434 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/image_evaluator.py
+-rw-r--r--  2.0 unx     2181 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/superres_evaluator.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/__init__.py
+-rw-r--r--  2.0 unx      666 b- defN 24-Apr-30 21:08 qai_hub_models/models/common.py
+-rw-r--r--  2.0 unx     7882 b- defN 24-Apr-30 21:08 qai_hub_models/models/protocols.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/__init__.py
+-rw-r--r--  2.0 unx     1655 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/common.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/__init__.py
+-rw-r--r--  2.0 unx     4346 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/app.py
+-rw-r--r--  2.0 unx     2904 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/demo.py
+-rw-r--r--  2.0 unx      890 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/evaluator.py
+-rw-r--r--  2.0 unx     2888 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/model.py
+-rw-r--r--  2.0 unx     8565 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/patches/move_datasets.diff
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/deeplab/__init__.py
+-rw-r--r--  2.0 unx     2449 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/deeplab/app.py
+-rw-r--r--  2.0 unx     2256 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/deeplab/demo.py
+-rw-r--r--  2.0 unx      915 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/deeplab/evaluator.py
+-rw-r--r--  2.0 unx     2015 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/deeplab/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/detr/__init__.py
+-rw-r--r--  2.0 unx     4604 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/detr/app.py
+-rw-r--r--  2.0 unx     3565 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/detr/coco_label_map.py
+-rw-r--r--  2.0 unx     2091 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/detr/demo.py
+-rw-r--r--  2.0 unx     2050 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/detr/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/fastsam/__init__.py
+-rw-r--r--  2.0 unx     4883 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/fastsam/app.py
+-rw-r--r--  2.0 unx     2022 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/fastsam/demo.py
+-rw-r--r--  2.0 unx     1935 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/fastsam/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet/__init__.py
+-rw-r--r--  2.0 unx     4548 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet/model.py
+-rw-r--r--  2.0 unx     1569 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet/test_utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet_quantized/__init__.py
+-rw-r--r--  2.0 unx     1165 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet_quantized/aimet_config.json
+-rw-r--r--  2.0 unx     2475 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet_quantized/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/imagenet_classifier/__init__.py
+-rw-r--r--  2.0 unx     3041 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/imagenet_classifier/app.py
+-rw-r--r--  2.0 unx     2432 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/imagenet_classifier/demo.py
+-rw-r--r--  2.0 unx     4661 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/imagenet_classifier/model.py
+-rw-r--r--  2.0 unx     3781 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/imagenet_classifier/test_utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/mediapipe/__init__.py
+-rw-r--r--  2.0 unx    30293 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/mediapipe/app.py
+-rw-r--r--  2.0 unx     4394 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/mediapipe/utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/quicksrnet/__init__.py
+-rw-r--r--  2.0 unx      985 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/quicksrnet/common.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/repaint/__init__.py
+-rw-r--r--  2.0 unx     3366 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/repaint/app.py
+-rw-r--r--  2.0 unx     2229 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/repaint/demo.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/sesr/__init__.py
+-rw-r--r--  2.0 unx     1029 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/sesr/common.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/super_resolution/__init__.py
+-rw-r--r--  2.0 unx     2143 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/super_resolution/app.py
+-rw-r--r--  2.0 unx     2775 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/super_resolution/demo.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/swin/__init__.py
+-rw-r--r--  2.0 unx     9175 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/swin/swin_transformer.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/video_classifier/__init__.py
+-rw-r--r--  2.0 unx    11183 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/video_classifier/app.py
+-rw-r--r--  2.0 unx     1581 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/video_classifier/demo.py
+-rw-r--r--  2.0 unx     1969 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/video_classifier/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/whisper/__init__.py
+-rw-r--r--  2.0 unx    10188 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/whisper/app.py
+-rw-r--r--  2.0 unx     1302 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/whisper/demo.py
+-rw-r--r--  2.0 unx    14119 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/whisper/model.py
+-rw-r--r--  2.0 unx     2848 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/whisper/test_utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/yolo/__init__.py
+-rw-r--r--  2.0 unx     7413 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/yolo/app.py
+-rw-r--r--  2.0 unx     2432 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/yolo/demo.py
+-rw-r--r--  2.0 unx     6081 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/yolo/utils.py
+-rw-r--r--  2.0 unx      450 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/__init__.py
+-rw-r--r--  2.0 unx     1405 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/conftest.py
+-rw-r--r--  2.0 unx      597 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/demo.py
+-rw-r--r--  2.0 unx     8437 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/export.py
+-rw-r--r--  2.0 unx     1023 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/info.yaml
+-rw-r--r--  2.0 unx     4838 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/model.py
+-rw-r--r--  2.0 unx     4545 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/perf.yaml
+-rw-r--r--  2.0 unx     2006 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/test.py
+-rw-r--r--  2.0 unx      532 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/patches/layer_norm.diff
+-rw-r--r--  2.0 unx     1987 b- defN 24-Apr-30 21:08 qai_hub_models/models/baichuan_7b_quantized/info.yaml
+-rw-r--r--  2.0 unx     2038 b- defN 24-Apr-30 21:08 qai_hub_models/models/baichuan_7b_quantized/perf.yaml
+-rw-r--r--  2.0 unx      559 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/__init__.py
+-rw-r--r--  2.0 unx     9705 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/app.py
+-rw-r--r--  2.0 unx     6397 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/demo.py
+-rw-r--r--  2.0 unx     7894 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/export.py
+-rw-r--r--  2.0 unx     1329 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/info.yaml
+-rw-r--r--  2.0 unx     5426 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/model.py
+-rw-r--r--  2.0 unx     8427 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/perf.yaml
+-rw-r--r--  2.0 unx       46 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1556 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/__init__.py
+-rw-r--r--  2.0 unx     1318 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/conftest.py
+-rw-r--r--  2.0 unx      543 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/demo.py
+-rw-r--r--  2.0 unx     8196 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/export.py
+-rw-r--r--  2.0 unx     1287 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/info.yaml
+-rw-r--r--  2.0 unx      708 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/model.py
+-rw-r--r--  2.0 unx     2707 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/perf.yaml
+-rw-r--r--  2.0 unx      857 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/test.py
+-rw-r--r--  2.0 unx      398 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/__init__.py
+-rw-r--r--  2.0 unx     3750 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/app.py
+-rw-r--r--  2.0 unx     1412 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/conftest.py
+-rw-r--r--  2.0 unx     2128 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/demo.py
+-rw-r--r--  2.0 unx     8474 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/export.py
+-rw-r--r--  2.0 unx     1334 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/info.yaml
+-rw-r--r--  2.0 unx     3831 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/model.py
+-rw-r--r--  2.0 unx     3358 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/perf.yaml
+-rw-r--r--  2.0 unx     1790 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/test.py
+-rw-r--r--  2.0 unx      455 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/__init__.py
+-rw-r--r--  2.0 unx     1423 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/conftest.py
+-rw-r--r--  2.0 unx     1091 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/demo.py
+-rw-r--r--  2.0 unx     8498 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/export.py
+-rw-r--r--  2.0 unx     1266 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/info.yaml
+-rw-r--r--  2.0 unx     2098 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/model.py
+-rw-r--r--  2.0 unx     3812 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/perf.yaml
+-rw-r--r--  2.0 unx     1862 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/test.py
+-rw-r--r--  2.0 unx      466 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/__init__.py
+-rw-r--r--  2.0 unx     1433 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/conftest.py
+-rw-r--r--  2.0 unx     1156 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/demo.py
+-rw-r--r--  2.0 unx     8914 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/export.py
+-rw-r--r--  2.0 unx     1306 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/info.yaml
+-rw-r--r--  2.0 unx     2861 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/model.py
+-rw-r--r--  2.0 unx     5887 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2192 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/test.py
+-rw-r--r--  2.0 unx      451 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/__init__.py
+-rw-r--r--  2.0 unx     1417 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/conftest.py
+-rw-r--r--  2.0 unx     1077 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/demo.py
+-rw-r--r--  2.0 unx     8492 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/export.py
+-rw-r--r--  2.0 unx     1285 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/info.yaml
+-rw-r--r--  2.0 unx     1581 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/model.py
+-rw-r--r--  2.0 unx     3781 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/perf.yaml
+-rw-r--r--  2.0 unx     1766 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/__init__.py
+-rw-r--r--  2.0 unx     1316 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/demo.py
+-rw-r--r--  2.0 unx     8169 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/export.py
+-rw-r--r--  2.0 unx     1310 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/info.yaml
+-rw-r--r--  2.0 unx      698 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/model.py
+-rw-r--r--  2.0 unx     4525 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/perf.yaml
+-rw-r--r--  2.0 unx      841 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/__init__.py
+-rw-r--r--  2.0 unx     1319 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/conftest.py
+-rw-r--r--  2.0 unx      896 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/demo.py
+-rw-r--r--  2.0 unx     8186 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/export.py
+-rw-r--r--  2.0 unx     1188 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/info.yaml
+-rw-r--r--  2.0 unx      661 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/model.py
+-rw-r--r--  2.0 unx     3401 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/requirements.txt
+-rw-r--r--  2.0 unx     1316 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/__init__.py
+-rw-r--r--  2.0 unx     1323 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/conftest.py
+-rw-r--r--  2.0 unx      906 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/demo.py
+-rw-r--r--  2.0 unx     8202 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/export.py
+-rw-r--r--  2.0 unx     1215 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/info.yaml
+-rw-r--r--  2.0 unx      668 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/model.py
+-rw-r--r--  2.0 unx     3420 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/requirements.txt
+-rw-r--r--  2.0 unx     1373 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/test.py
+-rw-r--r--  2.0 unx      481 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/__init__.py
+-rw-r--r--  2.0 unx     1318 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/conftest.py
+-rw-r--r--  2.0 unx      893 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/demo.py
+-rw-r--r--  2.0 unx     8182 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/export.py
+-rw-r--r--  2.0 unx     1185 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/info.yaml
+-rw-r--r--  2.0 unx      659 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/model.py
+-rw-r--r--  2.0 unx     3406 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/requirements.txt
+-rw-r--r--  2.0 unx     1636 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/test.py
+-rw-r--r--  2.0 unx      488 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/__init__.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/conftest.py
+-rw-r--r--  2.0 unx      903 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/demo.py
+-rw-r--r--  2.0 unx     8198 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/export.py
+-rw-r--r--  2.0 unx     1212 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/info.yaml
+-rw-r--r--  2.0 unx      666 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/model.py
+-rw-r--r--  2.0 unx     3413 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/requirements.txt
+-rw-r--r--  2.0 unx     1344 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/test.py
+-rw-r--r--  2.0 unx      477 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/__init__.py
+-rw-r--r--  2.0 unx     1320 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/conftest.py
+-rw-r--r--  2.0 unx      549 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/demo.py
+-rw-r--r--  2.0 unx     8184 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/export.py
+-rw-r--r--  2.0 unx     1361 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/info.yaml
+-rw-r--r--  2.0 unx      714 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/model.py
+-rw-r--r--  2.0 unx     4559 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/perf.yaml
+-rw-r--r--  2.0 unx      867 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/test.py
+-rw-r--r--  2.0 unx      463 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/__init__.py
+-rw-r--r--  2.0 unx     1405 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/conftest.py
+-rw-r--r--  2.0 unx      939 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/demo.py
+-rw-r--r--  2.0 unx     8283 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/export.py
+-rw-r--r--  2.0 unx     1117 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/info.yaml
+-rw-r--r--  2.0 unx     3473 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/model.py
+-rw-r--r--  2.0 unx     4597 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/perf.yaml
+-rw-r--r--  2.0 unx     1831 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/test.py
+-rw-r--r--  2.0 unx      418 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/__init__.py
+-rw-r--r--  2.0 unx     3207 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/app.py
+-rw-r--r--  2.0 unx     1416 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/conftest.py
+-rw-r--r--  2.0 unx     3172 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/demo.py
+-rw-r--r--  2.0 unx     7951 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/export.py
+-rw-r--r--  2.0 unx     1070 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/info.yaml
+-rw-r--r--  2.0 unx     2414 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/model.py
+-rw-r--r--  2.0 unx     3431 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/perf.yaml
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/requirements.txt
+-rw-r--r--  2.0 unx     2492 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/test.py
+-rw-r--r--  2.0 unx      440 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/conftest.py
+-rw-r--r--  2.0 unx      762 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/demo.py
+-rw-r--r--  2.0 unx     8545 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/export.py
+-rw-r--r--  2.0 unx     1301 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/info.yaml
+-rw-r--r--  2.0 unx      683 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/model.py
+-rw-r--r--  2.0 unx     3402 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/perf.yaml
+-rw-r--r--  2.0 unx       64 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/requirements.txt
+-rw-r--r--  2.0 unx     1332 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/test.py
+-rw-r--r--  2.0 unx      440 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/conftest.py
+-rw-r--r--  2.0 unx      762 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/demo.py
+-rw-r--r--  2.0 unx     8545 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/export.py
+-rw-r--r--  2.0 unx     1300 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/info.yaml
+-rw-r--r--  2.0 unx      683 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/model.py
+-rw-r--r--  2.0 unx     3409 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/perf.yaml
+-rw-r--r--  2.0 unx       64 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/requirements.txt
+-rw-r--r--  2.0 unx     1332 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/test.py
+-rw-r--r--  2.0 unx      410 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/__init__.py
+-rw-r--r--  2.0 unx     2683 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/app.py
+-rw-r--r--  2.0 unx     1411 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/conftest.py
+-rw-r--r--  2.0 unx     2315 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/demo.py
+-rw-r--r--  2.0 unx     8450 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/export.py
+-rw-r--r--  2.0 unx     1241 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/info.yaml
+-rw-r--r--  2.0 unx     1989 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/model.py
+-rw-r--r--  2.0 unx     4576 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/perf.yaml
+-rw-r--r--  2.0 unx     1637 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/test.py
+-rw-r--r--  2.0 unx      487 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/__init__.py
+-rw-r--r--  2.0 unx     1417 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/conftest.py
+-rw-r--r--  2.0 unx      607 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/demo.py
+-rw-r--r--  2.0 unx     8331 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/export.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/info.yaml
+-rw-r--r--  2.0 unx      648 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/model.py
+-rw-r--r--  2.0 unx     4585 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/requirements.txt
+-rw-r--r--  2.0 unx      804 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/__init__.py
+-rw-r--r--  2.0 unx     1408 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/conftest.py
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/demo.py
+-rw-r--r--  2.0 unx     8295 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/export.py
+-rw-r--r--  2.0 unx     1298 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/info.yaml
+-rw-r--r--  2.0 unx      580 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/model.py
+-rw-r--r--  2.0 unx     4571 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/requirements.txt
+-rw-r--r--  2.0 unx      746 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/__init__.py
+-rw-r--r--  2.0 unx     1418 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/conftest.py
+-rw-r--r--  2.0 unx      627 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/demo.py
+-rw-r--r--  2.0 unx     8731 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/export.py
+-rw-r--r--  2.0 unx     1347 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/info.yaml
+-rw-r--r--  2.0 unx     1169 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/model.py
+-rw-r--r--  2.0 unx     5112 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/requirements.txt
+-rw-r--r--  2.0 unx      840 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/__init__.py
+-rw-r--r--  2.0 unx     1408 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/conftest.py
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/demo.py
+-rw-r--r--  2.0 unx     8295 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/export.py
+-rw-r--r--  2.0 unx     1275 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/info.yaml
+-rw-r--r--  2.0 unx      580 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/model.py
+-rw-r--r--  2.0 unx     4591 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/requirements.txt
+-rw-r--r--  2.0 unx      746 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/__init__.py
+-rw-r--r--  2.0 unx     1418 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/conftest.py
+-rw-r--r--  2.0 unx      627 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/demo.py
+-rw-r--r--  2.0 unx     8731 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/export.py
+-rw-r--r--  2.0 unx     1347 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/info.yaml
+-rw-r--r--  2.0 unx     1156 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/model.py
+-rw-r--r--  2.0 unx     5127 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/requirements.txt
+-rw-r--r--  2.0 unx      840 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/__init__.py
+-rw-r--r--  2.0 unx     1408 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/conftest.py
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/demo.py
+-rw-r--r--  2.0 unx     8295 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/export.py
+-rw-r--r--  2.0 unx     1279 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/info.yaml
+-rw-r--r--  2.0 unx      580 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/model.py
+-rw-r--r--  2.0 unx     4585 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/requirements.txt
+-rw-r--r--  2.0 unx      746 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/test.py
+-rw-r--r--  2.0 unx      485 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/__init__.py
+-rw-r--r--  2.0 unx     1415 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/conftest.py
+-rw-r--r--  2.0 unx      601 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/demo.py
+-rw-r--r--  2.0 unx     8323 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/export.py
+-rw-r--r--  2.0 unx     1327 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/info.yaml
+-rw-r--r--  2.0 unx      640 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/model.py
+-rw-r--r--  2.0 unx     4582 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/requirements.txt
+-rw-r--r--  2.0 unx      794 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/__init__.py
+-rw-r--r--  2.0 unx     1418 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/conftest.py
+-rw-r--r--  2.0 unx      627 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/demo.py
+-rw-r--r--  2.0 unx     8731 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/export.py
+-rw-r--r--  2.0 unx     1347 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/info.yaml
+-rw-r--r--  2.0 unx     1156 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/model.py
+-rw-r--r--  2.0 unx     5123 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/requirements.txt
+-rw-r--r--  2.0 unx      840 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/demo.py
+-rw-r--r--  2.0 unx     8160 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/export.py
+-rw-r--r--  2.0 unx     1295 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/info.yaml
+-rw-r--r--  2.0 unx      743 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/model.py
+-rw-r--r--  2.0 unx     4547 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/perf.yaml
+-rw-r--r--  2.0 unx      840 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/test.py
+-rw-r--r--  2.0 unx      573 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/__init__.py
+-rw-r--r--  2.0 unx     1324 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/conftest.py
+-rw-r--r--  2.0 unx      578 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/demo.py
+-rw-r--r--  2.0 unx     8623 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/export.py
+-rw-r--r--  2.0 unx     1325 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/info.yaml
+-rw-r--r--  2.0 unx     4298 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/model.py
+-rw-r--r--  2.0 unx     6623 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/perf.yaml
+-rw-r--r--  2.0 unx      885 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/test.py
+-rw-r--r--  2.0 unx      430 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/__init__.py
+-rw-r--r--  2.0 unx     8458 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/app.py
+-rw-r--r--  2.0 unx     1409 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/conftest.py
+-rw-r--r--  2.0 unx     1739 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/demo.py
+-rw-r--r--  2.0 unx     8441 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/export.py
+-rw-r--r--  2.0 unx     1195 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/info.yaml
+-rw-r--r--  2.0 unx     3360 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/model.py
+-rw-r--r--  2.0 unx     4557 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/perf.yaml
+-rw-r--r--  2.0 unx       51 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/requirements.txt
+-rw-r--r--  2.0 unx     1420 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/test.py
+-rw-r--r--  2.0 unx      434 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/__init__.py
+-rw-r--r--  2.0 unx     2133 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/app.py
+-rw-r--r--  2.0 unx     1426 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py
+-rw-r--r--  2.0 unx     1517 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/demo.py
+-rw-r--r--  2.0 unx     7848 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/export.py
+-rw-r--r--  2.0 unx     1248 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/info.yaml
+-rw-r--r--  2.0 unx     7587 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/model.py
+-rw-r--r--  2.0 unx     3446 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml
+-rw-r--r--  2.0 unx       72 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/requirements.txt
+-rw-r--r--  2.0 unx     2560 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/test.py
+-rw-r--r--  2.0 unx      477 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/__init__.py
+-rw-r--r--  2.0 unx     1317 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/conftest.py
+-rw-r--r--  2.0 unx      546 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/demo.py
+-rw-r--r--  2.0 unx     8172 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/export.py
+-rw-r--r--  2.0 unx     1359 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/info.yaml
+-rw-r--r--  2.0 unx      756 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/model.py
+-rw-r--r--  2.0 unx     4565 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/perf.yaml
+-rw-r--r--  2.0 unx      861 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/test.py
+-rw-r--r--  2.0 unx      584 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/__init__.py
+-rw-r--r--  2.0 unx     1327 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/conftest.py
+-rw-r--r--  2.0 unx      591 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/demo.py
+-rw-r--r--  2.0 unx     8636 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/export.py
+-rw-r--r--  2.0 unx     1556 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/info.yaml
+-rw-r--r--  2.0 unx     7762 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/model.py
+-rw-r--r--  2.0 unx     5106 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/perf.yaml
+-rw-r--r--  2.0 unx      901 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/test.py
+-rw-r--r--  2.0 unx      455 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/__init__.py
+-rw-r--r--  2.0 unx     1411 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/conftest.py
+-rw-r--r--  2.0 unx      911 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/demo.py
+-rw-r--r--  2.0 unx     8460 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/export.py
+-rw-r--r--  2.0 unx     1082 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/info.yaml
+-rw-r--r--  2.0 unx     4977 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/model.py
+-rw-r--r--  2.0 unx     4545 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/perf.yaml
+-rw-r--r--  2.0 unx      171 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/requirements.txt
+-rw-r--r--  2.0 unx     2074 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/test.py
+-rw-r--r--  2.0 unx      404 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/__init__.py
+-rw-r--r--  2.0 unx     3986 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/app.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/conftest.py
+-rw-r--r--  2.0 unx     2072 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/demo.py
+-rw-r--r--  2.0 unx     7919 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/export.py
+-rw-r--r--  2.0 unx     1112 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/info.yaml
+-rw-r--r--  2.0 unx     3788 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/model.py
+-rw-r--r--  2.0 unx     3366 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/perf.yaml
+-rw-r--r--  2.0 unx       39 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/requirements.txt
+-rw-r--r--  2.0 unx     1714 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/test.py
+-rw-r--r--  2.0 unx     1997 b- defN 24-Apr-30 21:08 qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml
+-rw-r--r--  2.0 unx     2039 b- defN 24-Apr-30 21:08 qai_hub_models/models/llama_v2_7b_chat_quantized/perf.yaml
+-rw-r--r--  2.0 unx      412 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/__init__.py
+-rw-r--r--  2.0 unx     2099 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/app.py
+-rw-r--r--  2.0 unx     1413 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/conftest.py
+-rw-r--r--  2.0 unx     2862 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/demo.py
+-rw-r--r--  2.0 unx    10089 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/export.py
+-rw-r--r--  2.0 unx     1469 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/info.yaml
+-rw-r--r--  2.0 unx     7669 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/model.py
+-rw-r--r--  2.0 unx     8427 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/perf.yaml
+-rw-r--r--  2.0 unx     1441 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/test.py
+-rw-r--r--  2.0 unx      412 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/__init__.py
+-rw-r--r--  2.0 unx     9819 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/app.py
+-rw-r--r--  2.0 unx     1413 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/conftest.py
+-rw-r--r--  2.0 unx     2832 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/demo.py
+-rw-r--r--  2.0 unx    10044 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/export.py
+-rw-r--r--  2.0 unx     1374 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/info.yaml
+-rw-r--r--  2.0 unx     6017 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/model.py
+-rw-r--r--  2.0 unx     8437 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/perf.yaml
+-rw-r--r--  2.0 unx     1442 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/test.py
+-rw-r--r--  2.0 unx      412 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/__init__.py
+-rw-r--r--  2.0 unx     4430 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/app.py
+-rw-r--r--  2.0 unx     1413 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/conftest.py
+-rw-r--r--  2.0 unx     2889 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/demo.py
+-rw-r--r--  2.0 unx    10045 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/export.py
+-rw-r--r--  2.0 unx     1387 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/info.yaml
+-rw-r--r--  2.0 unx     5801 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/model.py
+-rw-r--r--  2.0 unx     8435 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/perf.yaml
+-rw-r--r--  2.0 unx     1443 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/test.py
+-rw-r--r--  2.0 unx      362 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/__init__.py
+-rw-r--r--  2.0 unx     1411 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/app.py
+-rw-r--r--  2.0 unx     1321 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/conftest.py
+-rw-r--r--  2.0 unx     2674 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/demo.py
+-rw-r--r--  2.0 unx     8479 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/export.py
+-rw-r--r--  2.0 unx     1455 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/info.yaml
+-rw-r--r--  2.0 unx    12352 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/model.py
+-rw-r--r--  2.0 unx     4583 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/perf.yaml
+-rw-r--r--  2.0 unx       15 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/requirements.txt
+-rw-r--r--  2.0 unx     1396 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/test.py
+-rw-r--r--  2.0 unx     2492 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/utils.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/demo.py
+-rw-r--r--  2.0 unx     8160 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/export.py
+-rw-r--r--  2.0 unx     1333 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/info.yaml
+-rw-r--r--  2.0 unx      699 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/model.py
+-rw-r--r--  2.0 unx     4528 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/perf.yaml
+-rw-r--r--  2.0 unx      882 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/test.py
+-rw-r--r--  2.0 unx      474 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/__init__.py
+-rw-r--r--  2.0 unx     1411 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/conftest.py
+-rw-r--r--  2.0 unx      540 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/demo.py
+-rw-r--r--  2.0 unx     8172 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/export.py
+-rw-r--r--  2.0 unx     1380 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/info.yaml
+-rw-r--r--  2.0 unx     2457 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/model.py
+-rw-r--r--  2.0 unx     4553 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/perf.yaml
+-rw-r--r--  2.0 unx     1091 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/test.py
+-rw-r--r--  2.0 unx      485 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/__init__.py
+-rw-r--r--  2.0 unx     1421 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/conftest.py
+-rw-r--r--  2.0 unx      585 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/demo.py
+-rw-r--r--  2.0 unx     8636 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/export.py
+-rw-r--r--  2.0 unx     1362 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/info.yaml
+-rw-r--r--  2.0 unx     3429 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/model.py
+-rw-r--r--  2.0 unx     6626 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/perf.yaml
+-rw-r--r--  2.0 unx     1002 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/__init__.py
+-rw-r--r--  2.0 unx     1323 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/conftest.py
+-rw-r--r--  2.0 unx      556 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/demo.py
+-rw-r--r--  2.0 unx     8216 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/export.py
+-rw-r--r--  2.0 unx     1340 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/info.yaml
+-rw-r--r--  2.0 unx      721 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/model.py
+-rw-r--r--  2.0 unx     3392 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/perf.yaml
+-rw-r--r--  2.0 unx      879 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/test.py
+-rw-r--r--  2.0 unx      607 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/__init__.py
+-rw-r--r--  2.0 unx     1333 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/conftest.py
+-rw-r--r--  2.0 unx      748 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/demo.py
+-rw-r--r--  2.0 unx     8368 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/export.py
+-rw-r--r--  2.0 unx     1374 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/info.yaml
+-rw-r--r--  2.0 unx     3075 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/model.py
+-rw-r--r--  2.0 unx     5123 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml
+-rw-r--r--  2.0 unx      917 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/__init__.py
+-rw-r--r--  2.0 unx     1323 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/conftest.py
+-rw-r--r--  2.0 unx      556 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/demo.py
+-rw-r--r--  2.0 unx     8216 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/export.py
+-rw-r--r--  2.0 unx     1338 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/info.yaml
+-rw-r--r--  2.0 unx      721 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/model.py
+-rw-r--r--  2.0 unx     3396 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/perf.yaml
+-rw-r--r--  2.0 unx      879 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/test.py
+-rw-r--r--  2.0 unx      394 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/__init__.py
+-rw-r--r--  2.0 unx     3958 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/app.py
+-rw-r--r--  2.0 unx     1410 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/conftest.py
+-rw-r--r--  2.0 unx     3404 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/demo.py
+-rw-r--r--  2.0 unx    10000 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/export.py
+-rw-r--r--  2.0 unx     1494 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/info.yaml
+-rw-r--r--  2.0 unx     5374 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/model.py
+-rw-r--r--  2.0 unx     6070 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/perf.yaml
+-rw-r--r--  2.0 unx       29 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/requirements.txt
+-rw-r--r--  2.0 unx     2118 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/test.py
+-rw-r--r--  2.0 unx      402 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/__init__.py
+-rw-r--r--  2.0 unx    12008 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/app.py
+-rw-r--r--  2.0 unx     1407 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/conftest.py
+-rw-r--r--  2.0 unx     2053 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/demo.py
+-rw-r--r--  2.0 unx     8452 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/export.py
+-rw-r--r--  2.0 unx     1246 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/info.yaml
+-rw-r--r--  2.0 unx     5084 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/model.py
+-rw-r--r--  2.0 unx     4572 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/perf.yaml
+-rw-r--r--  2.0 unx       31 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/requirements.txt
+-rw-r--r--  2.0 unx     1321 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/__init__.py
+-rw-r--r--  2.0 unx     1414 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/conftest.py
+-rw-r--r--  2.0 unx      972 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/demo.py
+-rw-r--r--  2.0 unx     8462 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/export.py
+-rw-r--r--  2.0 unx     1248 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/info.yaml
+-rw-r--r--  2.0 unx     3170 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/model.py
+-rw-r--r--  2.0 unx     4555 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/perf.yaml
+-rw-r--r--  2.0 unx     1422 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/__init__.py
+-rw-r--r--  2.0 unx     1424 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/conftest.py
+-rw-r--r--  2.0 unx      891 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/demo.py
+-rw-r--r--  2.0 unx     8898 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/export.py
+-rw-r--r--  2.0 unx     1274 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/info.yaml
+-rw-r--r--  2.0 unx     4587 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/model.py
+-rw-r--r--  2.0 unx     3932 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2921 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/test.py
+-rw-r--r--  2.0 unx      473 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/__init__.py
+-rw-r--r--  2.0 unx     1415 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/conftest.py
+-rw-r--r--  2.0 unx      976 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/demo.py
+-rw-r--r--  2.0 unx     8466 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/export.py
+-rw-r--r--  2.0 unx     1242 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/info.yaml
+-rw-r--r--  2.0 unx     3177 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/model.py
+-rw-r--r--  2.0 unx     4554 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/perf.yaml
+-rw-r--r--  2.0 unx     1428 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/test.py
+-rw-r--r--  2.0 unx      484 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/__init__.py
+-rw-r--r--  2.0 unx     1425 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/conftest.py
+-rw-r--r--  2.0 unx      900 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/demo.py
+-rw-r--r--  2.0 unx     8902 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/export.py
+-rw-r--r--  2.0 unx     1282 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/info.yaml
+-rw-r--r--  2.0 unx     4597 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/model.py
+-rw-r--r--  2.0 unx     3935 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2912 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/__init__.py
+-rw-r--r--  2.0 unx     1414 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/conftest.py
+-rw-r--r--  2.0 unx      972 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/demo.py
+-rw-r--r--  2.0 unx     8462 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/export.py
+-rw-r--r--  2.0 unx     1238 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/info.yaml
+-rw-r--r--  2.0 unx     3170 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/model.py
+-rw-r--r--  2.0 unx     4546 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/perf.yaml
+-rw-r--r--  2.0 unx     1422 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/__init__.py
+-rw-r--r--  2.0 unx     1424 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/conftest.py
+-rw-r--r--  2.0 unx      891 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/demo.py
+-rw-r--r--  2.0 unx     8898 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/export.py
+-rw-r--r--  2.0 unx     1278 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/info.yaml
+-rw-r--r--  2.0 unx     4577 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/model.py
+-rw-r--r--  2.0 unx     3932 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2859 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/test.py
+-rw-r--r--  2.0 unx      481 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/__init__.py
+-rw-r--r--  2.0 unx     1423 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/conftest.py
+-rw-r--r--  2.0 unx     1280 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/demo.py
+-rw-r--r--  2.0 unx     8498 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/export.py
+-rw-r--r--  2.0 unx     1206 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/info.yaml
+-rw-r--r--  2.0 unx     5435 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/model.py
+-rw-r--r--  2.0 unx     4580 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml
+-rw-r--r--  2.0 unx       44 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/requirements.txt
+-rw-r--r--  2.0 unx     1480 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/__init__.py
+-rw-r--r--  2.0 unx     1417 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/conftest.py
+-rw-r--r--  2.0 unx     1256 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/demo.py
+-rw-r--r--  2.0 unx     7935 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/export.py
+-rw-r--r--  2.0 unx     1330 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/info.yaml
+-rw-r--r--  2.0 unx     4443 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/model.py
+-rw-r--r--  2.0 unx     3412 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/perf.yaml
+-rw-r--r--  2.0 unx       44 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/requirements.txt
+-rw-r--r--  2.0 unx     1440 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/test.py
+-rw-r--r--  2.0 unx      469 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/__init__.py
+-rw-r--r--  2.0 unx     1311 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/conftest.py
+-rw-r--r--  2.0 unx      524 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/demo.py
+-rw-r--r--  2.0 unx     8148 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/export.py
+-rw-r--r--  2.0 unx     1291 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/info.yaml
+-rw-r--r--  2.0 unx      635 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/model.py
+-rw-r--r--  2.0 unx     4558 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/perf.yaml
+-rw-r--r--  2.0 unx      984 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/demo.py
+-rw-r--r--  2.0 unx     8160 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/export.py
+-rw-r--r--  2.0 unx     1312 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/info.yaml
+-rw-r--r--  2.0 unx      609 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/model.py
+-rw-r--r--  2.0 unx     4570 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/perf.yaml
+-rw-r--r--  2.0 unx      961 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/__init__.py
+-rw-r--r--  2.0 unx     1324 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/conftest.py
+-rw-r--r--  2.0 unx      578 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/demo.py
+-rw-r--r--  2.0 unx     8623 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/export.py
+-rw-r--r--  2.0 unx     1346 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/info.yaml
+-rw-r--r--  2.0 unx     3210 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/model.py
+-rw-r--r--  2.0 unx     6649 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/perf.yaml
+-rw-r--r--  2.0 unx      921 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/__init__.py
+-rw-r--r--  2.0 unx     1313 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/conftest.py
+-rw-r--r--  2.0 unx      530 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/demo.py
+-rw-r--r--  2.0 unx     8156 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/export.py
+-rw-r--r--  2.0 unx     1310 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/info.yaml
+-rw-r--r--  2.0 unx      607 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/model.py
+-rw-r--r--  2.0 unx     4539 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/perf.yaml
+-rw-r--r--  2.0 unx      955 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/test.py
+-rw-r--r--  2.0 unx      482 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/__init__.py
+-rw-r--r--  2.0 unx     1323 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/conftest.py
+-rw-r--r--  2.0 unx      562 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/demo.py
+-rw-r--r--  2.0 unx     8619 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/export.py
+-rw-r--r--  2.0 unx     1343 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/info.yaml
+-rw-r--r--  2.0 unx     3012 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/model.py
+-rw-r--r--  2.0 unx     6614 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/perf.yaml
+-rw-r--r--  2.0 unx      917 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/__init__.py
+-rw-r--r--  2.0 unx     1313 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/conftest.py
+-rw-r--r--  2.0 unx      530 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/demo.py
+-rw-r--r--  2.0 unx     8156 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/export.py
+-rw-r--r--  2.0 unx     1303 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/info.yaml
+-rw-r--r--  2.0 unx      607 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/model.py
+-rw-r--r--  2.0 unx     4553 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/perf.yaml
+-rw-r--r--  2.0 unx      955 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/test.py
+-rw-r--r--  2.0 unx      473 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/__init__.py
+-rw-r--r--  2.0 unx     1315 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/conftest.py
+-rw-r--r--  2.0 unx      536 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/demo.py
+-rw-r--r--  2.0 unx     8164 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/export.py
+-rw-r--r--  2.0 unx     1324 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/info.yaml
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/model.py
+-rw-r--r--  2.0 unx     4568 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/perf.yaml
+-rw-r--r--  2.0 unx      897 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/test.py
+-rw-r--r--  2.0 unx      484 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/__init__.py
+-rw-r--r--  2.0 unx     1325 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/conftest.py
+-rw-r--r--  2.0 unx      581 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/demo.py
+-rw-r--r--  2.0 unx     8647 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/export.py
+-rw-r--r--  2.0 unx     1365 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/info.yaml
+-rw-r--r--  2.0 unx     3017 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/model.py
+-rw-r--r--  2.0 unx     5115 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/perf.yaml
+-rw-r--r--  2.0 unx      925 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/demo.py
+-rw-r--r--  2.0 unx     8160 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/export.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/info.yaml
+-rw-r--r--  2.0 unx      704 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/model.py
+-rw-r--r--  2.0 unx     4554 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/perf.yaml
+-rw-r--r--  2.0 unx      840 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/__init__.py
+-rw-r--r--  2.0 unx     1324 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/conftest.py
+-rw-r--r--  2.0 unx      578 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/demo.py
+-rw-r--r--  2.0 unx     8643 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/export.py
+-rw-r--r--  2.0 unx     1362 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/info.yaml
+-rw-r--r--  2.0 unx     3008 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/model.py
+-rw-r--r--  2.0 unx     5084 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/perf.yaml
+-rw-r--r--  2.0 unx      921 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/test.py
+-rw-r--r--  2.0 unx      404 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/__init__.py
+-rw-r--r--  2.0 unx     5101 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/app.py
+-rw-r--r--  2.0 unx     1402 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/conftest.py
+-rw-r--r--  2.0 unx     3088 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/demo.py
+-rw-r--r--  2.0 unx    10468 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/export.py
+-rw-r--r--  2.0 unx     1391 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/info.yaml
+-rw-r--r--  2.0 unx    12000 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/model.py
+-rw-r--r--  2.0 unx     3411 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/perf.yaml
+-rw-r--r--  2.0 unx       37 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/requirements.txt
+-rw-r--r--  2.0 unx     3062 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/test.py
+-rw-r--r--  2.0 unx      826 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/utils.py
+-rw-r--r--  2.0 unx      464 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/__init__.py
+-rw-r--r--  2.0 unx     1406 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/conftest.py
+-rw-r--r--  2.0 unx      923 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/demo.py
+-rw-r--r--  2.0 unx     8287 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/export.py
+-rw-r--r--  2.0 unx     1104 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/info.yaml
+-rw-r--r--  2.0 unx     2984 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/model.py
+-rw-r--r--  2.0 unx     4547 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/perf.yaml
+-rw-r--r--  2.0 unx     1471 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/__init__.py
+-rw-r--r--  2.0 unx     1416 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/conftest.py
+-rw-r--r--  2.0 unx      990 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/demo.py
+-rw-r--r--  2.0 unx     8431 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/export.py
+-rw-r--r--  2.0 unx     1151 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/info.yaml
+-rw-r--r--  2.0 unx     4269 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/model.py
+-rw-r--r--  2.0 unx     3925 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2927 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/__init__.py
+-rw-r--r--  2.0 unx     1318 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/conftest.py
+-rw-r--r--  2.0 unx      543 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/demo.py
+-rw-r--r--  2.0 unx     8176 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/export.py
+-rw-r--r--  2.0 unx     1353 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/info.yaml
+-rw-r--r--  2.0 unx      713 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/model.py
+-rw-r--r--  2.0 unx     4561 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/perf.yaml
+-rw-r--r--  2.0 unx      857 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/test.py
+-rw-r--r--  2.0 unx      584 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/__init__.py
+-rw-r--r--  2.0 unx     1328 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/conftest.py
+-rw-r--r--  2.0 unx      588 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/demo.py
+-rw-r--r--  2.0 unx     8639 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/export.py
+-rw-r--r--  2.0 unx     1383 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/info.yaml
+-rw-r--r--  2.0 unx     5989 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/model.py
+-rw-r--r--  2.0 unx     5481 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/perf.yaml
+-rw-r--r--  2.0 unx      899 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/test.py
+-rw-r--r--  2.0 unx      396 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/__init__.py
+-rw-r--r--  2.0 unx     3793 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/app.py
+-rw-r--r--  2.0 unx     1404 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/conftest.py
+-rw-r--r--  2.0 unx     1657 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/demo.py
+-rw-r--r--  2.0 unx     8422 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/export.py
+-rw-r--r--  2.0 unx     1260 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/info.yaml
+-rw-r--r--  2.0 unx     4770 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/model.py
+-rw-r--r--  2.0 unx     4518 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/perf.yaml
+-rw-r--r--  2.0 unx     1355 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/test.py
+-rw-r--r--  2.0 unx      473 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/__init__.py
+-rw-r--r--  2.0 unx     1318 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/conftest.py
+-rw-r--r--  2.0 unx      539 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/demo.py
+-rw-r--r--  2.0 unx     8177 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/export.py
+-rw-r--r--  2.0 unx     1325 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/info.yaml
+-rw-r--r--  2.0 unx      696 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/model.py
+-rw-r--r--  2.0 unx     4551 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/perf.yaml
+-rw-r--r--  2.0 unx      851 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/test.py
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/__init__.py
+-rw-r--r--  2.0 unx     1328 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/conftest.py
+-rw-r--r--  2.0 unx      584 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/demo.py
+-rw-r--r--  2.0 unx     8592 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/export.py
+-rw-r--r--  2.0 unx     1358 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/info.yaml
+-rw-r--r--  2.0 unx     3023 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/model.py
+-rw-r--r--  2.0 unx     6622 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/perf.yaml
+-rw-r--r--  2.0 unx      895 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/test.py
+-rw-r--r--  2.0 unx      540 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/__init__.py
+-rw-r--r--  2.0 unx     7966 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/app.py
+-rw-r--r--  2.0 unx     5765 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/demo.py
+-rw-r--r--  2.0 unx     7704 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/export.py
+-rw-r--r--  2.0 unx     1355 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/info.yaml
+-rw-r--r--  2.0 unx     3604 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/model.py
+-rw-r--r--  2.0 unx     6384 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/perf.yaml
+-rw-r--r--  2.0 unx       46 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1599 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/test.py
+-rw-r--r--  2.0 unx      404 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/__init__.py
+-rw-r--r--  2.0 unx     4155 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/app.py
+-rw-r--r--  2.0 unx     1408 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/conftest.py
+-rw-r--r--  2.0 unx     2847 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/demo.py
+-rw-r--r--  2.0 unx     8043 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/export.py
+-rw-r--r--  2.0 unx     1084 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/info.yaml
+-rw-r--r--  2.0 unx     8406 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/model.py
+-rw-r--r--  2.0 unx     3385 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/perf.yaml
+-rw-r--r--  2.0 unx       11 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/requirements.txt
+-rw-r--r--  2.0 unx     2497 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/conftest.py
+-rw-r--r--  2.0 unx      531 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/demo.py
+-rw-r--r--  2.0 unx     8180 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/export.py
+-rw-r--r--  2.0 unx     1383 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/info.yaml
+-rw-r--r--  2.0 unx     1241 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/model.py
+-rw-r--r--  2.0 unx     3405 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/perf.yaml
+-rw-r--r--  2.0 unx     1358 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/__init__.py
+-rw-r--r--  2.0 unx     1315 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/conftest.py
+-rw-r--r--  2.0 unx      534 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/demo.py
+-rw-r--r--  2.0 unx     8184 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/export.py
+-rw-r--r--  2.0 unx     1378 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/info.yaml
+-rw-r--r--  2.0 unx     1242 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/model.py
+-rw-r--r--  2.0 unx     3404 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/perf.yaml
+-rw-r--r--  2.0 unx     1364 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/conftest.py
+-rw-r--r--  2.0 unx      531 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/demo.py
+-rw-r--r--  2.0 unx     8180 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/export.py
+-rw-r--r--  2.0 unx     1376 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/info.yaml
+-rw-r--r--  2.0 unx     1241 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/model.py
+-rw-r--r--  2.0 unx     3398 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/perf.yaml
+-rw-r--r--  2.0 unx     1476 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/test.py
+-rw-r--r--  2.0 unx      396 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/__init__.py
+-rw-r--r--  2.0 unx    10207 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/app.py
+-rw-r--r--  2.0 unx     1310 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/conftest.py
+-rw-r--r--  2.0 unx     1779 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/demo.py
+-rw-r--r--  2.0 unx     9944 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/export.py
+-rw-r--r--  2.0 unx     1370 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/info.yaml
+-rw-r--r--  2.0 unx    10482 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/model.py
+-rw-r--r--  2.0 unx     6080 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/perf.yaml
+-rw-r--r--  2.0 unx       42 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/requirements.txt
+-rw-r--r--  2.0 unx     2357 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/test.py
+-rw-r--r--  2.0 unx      348 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/__init__.py
+-rw-r--r--  2.0 unx     1305 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/app.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/conftest.py
+-rw-r--r--  2.0 unx     2509 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/demo.py
+-rw-r--r--  2.0 unx     8470 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/export.py
+-rw-r--r--  2.0 unx     1310 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/info.yaml
+-rw-r--r--  2.0 unx     2666 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/model.py
+-rw-r--r--  2.0 unx     4595 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/perf.yaml
+-rw-r--r--  2.0 unx     1215 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/test.py
+-rw-r--r--  2.0 unx      466 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/__init__.py
+-rw-r--r--  2.0 unx     1308 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/conftest.py
+-rw-r--r--  2.0 unx      515 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/demo.py
+-rw-r--r--  2.0 unx     8189 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/export.py
+-rw-r--r--  2.0 unx     1342 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/info.yaml
+-rw-r--r--  2.0 unx      685 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/model.py
+-rw-r--r--  2.0 unx     3394 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/perf.yaml
+-rw-r--r--  2.0 unx      807 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/test.py
+-rw-r--r--  2.0 unx      444 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/__init__.py
+-rw-r--r--  2.0 unx     1320 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/conftest.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/demo.py
+-rw-r--r--  2.0 unx     9996 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/export.py
+-rw-r--r--  2.0 unx     1849 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/info.yaml
+-rw-r--r--  2.0 unx      558 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/model.py
+-rw-r--r--  2.0 unx     6064 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/perf.yaml
+-rw-r--r--  2.0 unx       31 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/requirements.txt
+-rw-r--r--  2.0 unx      696 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/test.py
+-rw-r--r--  2.0 unx      445 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/__init__.py
+-rw-r--r--  2.0 unx     1321 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/conftest.py
+-rw-r--r--  2.0 unx      486 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/demo.py
+-rw-r--r--  2.0 unx    10000 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/export.py
+-rw-r--r--  2.0 unx     1848 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/info.yaml
+-rw-r--r--  2.0 unx      560 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/model.py
+-rw-r--r--  2.0 unx     6078 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/perf.yaml
+-rw-r--r--  2.0 unx       38 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/requirements.txt
+-rw-r--r--  2.0 unx      696 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/test.py
+-rw-r--r--  2.0 unx      444 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/__init__.py
+-rw-r--r--  2.0 unx     1320 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/conftest.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/demo.py
+-rw-r--r--  2.0 unx     9996 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/export.py
+-rw-r--r--  2.0 unx     1849 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/info.yaml
+-rw-r--r--  2.0 unx      558 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/model.py
+-rw-r--r--  2.0 unx     6057 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/perf.yaml
+-rw-r--r--  2.0 unx       31 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/requirements.txt
+-rw-r--r--  2.0 unx      696 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/__init__.py
+-rw-r--r--  2.0 unx     1317 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/conftest.py
+-rw-r--r--  2.0 unx      542 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/demo.py
+-rw-r--r--  2.0 unx     8172 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/export.py
+-rw-r--r--  2.0 unx     1298 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/info.yaml
+-rw-r--r--  2.0 unx      710 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/model.py
+-rw-r--r--  2.0 unx     4567 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/perf.yaml
+-rw-r--r--  2.0 unx      855 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/test.py
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/__init__.py
+-rw-r--r--  2.0 unx     1327 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/conftest.py
+-rw-r--r--  2.0 unx      587 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/demo.py
+-rw-r--r--  2.0 unx     8588 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/export.py
+-rw-r--r--  2.0 unx     1333 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/info.yaml
+-rw-r--r--  2.0 unx     3214 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/model.py
+-rw-r--r--  2.0 unx     6629 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/perf.yaml
+-rw-r--r--  2.0 unx      932 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/test.py
+-rw-r--r--  2.0 unx      461 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/__init__.py
+-rw-r--r--  2.0 unx     1403 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/conftest.py
+-rw-r--r--  2.0 unx      742 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/demo.py
+-rw-r--r--  2.0 unx     8275 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/export.py
+-rw-r--r--  2.0 unx     1156 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/info.yaml
+-rw-r--r--  2.0 unx     3403 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/model.py
+-rw-r--r--  2.0 unx     4545 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/perf.yaml
+-rw-r--r--  2.0 unx     1402 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/__init__.py
+-rw-r--r--  2.0 unx     1413 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/conftest.py
+-rw-r--r--  2.0 unx      956 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/demo.py
+-rw-r--r--  2.0 unx     8711 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/export.py
+-rw-r--r--  2.0 unx     1191 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/info.yaml
+-rw-r--r--  2.0 unx     3915 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/model.py
+-rw-r--r--  2.0 unx     3919 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/perf.yaml
+-rw-r--r--  2.0 unx     1607 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/test.py
+-rw-r--r--  2.0 unx      436 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/__init__.py
+-rw-r--r--  2.0 unx     1071 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/app.py
+-rw-r--r--  2.0 unx     1405 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/conftest.py
+-rw-r--r--  2.0 unx     1027 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/demo.py
+-rw-r--r--  2.0 unx     8178 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/export.py
+-rw-r--r--  2.0 unx     1168 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/info.yaml
+-rw-r--r--  2.0 unx     4686 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/model.py
+-rw-r--r--  2.0 unx     4574 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/perf.yaml
+-rw-r--r--  2.0 unx     1845 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/test.py
+-rw-r--r--  2.0 unx      436 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/__init__.py
+-rw-r--r--  2.0 unx     2033 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/app.py
+-rw-r--r--  2.0 unx     1405 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/conftest.py
+-rw-r--r--  2.0 unx      909 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/demo.py
+-rw-r--r--  2.0 unx     8198 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/export.py
+-rw-r--r--  2.0 unx     1131 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/info.yaml
+-rw-r--r--  2.0 unx    11972 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/model.py
+-rw-r--r--  2.0 unx     3408 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/perf.yaml
+-rw-r--r--  2.0 unx       83 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/requirements.txt
+-rw-r--r--  2.0 unx     2322 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/test.py
+-rw-r--r--  2.0 unx      447 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/__init__.py
+-rw-r--r--  2.0 unx     1415 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/conftest.py
+-rw-r--r--  2.0 unx      853 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/demo.py
+-rw-r--r--  2.0 unx     8614 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/export.py
+-rw-r--r--  2.0 unx     1318 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/info.yaml
+-rw-r--r--  2.0 unx     3251 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/model.py
+-rw-r--r--  2.0 unx     4045 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/perf.yaml
+-rw-r--r--  2.0 unx       83 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1558 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/test.py
+-rw-r--r--  2.0 unx      415 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/__init__.py
+-rw-r--r--  2.0 unx      892 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/app.py
+-rw-r--r--  2.0 unx     1409 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/conftest.py
+-rw-r--r--  2.0 unx      926 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/demo.py
+-rw-r--r--  2.0 unx     8252 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/export.py
+-rw-r--r--  2.0 unx     1171 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/info.yaml
+-rw-r--r--  2.0 unx     8031 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/model.py
+-rw-r--r--  2.0 unx     2768 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/perf.yaml
+-rw-r--r--  2.0 unx      100 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/requirements.txt
+-rw-r--r--  2.0 unx     2270 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/test.py
+-rw-r--r--  2.0 unx      459 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/__init__.py
+-rw-r--r--  2.0 unx     1419 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/conftest.py
+-rw-r--r--  2.0 unx      804 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/demo.py
+-rw-r--r--  2.0 unx     8635 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/export.py
+-rw-r--r--  2.0 unx     1354 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/info.yaml
+-rw-r--r--  2.0 unx     3540 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/model.py
+-rw-r--r--  2.0 unx     2777 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/perf.yaml
+-rw-r--r--  2.0 unx      100 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1542 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/test.py
+-rw-r--r--  2.0 unx      419 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/__init__.py
+-rw-r--r--  2.0 unx     7698 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/app.py
+-rw-r--r--  2.0 unx     1315 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/conftest.py
+-rw-r--r--  2.0 unx     3155 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/demo.py
+-rw-r--r--  2.0 unx     8255 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/export.py
+-rw-r--r--  2.0 unx     1287 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/info.yaml
+-rw-r--r--  2.0 unx     4663 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/model.py
+-rw-r--r--  2.0 unx     3409 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/perf.yaml
+-rw-r--r--  2.0 unx       64 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/requirements.txt
+-rw-r--r--  2.0 unx     2536 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/test.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/test/__init__.py
+-rw-r--r--  2.0 unx     1043 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_async_compile_jobs.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/test/e2e/__init__.py
+-rw-r--r--  2.0 unx     1661 b- defN 24-Apr-30 21:08 qai_hub_models/test/e2e/test_aimet_compile.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1493 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_utils/perf.yaml
+-rw-r--r--  2.0 unx     3229 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_utils/test_info_specs.py
+-rw-r--r--  2.0 unx     6525 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_utils/test_perf_summary.py
+-rw-r--r--  2.0 unx     3295 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_utils/test_qai_hub_helpers.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/utils/__init__.py
+-rw-r--r--  2.0 unx    17286 b- defN 24-Apr-30 21:08 qai_hub_models/utils/args.py
+-rw-r--r--  2.0 unx    34094 b- defN 24-Apr-30 21:08 qai_hub_models/utils/asset_loaders.py
+-rw-r--r--  2.0 unx     5946 b- defN 24-Apr-30 21:08 qai_hub_models/utils/base_model.py
+-rw-r--r--  2.0 unx     9261 b- defN 24-Apr-30 21:08 qai_hub_models/utils/bounding_box_processing.py
+-rw-r--r--  2.0 unx     1771 b- defN 24-Apr-30 21:08 qai_hub_models/utils/camera_capture.py
+-rw-r--r--  2.0 unx     5276 b- defN 24-Apr-30 21:08 qai_hub_models/utils/compare.py
+-rw-r--r--  2.0 unx    32743 b- defN 24-Apr-30 21:08 qai_hub_models/utils/config_loaders.py
+-rw-r--r--  2.0 unx     3066 b- defN 24-Apr-30 21:08 qai_hub_models/utils/display.py
+-rw-r--r--  2.0 unx     6403 b- defN 24-Apr-30 21:08 qai_hub_models/utils/draw.py
+-rw-r--r--  2.0 unx     1549 b- defN 24-Apr-30 21:08 qai_hub_models/utils/huggingface.py
+-rw-r--r--  2.0 unx    13246 b- defN 24-Apr-30 21:08 qai_hub_models/utils/image_processing.py
+-rw-r--r--  2.0 unx    12482 b- defN 24-Apr-30 21:08 qai_hub_models/utils/inference.py
+-rw-r--r--  2.0 unx     1308 b- defN 24-Apr-30 21:08 qai_hub_models/utils/input_spec.py
+-rw-r--r--  2.0 unx     4559 b- defN 24-Apr-30 21:08 qai_hub_models/utils/measurement.py
+-rw-r--r--  2.0 unx     1577 b- defN 24-Apr-30 21:08 qai_hub_models/utils/model_adapters.py
+-rw-r--r--  2.0 unx     1406 b- defN 24-Apr-30 21:08 qai_hub_models/utils/path_helpers.py
+-rw-r--r--  2.0 unx     5007 b- defN 24-Apr-30 21:08 qai_hub_models/utils/printing.py
+-rw-r--r--  2.0 unx     5365 b- defN 24-Apr-30 21:08 qai_hub_models/utils/qai_hub_helpers.py
+-rw-r--r--  2.0 unx     1463 b- defN 24-Apr-30 21:08 qai_hub_models/utils/qnn_helpers.py
+-rw-r--r--  2.0 unx     2170 b- defN 24-Apr-30 21:08 qai_hub_models/utils/quantization.py
+-rw-r--r--  2.0 unx    17774 b- defN 24-Apr-30 21:08 qai_hub_models/utils/quantization_aimet.py
+-rw-r--r--  2.0 unx      754 b- defN 24-Apr-30 21:08 qai_hub_models/utils/test_compare.py
+-rw-r--r--  2.0 unx     3173 b- defN 24-Apr-30 21:08 qai_hub_models/utils/testing.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/__init__.py
+-rw-r--r--  2.0 unx      876 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/config_loader.py
+-rw-r--r--  2.0 unx     1233 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/default_config.json
+-rw-r--r--  2.0 unx      946 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/default_config_legacy_v1.json
+-rw-r--r--  2.0 unx      955 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/default_config_legacy_v2.json
+-rw-r--r--  2.0 unx      919 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/default_config_per_channel_qnn.json
+-rw-r--r--  2.0 unx     1187 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/repo.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/utils/scorecard/__init__.py
+-rw-r--r--  2.0 unx     1289 b- defN 24-Apr-30 21:08 qai_hub_models/utils/scorecard/common.py
+-rw-r--r--  2.0 unx    12491 b- defN 24-Apr-30 21:08 qai_hub_models/utils/scorecard/job_summary.py
+-rw-r--r--  2.0 unx    13385 b- defN 24-Apr-30 21:08 qai_hub_models/utils/scorecard/model_card.py
+-rw-r--r--  2.0 unx    11415 b- defN 24-Apr-30 21:08 qai_hub_models/utils/scorecard/perf_summary.py
+-rw-r--r--  2.0 unx     1481 b- defN 24-Apr-30 21:10 qai_hub_models-0.5.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    43084 b- defN 24-Apr-30 21:10 qai_hub_models-0.5.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-30 21:10 qai_hub_models-0.5.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       15 b- defN 24-Apr-30 21:10 qai_hub_models-0.5.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    96763 b- defN 24-Apr-30 21:10 qai_hub_models-0.5.1.dist-info/RECORD
+942 files, 2799879 bytes uncompressed, 898180 bytes compressed:  67.9%
```

## zipnote {}

```diff
@@ -30,14 +30,17 @@
 
 Filename: qai_hub_models/datasets/common.py
 Comment: 
 
 Filename: qai_hub_models/datasets/imagenette.py
 Comment: 
 
+Filename: qai_hub_models/datasets/pascal_voc.py
+Comment: 
+
 Filename: qai_hub_models/evaluators/__init__.py
 Comment: 
 
 Filename: qai_hub_models/evaluators/base_evaluators.py
 Comment: 
 
 Filename: qai_hub_models/evaluators/classification_evaluator.py
@@ -93,14 +96,17 @@
 
 Filename: qai_hub_models/models/_shared/deeplab/demo.py
 Comment: 
 
 Filename: qai_hub_models/models/_shared/deeplab/evaluator.py
 Comment: 
 
+Filename: qai_hub_models/models/_shared/deeplab/model.py
+Comment: 
+
 Filename: qai_hub_models/models/_shared/detr/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/_shared/detr/app.py
 Comment: 
 
 Filename: qai_hub_models/models/_shared/detr/coco_label_map.py
@@ -348,14 +354,62 @@
 
 Filename: qai_hub_models/models/ddrnet23_slim/perf.yaml
 Comment: 
 
 Filename: qai_hub_models/models/ddrnet23_slim/test.py
 Comment: 
 
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet/conftest.py
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet/demo.py
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet/export.py
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet/info.yaml
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet/model.py
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet/perf.yaml
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet/test.py
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/conftest.py
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/demo.py
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/export.py
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/info.yaml
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/model.py
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/perf.yaml
+Comment: 
+
+Filename: qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/test.py
+Comment: 
+
 Filename: qai_hub_models/models/deeplabv3_resnet50/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/deeplabv3_resnet50/conftest.py
 Comment: 
 
 Filename: qai_hub_models/models/deeplabv3_resnet50/demo.py
@@ -2751,23 +2805,23 @@
 
 Filename: qai_hub_models/utils/scorecard/model_card.py
 Comment: 
 
 Filename: qai_hub_models/utils/scorecard/perf_summary.py
 Comment: 
 
-Filename: qai_hub_models-0.5.0.dist-info/LICENSE
+Filename: qai_hub_models-0.5.1.dist-info/LICENSE
 Comment: 
 
-Filename: qai_hub_models-0.5.0.dist-info/METADATA
+Filename: qai_hub_models-0.5.1.dist-info/METADATA
 Comment: 
 
-Filename: qai_hub_models-0.5.0.dist-info/WHEEL
+Filename: qai_hub_models-0.5.1.dist-info/WHEEL
 Comment: 
 
-Filename: qai_hub_models-0.5.0.dist-info/top_level.txt
+Filename: qai_hub_models-0.5.1.dist-info/top_level.txt
 Comment: 
 
-Filename: qai_hub_models-0.5.0.dist-info/RECORD
+Filename: qai_hub_models-0.5.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## qai_hub_models/_version.py

```diff
@@ -1,5 +1,5 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
-__version__ = "0.5.0"
+__version__ = "0.5.1"
```

## qai_hub_models/datasets/imagenette.py

```diff
@@ -46,15 +46,14 @@
     Class for using the Imagenette dataset published here:
         https://github.com/fastai/imagenette
 
     Contains ~4k images spanning 10 of the imagenet classes.
     """
 
     def __init__(self):
-        self._download_data()
         BaseDataset.__init__(self, str(IMAGENETTE_ASSET.path(extracted=True)))
         # Avoid circular import
         from qai_hub_models.models._shared.imagenet_classifier.app import (
             IMAGENET_TRANSFORM,
         )
 
         ImageNet.__init__(
```

## qai_hub_models/models/_shared/deeplab/app.py

```diff
@@ -9,36 +9,29 @@
 import numpy as np
 import PIL.Image
 import torch
 from PIL.Image import Image
 from torchvision import transforms
 
 from qai_hub_models.utils.draw import create_color_map
-from qai_hub_models.utils.image_processing import normalize_image_transform
 
 
 def preprocess_image(image: Image) -> torch.Tensor:
     """
     Preprocesses images to be run through torch DeepLabV3 segmenter
     as prescribed here:
     https://pytorch.org/hub/pytorch_vision_resnet/
 
     Parameters:
         image: Input image to be run through the classifier model.
 
     Returns:
         torch tensor to be directly passed to the model.
     """
-    transform = transforms.Compose(
-        [
-            transforms.ToTensor(),
-            normalize_image_transform(),
-        ]
-    )
-    out_tensor: torch.Tensor = transform(image)  # type: ignore
+    out_tensor: torch.Tensor = transforms.ToTensor()(image)  # type: ignore
     return out_tensor.unsqueeze(0)
 
 
 class DeepLabV3App:
     """
     This class consists of light-weight "app code" that is required to
     perform end to end inference with DeepLabV3.
```

## qai_hub_models/models/_shared/deeplab/demo.py

```diff
@@ -57,9 +57,9 @@
     # Resize / unpad annotated image
     image_annotated = pil_undo_resize_pad(
         image_annotated, orig_image.size, scale, padding
     )
 
     if not is_test:
         display_or_save_image(
-            image_annotated, args.output_dir, "annotated_image", "predicted image"
+            image_annotated, args.output_dir, "annotated_image.png", "predicted image"
         )
```

## qai_hub_models/models/_shared/yolo/app.py

```diff
@@ -146,14 +146,15 @@
             pred_boxes,
             pred_scores,
             pred_class_idx,
         )
 
         # Return raw output if requested
         if raw_output or isinstance(pixel_values_or_image, torch.Tensor):
+            print(pred_boxes, pred_scores, pred_class_idx)
             return (pred_boxes, pred_scores, pred_class_idx)
 
         # Add boxes to each batch
         for batch_idx in range(len(pred_boxes)):
             pred_boxes_batch = pred_boxes[batch_idx]
             for box in pred_boxes_batch:
                 draw_box_from_xyxy(
```

## qai_hub_models/models/_shared/yolo/utils.py

```diff
@@ -98,31 +98,39 @@
     # Get class ID of most likely score.
     scores, class_idx = get_most_likely_score(scores)
 
     return boxes, scores, class_idx
 
 
 def detect_postprocess_split_input(
-    xy: torch.Tensor, wh: torch.Tensor, scores: torch.Tensor
+    xy: torch.Tensor,
+    wh: torch.Tensor,
+    scores: torch.Tensor,
+    class_dtype: torch.dtype = torch.float32,
 ):
     """
     Same as `detect_postprocess` with inputs split into separate tensors.
     """
     boxes = box_transform_xywh2xyxy_split_input(xy, wh)
-    conf = scores[:, :, 0:1]
+    conf = scores[:, :, 0]
     scores = scores[:, :, 1:]
 
-    # Combine confidence and scores.
-    scores *= conf
-
     # Get class ID of most likely score.
     # (#10357) QNN has a bug where passing a result of Mul into ReduceMax returns all 0s
     scores, class_idx = torch.max(scores + 1e-10, -1, keepdim=False)
 
-    return boxes, scores, class_idx
+    # Combine confidence and scores.
+    # Original repo does this before the max operation, but that is more
+    # expensive by a factor of NUM_CLASSES and mathematically equivalent to this.
+    scores *= conf
+
+    # Quantized model runtime doesn't like int32 outputs, so cast class idx to int8.
+    # This is a no-op for coco models, but for datasets with >128 classes, this
+    # should be int32 for the unquantized model.
+    return boxes, scores, class_idx.to(class_dtype)
 
 
 def get_most_likely_score(scores: torch.Tensor):
     """
     Returns most likely score and class id
 
     Args:
```

## qai_hub_models/models/aotgan/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.aotgan import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.aotgan.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/aotgan/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "aotgan"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "aotgan",
             "AOT-GAN",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image,mask"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image,mask", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/aotgan/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: AOT-GAN
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:31.715297Z'
+    timestamp: '2024-04-23T18:42:31.302216Z'
   - torchscript_onnx_tflite:
       inference_time: 126778.0
       throughput: 7.887803877644386
       estimated_peak_memory_range:
         min: 2174976
         max: 256099504
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:31.716207Z'
-  - torchscript_onnx_ort:
-      inference_time: 6132971.0
-      throughput: 0.16305311080062176
-      estimated_peak_memory_range:
-        min: 252043264
-        max: 358570048
-      primary_compute_unit: CPU
-      precision: fp32
+    timestamp: '2024-04-23T18:42:31.302592Z'
+  - torchscript_onnx_tflite:
+      inference_time: 171670.0
+      throughput: 5.825129609133803
+      estimated_peak_memory_range:
+        min: 3219456
+        max: 6614600
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 235
         layers_on_gpu: 0
-        layers_on_cpu: 234
-        total_layers: 234
-      job_id: jz5w21zm5
+        layers_on_cpu: 0
+        total_layers: 235
+      job_id: jopr8dz05
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 162527.0
+      throughput: 6.15282383850068
+      estimated_peak_memory_range:
+        min: 4337664
+        max: 32953256
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 275
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 275
+      job_id: j1p80rlkg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:31.716494Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:31.716522Z'
+    timestamp: '2024-04-23T18:42:31.302905Z'
```

## qai_hub_models/models/baichuan_7b_quantized/info.yaml

```diff
@@ -21,15 +21,15 @@
 source_repo: https://github.com/baichuan-inc/Baichuan-7B/
 technical_details:
   Number of parameters: 7B
   Model size: 3.9GB
   Model-1 (Prompt Processor): Baichuan-PromptProcessor-Quantized
   Max context length: 1024
   Prompt processor input: 1024 tokens
-  Prompt processor output: 1 output token + KVCache for token generator
+  Prompt processor output: 1024 output tokens + KVCache for token generator
   Model-2 (Token Generator): Baichuan-TokenGenerator-KVCache-Quantized
   Token generator input: 1 input token + past KVCache
   Token generator output: 1 output token + KVCache for next iteration
   Decoding length: 1024 (1 output token + 1023 from KVCache)
   Use: Initiate conversation with prompt-processor and then token generator for subsequent iterations.
   QNN-SDK: "2.19"
 applicable_scenarios:
```

## qai_hub_models/models/baichuan_7b_quantized/perf.yaml

```diff
@@ -32,15 +32,15 @@
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
     timestamp: '2024-02-16T22:23:17.643089Z'
     torchscript_onnx_qnn:
       inference_time: 2599326
-      throughput: 0.38
+      throughput: 393.94
       estimated_peak_memory_range:
         min: 53248
         max: 40255040
       layer_info:
         layers_on_npu: 31772
         layers_on_gpu: 0
         layers_on_cpu: 0
```

## qai_hub_models/models/controlnet_quantized/export.py

```diff
@@ -34,14 +34,15 @@
     "UNet_Quantized",
     "ControlNet_Quantized",
 ]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     profile_options: str = "",
     **additional_model_kwargs,
@@ -59,14 +60,16 @@
 
     Each of the last three steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         components: List of sub-components of the model that will be exported.
             Each component is compiled and profiled separately.
             Defaults to ALL_COMPONENTS if not specified.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling.
@@ -79,14 +82,18 @@
     Returns:
         A Mapping from component_name to a 2-tuple of:
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "controlnet_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     component_arg = components
     components = components or DEFAULT_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
@@ -132,15 +139,15 @@
         for component_name in components:
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             print(f"Profiling model {component_name} on a hosted device.")
             submitted_profile_job = hub.submit_profile_job(
                 model=uploaded_models[component_name],
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             profile_jobs[component_name] = cast(
                 hub.client.ProfileJob, submitted_profile_job
             )
 
@@ -154,15 +161,15 @@
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
             submitted_inference_job = hub.submit_inference_job(
                 model=uploaded_models[component_name],
                 inputs=sample_inputs,
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
```

## qai_hub_models/models/convnext_tiny/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.convnext_tiny import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.convnext_tiny.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/convnext_tiny/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "convnext_tiny"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "convnext_tiny",
             "ConvNext-Tiny",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/ddrnet23_slim/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.ddrnet23_slim import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.ddrnet23_slim.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/ddrnet23_slim/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "ddrnet23_slim"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "ddrnet23_slim",
             "DDRNet23-Slim",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/ddrnet23_slim/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: DDRNet23-Slim
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:31.847278Z'
+    timestamp: '2024-04-23T18:42:31.356614Z'
   - torchscript_onnx_tflite:
       inference_time: 4569.0
       throughput: 218.8662727073758
       estimated_peak_memory_range:
         min: 16384
         max: 71802832
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:31.847348Z'
-  - torchscript_onnx_ort:
-      inference_time: 269630.0
-      throughput: 3.708786114304788
+    timestamp: '2024-04-23T18:42:31.356678Z'
+  - torchscript_onnx_tflite:
+      inference_time: 6682.0
+      throughput: 149.655791679138
       estimated_peak_memory_range:
-        min: 28385280
-        max: 79610080
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 1011712
+        max: 3063152
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 131
         layers_on_gpu: 0
-        layers_on_cpu: 105
-        total_layers: 105
-      job_id: jep20vdqg
+        layers_on_cpu: 0
+        total_layers: 131
+      job_id: jnp1y108p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:31.847391Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:31.847396Z'
+    timestamp: '2024-04-23T18:42:31.356703Z'
```

## qai_hub_models/models/deeplabv3_resnet50/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.deeplabv3_resnet50 import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.deeplabv3_resnet50.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/deeplabv3_resnet50/demo.py

```diff
@@ -1,16 +1,16 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from qai_hub_models.models._shared.deeplab.demo import deeplabv3_demo
+from qai_hub_models.models._shared.deeplab.model import NUM_CLASSES
 from qai_hub_models.models.deeplabv3_resnet50.model import (
     MODEL_ASSET_VERSION,
     MODEL_ID,
-    NUM_CLASSES,
     DeepLabV3_ResNet50,
 )
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 
 # Demo image comes from https://github.com/pytorch/hub/raw/master/images/deeplab1.png
 # and has had alpha channel removed for use as input
 INPUT_IMAGE_LOCAL_PATH = "deeplabv3_demo.png"
```

## qai_hub_models/models/deeplabv3_resnet50/export.py

```diff
@@ -22,26 +22,28 @@
     get_model_kwargs,
 )
 from qai_hub_models.utils.base_model import TargetRuntime
 from qai_hub_models.utils.compare import torch_inference
 from qai_hub_models.utils.input_spec import make_torch_inputs
 from qai_hub_models.utils.printing import (
     print_inference_metrics,
+    print_on_target_demo_cmd,
     print_profile_metrics_from_job,
 )
 from qai_hub_models.utils.qai_hub_helpers import (
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "deeplabv3_resnet50"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "deeplabv3_resnet50",
             "DeepLabV3-ResNet50",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0,output_1",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -156,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
@@ -184,14 +193,17 @@
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
         inference_result = transpose_channel_last_to_first(
             "output_0,output_1", inference_result, target_runtime
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
+    if not skip_summary:
+        print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
+
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
     parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
```

## qai_hub_models/models/deeplabv3_resnet50/info.yaml

```diff
@@ -12,23 +12,23 @@
 research_paper_title: Rethinking Atrous Convolution for Semantic Image Segmentation
 license: https://github.com/pytorch/vision/blob/main/LICENSE
 deploy_license: https://qaihub-public-assets.s3.us-west-2.amazonaws.com/qai-hub-models/Qualcomm+AI+Hub+Proprietary+License.pdf
 source_repo:
   https://github.com/pytorch/vision/blob/main/torchvision/models/segmentation/deeplabv3.py
 technical_details:
   Model checkpoint: COCO_WITH_VOC_LABELS_V1
-  Input resolution: 224x224
+  Input resolution: 513x513
   Number of parameters: 39.6M
   Model size: 151 MB
 applicable_scenarios:
   - Anomaly Detection
   - Inventory Management
 related_models:
   - sam
-  - unet_segmentation
+  - deeplabv3_plus_mobilenet
   - fcn_resnet50
 form_factors:
   - Phone
   - Tablet
   - IoT
 has_static_banner: yes
 has_animated_banner: yes
```

## qai_hub_models/models/deeplabv3_resnet50/model.py

```diff
@@ -1,72 +1,31 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
-import torch
 import torchvision.models as tv_models
 
-from qai_hub_models.evaluators.base_evaluators import BaseEvaluator
-from qai_hub_models.models._shared.deeplab.evaluator import DeepLabV3Evaluator
-from qai_hub_models.utils.base_model import BaseModel, TargetRuntime
-from qai_hub_models.utils.input_spec import InputSpec
+from qai_hub_models.models._shared.deeplab.model import DeepLabV3Model
+from qai_hub_models.utils.base_model import TargetRuntime
 
 MODEL_ID = __name__.split(".")[-2]
-MODEL_ASSET_VERSION = 1
+MODEL_ASSET_VERSION = 2
 DEFAULT_WEIGHTS = "COCO_WITH_VOC_LABELS_V1"
-NUM_CLASSES = 21
 
 
-class DeepLabV3_ResNet50(BaseModel):
+class DeepLabV3_ResNet50(DeepLabV3Model):
     """Exportable DeepLabV3_ResNet50 image segmentation applications, end-to-end."""
 
-    def __init__(
-        self,
-        deeplabv3_model: torch.nn.Module,
-    ) -> None:
-        super().__init__()
-        self.model = deeplabv3_model
-
     @classmethod
     def from_pretrained(cls, weights: str = DEFAULT_WEIGHTS) -> DeepLabV3_ResNet50:
         model = tv_models.segmentation.deeplabv3_resnet50(weights=weights).eval()
         return cls(model)
 
-    def get_evaluator(self) -> BaseEvaluator:
-        return DeepLabV3Evaluator(NUM_CLASSES)
-
-    def forward(self, image: torch.Tensor) -> torch.Tensor:
-        """
-        Run DeepLabV3_ResNet50 on `image`, and produce a tensor of classes for segmentation
-
-        Parameters:
-            image: Pixel values pre-processed for model consumption.
-                   Range: float[0, 1]
-                   3-channel Color Space: RGB
-
-        Returns:
-            tensor: Bx21xHxW tensor of class logits per pixel
-        """
-        return self.model(image)["out"]
-
-    @staticmethod
-    def get_input_spec(
-        batch_size: int = 1,
-        num_channels: int = 3,
-        height: int = 224,
-        width: int = 224,
-    ) -> InputSpec:
-        # Get the input specification ordered (name -> (shape, type)) pairs for this model.
-        #
-        # This can be used with the qai_hub python API to declare
-        # the model input specification upon submitting a profile job.
-        return {"image": ((batch_size, num_channels, height, width), "float32")}
-
     def get_hub_compile_options(
         self, target_runtime: TargetRuntime, other_compile_options: str = ""
     ) -> str:
         compile_options = super().get_hub_compile_options(
             target_runtime, other_compile_options
         )
         return compile_options + " --compute_unit gpu"
@@ -74,7 +33,10 @@
     def get_hub_profile_options(
         self, target_runtime: TargetRuntime, other_profile_options: str = ""
     ) -> str:
         profile_options = super().get_hub_profile_options(
             target_runtime, other_profile_options
         )
         return profile_options + " --compute_unit gpu"
+
+    def forward(self, image):
+        return super().forward(image)["out"]
```

## qai_hub_models/models/deeplabv3_resnet50/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,153 +21,129 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: DeepLabV3-ResNet50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 58164.0
-      throughput: 17.19276528436834
+      inference_time: 290847.0
+      throughput: 3.4382338480369405
       estimated_peak_memory_range:
-        min: 0
-        max: 172023904
+        min: 32768
+        max: 223952912
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 96
+        layers_on_gpu: 95
         layers_on_cpu: 0
-        total_layers: 96
-      job_id: j1pv09nr5
+        total_layers: 95
+      job_id: jz5wq3935
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 145136.0
-      throughput: 6.890089295557271
+      inference_time: 810711.0
+      throughput: 1.23348517535842
       estimated_peak_memory_range:
-        min: 770048
-        max: 8676784
+        min: 3481600
+        max: 11830488
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 82
+        layers_on_gpu: 83
         layers_on_cpu: 0
-        total_layers: 82
-      job_id: jlpeelnvp
+        total_layers: 83
+      job_id: jvgdoqvrp
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
-      estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 0
-      job_id: jz5w21rm5
-      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:31.913298Z'
+    timestamp: '2024-04-30T00:18:21.450422Z'
   - torchscript_onnx_tflite:
-      inference_time: 40277.0
-      throughput: 24.828065645405566
+      inference_time: 228363.0
+      throughput: 4.37899309432789
       estimated_peak_memory_range:
-        min: 4358144
-        max: 31124640
+        min: 102400
+        max: 31114256
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 96
+        layers_on_gpu: 95
         layers_on_cpu: 0
-        total_layers: 96
-      job_id: j7gjzw8e5
+        total_layers: 95
+      job_id: jmg9wy4wp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 105275.0
-      throughput: 9.49893137022085
+      inference_time: 588856.0
+      throughput: 1.6982080508647275
       estimated_peak_memory_range:
-        min: 704512
-        max: 27029136
+        min: 3207168
+        max: 37364864
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 82
+        layers_on_gpu: 83
         layers_on_cpu: 0
-        total_layers: 82
-      job_id: jygzo40x5
+        total_layers: 83
+      job_id: jz57xldvg
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
-      estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 0
-      job_id: jmg9jxq85
-      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:31.913393Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-30T00:18:21.450461Z'
+  - torchscript_onnx_tflite:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: jnp1yvm7p
+      job_id: jnp1ew88g
       job_status: Failed
+    torchscript_onnx_qnn:
+      inference_time: 821173.0
+      throughput: 1.217770189716418
+      estimated_peak_memory_range:
+        min: 3436544
+        max: 12462344
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 83
+        layers_on_cpu: 0
+        total_layers: 83
+      job_id: jqp4vdw8p
+      job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:31.913409Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:31.913414Z'
+    timestamp: '2024-04-30T00:18:21.450490Z'
```

## qai_hub_models/models/deeplabv3_resnet50/test.py

```diff
@@ -1,63 +1,52 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
-import numpy as np
 import pytest
 
 from qai_hub_models.models._shared.deeplab.app import DeepLabV3App
+from qai_hub_models.models._shared.deeplab.model import NUM_CLASSES
 from qai_hub_models.models.deeplabv3_resnet50.demo import INPUT_IMAGE_ADDRESS
 from qai_hub_models.models.deeplabv3_resnet50.demo import main as demo_main
 from qai_hub_models.models.deeplabv3_resnet50.model import (
     MODEL_ASSET_VERSION,
     MODEL_ID,
-    NUM_CLASSES,
     DeepLabV3_ResNet50,
 )
-from qai_hub_models.utils.asset_loaders import CachedWebModelAsset, load_image
-from qai_hub_models.utils.testing import assert_most_close, skip_clone_repo_check
+from qai_hub_models.utils.asset_loaders import (
+    CachedWebModelAsset,
+    load_image,
+    load_numpy,
+)
+from qai_hub_models.utils.testing import skip_clone_repo_check
 
-OUTPUT_IMAGE_LOCAL_PATH = "deeplabv3_demo_output.png"
-OUTPUT_IMAGE_ADDRESS = CachedWebModelAsset.from_asset_store(
-    MODEL_ID, MODEL_ASSET_VERSION, OUTPUT_IMAGE_LOCAL_PATH
+OUTPUT_IMAGE_MASK = CachedWebModelAsset.from_asset_store(
+    MODEL_ID, MODEL_ASSET_VERSION, "deeplab_output_mask.npy"
 )
 
 
 @skip_clone_repo_check
 def test_task():
     image = load_image(INPUT_IMAGE_ADDRESS)
-    output_image = load_image(OUTPUT_IMAGE_ADDRESS)
     app = DeepLabV3App(DeepLabV3_ResNet50.from_pretrained(), num_classes=NUM_CLASSES)
-    app_output_image = app.predict(image, False)
-
-    np.testing.assert_allclose(
-        np.asarray(app_output_image, dtype=np.float32) / 255,
-        np.asarray(output_image, dtype=np.float32) / 255,
-        rtol=0.02,
-        atol=0.2,
-    )
+    output_mask = app.predict(image, True)
+    output_mask_gt = load_numpy(OUTPUT_IMAGE_MASK)
+    assert (output_mask == output_mask_gt).mean() > 0.95
 
 
 @pytest.mark.trace
 @skip_clone_repo_check
 def test_trace():
     image = load_image(INPUT_IMAGE_ADDRESS)
-    output_image = load_image(OUTPUT_IMAGE_ADDRESS)
     app = DeepLabV3App(
         DeepLabV3_ResNet50.from_pretrained().convert_to_torchscript(),
         num_classes=NUM_CLASSES,
     )
-    app_output_image = app.predict(image, False)
-
-    assert_most_close(
-        np.asarray(app_output_image, dtype=np.float32) / 255,
-        np.asarray(output_image, dtype=np.float32) / 255,
-        diff_tol=0.005,
-        rtol=0.02,
-        atol=0.2,
-    )
+    output_mask = app.predict(image, True)
+    output_mask_gt = load_numpy(OUTPUT_IMAGE_MASK)
+    assert (output_mask == output_mask_gt).mean() > 0.95
 
 
 @skip_clone_repo_check
 def test_demo():
     demo_main(is_test=True)
```

## qai_hub_models/models/densenet121/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.densenet121 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.densenet121.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/densenet121/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "densenet121"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "densenet121",
             "DenseNet-121",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/densenet121/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: DenseNet-121
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:31.936971Z'
+    timestamp: '2024-04-23T18:42:31.432252Z'
   - torchscript_onnx_tflite:
       inference_time: 1282.0
       throughput: 780.0312012480499
       estimated_peak_memory_range:
         min: 12288
         max: 95228096
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:31.937210Z'
-  - torchscript_onnx_ort:
-      inference_time: 71373.0
-      throughput: 14.010900480573886
-      estimated_peak_memory_range:
-        min: 12173312
-        max: 150873232
-      primary_compute_unit: CPU
-      precision: fp32
+    timestamp: '2024-04-23T18:42:31.432349Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1944.0
+      throughput: 514.40329218107
+      estimated_peak_memory_range:
+        min: 20480
+        max: 2194800
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 312
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 312
+      job_id: j0pxnrjl5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 2008.0
+      throughput: 498.00796812749
+      estimated_peak_memory_range:
+        min: 12288
+        max: 40726728
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 372
         layers_on_gpu: 0
-        layers_on_cpu: 311
-        total_layers: 311
-      job_id: jopr8n775
+        layers_on_cpu: 0
+        total_layers: 372
+      job_id: jep20d6qg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:31.937310Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:31.937315Z'
+    timestamp: '2024-04-23T18:42:31.432464Z'
```

## qai_hub_models/models/detr_resnet101/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.detr_resnet101 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.detr_resnet101.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/detr_resnet101/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "detr_resnet101"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "detr_resnet101",
             "DETR-ResNet101",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/detr_resnet101/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: DETR-ResNet101
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:31.961967Z'
+    timestamp: '2024-04-23T18:42:31.456092Z'
   - torchscript_onnx_tflite:
       inference_time: 35573.0
       throughput: 28.111207938605123
       estimated_peak_memory_range:
         min: 28672
         max: 261178736
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:31.962230Z'
-  - torchscript_onnx_ort:
-      inference_time: 739045.0
-      throughput: 1.353097578631883
+    timestamp: '2024-04-23T18:42:31.456228Z'
+  - torchscript_onnx_tflite:
+      inference_time: 48057.0
+      throughput: 20.80862309340991
       estimated_peak_memory_range:
-        min: 73416704
-        max: 380529712
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 1380352
+        max: 12433288
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 715
-        total_layers: 715
-      job_id: jogk79knp
+        layers_on_npu: 910
+        layers_on_gpu: 2
+        layers_on_cpu: 0
+        total_layers: 912
+      job_id: j1gl68rmg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:31.962413Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:31.962419Z'
+    timestamp: '2024-04-23T18:42:31.456328Z'
```

## qai_hub_models/models/detr_resnet101_dc5/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.detr_resnet101_dc5 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.detr_resnet101_dc5.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/detr_resnet101_dc5/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "detr_resnet101_dc5"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "detr_resnet101_dc5",
             "DETR-ResNet101-DC5",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/detr_resnet101_dc5/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: DETR-ResNet101-DC5
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:31.985333Z'
+    timestamp: '2024-04-23T18:42:31.473830Z'
   - torchscript_onnx_tflite:
       inference_time: 311354.0
       throughput: 3.2117782331365583
       estimated_peak_memory_range:
         min: 90112
         max: 447334464
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:31.985607Z'
-  - torchscript_onnx_ort:
-      inference_time: 1115990.0
-      throughput: 0.8960653769299008
+    timestamp: '2024-04-23T18:42:31.473979Z'
+  - torchscript_onnx_tflite:
+      inference_time: 405436.0
+      throughput: 2.4664805296026993
       estimated_peak_memory_range:
-        min: 74452992
-        max: 382260256
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 6467584
+        max: 13861952
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 715
-        total_layers: 715
-      job_id: jwgok47kp
+        layers_on_npu: 911
+        layers_on_gpu: 2
+        layers_on_cpu: 0
+        total_layers: 913
+      job_id: jlpeex3vp
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:31.985802Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:31.985807Z'
+    timestamp: '2024-04-23T18:42:31.474090Z'
```

## qai_hub_models/models/detr_resnet50/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.detr_resnet50 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.detr_resnet50.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/detr_resnet50/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "detr_resnet50"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "detr_resnet50",
             "DETR-ResNet50",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/detr_resnet50/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: DETR-ResNet50
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.016941Z'
+    timestamp: '2024-04-23T18:42:31.491676Z'
   - torchscript_onnx_tflite:
       inference_time: 28469.0
       throughput: 35.12592644631002
       estimated_peak_memory_range:
         min: 1241088
         max: 215942624
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.017229Z'
-  - torchscript_onnx_ort:
-      inference_time: 436940.0
-      throughput: 2.2886437497139194
+    timestamp: '2024-04-23T18:42:31.491805Z'
+  - torchscript_onnx_tflite:
+      inference_time: 38866.0
+      throughput: 25.729429321257655
       estimated_peak_memory_range:
-        min: 184320
-        max: 280725376
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 1429504
+        max: 8463712
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 664
-        total_layers: 664
-      job_id: jz5w210m5
+        layers_on_npu: 842
+        layers_on_gpu: 2
+        layers_on_cpu: 0
+        total_layers: 844
+      job_id: jz570n39g
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.017429Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.017434Z'
+    timestamp: '2024-04-23T18:42:31.491909Z'
```

## qai_hub_models/models/detr_resnet50_dc5/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.detr_resnet50_dc5 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.detr_resnet50_dc5.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/detr_resnet50_dc5/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "detr_resnet50_dc5"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "detr_resnet50_dc5",
             "DETR-ResNet50-DC5",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/detr_resnet50_dc5/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: DETR-ResNet50-DC5
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.040136Z'
+    timestamp: '2024-04-23T18:42:31.509285Z'
   - torchscript_onnx_tflite:
       inference_time: 306266.0
       throughput: 3.26513553577609
       estimated_peak_memory_range:
         min: 16384
         max: 412400848
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.040372Z'
-  - torchscript_onnx_ort:
-      inference_time: 822481.0
-      throughput: 1.2158335572493468
+    timestamp: '2024-04-23T18:42:31.509384Z'
+  - torchscript_onnx_tflite:
+      inference_time: 400391.0
+      throughput: 2.497558636432887
       estimated_peak_memory_range:
-        min: 83013632
-        max: 364885312
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 7581696
+        max: 16235952
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 664
-        total_layers: 664
-      job_id: jqp4k961g
+        layers_on_npu: 843
+        layers_on_gpu: 2
+        layers_on_cpu: 0
+        total_layers: 845
+      job_id: jegnlq8q5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.040531Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.040536Z'
+    timestamp: '2024-04-23T18:42:31.509493Z'
```

## qai_hub_models/models/efficientnet_b0/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.efficientnet_b0 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.efficientnet_b0.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/efficientnet_b0/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "efficientnet_b0"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "efficientnet_b0",
             "EfficientNet-B0",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/efficientnet_b0/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: EfficientNet-B0
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.062096Z'
+    timestamp: '2024-04-23T18:42:31.527091Z'
   - torchscript_onnx_tflite:
       inference_time: 1177.0
       throughput: 849.6176720475786
       estimated_peak_memory_range:
         min: 16384
         max: 70869408
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.062267Z'
-  - torchscript_onnx_ort:
-      inference_time: 34902.0
-      throughput: 28.651653200389664
-      estimated_peak_memory_range:
-        min: 17244160
-        max: 92564240
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 167
-        total_layers: 167
-      job_id: j2p03v8np
+    timestamp: '2024-04-23T18:42:31.527166Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1635.0
+      throughput: 611.6207951070336
+      estimated_peak_memory_range:
+        min: 28672
+        max: 2553520
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 245
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 245
+      job_id: j1gl6qmmg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1694.0
+      throughput: 590.318772136954
+      estimated_peak_memory_range:
+        min: 622592
+        max: 68146216
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 243
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 243
+      job_id: j1pv0nkr5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.062317Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.062323Z'
+    timestamp: '2024-04-23T18:42:31.527250Z'
```

## qai_hub_models/models/esrgan/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.esrgan import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.esrgan.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/esrgan/export.py

```diff
@@ -34,14 +34,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "esrgan"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "esrgan",
             "ESRGAN",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,43 +112,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -156,15 +164,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/esrgan/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: ESRGAN
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.085976Z'
+    timestamp: '2024-04-23T18:42:31.551433Z'
   - torchscript_onnx_tflite:
       inference_time: 51233.0
       throughput: 19.518669607479556
       estimated_peak_memory_range:
         min: 94208
         max: 579142256
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.086528Z'
-  - torchscript_onnx_ort:
-      inference_time: 7783834.0
-      throughput: 0.12847139340330227
-      estimated_peak_memory_range:
-        min: 140189696
-        max: 373477056
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 675
-        total_layers: 675
-      job_id: jwgok4xkp
+    timestamp: '2024-04-23T18:42:31.551673Z'
+  - torchscript_onnx_tflite:
+      inference_time: 71702.0
+      throughput: 13.946612367855847
+      estimated_peak_memory_range:
+        min: 3293184
+        max: 6629192
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1024
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1024
+      job_id: jmg9jqn85
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 68263.0
+      throughput: 14.649224323572067
+      estimated_peak_memory_range:
+        min: 118784
+        max: 62391352
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1026
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1026
+      job_id: jqp4k2r1g
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.086691Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.086696Z'
+    timestamp: '2024-04-23T18:42:31.551903Z'
```

## qai_hub_models/models/facebook_denoiser/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.facebook_denoiser import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.facebook_denoiser.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/facebook_denoiser/export.py

```diff
@@ -33,14 +33,15 @@
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "facebook_denoiser"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "facebook_denoiser",
             "Facebook-Denoiser",
             device,
             skip_profiling,
             skip_inferencing,
@@ -104,40 +111,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -148,15 +156,15 @@
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=sample_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/facebook_denoiser/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Facebook-Denoiser
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.120738Z'
+    timestamp: '2024-04-23T18:42:31.576051Z'
   - torchscript_onnx_tflite:
       inference_time: 677141.0
       throughput: 1.476797299233099
       estimated_peak_memory_range:
         min: 363802624
         max: 387318224
       primary_compute_unit: CPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.120831Z'
-  - torchscript_onnx_ort:
-      inference_time: 900208.0
-      throughput: 1.1108543803209925
+    timestamp: '2024-04-23T18:42:31.576099Z'
+  - torchscript_onnx_tflite:
+      inference_time: 704020.0
+      throughput: 1.4204141927786142
       estimated_peak_memory_range:
-        min: 297889792
-        max: 352368240
+        min: 321875968
+        max: 538203832
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
-        layers_on_cpu: 107
-        total_layers: 107
-      job_id: jz5w21km5
+        layers_on_cpu: 209
+        total_layers: 209
+      job_id: j1p80kyog
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.120881Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.120888Z'
+    timestamp: '2024-04-23T18:42:31.576158Z'
```

## qai_hub_models/models/fastsam_s/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.fastsam_s import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.fastsam_s.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/fastsam_s/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "fastsam_s"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "fastsam_s",
             "FastSam-S",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,14 +113,15 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(
         model.to("cpu"), make_torch_inputs(input_spec), check_trace=False
     )
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
@@ -121,30 +129,30 @@
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_1,output_2,output_3,output_5",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -159,15 +167,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/fastsam_s/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: FastSam-S
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.143732Z'
+    timestamp: '2024-04-23T18:42:31.594002Z'
   - torchscript_onnx_tflite:
       inference_time: 6438.0
       throughput: 155.32774153463808
       estimated_peak_memory_range:
         min: 6541312
         max: 77737344
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.143846Z'
-  - torchscript_onnx_ort:
-      inference_time: 413910.0
-      throughput: 2.4159841511439684
+    timestamp: '2024-04-23T18:42:31.594052Z'
+  - torchscript_onnx_tflite:
+      inference_time: 8739.0
+      throughput: 114.42956860052638
       estimated_peak_memory_range:
-        min: 66248704
-        max: 155909472
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 7802880
+        max: 25345168
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 288
         layers_on_gpu: 0
-        layers_on_cpu: 201
-        total_layers: 201
-      job_id: jqp4k971g
+        layers_on_cpu: 0
+        total_layers: 288
+      job_id: jw56e0yyg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.143910Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.143916Z'
+    timestamp: '2024-04-23T18:42:31.594089Z'
```

## qai_hub_models/models/fastsam_x/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.fastsam_x import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.fastsam_x.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/fastsam_x/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "fastsam_x"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "fastsam_x",
             "FastSam-X",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,14 +113,15 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(
         model.to("cpu"), make_torch_inputs(input_spec), check_trace=False
     )
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
@@ -121,30 +129,30 @@
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_1,output_2,output_3,output_5",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -159,15 +167,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/fastsam_x/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: FastSam-X
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.166599Z'
+    timestamp: '2024-04-23T18:42:31.611861Z'
   - torchscript_onnx_tflite:
       inference_time: 36802.0
       throughput: 27.172436280636923
       estimated_peak_memory_range:
         min: 8462336
         max: 149995872
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.166760Z'
-  - torchscript_onnx_ort:
-      inference_time: 3463164.0
-      throughput: 0.2887532903437435
+    timestamp: '2024-04-23T18:42:31.611951Z'
+  - torchscript_onnx_tflite:
+      inference_time: 52081.0
+      throughput: 19.200860198536894
       estimated_peak_memory_range:
-        min: 151404544
-        max: 281837296
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 9240576
+        max: 13789008
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 420
         layers_on_gpu: 0
-        layers_on_cpu: 293
-        total_layers: 293
-      job_id: jep20v1qg
+        layers_on_cpu: 0
+        total_layers: 420
+      job_id: jmg9jql85
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.166866Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.166871Z'
+    timestamp: '2024-04-23T18:42:31.612007Z'
```

## qai_hub_models/models/fcn_resnet50/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.fcn_resnet50 import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.fcn_resnet50.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/fcn_resnet50/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "fcn_resnet50"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "fcn_resnet50",
             "FCN_ResNet50",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/fcn_resnet50/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: FCN_ResNet50
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.189836Z'
+    timestamp: '2024-04-23T18:42:31.629765Z'
   - torchscript_onnx_tflite:
       inference_time: 6385.0
       throughput: 156.61707126076743
       estimated_peak_memory_range:
         min: 4259840
         max: 81999104
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.189943Z'
-  - torchscript_onnx_ort:
-      inference_time: 582919.0
-      throughput: 1.7155042124205937
-      estimated_peak_memory_range:
-        min: 21987328
-        max: 53068064
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 57
-        total_layers: 57
-      job_id: jw56edkyg
+    timestamp: '2024-04-23T18:42:31.629842Z'
+  - torchscript_onnx_tflite:
+      inference_time: 8533.0
+      throughput: 117.19207781553968
+      estimated_peak_memory_range:
+        min: 4243456
+        max: 6395552
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 84
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 84
+      job_id: jvgdemx65
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 7887.0
+      throughput: 126.79092177000126
+      estimated_peak_memory_range:
+        min: 16384
+        max: 14326120
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 125
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 125
+      job_id: jo5mqln7p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.189971Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.189976Z'
+    timestamp: '2024-04-23T18:42:31.629884Z'
```

## qai_hub_models/models/ffnet_122ns_lowres/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.ffnet_122ns_lowres import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.ffnet_122ns_lowres.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/ffnet_122ns_lowres/export.py

```diff
@@ -34,14 +34,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "ffnet_122ns_lowres"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "ffnet_122ns_lowres",
             "FFNet-122NS-LowRes",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,43 +112,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -156,15 +164,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/ffnet_122ns_lowres/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: FFNet-122NS-LowRes
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.214242Z'
+    timestamp: '2024-04-23T18:42:31.654178Z'
   - torchscript_onnx_tflite:
       inference_time: 6839.0
       throughput: 146.22020763269484
       estimated_peak_memory_range:
         min: 569344
         max: 59671696
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.214416Z'
-  - torchscript_onnx_ort:
-      inference_time: 284999.0
-      throughput: 3.508784241348215
-      estimated_peak_memory_range:
-        min: 27045888
-        max: 100533632
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 151
-        total_layers: 151
-      job_id: jz5w21jm5
+    timestamp: '2024-04-23T18:42:31.654276Z'
+  - torchscript_onnx_tflite:
+      inference_time: 9658.0
+      throughput: 103.54110581901014
+      estimated_peak_memory_range:
+        min: 0
+        max: 4034800
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 216
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 216
+      job_id: jqpyry105
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 10822.0
+      throughput: 92.40436148586214
+      estimated_peak_memory_range:
+        min: 6328320
+        max: 38539008
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 348
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 348
+      job_id: jn5qed0e5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.214474Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.214480Z'
+    timestamp: '2024-04-23T18:42:31.654354Z'
```

## qai_hub_models/models/ffnet_40s/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.ffnet_40s import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.ffnet_40s.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/ffnet_40s/export.py

```diff
@@ -34,14 +34,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "ffnet_40s"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "ffnet_40s",
             "FFNet-40S",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,43 +112,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -156,15 +164,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/ffnet_40s/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: FFNet-40S
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.239491Z'
+    timestamp: '2024-04-23T18:42:31.678643Z'
   - torchscript_onnx_tflite:
       inference_time: 16867.0
       throughput: 59.28736586233474
       estimated_peak_memory_range:
         min: 32768
         max: 105460576
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.239587Z'
-  - torchscript_onnx_ort:
-      inference_time: 1282872.0
-      throughput: 0.7795009946432692
-      estimated_peak_memory_range:
-        min: 14479360
-        max: 54572560
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 67
-        total_layers: 67
-      job_id: jvgdezj65
+    timestamp: '2024-04-23T18:42:31.678725Z'
+  - torchscript_onnx_tflite:
+      inference_time: 22456.0
+      throughput: 44.53152832205201
+      estimated_peak_memory_range:
+        min: 32768
+        max: 1647568
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 92
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 92
+      job_id: jygzo0245
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 17241.0
+      throughput: 58.00127602807262
+      estimated_peak_memory_range:
+        min: 25214976
+        max: 52246888
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 140
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 140
+      job_id: jvgdemn65
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.239615Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.239621Z'
+    timestamp: '2024-04-23T18:42:31.678770Z'
```

## qai_hub_models/models/ffnet_40s_quantized/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.ffnet_40s_quantized import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.ffnet_40s_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/ffnet_40s_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_last_to_first,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "ffnet_40s_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "ffnet_40s_quantized",
             "FFNet-40S-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -123,15 +130,15 @@
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -139,15 +146,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -165,15 +172,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/ffnet_40s_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: FFNet-40S-Quantized
@@ -69,15 +72,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.269874Z'
+    timestamp: '2024-04-23T18:42:32.012713Z'
   - torchscript_onnx_tflite:
       inference_time: 4623.0
       throughput: 216.3097555699762
       estimated_peak_memory_range:
         min: 20480
         max: 67550048
       primary_compute_unit: NPU
@@ -107,16 +110,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.269932Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:32.012866Z'
+  - torchscript_onnx_tflite:
+      inference_time: 46106.0
+      throughput: 21.68915108662647
+      estimated_peak_memory_range:
+        min: 12288
+        max: 52922016
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 99
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 99
+      job_id: jegnlw0j5
+      job_status: Passed
+    torchscript_onnx_ort:
       inference_time: 362244.0
       throughput: 2.7605702233853426
       estimated_peak_memory_range:
         min: 159432704
         max: 207613904
       primary_compute_unit: CPU
       precision: fp32
@@ -128,18 +146,56 @@
       job_id: jegnl7jj5
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.269964Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.013041Z'
+  - torchscript_onnx_tflite:
+      inference_time: 206934.0
+      throughput: 4.832458658316178
+      estimated_peak_memory_range:
+        min: 2678784
+        max: 4932640
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 99
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 99
+      job_id: jep29omxg
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:32.013136Z'
+  - torchscript_onnx_tflite:
+      inference_time: 8927.0
+      throughput: 112.0197154699227
+      estimated_peak_memory_range:
+        min: 2711552
+        max: 19152008
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 99
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 99
+      job_id: jopr876k5
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.269969Z'
+    timestamp: '2024-04-23T18:42:32.013229Z'
```

## qai_hub_models/models/ffnet_54s/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.ffnet_54s import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.ffnet_54s.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/ffnet_54s/export.py

```diff
@@ -34,14 +34,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "ffnet_54s"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "ffnet_54s",
             "FFNet-54S",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,43 +112,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -156,15 +164,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/ffnet_54s/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: FFNet-54S
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.293297Z'
+    timestamp: '2024-04-23T18:42:32.053359Z'
   - torchscript_onnx_tflite:
       inference_time: 18446.0
       throughput: 54.21229534858506
       estimated_peak_memory_range:
         min: 1429504
         max: 120768592
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.293402Z'
-  - torchscript_onnx_ort:
-      inference_time: 1491103.0
-      throughput: 0.6706444826413736
-      estimated_peak_memory_range:
-        min: 131444736
-        max: 181106624
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 81
-        total_layers: 81
-      job_id: jn5qem8e5
+    timestamp: '2024-04-23T18:42:32.053423Z'
+  - torchscript_onnx_tflite:
+      inference_time: 25045.0
+      throughput: 39.92812936713915
+      estimated_peak_memory_range:
+        min: 2555904
+        max: 5156288
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 113
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 113
+      job_id: jn5qedee5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 19986.0
+      throughput: 50.035024517162014
+      estimated_peak_memory_range:
+        min: 25214976
+        max: 55043864
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 175
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 175
+      job_id: jwgok9k1p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.293435Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.293440Z'
+    timestamp: '2024-04-23T18:42:32.053476Z'
```

## qai_hub_models/models/ffnet_54s_quantized/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.ffnet_54s_quantized import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.ffnet_54s_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/ffnet_54s_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_last_to_first,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "ffnet_54s_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "ffnet_54s_quantized",
             "FFNet-54S-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -123,15 +130,15 @@
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -139,15 +146,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -165,15 +172,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/ffnet_54s_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: FFNet-54S-Quantized
@@ -69,15 +72,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.317104Z'
+    timestamp: '2024-04-23T18:42:32.077877Z'
   - torchscript_onnx_tflite:
       inference_time: 5099.0
       throughput: 196.11688566385567
       estimated_peak_memory_range:
         min: 16384
         max: 75082320
       primary_compute_unit: NPU
@@ -107,16 +110,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.317196Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:32.077918Z'
+  - torchscript_onnx_tflite:
+      inference_time: 49684.0
+      throughput: 20.127203928830205
+      estimated_peak_memory_range:
+        min: 126976
+        max: 56138256
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 120
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 120
+      job_id: jygzo0o45
+      job_status: Passed
+    torchscript_onnx_ort:
       inference_time: 420355.0
       throughput: 2.3789416088782103
       estimated_peak_memory_range:
         min: 187011072
         max: 248380464
       primary_compute_unit: CPU
       precision: fp32
@@ -128,18 +146,56 @@
       job_id: j1pv093z5
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.317235Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.077964Z'
+  - torchscript_onnx_tflite:
+      inference_time: 216291.0
+      throughput: 4.623400881220208
+      estimated_peak_memory_range:
+        min: 2650112
+        max: 4899184
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 120
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 120
+      job_id: jqpyj8drp
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:32.077990Z'
+  - torchscript_onnx_tflite:
+      inference_time: 10210.0
+      throughput: 97.94319294809011
+      estimated_peak_memory_range:
+        min: 2527232
+        max: 4340680
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 120
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 120
+      job_id: jvgdeme65
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.317241Z'
+    timestamp: '2024-04-23T18:42:32.078022Z'
```

## qai_hub_models/models/ffnet_78s/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.ffnet_78s import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.ffnet_78s.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/ffnet_78s/export.py

```diff
@@ -34,14 +34,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "ffnet_78s"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "ffnet_78s",
             "FFNet-78S",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,43 +112,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -156,15 +164,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/ffnet_78s/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: FFNet-78S
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.338018Z'
+    timestamp: '2024-04-23T18:42:32.105190Z'
   - torchscript_onnx_tflite:
       inference_time: 21728.0
       throughput: 46.02356406480118
       estimated_peak_memory_range:
         min: 0
         max: 133794256
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.338159Z'
-  - torchscript_onnx_ort:
-      inference_time: 1954272.0
-      throughput: 0.5116994973064138
-      estimated_peak_memory_range:
-        min: 148189184
-        max: 208699488
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 105
-        total_layers: 105
-      job_id: jvgdezw65
+    timestamp: '2024-04-23T18:42:32.105263Z'
+  - torchscript_onnx_tflite:
+      inference_time: 29631.0
+      throughput: 33.748439134690024
+      estimated_peak_memory_range:
+        min: 499712
+        max: 1916448
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 149
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 149
+      job_id: jegnlwlj5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 23601.0
+      throughput: 42.371085970933436
+      estimated_peak_memory_range:
+        min: 25165824
+        max: 55560608
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 235
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 235
+      job_id: j2p03x20p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.338194Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.338199Z'
+    timestamp: '2024-04-23T18:42:32.105324Z'
```

## qai_hub_models/models/ffnet_78s_lowres/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.ffnet_78s_lowres import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.ffnet_78s_lowres.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/ffnet_78s_lowres/export.py

```diff
@@ -34,14 +34,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "ffnet_78s_lowres"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "ffnet_78s_lowres",
             "FFNet-78S-LowRes",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,43 +112,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -156,15 +164,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/ffnet_78s_lowres/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: FFNet-78S-LowRes
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.360398Z'
+    timestamp: '2024-04-23T18:42:32.129479Z'
   - torchscript_onnx_tflite:
       inference_time: 7620.0
       throughput: 131.23359580052494
       estimated_peak_memory_range:
         min: 299008
         max: 53659920
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.360526Z'
-  - torchscript_onnx_ort:
-      inference_time: 393606.0
-      throughput: 2.540611677667515
-      estimated_peak_memory_range:
-        min: 18964480
-        max: 72131520
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 106
-        total_layers: 106
-      job_id: jep20v86g
+    timestamp: '2024-04-23T18:42:32.129559Z'
+  - torchscript_onnx_tflite:
+      inference_time: 10747.0
+      throughput: 93.04922303898762
+      estimated_peak_memory_range:
+        min: 655360
+        max: 2972672
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 149
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 149
+      job_id: jw56e0zng
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 11414.0
+      throughput: 87.61170492377782
+      estimated_peak_memory_range:
+        min: 6336512
+        max: 38367920
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 236
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 236
+      job_id: j7gjz8215
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.360568Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.360574Z'
+    timestamp: '2024-04-23T18:42:32.129627Z'
```

## qai_hub_models/models/ffnet_78s_quantized/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.ffnet_78s_quantized import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.ffnet_78s_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/ffnet_78s_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_last_to_first,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "ffnet_78s_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "ffnet_78s_quantized",
             "FFNet-78S-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -123,15 +130,15 @@
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -139,15 +146,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -165,15 +172,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/ffnet_78s_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: FFNet-78S-Quantized
@@ -69,15 +72,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.383420Z'
+    timestamp: '2024-04-23T18:42:32.154340Z'
   - torchscript_onnx_tflite:
       inference_time: 5988.0
       throughput: 167.000668002672
       estimated_peak_memory_range:
         min: 20480
         max: 87117952
       primary_compute_unit: NPU
@@ -107,16 +110,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.383520Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:32.154385Z'
+  - torchscript_onnx_tflite:
+      inference_time: 57755.0
+      throughput: 17.31451822353043
+      estimated_peak_memory_range:
+        min: 319488
+        max: 58248928
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 156
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 156
+      job_id: jz5708lng
+      job_status: Passed
+    torchscript_onnx_ort:
       inference_time: 547799.0
       throughput: 1.825487085591613
       estimated_peak_memory_range:
         min: 166916096
         max: 242608960
       primary_compute_unit: CPU
       precision: fp32
@@ -128,18 +146,56 @@
       job_id: jn5qemke5
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.383573Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.154436Z'
+  - torchscript_onnx_tflite:
+      inference_time: 235689.0
+      throughput: 4.242879387667647
+      estimated_peak_memory_range:
+        min: 2572288
+        max: 5196608
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 156
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 156
+      job_id: j2p02or25
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:32.154466Z'
+  - torchscript_onnx_tflite:
+      inference_time: 10675.0
+      throughput: 93.6768149882904
+      estimated_peak_memory_range:
+        min: 2576384
+        max: 4529144
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 156
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 156
+      job_id: jvgdemq65
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.383578Z'
+    timestamp: '2024-04-23T18:42:32.154494Z'
```

## qai_hub_models/models/googlenet/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.googlenet import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.googlenet.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/googlenet/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "googlenet"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "googlenet",
             "GoogLeNet",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/googlenet/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: GoogLeNet
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.443133Z'
+    timestamp: '2024-04-23T18:42:32.213322Z'
   - torchscript_onnx_tflite:
       inference_time: 650.0
       throughput: 1538.4615384615386
       estimated_peak_memory_range:
         min: 16384
         max: 45786064
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.443243Z'
-  - torchscript_onnx_ort:
-      inference_time: 41981.0
-      throughput: 23.820299659369716
+    timestamp: '2024-04-23T18:42:32.213386Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1043.0
+      throughput: 958.7727708533077
       estimated_peak_memory_range:
-        min: 0
-        max: 44006304
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 12288
+        max: 1850480
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 84
         layers_on_gpu: 0
-        layers_on_cpu: 84
+        layers_on_cpu: 0
         total_layers: 84
-      job_id: jegnl7yj5
+      job_id: jlpeenk8p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1090.0
+      throughput: 917.4311926605504
+      estimated_peak_memory_range:
+        min: 622592
+        max: 4955600
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 143
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 143
+      job_id: jnp1ymenp
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.443281Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.443286Z'
+    timestamp: '2024-04-23T18:42:32.213429Z'
```

## qai_hub_models/models/googlenet_quantized/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.googlenet_quantized import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.googlenet_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/googlenet_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "googlenet_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "googlenet_quantized",
             "GoogLeNetQuantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/googlenet_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: GoogLeNetQuantized
@@ -84,15 +87,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.468801Z'
+    timestamp: '2024-04-23T18:42:32.237563Z'
   - torchscript_onnx_tflite:
       inference_time: 229.0
       throughput: 4366.812227074236
       estimated_peak_memory_range:
         min: 12288
         max: 32807600
       primary_compute_unit: NPU
@@ -137,16 +140,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.468895Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:32.237614Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1013.0
+      throughput: 987.1668311944719
+      estimated_peak_memory_range:
+        min: 20480
+        max: 16869552
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 86
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 86
+      job_id: jz5708xqg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jegnlwev5
+      job_status: Failed
+    torchscript_onnx_ort:
       inference_time: 10247.0
       throughput: 97.58953840148337
       estimated_peak_memory_range:
         min: 2646016
         max: 50596416
       primary_compute_unit: CPU
       precision: fp32
@@ -158,18 +191,71 @@
       job_id: j1p3vw2mg
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.468936Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.237671Z'
+  - torchscript_onnx_tflite:
+      inference_time: 5919.0
+      throughput: 168.94745734076702
+      estimated_peak_memory_range:
+        min: 20480
+        max: 6396208
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 86
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 86
+      job_id: j1p8mj7z5
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:32.237693Z'
+  - torchscript_onnx_tflite:
+      inference_time: 322.0
+      throughput: 3105.590062111801
+      estimated_peak_memory_range:
+        min: 12288
+        max: 2046792
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 86
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 86
+      job_id: j0pxnzyj5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 365.0
+      throughput: 2739.72602739726
+      estimated_peak_memory_range:
+        min: 634880
+        max: 5391328
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 88
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 88
+      job_id: jep20zmxg
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.468942Z'
+    timestamp: '2024-04-23T18:42:32.237731Z'
```

## qai_hub_models/models/hrnet_pose/app.py

```diff
@@ -1,14 +1,14 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
-from typing import Callable, List, Tuple
+from typing import Callable, Dict, List, Tuple
 
 import numpy as np
 import torch
 from mmpose.apis import MMPoseInferencer
 from mmpose.codecs.utils import refine_keypoints
 from PIL.Image import Image, fromarray
 
@@ -90,14 +90,43 @@
         )
         self.pre_processor = self.inferencer.inferencer.model.data_preprocessor
 
     def predict(self, *args, **kwargs):
         # See predict_pose_keypoints.
         return self.predict_pose_keypoints(*args, **kwargs)
 
+    def preprocess_input(
+        self, pixel_values_or_image: torch.Tensor | np.ndarray | Image | List[Image]
+    ) -> Tuple[List[np.ndarray], Dict[str, torch.Tensor], torch.Tensor]:
+        # Convert from PIL / torch/ etc. to NHWC, RGB numpy frames, which is the required input type.
+        NHWC_int_numpy_frames, _ = app_to_net_image_inputs(pixel_values_or_image)
+
+        # MMPose does a lot of heavy lifting here. The preprocessor does the following:
+        # * runs a detector model to find people in each frame
+        # * for each bounding box...
+        # *     crop to the bounding box. resize bounding box to fit model input size using scaling factor
+        # *     Save bounding box coordinates and box scaling factor for use later
+        inputs = self.inferencer.preprocess(NHWC_int_numpy_frames, batch_size=1)
+
+        # We only get the first (highest probability) box and ignore the others.
+        # Other implementations may choose to run pose estimation on all boxes
+        # if they want to support multiple people in the same frame.
+        proc_inputs, _ = list(inputs)[0]
+        proc_inputs_ = proc_inputs["inputs"][0]
+
+        # RGB -> BGR
+        x = proc_inputs_[[2, 1, 0]]
+        # Convert to expected model input distrubtion
+        x = x.float() / 255.0
+
+        # Add batch dimension
+        x = torch.unsqueeze(x, 0)
+
+        return (NHWC_int_numpy_frames, proc_inputs, x)
+
     def predict_pose_keypoints(
         self,
         pixel_values_or_image: torch.Tensor | np.ndarray | Image | List[Image],
         raw_output=False,
     ) -> np.ndarray | List[Image]:
         """
         Predicts pose keypoints for a person in the image.
@@ -118,36 +147,17 @@
                 keypoints: np.ndarray, shape [B, N, 2]
                     Numpy array of keypoints within the images Each keypoint is an (x, y) pair of coordinates within the image.
 
             Otherwise, returns:
                 predicted_images: List[PIL.Image]
                     Images with keypoints drawn.
         """
-        # Convert from PIL / torch/ etc. to NHWC, RGB numpy frames, which is the required input type.
-        NHWC_int_numpy_frames, _ = app_to_net_image_inputs(pixel_values_or_image)
-
-        # MMPose does a lot of heavy lifting here. The preprocessor does the following:
-        # * runs a detetor model to find people in each frame
-        # * for each bounding box...
-        # *     crop to the bounding box. resize bounding box to fit model input size using scaling factor
-        # *     Save bounding box coordinates and box scaling factor for use later
-        inputs = self.inferencer.preprocess(NHWC_int_numpy_frames, batch_size=1)
-
-        # We only get the first (highest probability) box and ignore the others.
-        # Other implementations may choose to run pose estimation on all boxes
-        # if they want to support multiple people in the same frame.
-        proc_inputs, _ = list(inputs)[0]
-        proc_inputs_ = proc_inputs["inputs"][0]
-
-        # RGB -> BGR
-        x = proc_inputs_[[2, 1, 0], ...]
-        # Convert to expected model input distrubtion
-        x = (x - self.pre_processor.mean) / self.pre_processor.std
-        # Add batch dimension
-        x = torch.unsqueeze(x, 0)
+        (NHWC_int_numpy_frames, proc_inputs, x) = self.preprocess_input(
+            pixel_values_or_image
+        )
 
         # run inference
         heatmaps = self.model(x)
         heatmaps = heatmaps.detach().numpy()
 
         # create predictions from heatmap
         pred_kps, scores = get_max_preds(heatmaps)
```

## qai_hub_models/models/hrnet_pose/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.hrnet_pose import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.hrnet_pose.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/hrnet_pose/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "hrnet_pose"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "hrnet_pose",
             "HRNetPose",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/hrnet_pose/model.py

```diff
@@ -6,30 +6,39 @@
 
 import sys
 from importlib import reload
 
 import torch
 import torch.nn as nn
 
-from qai_hub_models.utils.asset_loaders import CachedWebModelAsset, SourceAsRoot
+from qai_hub_models.models.common import SampleInputsType
+from qai_hub_models.utils.asset_loaders import (
+    CachedWebModelAsset,
+    SourceAsRoot,
+    load_numpy,
+)
 from qai_hub_models.utils.base_model import BaseModel
+from qai_hub_models.utils.image_processing import normalize_image_torchvision
 from qai_hub_models.utils.input_spec import InputSpec
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 1
 # This model originally comes from https://github.com/leoxiaobin/deep-high-resolution-net.pytorch
 # but we'll use the weights from AIMET
 # Weights and config stored in S3 are sourced from
 # https://github.com/quic/aimet-model-zoo/blob/develop/aimet_zoo_torch/hrnet_posenet/models/model_cards/hrnet_posenet_w8a8.json
 # Weights are found here
 # https://github.com/quic/aimet-model-zoo/releases/download/phase_2_march_artifacts/hrnet_posenet_FP32_state_dict.pth
 DEFAULT_WEIGHTS = "hrnet_posenet_FP32_state_dict.pth"
 SOURCE_REPOSITORY = "https://github.com/leoxiaobin/deep-high-resolution-net.pytorch"
 COMMIT_HASH = "6f69e4676ad8d43d0d61b64b1b9726f0c369e7b1"
 CONFIG_FILE = "experiments/coco/hrnet/w32_256x192_adam_lr1e-3.yaml"
+SAMPLE_INPUTS = CachedWebModelAsset.from_asset_store(
+    MODEL_ID, MODEL_ASSET_VERSION, "sample_hrnet_inputs.npy"
+)
 
 
 class HRNetPose(BaseModel):
     def __init__(self, model: nn.Module) -> None:
         super().__init__()
         self.model = model
 
@@ -60,16 +69,23 @@
 
             cfg.merge_from_file(CONFIG_FILE)
             cfg.freeze()
             net = PoseHighResolutionNet(cfg)
             net.load_state_dict(weights)
             return cls(net).eval()
 
-    def forward(self, image: torch.Tensor):
+    def forward(self, image):
+        """
+        Image inputs are expected to be in RGB format in the range [0, 1].
+        """
+        image = normalize_image_torchvision(image)
         return self.model(image)
 
+    def sample_inputs(self, input_spec: InputSpec | None = None) -> SampleInputsType:
+        return {"image": [load_numpy(SAMPLE_INPUTS)]}
+
     @staticmethod
     def get_input_spec(
         height: int = 256,
         width: int = 192,
     ) -> InputSpec:
         return {"image": ((1, 3, height, width), "float32")}
```

## qai_hub_models/models/hrnet_pose/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: HRNetPose
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.492469Z'
+    timestamp: '2024-04-23T18:42:32.272940Z'
   - torchscript_onnx_tflite:
       inference_time: 1753.0
       throughput: 570.4506560182544
       estimated_peak_memory_range:
         min: 225280
         max: 107290736
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.492853Z'
-  - torchscript_onnx_ort:
-      inference_time: 158917.0
-      throughput: 6.292592988792892
-      estimated_peak_memory_range:
-        min: 16871424
-        max: 185399920
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 379
-        total_layers: 379
-      job_id: jmg9jxem5
+    timestamp: '2024-04-23T18:42:32.273098Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2294.0
+      throughput: 435.9197907585004
+      estimated_peak_memory_range:
+        min: 16384
+        max: 3533472
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 514
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 514
+      job_id: jogk7kyyp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 2291.0
+      throughput: 436.4906154517678
+      estimated_peak_memory_range:
+        min: 610304
+        max: 59474648
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 745
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 745
+      job_id: j1p3vrmxg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.492969Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.492975Z'
+    timestamp: '2024-04-23T18:42:32.273247Z'
```

## qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.huggingface_wavlm_base_plus import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.huggingface_wavlm_base_plus.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/huggingface_wavlm_base_plus/export.py

```diff
@@ -32,14 +32,15 @@
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -60,14 +61,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -81,14 +84,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "huggingface_wavlm_base_plus"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "huggingface_wavlm_base_plus",
             "HuggingFace-WavLM-Base-Plus",
             device,
             skip_profiling,
             skip_inferencing,
@@ -103,40 +110,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -147,15 +155,15 @@
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=sample_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: HuggingFace-WavLM-Base-Plus
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.541978Z'
+    timestamp: '2024-04-23T18:42:32.343742Z'
   - torchscript_onnx_tflite:
       inference_time: 789013.0
       throughput: 1.2674062404548467
       estimated_peak_memory_range:
         min: 148623360
         max: 174462192
       primary_compute_unit: CPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.542329Z'
-  - torchscript_onnx_ort:
-      inference_time: 825393.0
-      throughput: 1.211544076579278
+    timestamp: '2024-04-23T18:42:32.343896Z'
+  - torchscript_onnx_tflite:
+      inference_time: 928773.0
+      throughput: 1.0766893525113241
       estimated_peak_memory_range:
-        min: 72355840
-        max: 279092176
+        min: 150151168
+        max: 158231104
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
-        layers_on_cpu: 484
-        total_layers: 484
-      job_id: jqpyr7005
+        layers_on_cpu: 811
+        total_layers: 811
+      job_id: jqp4k2wqg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.542466Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.542471Z'
+    timestamp: '2024-04-23T18:42:32.343990Z'
```

## qai_hub_models/models/inception_v3/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.inception_v3 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.inception_v3.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/inception_v3/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "inception_v3"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "inception_v3",
             "Inception-v3",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/inception_v3/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Inception-v3
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.563810Z'
+    timestamp: '2024-04-23T18:42:32.362074Z'
   - torchscript_onnx_tflite:
       inference_time: 1019.0
       throughput: 981.3542688910696
       estimated_peak_memory_range:
         min: 12288
         max: 51945968
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.563949Z'
-  - torchscript_onnx_ort:
-      inference_time: 73090.0
-      throughput: 13.681762210972773
-      estimated_peak_memory_range:
-        min: 1110016
-        max: 60531168
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 123
-        total_layers: 123
-      job_id: j1p3v60mg
+    timestamp: '2024-04-23T18:42:32.362144Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1335.0
+      throughput: 749.0636704119851
+      estimated_peak_memory_range:
+        min: 24576
+        max: 1812440
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 129
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 129
+      job_id: jogk7klyp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1397.0
+      throughput: 715.8196134574088
+      estimated_peak_memory_range:
+        min: 36864
+        max: 150659520
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 219
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 219
+      job_id: j1p3vr4xg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.563990Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.563995Z'
+    timestamp: '2024-04-23T18:42:32.362196Z'
```

## qai_hub_models/models/inception_v3_quantized/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.inception_v3_quantized import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.inception_v3_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/inception_v3_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "inception_v3_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "inception_v3_quantized",
             "Inception-v3-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
@@ -196,14 +203,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/inception_v3_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Inception-v3-Quantized
@@ -69,15 +72,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.586344Z'
+    timestamp: '2024-04-23T18:42:32.386127Z'
   - torchscript_onnx_tflite:
       inference_time: 492.0
       throughput: 2032.520325203252
       estimated_peak_memory_range:
         min: 12288
         max: 64321136
       primary_compute_unit: NPU
@@ -107,16 +110,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.586427Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:32.386165Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2624.0
+      throughput: 381.0975609756098
+      estimated_peak_memory_range:
+        min: 12288
+        max: 20812688
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: j7gjz8075
+      job_status: Passed
+    torchscript_onnx_ort:
       inference_time: 26460.0
       throughput: 37.79289493575208
       estimated_peak_memory_range:
         min: 17575936
         max: 85502320
       primary_compute_unit: CPU
       precision: fp32
@@ -128,18 +146,56 @@
       job_id: jygzond45
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.586468Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.386219Z'
+  - torchscript_onnx_tflite:
+      inference_time: 7950.0
+      throughput: 125.78616352201257
+      estimated_peak_memory_range:
+        min: 45056
+        max: 4402544
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jn5qr427p
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:32.386245Z'
+  - torchscript_onnx_tflite:
+      inference_time: 641.0
+      throughput: 1560.0624024960998
+      estimated_peak_memory_range:
+        min: 12288
+        max: 1923000
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jlpeenr7p
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.586474Z'
+    timestamp: '2024-04-23T18:42:32.386270Z'
```

## qai_hub_models/models/lama_dilated/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.lama_dilated import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.lama_dilated.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/lama_dilated/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "lama_dilated"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "lama_dilated",
             "LaMa-Dilated",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image,mask"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image,mask", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/lama_dilated/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: LaMa-Dilated
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.607133Z'
+    timestamp: '2024-04-23T18:42:32.412866Z'
   - torchscript_onnx_tflite:
       inference_time: 60997.0
       throughput: 16.39424889748676
       estimated_peak_memory_range:
         min: 2707456
         max: 271146544
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.607354Z'
-  - torchscript_onnx_ort:
-      inference_time: 4472091.0
-      throughput: 0.2236090455225531
-      estimated_peak_memory_range:
-        min: 91983872
-        max: 195795248
-      primary_compute_unit: CPU
-      precision: fp32
+    timestamp: '2024-04-23T18:42:32.412968Z'
+  - torchscript_onnx_tflite:
+      inference_time: 87453.0
+      throughput: 11.434713503253176
+      estimated_peak_memory_range:
+        min: 3260416
+        max: 139194808
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 347
         layers_on_gpu: 0
-        layers_on_cpu: 220
-        total_layers: 220
-      job_id: jnp1y6zkp
+        layers_on_cpu: 0
+        total_layers: 347
+      job_id: jopr871v5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 82234.0
+      throughput: 12.160420264124328
+      estimated_peak_memory_range:
+        min: 3178496
+        max: 33096560
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 332
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 332
+      job_id: j1p80kwzg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.607436Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.607442Z'
+    timestamp: '2024-04-23T18:42:32.413058Z'
```

## qai_hub_models/models/litehrnet/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.litehrnet import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.litehrnet.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/litehrnet/export.py

```diff
@@ -33,14 +33,15 @@
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "litehrnet"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "litehrnet",
             "LiteHRNet",
             device,
             skip_profiling,
             skip_inferencing,
@@ -104,40 +111,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -148,15 +156,15 @@
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=sample_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/litehrnet/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: LiteHRNet
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.630164Z'
+    timestamp: '2024-04-23T18:42:32.436998Z'
   - torchscript_onnx_tflite:
       inference_time: 10344.0
       throughput: 96.67440061871616
       estimated_peak_memory_range:
         min: 20480
         max: 73273328
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.630465Z'
-  - torchscript_onnx_ort:
-      inference_time: 37860.0
-      throughput: 26.413100898045432
+    timestamp: '2024-04-23T18:42:32.437166Z'
+  - torchscript_onnx_tflite:
+      inference_time: 15632.0
+      throughput: 63.97134083930399
       estimated_peak_memory_range:
-        min: 10416128
-        max: 398877440
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 6529024
+        max: 10764512
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 1226
         layers_on_gpu: 0
-        layers_on_cpu: 939
-        total_layers: 939
-      job_id: jo5mq8xyp
+        layers_on_cpu: 10
+        total_layers: 1236
+      job_id: j1gl6qeeg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.630724Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.630734Z'
+    timestamp: '2024-04-23T18:42:32.437300Z'
```

## qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml

```diff
@@ -22,15 +22,15 @@
 source_repo: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
 technical_details:
   Number of parameters: 7B
   Model size: 3.6GB
   Model-1 (Prompt Processor): Llama-PromptProcessor-Quantized
   Max context length: 1024
   Prompt processor input: 1024 tokens
-  Prompt processor output: 1 output token + KVCache for token generator
+  Prompt processor output: 1024 output tokens + KVCache for token generator
   Model-2 (Token Generator): Llama-TokenGenerator-KVCache-Quantized
   Token generator input: 1 input token + past KVCache
   Token generator output: 1 output token + KVCache for next iteration
   Decoding length: 1024 (1 output token + 1023 from KVCache)
   Use: Initiate conversation with prompt-processor and then token generator for subsequent iterations.
   QNN-SDK: "2.19"
 applicable_scenarios:
```

## qai_hub_models/models/llama_v2_7b_chat_quantized/perf.yaml

```diff
@@ -32,15 +32,15 @@
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
     timestamp: '2024-01-26T00:34:02.549319Z'
     torchscript_onnx_qnn:
       inference_time: 2578521
-      throughput: 0.38
+      throughput: 397.13
       estimated_peak_memory_range:
         min: 12124160
         max: 17599768
       layer_info:
         layers_on_npu: 31769
         layers_on_gpu: 0
         layers_on_cpu: 0
```

## qai_hub_models/models/mediapipe_face/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.mediapipe_face import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.mediapipe_face.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/mediapipe_face/export.py

```diff
@@ -30,14 +30,15 @@
 )
 
 ALL_COMPONENTS = ["MediaPipeFaceDetector", "MediaPipeFaceLandmarkDetector"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
@@ -59,14 +60,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         components: List of sub-components of the model that will be exported.
             Each component is compiled and profiled separately.
             Defaults to ALL_COMPONENTS if not specified.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
@@ -83,14 +86,18 @@
         A Mapping from component_name to a 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "mediapipe_face"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     component_arg = components
     components = components or ALL_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
@@ -116,27 +123,28 @@
     if "MediaPipeFaceLandmarkDetector" in components:
         components_dict["MediaPipeFaceLandmarkDetector"] = model.face_landmark_detector  # type: ignore
 
     compile_jobs: Dict[str, hub.client.CompileJob] = {}
     for component_name, component in components_dict.items():
         # Trace the model
         input_spec = component.get_input_spec()
+        component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
             target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
-            device=hub.Device(device),
+            device=hub_device,
             name=f"{model_name}_{component_name}",
             options=model_compile_options,
         )
         compile_jobs[component_name] = cast(
             hub.client.CompileJob, submitted_compile_job
         )
 
@@ -146,15 +154,15 @@
         for component_name in components:
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             print(f"Profiling model {component_name} on a hosted device.")
             submitted_profile_job = hub.submit_profile_job(
                 model=compile_jobs[component_name].get_target_model(),
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             profile_jobs[component_name] = cast(
                 hub.client.ProfileJob, submitted_profile_job
             )
 
@@ -168,15 +176,15 @@
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
                 inputs=sample_inputs,
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
 
@@ -215,15 +223,18 @@
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
     parser = export_parser(
-        model_cls=Model, components=ALL_COMPONENTS, supports_ort=False
+        model_cls=Model,
+        components=ALL_COMPONENTS,
+        supports_qnn=False,
+        supports_ort=False,
     )
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mediapipe_face/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: MediaPipeFaceDetector
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.651181Z'
+    timestamp: '2024-04-23T18:42:32.455108Z'
   - torchscript_onnx_tflite:
       inference_time: 544.0
       throughput: 1838.235294117647
       estimated_peak_memory_range:
         min: 12288
         max: 28679584
       primary_compute_unit: NPU
@@ -137,46 +134,53 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.651255Z'
-  - torchscript_onnx_ort:
-      inference_time: 17041.0
-      throughput: 58.68200222991609
-      estimated_peak_memory_range:
-        min: 315392
-        max: 42258624
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 84
-        total_layers: 84
-      job_id: j1pv07675
+    timestamp: '2024-04-23T18:42:32.455176Z'
+  - torchscript_onnx_tflite:
+      inference_time: 784.0
+      throughput: 1275.5102040816328
+      estimated_peak_memory_range:
+        min: 24576
+        max: 1602632
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 112
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 112
+      job_id: jlpeen47p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 840.0
+      throughput: 1190.4761904761904
+      estimated_peak_memory_range:
+        min: 815104
+        max: 6172048
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 148
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 148
+      job_id: j0pxnzej5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.651281Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.651287Z'
+    timestamp: '2024-04-23T18:42:32.455221Z'
 - name: MediaPipeFaceLandmarkDetector
   performance_metrics:
   - torchscript_onnx_tflite:
       inference_time: 315.0
       throughput: 3174.6031746031745
       estimated_peak_memory_range:
         min: 24576
@@ -223,15 +227,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.651349Z'
+    timestamp: '2024-04-23T18:42:32.455272Z'
   - torchscript_onnx_tflite:
       inference_time: 230.0
       throughput: 4347.826086956522
       estimated_peak_memory_range:
         min: 12288
         max: 25090016
       primary_compute_unit: NPU
@@ -276,39 +280,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.651411Z'
-  - torchscript_onnx_ort:
-      inference_time: 4387.0
-      throughput: 227.94620469569182
-      estimated_peak_memory_range:
-        min: 0
-        max: 40117968
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 80
-        total_layers: 80
-      job_id: j7gjzqv75
+    timestamp: '2024-04-23T18:42:32.455318Z'
+  - torchscript_onnx_tflite:
+      inference_time: 326.0
+      throughput: 3067.4846625766872
+      estimated_peak_memory_range:
+        min: 24576
+        max: 1871744
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 101
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 101
+      job_id: jygzo0vz5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 396.0
+      throughput: 2525.252525252525
+      estimated_peak_memory_range:
+        min: 458752
+        max: 81438752
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 107
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 107
+      job_id: jo5mqlvyp
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.651435Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.651440Z'
+    timestamp: '2024-04-23T18:42:32.455361Z'
```

## qai_hub_models/models/mediapipe_hand/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.mediapipe_hand import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.mediapipe_hand.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/mediapipe_hand/export.py

```diff
@@ -30,14 +30,15 @@
 )
 
 ALL_COMPONENTS = ["MediaPipeHandDetector", "MediaPipeHandLandmarkDetector"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
@@ -59,14 +60,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         components: List of sub-components of the model that will be exported.
             Each component is compiled and profiled separately.
             Defaults to ALL_COMPONENTS if not specified.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
@@ -83,14 +86,18 @@
         A Mapping from component_name to a 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "mediapipe_hand"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     component_arg = components
     components = components or ALL_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
@@ -116,27 +123,28 @@
     if "MediaPipeHandLandmarkDetector" in components:
         components_dict["MediaPipeHandLandmarkDetector"] = model.hand_landmark_detector  # type: ignore
 
     compile_jobs: Dict[str, hub.client.CompileJob] = {}
     for component_name, component in components_dict.items():
         # Trace the model
         input_spec = component.get_input_spec()
+        component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
             target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
-            device=hub.Device(device),
+            device=hub_device,
             name=f"{model_name}_{component_name}",
             options=model_compile_options,
         )
         compile_jobs[component_name] = cast(
             hub.client.CompileJob, submitted_compile_job
         )
 
@@ -146,15 +154,15 @@
         for component_name in components:
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             print(f"Profiling model {component_name} on a hosted device.")
             submitted_profile_job = hub.submit_profile_job(
                 model=compile_jobs[component_name].get_target_model(),
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             profile_jobs[component_name] = cast(
                 hub.client.ProfileJob, submitted_profile_job
             )
 
@@ -168,15 +176,15 @@
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
                 inputs=sample_inputs,
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
```

## qai_hub_models/models/mediapipe_hand/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: MediaPipeHandDetector
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.692034Z'
+    timestamp: '2024-04-23T18:42:32.499757Z'
   - torchscript_onnx_tflite:
       inference_time: 679.0
       throughput: 1472.7540500736377
       estimated_peak_memory_range:
         min: 12288
         max: 52020064
       primary_compute_unit: NPU
@@ -137,46 +134,53 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.692119Z'
-  - torchscript_onnx_ort:
-      inference_time: 41236.0
-      throughput: 24.25065476767873
-      estimated_peak_memory_range:
-        min: 348160
-        max: 56481568
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 116
-        total_layers: 116
-      job_id: jep20exxg
+    timestamp: '2024-04-23T18:42:32.499831Z'
+  - torchscript_onnx_tflite:
+      inference_time: 959.0
+      throughput: 1042.752867570386
+      estimated_peak_memory_range:
+        min: 24576
+        max: 3871952
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 152
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 152
+      job_id: j1p3vr8xg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1020.0
+      throughput: 980.3921568627451
+      estimated_peak_memory_range:
+        min: 806912
+        max: 7974248
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 197
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 197
+      job_id: jnp1ym3kp
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.692149Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.692154Z'
+    timestamp: '2024-04-23T18:42:32.499901Z'
 - name: MediaPipeHandLandmarkDetector
   performance_metrics:
   - torchscript_onnx_tflite:
       inference_time: 1259.0
       throughput: 794.2811755361398
       estimated_peak_memory_range:
         min: 24576
@@ -223,15 +227,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.692237Z'
+    timestamp: '2024-04-23T18:42:32.499968Z'
   - torchscript_onnx_tflite:
       inference_time: 901.0
       throughput: 1109.8779134295228
       estimated_peak_memory_range:
         min: 12288
         max: 56691584
       primary_compute_unit: NPU
@@ -276,39 +280,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.692320Z'
-  - torchscript_onnx_ort:
-      inference_time: 38223.0
-      throughput: 26.162258326138712
-      estimated_peak_memory_range:
-        min: 425984
-        max: 56359008
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 116
-        total_layers: 116
-      job_id: jqpyrmzr5
+    timestamp: '2024-04-23T18:42:32.500029Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1206.0
+      throughput: 829.1873963515754
+      estimated_peak_memory_range:
+        min: 40960
+        max: 2078488
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 159
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 159
+      job_id: jwgok9m4p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1308.0
+      throughput: 764.525993883792
+      estimated_peak_memory_range:
+        min: 811008
+        max: 8238832
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 210
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 210
+      job_id: jvgdem0k5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.692348Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.692352Z'
+    timestamp: '2024-04-23T18:42:32.500084Z'
```

## qai_hub_models/models/mediapipe_pose/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.mediapipe_pose import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.mediapipe_pose.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/mediapipe_pose/export.py

```diff
@@ -30,14 +30,15 @@
 )
 
 ALL_COMPONENTS = ["MediaPipePoseDetector", "MediaPipePoseLandmarkDetector"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
@@ -59,14 +60,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         components: List of sub-components of the model that will be exported.
             Each component is compiled and profiled separately.
             Defaults to ALL_COMPONENTS if not specified.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
@@ -83,14 +86,18 @@
         A Mapping from component_name to a 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "mediapipe_pose"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     component_arg = components
     components = components or ALL_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
@@ -116,27 +123,28 @@
     if "MediaPipePoseLandmarkDetector" in components:
         components_dict["MediaPipePoseLandmarkDetector"] = model.pose_landmark_detector  # type: ignore
 
     compile_jobs: Dict[str, hub.client.CompileJob] = {}
     for component_name, component in components_dict.items():
         # Trace the model
         input_spec = component.get_input_spec()
+        component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
             target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
-            device=hub.Device(device),
+            device=hub_device,
             name=f"{model_name}_{component_name}",
             options=model_compile_options,
         )
         compile_jobs[component_name] = cast(
             hub.client.CompileJob, submitted_compile_job
         )
 
@@ -146,15 +154,15 @@
         for component_name in components:
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             print(f"Profiling model {component_name} on a hosted device.")
             submitted_profile_job = hub.submit_profile_job(
                 model=compile_jobs[component_name].get_target_model(),
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             profile_jobs[component_name] = cast(
                 hub.client.ProfileJob, submitted_profile_job
             )
 
@@ -168,15 +176,15 @@
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
                 inputs=sample_inputs,
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
```

## qai_hub_models/models/mediapipe_pose/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: MediaPipePoseDetector
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.732493Z'
+    timestamp: '2024-04-23T18:42:32.544709Z'
   - torchscript_onnx_tflite:
       inference_time: 612.0
       throughput: 1633.986928104575
       estimated_peak_memory_range:
         min: 16384
         max: 40580928
       primary_compute_unit: NPU
@@ -137,46 +134,53 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.732591Z'
-  - torchscript_onnx_ort:
-      inference_time: 42472.0
-      throughput: 23.544923714447165
-      estimated_peak_memory_range:
-        min: 0
-        max: 42875888
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 81
-        total_layers: 81
-      job_id: jz5w24wz5
+    timestamp: '2024-04-23T18:42:32.544766Z'
+  - torchscript_onnx_tflite:
+      inference_time: 845.0
+      throughput: 1183.4319526627219
+      estimated_peak_memory_range:
+        min: 32768
+        max: 1538160
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 107
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 107
+      job_id: j2p03xdep
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 886.0
+      throughput: 1128.6681715575621
+      estimated_peak_memory_range:
+        min: 12288
+        max: 104292296
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 140
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 140
+      job_id: j1pv0nem5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.732621Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.732626Z'
+    timestamp: '2024-04-23T18:42:32.544811Z'
 - name: MediaPipePoseLandmarkDetector
   performance_metrics:
   - torchscript_onnx_tflite:
       inference_time: 1206.0
       throughput: 829.1873963515754
       estimated_peak_memory_range:
         min: 16384
@@ -223,15 +227,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.732805Z'
+    timestamp: '2024-04-23T18:42:32.544899Z'
   - torchscript_onnx_tflite:
       inference_time: 880.0
       throughput: 1136.3636363636363
       estimated_peak_memory_range:
         min: 16384
         max: 87924496
       primary_compute_unit: NPU
@@ -276,39 +280,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.732969Z'
-  - torchscript_onnx_ort:
-      inference_time: 28143.0
-      throughput: 35.532814554240844
-      estimated_peak_memory_range:
-        min: 0
-        max: 78807408
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 172
-        total_layers: 172
-      job_id: jmg9jd0q5
+    timestamp: '2024-04-23T18:42:32.544985Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1247.0
+      throughput: 801.924619085806
+      estimated_peak_memory_range:
+        min: 12288
+        max: 2817072
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 230
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 230
+      job_id: j1p80k68g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1291.0
+      throughput: 774.5933384972889
+      estimated_peak_memory_range:
+        min: 24576
+        max: 13908424
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 306
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 306
+      job_id: j7gjz8o85
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.733019Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.733023Z'
+    timestamp: '2024-04-23T18:42:32.545055Z'
```

## qai_hub_models/models/mediapipe_selfie/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.mediapipe_selfie import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.mediapipe_selfie.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/mediapipe_selfie/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "mediapipe_selfie"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "mediapipe_selfie",
             "MediaPipe-Selfie-Segmentation",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/mediapipe_selfie/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: MediaPipe-Selfie-Segmentation
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.778315Z'
+    timestamp: '2024-04-23T18:42:32.589824Z'
   - torchscript_onnx_tflite:
       inference_time: 536.0
       throughput: 1865.6716417910447
       estimated_peak_memory_range:
         min: 12288
         max: 23055696
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.778430Z'
-  - torchscript_onnx_ort:
-      inference_time: 8168.0
-      throughput: 122.42899118511264
-      estimated_peak_memory_range:
-        min: 225280
-        max: 49546208
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 110
-        total_layers: 110
-      job_id: jegnlklv5
+    timestamp: '2024-04-23T18:42:32.589884Z'
+  - torchscript_onnx_tflite:
+      inference_time: 785.0
+      throughput: 1273.8853503184714
+      estimated_peak_memory_range:
+        min: 12288
+        max: 2039720
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 118
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 118
+      job_id: j0pxnzd95
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 772.0
+      throughput: 1295.3367875647668
+      estimated_peak_memory_range:
+        min: 819200
+        max: 8273816
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 138
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 138
+      job_id: jep20zvmg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.778470Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.778476Z'
+    timestamp: '2024-04-23T18:42:32.589938Z'
```

## qai_hub_models/models/mnasnet05/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.mnasnet05 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.mnasnet05.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/mnasnet05/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "mnasnet05"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "mnasnet05",
             "MNASNet05",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/mnasnet05/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: MNASNet05
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.802104Z'
+    timestamp: '2024-04-23T18:42:32.613841Z'
   - torchscript_onnx_tflite:
       inference_time: 530.0
       throughput: 1886.7924528301887
       estimated_peak_memory_range:
         min: 12288
         max: 45612800
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.802197Z'
-  - torchscript_onnx_ort:
-      inference_time: 12908.0
-      throughput: 77.47133560582584
+    timestamp: '2024-04-23T18:42:32.613900Z'
+  - torchscript_onnx_tflite:
+      inference_time: 799.0
+      throughput: 1251.5644555694619
+      estimated_peak_memory_range:
+        min: 20480
+        max: 1900528
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 71
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 71
+      job_id: j1p3vrwzg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 852.0
+      throughput: 1173.7089201877934
       estimated_peak_memory_range:
         min: 0
-        max: 31498400
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 56
-        total_layers: 56
-      job_id: jn5qeve75
+        max: 47875160
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 103
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 103
+      job_id: jlpeenl0p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.802219Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.802226Z'
+    timestamp: '2024-04-23T18:42:32.613941Z'
```

## qai_hub_models/models/mobilenet_v2/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.mobilenet_v2 import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.mobilenet_v2.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/mobilenet_v2/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "mobilenet_v2"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "mobilenet_v2",
             "MobileNet-v2",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/mobilenet_v2/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: MobileNet-v2
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.825870Z'
+    timestamp: '2024-04-23T18:42:32.638108Z'
   - torchscript_onnx_tflite:
       inference_time: 651.0
       throughput: 1536.0983102918588
       estimated_peak_memory_range:
         min: 16384
         max: 56986240
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.825968Z'
-  - torchscript_onnx_ort:
-      inference_time: 19487.0
-      throughput: 51.31626212346693
-      estimated_peak_memory_range:
-        min: 2371584
-        max: 33778432
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 57
-        total_layers: 57
-      job_id: jlpeeye7p
+    timestamp: '2024-04-23T18:42:32.638177Z'
+  - torchscript_onnx_tflite:
+      inference_time: 957.0
+      throughput: 1044.932079414838
+      estimated_peak_memory_range:
+        min: 57344
+        max: 1611720
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 72
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 72
+      job_id: jvgdemzl5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1265.0
+      throughput: 790.5138339920949
+      estimated_peak_memory_range:
+        min: 618496
+        max: 128931224
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 105
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 105
+      job_id: jo5mql8qp
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.825993Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.826002Z'
+    timestamp: '2024-04-23T18:42:32.638216Z'
```

## qai_hub_models/models/mobilenet_v2_quantized/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.mobilenet_v2_quantized import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.mobilenet_v2_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/mobilenet_v2_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "mobilenet_v2_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "mobilenet_v2_quantized",
             "MobileNet-v2-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/mobilenet_v2_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: MobileNet-v2-Quantized
@@ -84,15 +87,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.849738Z'
+    timestamp: '2024-04-23T18:42:32.662420Z'
   - torchscript_onnx_tflite:
       inference_time: 233.0
       throughput: 4291.845493562232
       estimated_peak_memory_range:
         min: 12288
         max: 37162256
       primary_compute_unit: NPU
@@ -137,16 +140,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.849816Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:32.662476Z'
+  - torchscript_onnx_tflite:
+      inference_time: 949.0
+      throughput: 1053.740779768177
+      estimated_peak_memory_range:
+        min: 12288
+        max: 23229040
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 74
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 74
+      job_id: jogk7k8op
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jwgok98dp
+      job_status: Failed
+    torchscript_onnx_ort:
       inference_time: 6507.0
       throughput: 153.68065160596282
       estimated_peak_memory_range:
         min: 335872
         max: 43247408
       primary_compute_unit: CPU
       precision: fp32
@@ -158,18 +191,71 @@
       job_id: jqp4k3kqg
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.849852Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.662525Z'
+  - torchscript_onnx_tflite:
+      inference_time: 7442.0
+      throughput: 134.37248051599033
+      estimated_peak_memory_range:
+        min: 12288
+        max: 11587968
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 72
+        layers_on_gpu: 2
+        layers_on_cpu: 0
+        total_layers: 74
+      job_id: j1gl2wkep
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:32.662544Z'
+  - torchscript_onnx_tflite:
+      inference_time: 325.0
+      throughput: 3076.923076923077
+      estimated_peak_memory_range:
+        min: 20480
+        max: 1768808
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 74
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 74
+      job_id: j1p80k18g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 695.0
+      throughput: 1438.8489208633093
+      estimated_peak_memory_range:
+        min: 20480
+        max: 131789128
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 73
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 73
+      job_id: jw56e0w7g
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.849857Z'
+    timestamp: '2024-04-23T18:42:32.662578Z'
```

## qai_hub_models/models/mobilenet_v3_large/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.mobilenet_v3_large import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.mobilenet_v3_large.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/mobilenet_v3_large/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "mobilenet_v3_large"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "mobilenet_v3_large",
             "MobileNet-v3-Large",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/mobilenet_v3_large/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: MobileNet-v3-Large
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.872299Z'
+    timestamp: '2024-04-23T18:42:32.697406Z'
   - torchscript_onnx_tflite:
       inference_time: 691.0
       throughput: 1447.178002894356
       estimated_peak_memory_range:
         min: 16384
         max: 61060464
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.872367Z'
-  - torchscript_onnx_ort:
-      inference_time: 19040.0
-      throughput: 52.52100840336134
+    timestamp: '2024-04-23T18:42:32.697460Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1022.0
+      throughput: 978.4735812133073
       estimated_peak_memory_range:
-        min: 0
-        max: 57628960
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 24576
+        max: 1929640
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 136
         layers_on_gpu: 0
-        layers_on_cpu: 126
-        total_layers: 126
-      job_id: jep20e9xg
+        layers_on_cpu: 0
+        total_layers: 136
+      job_id: jz5w2r4j5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.872408Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.872414Z'
+    timestamp: '2024-04-23T18:42:32.697490Z'
```

## qai_hub_models/models/mobilenet_v3_large_quantized/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.mobilenet_v3_large_quantized import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.mobilenet_v3_large_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/mobilenet_v3_large_quantized/export.py

```diff
@@ -32,14 +32,15 @@
     export_without_hub_access,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -60,14 +61,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -81,14 +84,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "mobilenet_v3_large_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "mobilenet_v3_large_quantized",
             "MobileNet-v3-Large-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -119,15 +126,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -135,15 +142,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +164,15 @@
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: MobileNet-v3-Large-Quantized
@@ -69,15 +72,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.893025Z'
+    timestamp: '2024-04-23T18:42:32.715082Z'
   - torchscript_onnx_tflite:
       inference_time: 413.0
       throughput: 2421.3075060532688
       estimated_peak_memory_range:
         min: 12288
         max: 46829184
       primary_compute_unit: NPU
@@ -107,16 +110,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.893099Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:32.715131Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1547.0
+      throughput: 646.4124111182934
+      estimated_peak_memory_range:
+        min: 12288
+        max: 28081232
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 138
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 138
+      job_id: jegnlwnm5
+      job_status: Passed
+    torchscript_onnx_ort:
       inference_time: 10400.0
       throughput: 96.15384615384616
       estimated_peak_memory_range:
         min: 11681792
         max: 108762160
       primary_compute_unit: CPU
       precision: fp32
@@ -128,18 +146,56 @@
       job_id: jn5qevr75
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.893166Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.715190Z'
+  - torchscript_onnx_tflite:
+      inference_time: 5306.0
+      throughput: 188.46588767433096
+      estimated_peak_memory_range:
+        min: 40960
+        max: 2748408
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 138
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 138
+      job_id: jw56zo1vg
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:32.715215Z'
+  - torchscript_onnx_tflite:
+      inference_time: 667.0
+      throughput: 1499.2503748125937
+      estimated_peak_memory_range:
+        min: 40960
+        max: 1853728
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 138
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 138
+      job_id: jo5mqlmqp
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.893172Z'
+    timestamp: '2024-04-23T18:42:32.715252Z'
```

## qai_hub_models/models/mobilenet_v3_small/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.mobilenet_v3_small import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.mobilenet_v3_small.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/mobilenet_v3_small/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "mobilenet_v3_small"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "mobilenet_v3_small",
             "MobileNet-v3-Small",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/mobilenet_v3_small/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: MobileNet-v3-Small
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.913835Z'
+    timestamp: '2024-04-23T18:42:32.742071Z'
   - torchscript_onnx_tflite:
       inference_time: 547.0
       throughput: 1828.1535648994516
       estimated_peak_memory_range:
         min: 12288
         max: 40731056
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.913910Z'
-  - torchscript_onnx_ort:
-      inference_time: 8623.0
-      throughput: 115.96892032935173
+    timestamp: '2024-04-23T18:42:32.742122Z'
+  - torchscript_onnx_tflite:
+      inference_time: 844.0
+      throughput: 1184.8341232227488
       estimated_peak_memory_range:
-        min: 0
-        max: 53013952
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 12288
+        max: 1902856
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 124
         layers_on_gpu: 0
-        layers_on_cpu: 114
-        total_layers: 114
-      job_id: j1pv07r75
+        layers_on_cpu: 0
+        total_layers: 124
+      job_id: j1gl6qxlg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.913950Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.913955Z'
+    timestamp: '2024-04-23T18:42:32.742147Z'
```

## qai_hub_models/models/openai_clip/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.openai_clip import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.openai_clip.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/openai_clip/export.py

```diff
@@ -30,14 +30,15 @@
 )
 
 ALL_COMPONENTS = ["CLIPTextEncoder", "CLIPImageEncoder"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
@@ -59,14 +60,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         components: List of sub-components of the model that will be exported.
             Each component is compiled and profiled separately.
             Defaults to ALL_COMPONENTS if not specified.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
@@ -83,14 +86,18 @@
         A Mapping from component_name to a 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "openai_clip"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     component_arg = components
     components = components or ALL_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
@@ -116,27 +123,28 @@
     if "CLIPImageEncoder" in components:
         components_dict["CLIPImageEncoder"] = model.image_encoder  # type: ignore
 
     compile_jobs: Dict[str, hub.client.CompileJob] = {}
     for component_name, component in components_dict.items():
         # Trace the model
         input_spec = component.get_input_spec()
+        component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
             target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
-            device=hub.Device(device),
+            device=hub_device,
             name=f"{model_name}_{component_name}",
             options=model_compile_options,
         )
         compile_jobs[component_name] = cast(
             hub.client.CompileJob, submitted_compile_job
         )
 
@@ -146,15 +154,15 @@
         for component_name in components:
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             print(f"Profiling model {component_name} on a hosted device.")
             submitted_profile_job = hub.submit_profile_job(
                 model=compile_jobs[component_name].get_target_model(),
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             profile_jobs[component_name] = cast(
                 hub.client.ProfileJob, submitted_profile_job
             )
 
@@ -168,15 +176,15 @@
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
                 inputs=sample_inputs,
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
```

## qai_hub_models/models/openai_clip/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: CLIPTextEncoder
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.934211Z'
+    timestamp: '2024-04-23T18:42:32.759891Z'
   - torchscript_onnx_tflite:
       inference_time: 11237.0
       throughput: 88.99172376968941
       estimated_peak_memory_range:
         min: 16384
         max: 219358080
       primary_compute_unit: NPU
@@ -107,46 +104,38 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.934371Z'
-  - torchscript_onnx_ort:
-      inference_time: 55778.0
-      throughput: 17.92821542543655
-      estimated_peak_memory_range:
-        min: 761856
-        max: 125360048
-      primary_compute_unit: CPU
-      precision: fp32
+    timestamp: '2024-04-23T18:42:32.759974Z'
+  - torchscript_onnx_tflite:
+      inference_time: 15367.0
+      throughput: 65.07451031430989
+      estimated_peak_memory_range:
+        min: 49152
+        max: 3357800
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 574
         layers_on_gpu: 0
-        layers_on_cpu: 290
-        total_layers: 290
-      job_id: jmg9jdyv5
+        layers_on_cpu: 2
+        total_layers: 576
+      job_id: j7gjz8785
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.934453Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.934458Z'
+    timestamp: '2024-04-23T18:42:32.760069Z'
 - name: CLIPImageEncoder
   performance_metrics:
   - torchscript_onnx_tflite:
       inference_time: 126657.0
       throughput: 7.895339381163299
       estimated_peak_memory_range:
         min: 163840
@@ -178,15 +167,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.934627Z'
+    timestamp: '2024-04-23T18:42:32.760150Z'
   - torchscript_onnx_tflite:
       inference_time: 96976.0
       throughput: 10.31182973106748
       estimated_peak_memory_range:
         min: 229376
         max: 865695568
       primary_compute_unit: NPU
@@ -216,39 +205,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.934785Z'
-  - torchscript_onnx_ort:
-      inference_time: 365962.0
-      throughput: 2.732524141850793
-      estimated_peak_memory_range:
-        min: 954368
-        max: 125503856
-      primary_compute_unit: CPU
-      precision: fp32
+    timestamp: '2024-04-23T18:42:32.760227Z'
+  - torchscript_onnx_tflite:
+      inference_time: 127012.0
+      throughput: 7.873271816836205
+      estimated_peak_memory_range:
+        min: 184320
+        max: 4508448
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 576
         layers_on_gpu: 0
-        layers_on_cpu: 280
-        total_layers: 280
-      job_id: jnp1y6wlp
+        layers_on_cpu: 0
+        total_layers: 576
+      job_id: jlpeenz0p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.934858Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.934863Z'
+    timestamp: '2024-04-23T18:42:32.760295Z'
```

## qai_hub_models/models/openpose/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.openpose import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.openpose.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/openpose/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "openpose"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "openpose",
             "OpenPose",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0,output_1",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/openpose/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: OpenPose
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.970561Z'
+    timestamp: '2024-04-23T18:42:32.792235Z'
   - torchscript_onnx_tflite:
       inference_time: 8779.0
       throughput: 113.90818999886092
       estimated_peak_memory_range:
         min: 196608
         max: 34017488
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.970679Z'
-  - torchscript_onnx_ort:
-      inference_time: 948410.0
-      throughput: 1.0543963053953458
-      estimated_peak_memory_range:
-        min: 28573696
-        max: 81966016
-      primary_compute_unit: CPU
-      precision: fp32
+    timestamp: '2024-04-23T18:42:32.792298Z'
+  - torchscript_onnx_tflite:
+      inference_time: 11875.0
+      throughput: 84.21052631578948
+      estimated_peak_memory_range:
+        min: 139264
+        max: 2225560
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 103
         layers_on_gpu: 0
-        layers_on_cpu: 103
+        layers_on_cpu: 0
         total_layers: 103
-      job_id: j2p036lep
+      job_id: j0pxnzl15
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 11826.0
+      throughput: 84.5594452900389
+      estimated_peak_memory_range:
+        min: 663552
+        max: 242581864
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 186
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 186
+      job_id: jep20zr4g
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.970720Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.970725Z'
+    timestamp: '2024-04-23T18:42:32.792350Z'
```

## qai_hub_models/models/quicksrnetlarge/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.quicksrnetlarge import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.quicksrnetlarge.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/quicksrnetlarge/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "quicksrnetlarge"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "quicksrnetlarge",
             "QuickSRNetLarge",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/quicksrnetlarge/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: QuickSRNetLarge
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:32.994123Z'
+    timestamp: '2024-04-23T18:42:32.816274Z'
   - torchscript_onnx_tflite:
       inference_time: 1917.0
       throughput: 521.6484089723526
       estimated_peak_memory_range:
         min: 16384
         max: 28332832
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:32.994193Z'
-  - torchscript_onnx_ort:
-      inference_time: 139583.0
-      throughput: 7.164196213005882
-      estimated_peak_memory_range:
-        min: 577536
-        max: 14778160
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 15
-        total_layers: 15
-      job_id: jwgok83dp
+    timestamp: '2024-04-23T18:42:32.816325Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2485.0
+      throughput: 402.4144869215292
+      estimated_peak_memory_range:
+        min: 32768
+        max: 1755936
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 28
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 31
+      job_id: j1p80kexg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 2097.0
+      throughput: 476.87172150691464
+      estimated_peak_memory_range:
+        min: 225280
+        max: 13035320
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 31
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 31
+      job_id: jw56e080g
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:32.994212Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:32.994218Z'
+    timestamp: '2024-04-23T18:42:32.816351Z'
```

## qai_hub_models/models/quicksrnetlarge_quantized/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.quicksrnetlarge_quantized import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.quicksrnetlarge_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/quicksrnetlarge_quantized/export.py

```diff
@@ -34,14 +34,15 @@
     transpose_channel_last_to_first,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "quicksrnetlarge_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "quicksrnetlarge_quantized",
             "QuickSRNetLarge-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -124,15 +131,15 @@
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -140,15 +147,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -166,15 +173,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: QuickSRNetLarge-Quantized
@@ -54,15 +57,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.020831Z'
+    timestamp: '2024-04-23T18:42:32.840385Z'
   - torchscript_onnx_tflite:
       inference_time: 1167.0
       throughput: 856.898029134533
       estimated_peak_memory_range:
         min: 12288
         max: 25644128
       primary_compute_unit: NPU
@@ -77,24 +80,77 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.020859Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.840407Z'
+  - torchscript_onnx_tflite:
+      inference_time: 6024.0
+      throughput: 166.00265604249668
+      estimated_peak_memory_range:
+        min: 40960
+        max: 19668928
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 30
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 33
+      job_id: jz5w2ry65
+      job_status: Passed
+    reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.020865Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.840436Z'
+  - torchscript_onnx_tflite:
+      inference_time: 41995.0
+      throughput: 23.81235861412073
+      estimated_peak_memory_range:
+        min: 1863680
+        max: 4699224
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 30
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 33
+      job_id: j1p31omxg
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:32.840455Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1874.0
+      throughput: 533.6179295624333
+      estimated_peak_memory_range:
+        min: 24576
+        max: 6948872
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 30
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 33
+      job_id: jygzo0lk5
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.020870Z'
+    timestamp: '2024-04-23T18:42:32.840469Z'
```

## qai_hub_models/models/quicksrnetmedium/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.quicksrnetmedium import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.quicksrnetmedium.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/quicksrnetmedium/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "quicksrnetmedium"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "quicksrnetmedium",
             "QuickSRNetMedium",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/quicksrnetmedium/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: QuickSRNetMedium
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.040879Z'
+    timestamp: '2024-04-23T18:42:32.860888Z'
   - torchscript_onnx_tflite:
       inference_time: 871.0
       throughput: 1148.105625717566
       estimated_peak_memory_range:
         min: 16384
         max: 19182544
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.040928Z'
-  - torchscript_onnx_ort:
-      inference_time: 20332.0
-      throughput: 49.18355301987015
-      estimated_peak_memory_range:
-        min: 335872
-        max: 12174960
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 8
-        total_layers: 8
-      job_id: jz5709drg
+    timestamp: '2024-04-23T18:42:32.860931Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1365.0
+      throughput: 732.6007326007326
+      estimated_peak_memory_range:
+        min: 24576
+        max: 16231088
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 14
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 17
+      job_id: jz5708olg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1005.0
+      throughput: 995.0248756218906
+      estimated_peak_memory_range:
+        min: 221184
+        max: 6072368
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 17
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 17
+      job_id: jegnlw1r5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.040945Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.040950Z'
+    timestamp: '2024-04-23T18:42:32.860962Z'
```

## qai_hub_models/models/quicksrnetmedium_quantized/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.quicksrnetmedium_quantized import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.quicksrnetmedium_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/quicksrnetmedium_quantized/export.py

```diff
@@ -34,14 +34,15 @@
     transpose_channel_last_to_first,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "quicksrnetmedium_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "quicksrnetmedium_quantized",
             "QuickSRNetMedium-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -124,15 +131,15 @@
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -140,15 +147,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -166,15 +173,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: QuickSRNetMedium-Quantized
@@ -54,15 +57,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.064857Z'
+    timestamp: '2024-04-23T18:42:32.884809Z'
   - torchscript_onnx_tflite:
       inference_time: 871.0
       throughput: 1148.105625717566
       estimated_peak_memory_range:
         min: 16384
         max: 19479952
       primary_compute_unit: NPU
@@ -77,24 +80,77 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.064887Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.884829Z'
+  - torchscript_onnx_tflite:
+      inference_time: 3381.0
+      throughput: 295.77048210588583
+      estimated_peak_memory_range:
+        min: 12288
+        max: 15175488
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 16
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 19
+      job_id: j1p80kjxg
+      job_status: Passed
+    reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.064893Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.884857Z'
+  - torchscript_onnx_tflite:
+      inference_time: 15536.0
+      throughput: 64.36663233779609
+      estimated_peak_memory_range:
+        min: 1720320
+        max: 4755304
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 16
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 19
+      job_id: jwgondv4p
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:32.884871Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1396.0
+      throughput: 716.3323782234957
+      estimated_peak_memory_range:
+        min: 32768
+        max: 1677424
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 16
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 19
+      job_id: j1gl6qw8g
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.064899Z'
+    timestamp: '2024-04-23T18:42:32.884887Z'
```

## qai_hub_models/models/quicksrnetsmall/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.quicksrnetsmall import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.quicksrnetsmall.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/quicksrnetsmall/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "quicksrnetsmall"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "quicksrnetsmall",
             "QuickSRNetSmall",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/quicksrnetsmall/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: QuickSRNetSmall
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.083745Z'
+    timestamp: '2024-04-23T18:42:32.905242Z'
   - torchscript_onnx_tflite:
       inference_time: 914.0
       throughput: 1094.0919037199126
       estimated_peak_memory_range:
         min: 16384
         max: 18347856
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.083791Z'
-  - torchscript_onnx_ort:
-      inference_time: 11762.0
-      throughput: 85.01955449753443
-      estimated_peak_memory_range:
-        min: 253952
-        max: 10698672
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 5
-        total_layers: 5
-      job_id: j1p80178g
+    timestamp: '2024-04-23T18:42:32.905287Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1327.0
+      throughput: 753.5795026375282
+      estimated_peak_memory_range:
+        min: 28672
+        max: 8134240
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 8
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 11
+      job_id: j1p3vrolg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1021.0
+      throughput: 979.4319294809011
+      estimated_peak_memory_range:
+        min: 249856
+        max: 7951808
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 11
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 11
+      job_id: jlpeen61p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.083807Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.083813Z'
+    timestamp: '2024-04-23T18:42:32.905312Z'
```

## qai_hub_models/models/quicksrnetsmall_quantized/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.quicksrnetsmall_quantized import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.quicksrnetsmall_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/quicksrnetsmall_quantized/export.py

```diff
@@ -34,14 +34,15 @@
     transpose_channel_last_to_first,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "quicksrnetsmall_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "quicksrnetsmall_quantized",
             "QuickSRNetSmall-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -124,15 +131,15 @@
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -140,15 +147,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -166,15 +173,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: QuickSRNetSmall-Quantized
@@ -54,15 +57,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.108943Z'
+    timestamp: '2024-04-23T18:42:32.929166Z'
   - torchscript_onnx_tflite:
       inference_time: 1612.0
       throughput: 620.3473945409429
       estimated_peak_memory_range:
         min: 16384
         max: 18121488
       primary_compute_unit: NPU
@@ -77,24 +80,77 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.108969Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.929186Z'
+  - torchscript_onnx_tflite:
+      inference_time: 3227.0
+      throughput: 309.88534242330337
+      estimated_peak_memory_range:
+        min: 49152
+        max: 15102016
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 10
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 13
+      job_id: jqp4k24vg
+      job_status: Passed
+    reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.108975Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:32.929214Z'
+  - torchscript_onnx_tflite:
+      inference_time: 12108.0
+      throughput: 82.59002312520647
+      estimated_peak_memory_range:
+        min: 5685248
+        max: 13091440
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 10
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 13
+      job_id: j1pvr2w75
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:32.929227Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1388.0
+      throughput: 720.4610951008646
+      estimated_peak_memory_range:
+        min: 24576
+        max: 1828056
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 10
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 13
+      job_id: j0pxnzr15
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.108980Z'
+    timestamp: '2024-04-23T18:42:32.929244Z'
```

## qai_hub_models/models/real_esrgan_general_x4v3/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.real_esrgan_general_x4v3 import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.real_esrgan_general_x4v3.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/real_esrgan_general_x4v3/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "real_esrgan_general_x4v3"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "real_esrgan_general_x4v3",
             "Real-ESRGAN-General-x4v3",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/real_esrgan_general_x4v3/model.py

```diff
@@ -1,14 +1,15 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import os
+import sys
 
 import torch
 
 from qai_hub_models.evaluators.base_evaluators import BaseEvaluator
 from qai_hub_models.evaluators.superres_evaluator import SuperResolutionOutputEvaluator
 from qai_hub_models.utils.asset_loaders import SourceAsRoot
 from qai_hub_models.utils.base_model import BaseModel
@@ -116,17 +117,21 @@
 
                 response = requests.get(weights_url)
                 with open(weights_path, "wb") as file:
                     file.write(response.content)
                 print(f"Weights file downloaded as {weights_path}")
 
         # necessary import. `archs` comes from the realesrgan repo.
-        from realesrgan.archs.srvgg_arch import SRVGGNetCompact
+        # This can be imported only once per session
+        if "basicsr.archs.srvgg_arch" not in sys.modules:
+            import realesrgan.archs.srvgg_arch as srvgg_arch
+        else:
+            srvgg_arch = sys.modules["basicsr.archs.srvgg_arch"]
 
-        realesrgan_model = SRVGGNetCompact(
+        realesrgan_model = srvgg_arch.SRVGGNetCompact(
             num_in_ch=3,
             num_out_ch=3,
             num_feat=64,
             num_conv=32,
             upscale=4,
             act_type="prelu",
         )
```

## qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Real-ESRGAN-General-x4v3
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.136582Z'
+    timestamp: '2024-04-23T18:42:32.949581Z'
   - torchscript_onnx_tflite:
       inference_time: 5369.0
       throughput: 186.25442354255912
       estimated_peak_memory_range:
         min: 20480
         max: 55365360
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.136676Z'
-  - torchscript_onnx_ort:
-      inference_time: 398793.0
-      throughput: 2.507566582161673
-      estimated_peak_memory_range:
-        min: 626688
-        max: 35598960
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 70
-        total_layers: 70
-      job_id: jlpeeyv0p
+    timestamp: '2024-04-23T18:42:32.949640Z'
+  - torchscript_onnx_tflite:
+      inference_time: 7123.0
+      throughput: 140.39028499227854
+      estimated_peak_memory_range:
+        min: 15777792
+        max: 23652120
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 69
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 72
+      job_id: jopr87d95
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 7016.0
+      throughput: 142.53135689851769
+      estimated_peak_memory_range:
+        min: 32768
+        max: 10477536
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 72
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 72
+      job_id: j1p80krxg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.136705Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.136710Z'
+    timestamp: '2024-04-23T18:42:32.949674Z'
```

## qai_hub_models/models/real_esrgan_x4plus/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.real_esrgan_x4plus import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.real_esrgan_x4plus.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/real_esrgan_x4plus/export.py

```diff
@@ -33,14 +33,15 @@
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "real_esrgan_x4plus"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "real_esrgan_x4plus",
             "Real-ESRGAN-x4plus",
             device,
             skip_profiling,
             skip_inferencing,
@@ -104,40 +111,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -148,15 +156,15 @@
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=sample_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/real_esrgan_x4plus/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Real-ESRGAN-x4plus
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.162732Z'
+    timestamp: '2024-04-23T18:42:32.973754Z'
   - torchscript_onnx_qnn:
       inference_time: 50526.0
       throughput: 19.79179036535645
       estimated_peak_memory_range:
         min: 53248
         max: 259398784
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.163018Z'
-  - torchscript_onnx_ort:
-      inference_time: 7662961.0
-      throughput: 0.13049785846489367
+    timestamp: '2024-04-23T18:42:32.973885Z'
+  - torchscript_onnx_qnn:
+      inference_time: 67718.0
+      throughput: 14.767122478513837
       estimated_peak_memory_range:
-        min: 194519040
-        max: 419255440
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 163840
+        max: 107805352
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 1031
         layers_on_gpu: 0
-        layers_on_cpu: 677
-        total_layers: 677
-      job_id: jz5709jrg
+        layers_on_cpu: 0
+        total_layers: 1031
+      job_id: j1p3vr7lg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.163186Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.163195Z'
+    timestamp: '2024-04-23T18:42:32.974009Z'
```

## qai_hub_models/models/regnet/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.regnet import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.regnet.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/regnet/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "regnet"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "regnet",
             "RegNet",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/regnet/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: RegNet
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.184487Z'
+    timestamp: '2024-04-23T18:42:32.991772Z'
   - torchscript_onnx_tflite:
       inference_time: 1616.0
       throughput: 618.8118811881188
       estimated_peak_memory_range:
         min: 12288
         max: 134209840
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.184605Z'
-  - torchscript_onnx_ort:
-      inference_time: 76051.0
-      throughput: 13.149071018132569
-      estimated_peak_memory_range:
-        min: 10616832
-        max: 56250448
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 85
-        total_layers: 85
-      job_id: jqpyrm445
+    timestamp: '2024-04-23T18:42:32.991836Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2329.0
+      throughput: 429.36882782310005
+      estimated_peak_memory_range:
+        min: 24576
+        max: 2315288
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 114
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 114
+      job_id: jvgdemme5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 2130.0
+      throughput: 469.4835680751174
+      estimated_peak_memory_range:
+        min: 12288
+        max: 56502216
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 188
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 188
+      job_id: jo5mqllwp
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.184644Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.184650Z'
+    timestamp: '2024-04-23T18:42:32.991902Z'
```

## qai_hub_models/models/resnet101/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.resnet101 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.resnet101.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/resnet101/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "resnet101"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "resnet101",
             "ResNet101",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/resnet101/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: ResNet101
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.231297Z'
+    timestamp: '2024-04-23T18:42:33.051295Z'
   - torchscript_onnx_tflite:
       inference_time: 2446.0
       throughput: 408.8307440719542
       estimated_peak_memory_range:
         min: 212992
         max: 104476752
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.231479Z'
-  - torchscript_onnx_ort:
-      inference_time: 176170.0
-      throughput: 5.6763353578929445
-      estimated_peak_memory_range:
-        min: 6721536
-        max: 58771664
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 110
-        total_layers: 110
-      job_id: jvgde2rl5
+    timestamp: '2024-04-23T18:42:33.051366Z'
+  - torchscript_onnx_tflite:
+      inference_time: 3443.0
+      throughput: 290.4443799012489
+      estimated_peak_memory_range:
+        min: 24576
+        max: 2329152
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 147
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 147
+      job_id: jep20zqrg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 3473.0
+      throughput: 287.93550244745177
+      estimated_peak_memory_range:
+        min: 622592
+        max: 217592784
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 245
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 245
+      job_id: jn5qedxn5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.231519Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.231525Z'
+    timestamp: '2024-04-23T18:42:33.051462Z'
```

## qai_hub_models/models/resnet101_quantized/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.resnet101_quantized import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.resnet101_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/resnet101_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "resnet101_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "resnet101_quantized",
             "ResNet101Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/resnet101_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: ResNet101Quantized
@@ -84,15 +87,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.272224Z'
+    timestamp: '2024-04-23T18:42:33.075407Z'
   - torchscript_onnx_tflite:
       inference_time: 922.0
       throughput: 1084.5986984815618
       estimated_peak_memory_range:
         min: 16384
         max: 92718400
       primary_compute_unit: NPU
@@ -137,16 +140,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.272344Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:33.075467Z'
+  - torchscript_onnx_tflite:
+      inference_time: 4806.0
+      throughput: 208.07324178110696
+      estimated_peak_memory_range:
+        min: 24576
+        max: 27299616
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 150
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 150
+      job_id: jvgdemjr5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1gl6qnjg
+      job_status: Failed
+    torchscript_onnx_ort:
       inference_time: 53190.0
       throughput: 18.80052641473961
       estimated_peak_memory_range:
         min: 12480512
         max: 88971072
       primary_compute_unit: CPU
       precision: fp32
@@ -158,18 +191,71 @@
       job_id: j1gl6lelg
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.272399Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.075531Z'
+  - torchscript_onnx_tflite:
+      inference_time: 17430.0
+      throughput: 57.37234652897303
+      estimated_peak_memory_range:
+        min: 16384
+        max: 2096256
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 150
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 150
+      job_id: jlpew6v7p
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:33.075558Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1196.0
+      throughput: 836.1204013377926
+      estimated_peak_memory_range:
+        min: 24576
+        max: 2116496
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 150
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 150
+      job_id: jygzo01o5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1433.0
+      throughput: 697.8367062107467
+      estimated_peak_memory_range:
+        min: 61440
+        max: 10820048
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 148
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 148
+      job_id: jlpeen9op
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.272409Z'
+    timestamp: '2024-04-23T18:42:33.075622Z'
```

## qai_hub_models/models/resnet18/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.resnet18 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.resnet18.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/resnet18/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "resnet18"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "resnet18",
             "ResNet18",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/resnet18/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: ResNet18
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.298377Z'
+    timestamp: '2024-04-23T18:42:33.110933Z'
   - torchscript_onnx_tflite:
       inference_time: 987.0
       throughput: 1013.1712259371834
       estimated_peak_memory_range:
         min: 12288
         max: 24202432
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.298441Z'
-  - torchscript_onnx_ort:
-      inference_time: 50603.0
-      throughput: 19.76167420903899
-      estimated_peak_memory_range:
-        min: 3657728
-        max: 22461600
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 26
-        total_layers: 26
-      job_id: jygzonv65
+    timestamp: '2024-04-23T18:42:33.110984Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1376.0
+      throughput: 726.7441860465116
+      estimated_peak_memory_range:
+        min: 20480
+        max: 1963688
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 38
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 38
+      job_id: j2p03xqnp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1485.0
+      throughput: 673.4006734006734
+      estimated_peak_memory_range:
+        min: 16384
+        max: 83668248
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 53
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 53
+      job_id: j1gl6qrmg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.298460Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.298465Z'
+    timestamp: '2024-04-23T18:42:33.111013Z'
```

## qai_hub_models/models/resnet18_quantized/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.resnet18_quantized import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.resnet18_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/resnet18_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "resnet18_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "resnet18_quantized",
             "ResNet18Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/resnet18_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: ResNet18Quantized
@@ -84,15 +87,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.321715Z'
+    timestamp: '2024-04-23T18:42:33.135184Z'
   - torchscript_onnx_tflite:
       inference_time: 351.0
       throughput: 2849.002849002849
       estimated_peak_memory_range:
         min: 12288
         max: 24268608
       primary_compute_unit: NPU
@@ -137,16 +140,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.321775Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:33.135231Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1555.0
+      throughput: 643.0868167202573
+      estimated_peak_memory_range:
+        min: 16384
+        max: 14843920
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 41
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 41
+      job_id: jygzo0kx5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jqpyry0l5
+      job_status: Failed
+    torchscript_onnx_ort:
       inference_time: 11826.0
       throughput: 84.5594452900389
       estimated_peak_memory_range:
         min: 1556480
         max: 29105488
       primary_compute_unit: CPU
       precision: fp32
@@ -158,18 +191,71 @@
       job_id: jvgde20e5
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.321799Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.135269Z'
+  - torchscript_onnx_tflite:
+      inference_time: 7308.0
+      throughput: 136.83634373289544
+      estimated_peak_memory_range:
+        min: 12288
+        max: 6786960
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 41
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 41
+      job_id: jygzjz7zp
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:33.135284Z'
+  - torchscript_onnx_tflite:
+      inference_time: 463.0
+      throughput: 2159.827213822894
+      estimated_peak_memory_range:
+        min: 20480
+        max: 15182520
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 41
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 41
+      job_id: jlpeen3vp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 680.0
+      throughput: 1470.5882352941176
+      estimated_peak_memory_range:
+        min: 24576
+        max: 60765408
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 39
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 39
+      job_id: jo5mqly9p
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.321804Z'
+    timestamp: '2024-04-23T18:42:33.135313Z'
```

## qai_hub_models/models/resnet50/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.resnet50 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.resnet50.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/resnet50/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "resnet50"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "resnet50",
             "ResNet50",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/resnet50/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: ResNet50
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.345425Z'
+    timestamp: '2024-04-23T18:42:33.170270Z'
   - torchscript_onnx_tflite:
       inference_time: 1648.0
       throughput: 606.7961165048544
       estimated_peak_memory_range:
         min: 16384
         max: 69510112
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.345522Z'
-  - torchscript_onnx_ort:
-      inference_time: 87191.0
-      throughput: 11.469073642921861
-      estimated_peak_memory_range:
-        min: 3137536
-        max: 34987136
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 59
-        total_layers: 59
-      job_id: j2p036k6p
+    timestamp: '2024-04-23T18:42:33.170321Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2299.0
+      throughput: 434.97172683775557
+      estimated_peak_memory_range:
+        min: 24576
+        max: 2160472
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 79
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 79
+      job_id: jvgdey1z5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 2343.0
+      throughput: 426.8032437046522
+      estimated_peak_memory_range:
+        min: 626688
+        max: 186221872
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: jopr8m375
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.345551Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.345557Z'
+    timestamp: '2024-04-23T18:42:33.170363Z'
```

## qai_hub_models/models/resnext101/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.resnext101 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.resnext101.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/resnext101/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "resnext101"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "resnext101",
             "ResNeXt101",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/resnext101/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: ResNeXt101
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.501047Z'
+    timestamp: '2024-04-23T18:42:33.282825Z'
   - torchscript_onnx_tflite:
       inference_time: 4816.0
       throughput: 207.64119601328903
       estimated_peak_memory_range:
         min: 20480
         max: 366481792
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.501140Z'
-  - torchscript_onnx_ort:
-      inference_time: 330933.0
-      throughput: 3.0217596915387706
-      estimated_peak_memory_range:
-        min: 14622720
-        max: 74979712
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 110
-        total_layers: 110
-      job_id: j1pv07ej5
+    timestamp: '2024-04-23T18:42:33.282896Z'
+  - torchscript_onnx_tflite:
+      inference_time: 6712.0
+      throughput: 148.98688915375448
+      estimated_peak_memory_range:
+        min: 36864
+        max: 3053288
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 147
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 147
+      job_id: j7gjz6215
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 6586.0
+      throughput: 151.83723048891588
+      estimated_peak_memory_range:
+        min: 16384
+        max: 36067624
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 245
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 245
+      job_id: jmg9j7ym5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.501174Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.501179Z'
+    timestamp: '2024-04-23T18:42:33.282959Z'
```

## qai_hub_models/models/resnext101_quantized/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.resnext101_quantized import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.resnext101_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/resnext101_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "resnext101_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "resnext101_quantized",
             "ResNeXt101Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/resnext101_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: ResNeXt101Quantized
@@ -69,15 +72,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.523763Z'
+    timestamp: '2024-04-23T18:42:33.306938Z'
   - torchscript_onnx_tflite:
       inference_time: 2167.0
       throughput: 461.4674665436087
       estimated_peak_memory_range:
         min: 12288
         max: 262604528
       primary_compute_unit: NPU
@@ -107,16 +110,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.523816Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:33.306993Z'
+  - torchscript_onnx_tflite:
+      inference_time: 10468.0
+      throughput: 95.52923194497517
+      estimated_peak_memory_range:
+        min: 32768
+        max: 199144352
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 150
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 150
+      job_id: jlpee0v8p
+      job_status: Passed
+    torchscript_onnx_ort:
       inference_time: 88885.0
       throughput: 11.250492209034146
       estimated_peak_memory_range:
         min: 8159232
         max: 88001424
       primary_compute_unit: CPU
       precision: fp32
@@ -128,18 +146,56 @@
       job_id: jmg9jdxl5
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.523856Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.307043Z'
+  - torchscript_onnx_tflite:
+      inference_time: 134216.0
+      throughput: 7.450676521428146
+      estimated_peak_memory_range:
+        min: 24576
+        max: 357047544
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 14
+        layers_on_gpu: 125
+        layers_on_cpu: 11
+        total_layers: 150
+      job_id: jmg9yo4q5
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:33.307071Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2909.0
+      throughput: 343.7607425232039
+      estimated_peak_memory_range:
+        min: 16384
+        max: 2753672
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 150
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 150
+      job_id: j1p3vlqxg
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.523862Z'
+    timestamp: '2024-04-23T18:42:33.307097Z'
```

## qai_hub_models/models/resnext50/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.resnext50 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.resnext50.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/resnext50/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "resnext50"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "resnext50",
             "ResNeXt50",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/resnext50/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: ResNeXt50
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.545672Z'
+    timestamp: '2024-04-23T18:42:33.333746Z'
   - torchscript_onnx_tflite:
       inference_time: 1788.0
       throughput: 559.2841163310962
       estimated_peak_memory_range:
         min: 16384
         max: 164107600
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.545745Z'
-  - torchscript_onnx_ort:
-      inference_time: 86849.0
-      throughput: 11.514237354488825
-      estimated_peak_memory_range:
-        min: 11636736
-        max: 43512800
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 59
-        total_layers: 59
-      job_id: jegnlk7r5
+    timestamp: '2024-04-23T18:42:33.333800Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2497.0
+      throughput: 400.4805766920304
+      estimated_peak_memory_range:
+        min: 53248
+        max: 2221936
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 79
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 79
+      job_id: jnp1yk3kp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 2594.0
+      throughput: 385.50501156515037
+      estimated_peak_memory_range:
+        min: 618496
+        max: 68165536
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: j0pxn8mj5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.545770Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.545776Z'
+    timestamp: '2024-04-23T18:42:33.333840Z'
```

## qai_hub_models/models/resnext50_quantized/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.resnext50_quantized import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.resnext50_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/resnext50_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "resnext50_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "resnext50_quantized",
             "ResNeXt50Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/resnext50_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: ResNeXt50Quantized
@@ -69,15 +72,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.568493Z'
+    timestamp: '2024-04-23T18:42:33.357768Z'
   - torchscript_onnx_tflite:
       inference_time: 724.0
       throughput: 1381.2154696132598
       estimated_peak_memory_range:
         min: 12288
         max: 99522896
       primary_compute_unit: NPU
@@ -107,16 +110,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.568546Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:33.357805Z'
+  - torchscript_onnx_tflite:
+      inference_time: 3105.0
+      throughput: 322.061191626409
+      estimated_peak_memory_range:
+        min: 12288
+        max: 54933392
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 82
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 82
+      job_id: jygzoq8z5
+      job_status: Passed
+    torchscript_onnx_ort:
       inference_time: 31790.0
       throughput: 31.456432840515884
       estimated_peak_memory_range:
         min: 8765440
         max: 56053712
       primary_compute_unit: CPU
       precision: fp32
@@ -128,18 +146,56 @@
       job_id: j1p8014xg
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.568576Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.357842Z'
+  - torchscript_onnx_tflite:
+      inference_time: 64556.0
+      throughput: 15.49042691616581
+      estimated_peak_memory_range:
+        min: 0
+        max: 94711912
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 14
+        layers_on_gpu: 57
+        layers_on_cpu: 11
+        total_layers: 82
+      job_id: jnp1wo8kg
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:33.357866Z'
+  - torchscript_onnx_tflite:
+      inference_time: 987.0
+      throughput: 1013.1712259371834
+      estimated_peak_memory_range:
+        min: 24576
+        max: 1688688
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 82
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 82
+      job_id: jogk7woyp
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.568582Z'
+    timestamp: '2024-04-23T18:42:33.357887Z'
```

## qai_hub_models/models/sam/conftest.py

```diff
@@ -1,31 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.sam import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.sam.Model.from_pretrained",
-        return_value=Model.from_pretrained(
-            model_type="vit_b",
-        ),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/sam/export.py

```diff
@@ -10,15 +10,15 @@
 import os
 import warnings
 from pathlib import Path
 from typing import Any, Dict, List, Mapping, Optional, Tuple, cast
 
 import qai_hub as hub
 import torch
-from torch.utils.mobile_optimizer import MobileOptimizerType, optimize_for_mobile
+from torch.utils import mobile_optimizer
 
 from qai_hub_models.models.sam import Model
 from qai_hub_models.utils.args import export_parser, get_model_kwargs
 from qai_hub_models.utils.base_model import BaseModel, TargetRuntime
 from qai_hub_models.utils.compare import torch_inference
 from qai_hub_models.utils.input_spec import make_torch_inputs
 from qai_hub_models.utils.printing import (
@@ -32,14 +32,15 @@
 
 ALL_COMPONENTS = ["SAMDecoder", "SAMEncoder"]
 DEFAULT_COMPONENTS = ["SAMDecoder"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         components: List of sub-components of the model that will be exported.
             Each component is compiled and profiled separately.
             Defaults to ALL_COMPONENTS if not specified.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
@@ -85,14 +88,18 @@
         A Mapping from component_name to a 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "sam"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     component_arg = components
     components = components or DEFAULT_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
@@ -118,36 +125,37 @@
     if "SAMEncoder" in components:
         components_dict["SAMEncoder"] = model.get_sam_encoder()  # type: ignore
 
     compile_jobs: Dict[str, hub.client.CompileJob] = {}
     for component_name, component in components_dict.items():
         # Trace the model
         input_spec = component.get_input_spec()
+        component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
-        source_model = optimize_for_mobile(
+        source_model = mobile_optimizer.optimize_for_mobile(
             source_model,
             optimization_blocklist={
-                MobileOptimizerType.HOIST_CONV_PACKED_PARAMS,  # type: ignore
-                MobileOptimizerType.INSERT_FOLD_PREPACK_OPS,  # type: ignore
-                MobileOptimizerType.CONV_BN_FUSION,  # type: ignore
+                mobile_optimizer.MobileOptimizerType.HOIST_CONV_PACKED_PARAMS,  # type: ignore
+                mobile_optimizer.MobileOptimizerType.INSERT_FOLD_PREPACK_OPS,  # type: ignore
+                mobile_optimizer.MobileOptimizerType.CONV_BN_FUSION,  # type: ignore
             },
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
             target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
-            device=hub.Device(device),
+            device=hub_device,
             name=f"{model_name}_{component_name}",
             options=model_compile_options,
         )
         compile_jobs[component_name] = cast(
             hub.client.CompileJob, submitted_compile_job
         )
 
@@ -157,15 +165,15 @@
         for component_name in components:
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             print(f"Profiling model {component_name} on a hosted device.")
             submitted_profile_job = hub.submit_profile_job(
                 model=compile_jobs[component_name].get_target_model(),
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             profile_jobs[component_name] = cast(
                 hub.client.ProfileJob, submitted_profile_job
             )
 
@@ -179,15 +187,15 @@
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
                 inputs=sample_inputs,
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
```

## qai_hub_models/models/sam/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: SAMDecoder
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.589529Z'
+    timestamp: '2024-04-23T18:42:33.384775Z'
   - torchscript_onnx_tflite:
       inference_time: 33609.0
       throughput: 29.75393495789818
       estimated_peak_memory_range:
         min: 61440
         max: 246507888
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.589602Z'
-  - torchscript_onnx_ort:
-      inference_time: 75754.0
-      throughput: 13.200623069408875
+    timestamp: '2024-04-23T18:42:33.384834Z'
+  - torchscript_onnx_tflite:
+      inference_time: 48295.0
+      throughput: 20.706077233668083
       estimated_peak_memory_range:
-        min: 62357504
-        max: 174265200
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 3977216
+        max: 12384360
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 340
         layers_on_gpu: 0
-        layers_on_cpu: 284
-        total_layers: 284
-      job_id: j1p3v6wlg
+        layers_on_cpu: 0
+        total_layers: 340
+      job_id: jnp1yk7kp
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.589653Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.589658Z'
+    timestamp: '2024-04-23T18:42:33.384881Z'
```

## qai_hub_models/models/sesr_m5/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.sesr_m5 import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.sesr_m5.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/sesr_m5/export.py

```diff
@@ -34,14 +34,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "sesr_m5"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "sesr_m5",
             "SESR-M5",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,43 +112,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -156,15 +164,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/sesr_m5/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: SESR-M5
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.610579Z'
+    timestamp: '2024-04-23T18:42:33.402615Z'
   - torchscript_onnx_tflite:
       inference_time: 1608.0
       throughput: 621.8905472636816
       estimated_peak_memory_range:
         min: 16384
         max: 24474768
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.610631Z'
-  - torchscript_onnx_ort:
-      inference_time: 116893.0
-      throughput: 8.554832196966457
-      estimated_peak_memory_range:
-        min: 31080448
-        max: 45827008
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 17
-        total_layers: 17
-      job_id: jmg9jddl5
+    timestamp: '2024-04-23T18:42:33.402651Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2223.0
+      throughput: 449.842555105713
+      estimated_peak_memory_range:
+        min: 20480
+        max: 8844744
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 22
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 25
+      job_id: j0pxn8d95
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 2148.0
+      throughput: 465.54934823091247
+      estimated_peak_memory_range:
+        min: 229376
+        max: 4684448
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 31
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 31
+      job_id: jep20qvmg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.610651Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.610657Z'
+    timestamp: '2024-04-23T18:42:33.402678Z'
```

## qai_hub_models/models/sesr_m5_quantized/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.sesr_m5_quantized import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.sesr_m5_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/sesr_m5_quantized/export.py

```diff
@@ -32,14 +32,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -60,14 +61,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -81,14 +84,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "sesr_m5_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "sesr_m5_quantized",
             "SESR-M5-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -119,15 +126,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -135,15 +142,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -161,15 +168,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/sesr_m5_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: SESR-M5-Quantized
@@ -54,15 +57,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.648019Z'
+    timestamp: '2024-04-23T18:42:33.426676Z'
   - torchscript_onnx_tflite:
       inference_time: 1067.0
       throughput: 937.207122774133
       estimated_peak_memory_range:
         min: 12288
         max: 21689744
       primary_compute_unit: NPU
@@ -77,24 +80,77 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.648046Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.426696Z'
+  - torchscript_onnx_tflite:
+      inference_time: 3752.0
+      throughput: 266.52452025586354
+      estimated_peak_memory_range:
+        min: 49152
+        max: 14587664
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 13
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 16
+      job_id: jwgok74dp
+      job_status: Passed
+    reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.648052Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.426710Z'
+  - torchscript_onnx_tflite:
+      inference_time: 12810.0
+      throughput: 78.06401249024199
+      estimated_peak_memory_range:
+        min: 5787648
+        max: 13604584
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 12
+        layers_on_gpu: 0
+        layers_on_cpu: 4
+        total_layers: 16
+      job_id: jvgdq6vk5
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:33.426724Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1743.0
+      throughput: 573.7234652897304
+      estimated_peak_memory_range:
+        min: 28672
+        max: 1454440
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 13
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 16
+      job_id: j1p3vlwzg
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.648057Z'
+    timestamp: '2024-04-23T18:42:33.426738Z'
```

## qai_hub_models/models/shufflenet_v2/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.shufflenet_v2 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.shufflenet_v2.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/shufflenet_v2/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "shufflenet_v2"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "shufflenet_v2",
             "Shufflenet-v2",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/shufflenet_v2/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Shufflenet-v2
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.741217Z'
+    timestamp: '2024-04-23T18:42:33.447120Z'
   - torchscript_onnx_tflite:
       inference_time: 855.0
       throughput: 1169.5906432748538
       estimated_peak_memory_range:
         min: 16384
         max: 33284208
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.741324Z'
-  - torchscript_onnx_ort:
-      inference_time: 3481.0
-      throughput: 287.2737719046251
-      estimated_peak_memory_range:
-        min: 282624
-        max: 62466080
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 138
-        total_layers: 138
-      job_id: jep20ee4g
+    timestamp: '2024-04-23T18:42:33.447190Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1291.0
+      throughput: 774.5933384972889
+      estimated_peak_memory_range:
+        min: 20480
+        max: 6952312
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 204
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 204
+      job_id: jz5w201j5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 803.0
+      throughput: 1245.3300124533
+      estimated_peak_memory_range:
+        min: 618496
+        max: 103577192
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 158
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 158
+      job_id: j0pxn8x95
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.741367Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.741372Z'
+    timestamp: '2024-04-23T18:42:33.447245Z'
```

## qai_hub_models/models/shufflenet_v2_quantized/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.shufflenet_v2_quantized import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.shufflenet_v2_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/shufflenet_v2_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "shufflenet_v2_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "shufflenet_v2_quantized",
             "Shufflenet-v2Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/shufflenet_v2_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Shufflenet-v2Quantized
@@ -69,15 +72,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.790579Z'
+    timestamp: '2024-04-23T18:42:33.471327Z'
   - torchscript_onnx_tflite:
       inference_time: 464.0
       throughput: 2155.1724137931033
       estimated_peak_memory_range:
         min: 12288
         max: 22792592
       primary_compute_unit: NPU
@@ -107,24 +110,107 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.790668Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.471384Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1064.0
+      throughput: 939.8496240601504
+      estimated_peak_memory_range:
+        min: 12288
+        max: 16582800
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 207
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 207
+      job_id: jogk7w8op
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jwgok78dp
+      job_status: Failed
+    reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.790674Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.471427Z'
+  - torchscript_onnx_tflite:
+      inference_time: 10090.0
+      throughput: 99.10802775024777
+      estimated_peak_memory_range:
+        min: 12288
+        max: 6455280
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 44
+        layers_on_gpu: 9
+        layers_on_cpu: 154
+        total_layers: 207
+      job_id: jz5w3y9jp
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:33.471459Z'
+  - torchscript_onnx_tflite:
+      inference_time: 667.0
+      throughput: 1499.2503748125937
+      estimated_peak_memory_range:
+        min: 24576
+        max: 2164120
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 207
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 207
+      job_id: jn5qexvm5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 618.0
+      throughput: 1618.1229773462783
+      estimated_peak_memory_range:
+        min: 634880
+        max: 8982056
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 124
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 124
+      job_id: j7gjz6q85
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.790679Z'
+    timestamp: '2024-04-23T18:42:33.471511Z'
```

## qai_hub_models/models/sinet/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.sinet import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.sinet.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/sinet/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "sinet"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "sinet",
             "SINet",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/sinet/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: SINet
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.859879Z'
+    timestamp: '2024-04-23T18:42:33.500477Z'
   - torchscript_onnx_tflite:
       inference_time: 1171.0
       throughput: 853.9709649871904
       estimated_peak_memory_range:
         min: 12288
         max: 25301888
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.859978Z'
-  - torchscript_onnx_ort:
-      inference_time: 10435.0
-      throughput: 95.83133684714902
-      estimated_peak_memory_range:
-        min: 319488
-        max: 87361408
-      primary_compute_unit: CPU
-      precision: fp32
+    timestamp: '2024-04-23T18:42:33.500551Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1823.0
+      throughput: 548.5463521667581
+      estimated_peak_memory_range:
+        min: 24576
+        max: 1974184
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 240
         layers_on_gpu: 0
-        layers_on_cpu: 200
-        total_layers: 200
-      job_id: j7gjzqqx5
+        layers_on_cpu: 0
+        total_layers: 240
+      job_id: jz57014rg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1185.0
+      throughput: 843.8818565400844
+      estimated_peak_memory_range:
+        min: 634880
+        max: 5992992
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 186
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 186
+      job_id: jopr8m0e5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.860021Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.860031Z'
+    timestamp: '2024-04-23T18:42:33.500612Z'
```

## qai_hub_models/models/squeezenet1_1/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.squeezenet1_1 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.squeezenet1_1.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/squeezenet1_1/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "squeezenet1_1"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "squeezenet1_1",
             "SqueezeNet-1_1",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/squeezenet1_1/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: SqueezeNet-1_1
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.903003Z'
+    timestamp: '2024-04-23T18:42:33.524625Z'
   - torchscript_onnx_tflite:
       inference_time: 453.0
       throughput: 2207.5055187637968
       estimated_peak_memory_range:
         min: 12288
         max: 22540768
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.903066Z'
-  - torchscript_onnx_ort:
-      inference_time: 9264.0
-      throughput: 107.94473229706391
-      estimated_peak_memory_range:
-        min: 311296
-        max: 24698816
-      primary_compute_unit: CPU
-      precision: fp32
+    timestamp: '2024-04-23T18:42:33.524666Z'
+  - torchscript_onnx_tflite:
+      inference_time: 672.0
+      throughput: 1488.095238095238
+      estimated_peak_memory_range:
+        min: 12288
+        max: 1757808
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 41
         layers_on_gpu: 0
-        layers_on_cpu: 41
+        layers_on_cpu: 0
         total_layers: 41
-      job_id: jz5w24735
+      job_id: j1pv0ydm5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 718.0
+      throughput: 1392.757660167131
+      estimated_peak_memory_range:
+        min: 618496
+        max: 75568808
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 70
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 70
+      job_id: jnp1ykjlp
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.903088Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.903093Z'
+    timestamp: '2024-04-23T18:42:33.524695Z'
```

## qai_hub_models/models/squeezenet1_1_quantized/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.squeezenet1_1_quantized import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.squeezenet1_1_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/squeezenet1_1_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "squeezenet1_1_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "squeezenet1_1_quantized",
             "SqueezeNet-1_1Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/squeezenet1_1_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: SqueezeNet-1_1Quantized
@@ -84,15 +87,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.926625Z'
+    timestamp: '2024-04-23T18:42:33.548624Z'
   - torchscript_onnx_tflite:
       inference_time: 178.0
       throughput: 5617.9775280898875
       estimated_peak_memory_range:
         min: 12288
         max: 21783424
       primary_compute_unit: NPU
@@ -137,16 +140,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.926679Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:33.548662Z'
+  - torchscript_onnx_tflite:
+      inference_time: 645.0
+      throughput: 1550.3875968992247
+      estimated_peak_memory_range:
+        min: 12288
+        max: 14710928
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 43
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 43
+      job_id: jogk7w2op
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1pv0ylm5
+      job_status: Failed
+    torchscript_onnx_ort:
       inference_time: 3597.0
       throughput: 278.00945232137894
       estimated_peak_memory_range:
         min: 0
         max: 28318256
       primary_compute_unit: CPU
       precision: fp32
@@ -158,18 +191,71 @@
       job_id: jo5mq8mdp
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.926703Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.548701Z'
+  - torchscript_onnx_tflite:
+      inference_time: 4261.0
+      throughput: 234.6866932644919
+      estimated_peak_memory_range:
+        min: 90112
+        max: 1970416
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 43
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 43
+      job_id: jmg9yo4v5
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:33.548716Z'
+  - torchscript_onnx_tflite:
+      inference_time: 246.0
+      throughput: 4065.040650406504
+      estimated_peak_memory_range:
+        min: 12288
+        max: 1876064
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 43
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 43
+      job_id: j1gl69ylg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 507.0
+      throughput: 1972.3865877712033
+      estimated_peak_memory_range:
+        min: 528384
+        max: 12189432
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 47
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 47
+      job_id: jygzoql65
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.926709Z'
+    timestamp: '2024-04-23T18:42:33.548742Z'
```

## qai_hub_models/models/stable_diffusion_quantized/export.py

```diff
@@ -24,14 +24,15 @@
 
 ALL_COMPONENTS = ["TextEncoder_Quantized", "UNet_Quantized", "VAEDecoder_Quantized"]
 DEFAULT_COMPONENTS = ["TextEncoder_Quantized", "VAEDecoder_Quantized", "UNet_Quantized"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     profile_options: str = "",
     **additional_model_kwargs,
@@ -49,14 +50,16 @@
 
     Each of the last three steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         components: List of sub-components of the model that will be exported.
             Each component is compiled and profiled separately.
             Defaults to ALL_COMPONENTS if not specified.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling.
@@ -69,14 +72,18 @@
     Returns:
         A Mapping from component_name to a 2-tuple of:
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "stable_diffusion_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     component_arg = components
     components = components or DEFAULT_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
@@ -120,15 +127,15 @@
         for component_name in components:
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             print(f"Profiling model {component_name} on a hosted device.")
             submitted_profile_job = hub.submit_profile_job(
                 model=uploaded_models[component_name],
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             profile_jobs[component_name] = cast(
                 hub.client.ProfileJob, submitted_profile_job
             )
 
@@ -142,15 +149,15 @@
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
             submitted_inference_job = hub.submit_inference_job(
                 model=uploaded_models[component_name],
                 inputs=sample_inputs,
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
```

## qai_hub_models/models/stylegan2/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.stylegan2 import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.stylegan2.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/stylegan2/export.py

```diff
@@ -33,14 +33,15 @@
     export_without_hub_access,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "stylegan2"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "stylegan2",
             "StyleGAN2",
             device,
             skip_profiling,
             skip_inferencing,
@@ -104,40 +111,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_output output_0"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -148,15 +156,15 @@
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=sample_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/stylegan2/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: StyleGAN2
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:33.993139Z'
+    timestamp: '2024-04-23T18:42:33.583802Z'
   - torchscript_onnx_tflite:
       inference_time: 1012977.0
       throughput: 0.9871892451654875
       estimated_peak_memory_range:
         min: 954945536
         max: 980253632
       primary_compute_unit: CPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:33.993259Z'
-  - torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+    timestamp: '2024-04-23T18:42:33.583878Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1253049.0
+      throughput: 0.7980533881755622
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 941391872
+        max: 2204990360
+      primary_compute_unit: CPU
+      precision: fp32
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 0
-      job_id: j2p036j9p
-      job_status: Failed
+        layers_on_gpu: 78
+        layers_on_cpu: 402
+        total_layers: 480
+      job_id: j0pxn8015
+      job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:33.993275Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:33.993281Z'
+    timestamp: '2024-04-23T18:42:33.583943Z'
```

## qai_hub_models/models/swin_base/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.swin_base import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.swin_base.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/swin_base/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "swin_base"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "swin_base",
             "Swin-Base",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/swin_base/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Swin-Base
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.015592Z'
+    timestamp: '2024-04-23T18:42:33.601688Z'
   - torchscript_onnx_tflite:
       inference_time: 39474.0
       throughput: 25.333130668287986
       estimated_peak_memory_range:
         min: 73728
         max: 512044160
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.015822Z'
-  - torchscript_onnx_ort:
-      inference_time: 316019.0
-      throughput: 3.1643666994706012
+    timestamp: '2024-04-23T18:42:33.601867Z'
+  - torchscript_onnx_tflite:
+      inference_time: 61645.0
+      throughput: 16.221915808256956
       estimated_peak_memory_range:
-        min: 50855936
-        max: 473442224
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 28672
+        max: 3282368
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 1568
         layers_on_gpu: 0
-        layers_on_cpu: 1054
-        total_layers: 1054
-      job_id: jw56ew76g
+        layers_on_cpu: 0
+        total_layers: 1568
+      job_id: jw56e9o0g
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.015972Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.015979Z'
+    timestamp: '2024-04-23T18:42:33.602032Z'
```

## qai_hub_models/models/swin_small/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.swin_small import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.swin_small.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/swin_small/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "swin_small"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "swin_small",
             "Swin-Small",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/swin_small/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Swin-Small
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.038305Z'
+    timestamp: '2024-04-23T18:42:33.619812Z'
   - torchscript_onnx_tflite:
       inference_time: 29579.0
       throughput: 33.80776902532202
       estimated_peak_memory_range:
         min: 45056
         max: 479603376
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.038593Z'
-  - torchscript_onnx_ort:
-      inference_time: 191054.0
-      throughput: 5.234122290033184
+    timestamp: '2024-04-23T18:42:33.619995Z'
+  - torchscript_onnx_tflite:
+      inference_time: 45406.0
+      throughput: 22.023521120556754
       estimated_peak_memory_range:
-        min: 38006784
-        max: 457314976
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 94208
+        max: 3127248
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 1563
         layers_on_gpu: 0
-        layers_on_cpu: 1050
-        total_layers: 1050
-      job_id: jlpeey7op
+        layers_on_cpu: 0
+        total_layers: 1563
+      job_id: jz5701nlg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.038775Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.038781Z'
+    timestamp: '2024-04-23T18:42:33.620167Z'
```

## qai_hub_models/models/swin_tiny/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.swin_tiny import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.swin_tiny.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/swin_tiny/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "swin_tiny"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "swin_tiny",
             "Swin-Tiny",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/swin_tiny/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Swin-Tiny
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.060657Z'
+    timestamp: '2024-04-23T18:42:33.637970Z'
   - torchscript_onnx_tflite:
       inference_time: 18310.0
       throughput: 54.614964500273075
       estimated_peak_memory_range:
         min: 40960
         max: 293649808
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.060798Z'
-  - torchscript_onnx_ort:
-      inference_time: 103881.0
-      throughput: 9.626399437818273
+    timestamp: '2024-04-23T18:42:33.638080Z'
+  - torchscript_onnx_tflite:
+      inference_time: 28405.0
+      throughput: 35.205069530012324
       estimated_peak_memory_range:
-        min: 40472576
-        max: 272448208
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 57344
+        max: 3112384
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 837
         layers_on_gpu: 0
-        layers_on_cpu: 564
-        total_layers: 564
-      job_id: jvgde2dr5
+        layers_on_cpu: 0
+        total_layers: 837
+      job_id: jep20qd4g
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.060892Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.060899Z'
+    timestamp: '2024-04-23T18:42:33.638179Z'
```

## qai_hub_models/models/trocr/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.trocr import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.trocr.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/trocr/export.py

```diff
@@ -30,14 +30,15 @@
 )
 
 ALL_COMPONENTS = ["TrOCREncoder", "TrOCRDecoder"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
@@ -59,14 +60,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         components: List of sub-components of the model that will be exported.
             Each component is compiled and profiled separately.
             Defaults to ALL_COMPONENTS if not specified.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
@@ -83,14 +86,18 @@
         A Mapping from component_name to a 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "trocr"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     component_arg = components
     components = components or ALL_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
@@ -116,27 +123,28 @@
     if "TrOCRDecoder" in components:
         components_dict["TrOCRDecoder"] = model.decoder  # type: ignore
 
     compile_jobs: Dict[str, hub.client.CompileJob] = {}
     for component_name, component in components_dict.items():
         # Trace the model
         input_spec = component.get_input_spec()
+        component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
             target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
-            device=hub.Device(device),
+            device=hub_device,
             name=f"{model_name}_{component_name}",
             options=model_compile_options,
         )
         compile_jobs[component_name] = cast(
             hub.client.CompileJob, submitted_compile_job
         )
 
@@ -146,15 +154,15 @@
         for component_name in components:
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             print(f"Profiling model {component_name} on a hosted device.")
             submitted_profile_job = hub.submit_profile_job(
                 model=compile_jobs[component_name].get_target_model(),
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             profile_jobs[component_name] = cast(
                 hub.client.ProfileJob, submitted_profile_job
             )
 
@@ -168,15 +176,15 @@
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
                 inputs=sample_inputs,
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
```

## qai_hub_models/models/trocr/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: TrOCREncoder
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.085150Z'
+    timestamp: '2024-04-23T18:42:33.655937Z'
   - torchscript_onnx_tflite:
       inference_time: 162590.0
       throughput: 6.1504397564425854
       estimated_peak_memory_range:
         min: 5963776
         max: 327025904
       primary_compute_unit: NPU
@@ -107,46 +104,38 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.085260Z'
-  - torchscript_onnx_ort:
-      inference_time: 329378.0
-      throughput: 3.036025478325814
+    timestamp: '2024-04-23T18:42:33.656015Z'
+  - torchscript_onnx_tflite:
+      inference_time: 216411.0
+      throughput: 4.620837203284491
       estimated_peak_memory_range:
-        min: 0
-        max: 157362896
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 7274496
+        max: 10398120
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 592
         layers_on_gpu: 0
-        layers_on_cpu: 365
-        total_layers: 365
-      job_id: j2p036m9p
+        layers_on_cpu: 0
+        total_layers: 592
+      job_id: jlpee0x1p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.085323Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.085329Z'
+    timestamp: '2024-04-23T18:42:33.656082Z'
 - name: TrOCRDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
       inference_time: 2684.0
       throughput: 372.5782414307005
       estimated_peak_memory_range:
         min: 16384
@@ -178,15 +167,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.085406Z'
+    timestamp: '2024-04-23T18:42:33.656134Z'
   - torchscript_onnx_tflite:
       inference_time: 1948.0
       throughput: 513.347022587269
       estimated_peak_memory_range:
         min: 12288
         max: 192910976
       primary_compute_unit: NPU
@@ -216,39 +205,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.085483Z'
-  - torchscript_onnx_ort:
-      inference_time: 7508.0
-      throughput: 133.19126265316996
+    timestamp: '2024-04-23T18:42:33.656185Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2691.0
+      throughput: 371.6090672612412
       estimated_peak_memory_range:
-        min: 7491584
-        max: 135510624
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 16384
+        max: 2038272
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 370
         layers_on_gpu: 0
-        layers_on_cpu: 300
-        total_layers: 300
-      job_id: j1p801ekg
+        layers_on_cpu: 0
+        total_layers: 370
+      job_id: jygzoqyk5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.085537Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.085542Z'
+    timestamp: '2024-04-23T18:42:33.656227Z'
```

## qai_hub_models/models/unet_segmentation/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.unet_segmentation import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.unet_segmentation.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/unet_segmentation/export.py

```diff
@@ -35,14 +35,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -63,14 +64,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -84,14 +87,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "unet_segmentation"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "unet_segmentation",
             "Unet-Segmentation",
             device,
             skip_profiling,
             skip_inferencing,
@@ -106,43 +113,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -157,15 +165,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/unet_segmentation/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Unet-Segmentation
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.126218Z'
+    timestamp: '2024-04-23T18:42:33.687955Z'
   - torchscript_onnx_tflite:
       inference_time: 112866.0
       throughput: 8.860064146864424
       estimated_peak_memory_range:
         min: 5500928
         max: 359682512
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.126274Z'
-  - torchscript_onnx_ort:
-      inference_time: 11877125.0
-      throughput: 0.08419545975983245
-      estimated_peak_memory_range:
-        min: 1166123008
-        max: 1184789632
-      primary_compute_unit: CPU
-      precision: fp32
+    timestamp: '2024-04-23T18:42:33.687994Z'
+  - torchscript_onnx_tflite:
+      inference_time: 160844.0
+      throughput: 6.2172042475939415
+      estimated_peak_memory_range:
+        min: 323584
+        max: 237497504
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 31
         layers_on_gpu: 0
-        layers_on_cpu: 31
+        layers_on_cpu: 0
         total_layers: 31
-      job_id: j1pv072k5
+      job_id: jw56e900g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 150008.0
+      throughput: 6.666311130073063
+      estimated_peak_memory_range:
+        min: 9900032
+        max: 34159264
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 51
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 51
+      job_id: jo5mq11wp
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.126295Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.126300Z'
+    timestamp: '2024-04-23T18:42:33.688021Z'
```

## qai_hub_models/models/vit/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.vit import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.vit.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/vit/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "vit"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "vit",
             "VIT",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,42 +112,43 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(
         model.to("cpu"), make_torch_inputs(input_spec), check_trace=False
     )
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -155,15 +163,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/vit/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: VIT
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.149872Z'
+    timestamp: '2024-04-23T18:42:33.712224Z'
   - torchscript_onnx_tflite:
       inference_time: 89024.0
       throughput: 11.23292595255212
       estimated_peak_memory_range:
         min: 151552
         max: 407939792
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.149997Z'
-  - torchscript_onnx_ort:
-      inference_time: 370419.0
-      throughput: 2.6996455365410523
+    timestamp: '2024-04-23T18:42:33.712295Z'
+  - torchscript_onnx_tflite:
+      inference_time: 119402.0
+      throughput: 8.375069094320029
       estimated_peak_memory_range:
-        min: 7974912
-        max: 138230016
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 135168
+        max: 4419520
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 535
         layers_on_gpu: 0
-        layers_on_cpu: 285
-        total_layers: 285
-      job_id: jmg9jdow5
+        layers_on_cpu: 0
+        total_layers: 535
+      job_id: jqpyrkk75
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.150063Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.150070Z'
+    timestamp: '2024-04-23T18:42:33.712360Z'
```

## qai_hub_models/models/whisper_base_en/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.whisper_base_en import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.whisper_base_en.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/whisper_base_en/export.py

```diff
@@ -30,14 +30,15 @@
 )
 
 ALL_COMPONENTS = ["WhisperEncoder", "WhisperDecoder"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
@@ -59,14 +60,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         components: List of sub-components of the model that will be exported.
             Each component is compiled and profiled separately.
             Defaults to ALL_COMPONENTS if not specified.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
@@ -83,14 +86,18 @@
         A Mapping from component_name to a 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "whisper_base_en"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     component_arg = components
     components = components or ALL_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
@@ -116,27 +123,28 @@
     if "WhisperDecoder" in components:
         components_dict["WhisperDecoder"] = model.decoder  # type: ignore
 
     compile_jobs: Dict[str, hub.client.CompileJob] = {}
     for component_name, component in components_dict.items():
         # Trace the model
         input_spec = component.get_input_spec()
+        component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
             target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
-            device=hub.Device(device),
+            device=hub_device,
             name=f"{model_name}_{component_name}",
             options=model_compile_options,
         )
         compile_jobs[component_name] = cast(
             hub.client.CompileJob, submitted_compile_job
         )
 
@@ -146,15 +154,15 @@
         for component_name in components:
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             print(f"Profiling model {component_name} on a hosted device.")
             submitted_profile_job = hub.submit_profile_job(
                 model=compile_jobs[component_name].get_target_model(),
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             profile_jobs[component_name] = cast(
                 hub.client.ProfileJob, submitted_profile_job
             )
 
@@ -168,15 +176,15 @@
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
                 inputs=sample_inputs,
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
```

## qai_hub_models/models/whisper_base_en/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: WhisperEncoder
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.173016Z'
+    timestamp: '2024-04-23T18:42:33.730149Z'
   - torchscript_onnx_tflite:
       inference_time: 118628.0
       throughput: 8.42971305256769
       estimated_peak_memory_range:
         min: 36814848
         max: 61467824
       primary_compute_unit: GPU
@@ -107,46 +104,38 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.173102Z'
-  - torchscript_onnx_ort:
-      inference_time: 5372639.0
-      throughput: 0.186128269552449
+    timestamp: '2024-04-23T18:42:33.730201Z'
+  - torchscript_onnx_tflite:
+      inference_time: 157798.0
+      throughput: 6.337215934295745
       estimated_peak_memory_range:
-        min: 165142528
-        max: 204558448
+        min: 25370624
+        max: 124671888
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 7
-        layers_on_cpu: 6
-        total_layers: 13
-      job_id: jep20eorg
+        layers_on_gpu: 303
+        layers_on_cpu: 0
+        total_layers: 303
+      job_id: jlpee001p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.173121Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.173126Z'
+    timestamp: '2024-04-23T18:42:33.730243Z'
 - name: WhisperDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
       inference_time: 13793.0
       throughput: 72.50054375407815
       estimated_peak_memory_range:
         min: 5775360
@@ -178,15 +167,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.173228Z'
+    timestamp: '2024-04-23T18:42:33.730305Z'
   - torchscript_onnx_tflite:
       inference_time: 10194.0
       throughput: 98.09691975671964
       estimated_peak_memory_range:
         min: 3768320
         max: 98615936
       primary_compute_unit: NPU
@@ -216,39 +205,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.173327Z'
-  - torchscript_onnx_ort:
-      inference_time: 38739.0
-      throughput: 25.813779395441287
+    timestamp: '2024-04-23T18:42:33.730361Z'
+  - torchscript_onnx_tflite:
+      inference_time: 13928.0
+      throughput: 71.79781734635267
       estimated_peak_memory_range:
-        min: 43307008
-        max: 164573664
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 5758976
+        max: 8442936
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 447
         layers_on_gpu: 0
-        layers_on_cpu: 294
-        total_layers: 294
-      job_id: jqpyrm885
+        layers_on_cpu: 2
+        total_layers: 449
+      job_id: jygzoqqk5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.173391Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.173396Z'
+    timestamp: '2024-04-23T18:42:33.730409Z'
```

## qai_hub_models/models/whisper_small_en/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.whisper_small_en import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.whisper_small_en.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/whisper_small_en/export.py

```diff
@@ -30,14 +30,15 @@
 )
 
 ALL_COMPONENTS = ["WhisperEncoder", "WhisperDecoder"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
@@ -59,14 +60,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         components: List of sub-components of the model that will be exported.
             Each component is compiled and profiled separately.
             Defaults to ALL_COMPONENTS if not specified.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
@@ -83,14 +86,18 @@
         A Mapping from component_name to a 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "whisper_small_en"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     component_arg = components
     components = components or ALL_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
@@ -116,27 +123,28 @@
     if "WhisperDecoder" in components:
         components_dict["WhisperDecoder"] = model.decoder  # type: ignore
 
     compile_jobs: Dict[str, hub.client.CompileJob] = {}
     for component_name, component in components_dict.items():
         # Trace the model
         input_spec = component.get_input_spec()
+        component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
             target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
-            device=hub.Device(device),
+            device=hub_device,
             name=f"{model_name}_{component_name}",
             options=model_compile_options,
         )
         compile_jobs[component_name] = cast(
             hub.client.CompileJob, submitted_compile_job
         )
 
@@ -146,15 +154,15 @@
         for component_name in components:
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             print(f"Profiling model {component_name} on a hosted device.")
             submitted_profile_job = hub.submit_profile_job(
                 model=compile_jobs[component_name].get_target_model(),
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             profile_jobs[component_name] = cast(
                 hub.client.ProfileJob, submitted_profile_job
             )
 
@@ -168,15 +176,15 @@
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
                 inputs=sample_inputs,
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
```

## qai_hub_models/models/whisper_small_en/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: WhisperEncoder
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.211429Z'
+    timestamp: '2024-04-23T18:42:33.762329Z'
   - torchscript_onnx_tflite:
       inference_time: 465622.0
       throughput: 2.1476648440151025
       estimated_peak_memory_range:
         min: 110800896
         max: 143440272
       primary_compute_unit: GPU
@@ -107,46 +104,38 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.211536Z'
-  - torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+    timestamp: '2024-04-23T18:42:33.762404Z'
+  - torchscript_onnx_tflite:
+      inference_time: 602366.0
+      throughput: 1.66012025911157
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 72904704
+        max: 522853520
+      primary_compute_unit: GPU
+      precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 0
+        layers_on_gpu: 585
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: j1pv07mk5
-      job_status: Failed
+        total_layers: 585
+      job_id: j2p038w9p
+      job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.211553Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.211558Z'
+    timestamp: '2024-04-23T18:42:33.762472Z'
 - name: WhisperDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
       inference_time: 45614.0
       throughput: 21.92309378699522
       estimated_peak_memory_range:
         min: 16830464
@@ -178,15 +167,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.211699Z'
+    timestamp: '2024-04-23T18:42:33.762570Z'
   - torchscript_onnx_tflite:
       inference_time: 34559.0
       throughput: 28.936022454353424
       estimated_peak_memory_range:
         min: 15560704
         max: 1589538480
       primary_compute_unit: NPU
@@ -216,39 +205,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.211846Z'
-  - torchscript_onnx_ort:
-      inference_time: 97060.0
-      throughput: 10.302905419328251
+    timestamp: '2024-04-23T18:42:33.762666Z'
+  - torchscript_onnx_tflite:
+      inference_time: 45957.0
+      throughput: 21.75947080966991
       estimated_peak_memory_range:
-        min: 136572928
-        max: 369089808
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 16830464
+        max: 19552208
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 879
         layers_on_gpu: 0
-        layers_on_cpu: 582
-        total_layers: 582
-      job_id: j7gjzqyv5
+        layers_on_cpu: 2
+        total_layers: 881
+      job_id: j1p80dnkg
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.211956Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.211964Z'
+    timestamp: '2024-04-23T18:42:33.762757Z'
```

## qai_hub_models/models/whisper_tiny_en/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.whisper_tiny_en import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.whisper_tiny_en.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/whisper_tiny_en/export.py

```diff
@@ -30,14 +30,15 @@
 )
 
 ALL_COMPONENTS = ["WhisperEncoder", "WhisperDecoder"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
@@ -59,14 +60,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         components: List of sub-components of the model that will be exported.
             Each component is compiled and profiled separately.
             Defaults to ALL_COMPONENTS if not specified.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
@@ -83,14 +86,18 @@
         A Mapping from component_name to a 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "whisper_tiny_en"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     component_arg = components
     components = components or ALL_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
@@ -116,27 +123,28 @@
     if "WhisperDecoder" in components:
         components_dict["WhisperDecoder"] = model.decoder  # type: ignore
 
     compile_jobs: Dict[str, hub.client.CompileJob] = {}
     for component_name, component in components_dict.items():
         # Trace the model
         input_spec = component.get_input_spec()
+        component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
             target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
-            device=hub.Device(device),
+            device=hub_device,
             name=f"{model_name}_{component_name}",
             options=model_compile_options,
         )
         compile_jobs[component_name] = cast(
             hub.client.CompileJob, submitted_compile_job
         )
 
@@ -146,15 +154,15 @@
         for component_name in components:
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             print(f"Profiling model {component_name} on a hosted device.")
             submitted_profile_job = hub.submit_profile_job(
                 model=compile_jobs[component_name].get_target_model(),
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             profile_jobs[component_name] = cast(
                 hub.client.ProfileJob, submitted_profile_job
             )
 
@@ -168,15 +176,15 @@
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
                 inputs=sample_inputs,
-                device=hub.Device(device),
+                device=hub_device,
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
```

## qai_hub_models/models/whisper_tiny_en/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: WhisperEncoder
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.249472Z'
+    timestamp: '2024-04-23T18:42:33.794639Z'
   - torchscript_onnx_tflite:
       inference_time: 52682.0
       throughput: 18.981815420826848
       estimated_peak_memory_range:
         min: 0
         max: 28255008
       primary_compute_unit: GPU
@@ -107,46 +104,38 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.249530Z'
-  - torchscript_onnx_ort:
-      inference_time: 2208077.0
-      throughput: 0.4528827572589181
+    timestamp: '2024-04-23T18:42:33.794681Z'
+  - torchscript_onnx_tflite:
+      inference_time: 67311.0
+      throughput: 14.856412770572417
       estimated_peak_memory_range:
-        min: 123895808
-        max: 156042800
+        min: 17125376
+        max: 63332656
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 5
-        layers_on_cpu: 4
-        total_layers: 9
-      job_id: j0pxnxr35
+        layers_on_gpu: 209
+        layers_on_cpu: 0
+        total_layers: 209
+      job_id: jygzoq1o5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.249548Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.249553Z'
+    timestamp: '2024-04-23T18:42:33.794711Z'
 - name: WhisperDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
       inference_time: 7115.0
       throughput: 140.54813773717498
       estimated_peak_memory_range:
         min: 2977792
@@ -178,15 +167,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.249619Z'
+    timestamp: '2024-04-23T18:42:33.794757Z'
   - torchscript_onnx_tflite:
       inference_time: 5479.0
       throughput: 182.5150574922431
       estimated_peak_memory_range:
         min: 2871296
         max: 232253952
       primary_compute_unit: NPU
@@ -216,39 +205,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.249684Z'
-  - torchscript_onnx_ort:
-      inference_time: 18647.0
-      throughput: 53.62792942564488
+    timestamp: '2024-04-23T18:42:33.794805Z'
+  - torchscript_onnx_tflite:
+      inference_time: 7148.0
+      throughput: 139.89927252378288
       estimated_peak_memory_range:
-        min: 22511616
-        max: 106667856
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 2977792
+        max: 5388280
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 303
         layers_on_gpu: 0
-        layers_on_cpu: 198
-        total_layers: 198
-      job_id: jo5mq8kdp
+        layers_on_cpu: 2
+        total_layers: 305
+      job_id: jz5w20j35
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.249724Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.249729Z'
+    timestamp: '2024-04-23T18:42:33.794844Z'
```

## qai_hub_models/models/wideresnet50/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.wideresnet50 import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.wideresnet50.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/wideresnet50/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "wideresnet50"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "wideresnet50",
             "WideResNet50",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/wideresnet50/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: WideResNet50
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.287009Z'
+    timestamp: '2024-04-23T18:42:33.826943Z'
   - torchscript_onnx_tflite:
       inference_time: 3655.0
       throughput: 273.59781121751024
       estimated_peak_memory_range:
         min: 16384
         max: 97733152
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.287087Z'
-  - torchscript_onnx_ort:
-      inference_time: 244281.0
-      throughput: 4.093646251652809
-      estimated_peak_memory_range:
-        min: 4247552
-        max: 41412368
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 59
-        total_layers: 59
-      job_id: jogk780wp
+    timestamp: '2024-04-23T18:42:33.826991Z'
+  - torchscript_onnx_tflite:
+      inference_time: 4907.0
+      throughput: 203.79050336254332
+      estimated_peak_memory_range:
+        min: 28672
+        max: 2415760
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 79
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 79
+      job_id: j2p038n9p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 5790.0
+      throughput: 172.71157167530225
+      estimated_peak_memory_range:
+        min: 622592
+        max: 209332032
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: jw56e9k6g
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.287114Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.287120Z'
+    timestamp: '2024-04-23T18:42:33.827030Z'
```

## qai_hub_models/models/wideresnet50_quantized/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.wideresnet50_quantized import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.wideresnet50_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/wideresnet50_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "wideresnet50_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "wideresnet50_quantized",
             "WideResNet50-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image_tensor"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image_tensor", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/wideresnet50_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: WideResNet50-Quantized
@@ -84,15 +87,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.311033Z'
+    timestamp: '2024-04-23T18:42:33.850968Z'
   - torchscript_onnx_tflite:
       inference_time: 1351.0
       throughput: 740.1924500370096
       estimated_peak_memory_range:
         min: 12288
         max: 55206416
       primary_compute_unit: NPU
@@ -137,16 +140,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.311102Z'
-  - torchscript_onnx_ort:
+    timestamp: '2024-04-23T18:42:33.851014Z'
+  - torchscript_onnx_tflite:
+      inference_time: 8152.0
+      throughput: 122.6692836113837
+      estimated_peak_memory_range:
+        min: 12288
+        max: 25276096
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 82
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 82
+      job_id: j1pv0yxk5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jw56ex4yg
+      job_status: Failed
+    torchscript_onnx_ort:
       inference_time: 75852.0
       throughput: 13.183568000843747
       estimated_peak_memory_range:
         min: 4431872
         max: 54054544
       primary_compute_unit: CPU
       precision: fp32
@@ -158,18 +191,71 @@
       job_id: j7gjzq8v5
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.311130Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.851058Z'
+  - torchscript_onnx_tflite:
+      inference_time: 24077.0
+      throughput: 41.533413631266356
+      estimated_peak_memory_range:
+        min: 45056
+        max: 2559568
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 82
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 82
+      job_id: jnp1wo8lg
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:33.851077Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1831.0
+      throughput: 546.1496450027307
+      estimated_peak_memory_range:
+        min: 32768
+        max: 1466192
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 82
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 82
+      job_id: j2p038q9p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 2151.0
+      throughput: 464.9000464900046
+      estimated_peak_memory_range:
+        min: 622592
+        max: 7136072
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 80
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 80
+      job_id: jqp4k601g
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.311135Z'
+    timestamp: '2024-04-23T18:42:33.851108Z'
```

## qai_hub_models/models/xlsr/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.xlsr import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.xlsr.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/xlsr/export.py

```diff
@@ -34,14 +34,15 @@
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "xlsr"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "xlsr",
             "XLSR",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,43 +112,44 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -156,15 +164,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/xlsr/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: XLSR
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.334232Z'
+    timestamp: '2024-04-23T18:42:33.886071Z'
   - torchscript_onnx_tflite:
       inference_time: 1833.0
       throughput: 545.5537370430987
       estimated_peak_memory_range:
         min: 16384
         max: 19549104
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.334283Z'
-  - torchscript_onnx_ort:
-      inference_time: 14457.0
-      throughput: 69.17064397869544
-      estimated_peak_memory_range:
-        min: 348160
-        max: 14558736
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 14
-        total_layers: 14
-      job_id: jz5w24rm5
+    timestamp: '2024-04-23T18:42:33.886105Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2709.0
+      throughput: 369.139904023625
+      estimated_peak_memory_range:
+        min: 6631424
+        max: 8101008
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 13
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 16
+      job_id: jmg9jrn85
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 963.0
+      throughput: 1038.4215991692627
+      estimated_peak_memory_range:
+        min: 212992
+        max: 33066344
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 21
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 21
+      job_id: jqp4k7r1g
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.334301Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.334307Z'
+    timestamp: '2024-04-23T18:42:33.886129Z'
```

## qai_hub_models/models/xlsr_quantized/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.xlsr_quantized import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.xlsr_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/xlsr_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_last_to_first,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "xlsr_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "xlsr_quantized",
             "XLSR-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -123,15 +130,15 @@
         + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -139,15 +146,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -165,15 +172,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/xlsr_quantized/perf.yaml

```diff
@@ -5,16 +5,18 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -24,14 +26,15 @@
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: XLSR-Quantized
@@ -54,15 +57,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.357060Z'
+    timestamp: '2024-04-23T18:42:33.910002Z'
   - torchscript_onnx_tflite:
       inference_time: 1209.0
       throughput: 827.129859387924
       estimated_peak_memory_range:
         min: 53248
         max: 20193472
       primary_compute_unit: NPU
@@ -77,24 +80,77 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.357086Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.910020Z'
+  - torchscript_onnx_tflite:
+      inference_time: 3053.0
+      throughput: 327.54667540124467
+      estimated_peak_memory_range:
+        min: 57344
+        max: 15609680
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 16
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 19
+      job_id: jopr8r375
+      job_status: Passed
+    reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.357093Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.910035Z'
+  - torchscript_onnx_tflite:
+      inference_time: 15998.0
+      throughput: 62.50781347668458
+      estimated_peak_memory_range:
+        min: 45056
+        max: 17827664
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 5
+        layers_on_gpu: 9
+        layers_on_cpu: 5
+        total_layers: 19
+      job_id: jvgdq6vl5
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-04-23T18:42:33.910050Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1313.0
+      throughput: 761.6146230007616
+      estimated_peak_memory_range:
+        min: 28672
+        max: 5004672
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 16
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 19
+      job_id: j2p03w0np
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.357098Z'
+    timestamp: '2024-04-23T18:42:33.910063Z'
```

## qai_hub_models/models/yolov6/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.yolov6 import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.yolov6.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/yolov6/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "yolov6"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "yolov6",
             "Yolo-v6",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/yolov6/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Yolo-v6
@@ -84,15 +81,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.376242Z'
+    timestamp: '2024-04-23T18:42:33.930548Z'
   - torchscript_onnx_tflite:
       inference_time: 5649.0
       throughput: 177.02248185519562
       estimated_peak_memory_range:
         min: 16384
         max: 82704608
       primary_compute_unit: NPU
@@ -137,39 +134,46 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.376351Z'
-  - torchscript_onnx_ort:
-      inference_time: 126965.0
-      throughput: 7.876186350569054
-      estimated_peak_memory_range:
-        min: 0
-        max: 67148048
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 149
-        total_layers: 149
-      job_id: jopr8w775
+    timestamp: '2024-04-23T18:42:33.930616Z'
+  - torchscript_onnx_tflite:
+      inference_time: 7952.0
+      throughput: 125.75452716297787
+      estimated_peak_memory_range:
+        min: 217088
+        max: 3444928
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 182
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 182
+      job_id: jn5qenqo5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 6878.0
+      throughput: 145.39110206455365
+      estimated_peak_memory_range:
+        min: 4952064
+        max: 19343808
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 229
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 229
+      job_id: j7gjz9ve5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.376390Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.376396Z'
+    timestamp: '2024-04-23T18:42:33.930670Z'
```

## qai_hub_models/models/yolov7/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.yolov7 import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.yolov7.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/yolov7/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "yolov7"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "yolov7",
             "Yolo-v7",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,40 +112,41 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -153,15 +161,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/yolov7/model.py

```diff
@@ -32,20 +32,22 @@
 
     def __init__(
         self,
         yolov7_feature_extractor: torch.nn.Module,
         yolov7_detector: torch.nn.Module,
         include_postprocessing: bool = True,
         split_output: bool = False,
+        class_dtype: torch.dtype = torch.float32,
     ) -> None:
         super().__init__()
         self.yolov7_feature_extractor = yolov7_feature_extractor
         self.yolov7_detector = yolov7_detector
         self.include_postprocessing = include_postprocessing
         self.split_output = split_output
+        self.class_dtype = class_dtype
 
     # All image input spatial dimensions should be a multiple of this stride.
     STRIDE_MULTIPLE = 32
 
     def get_evaluator(self) -> BaseEvaluator:
         return DetectionEvaluator(640, 640)
 
@@ -127,15 +129,17 @@
         detector_output = self.yolov7_detector(feature_extraction_output)
 
         if not self.include_postprocessing:
             if self.split_output:
                 return detector_output
             return torch.cat(detector_output, -1)
 
-        return detect_postprocess_split_input(*detector_output)
+        return detect_postprocess_split_input(
+            *detector_output, class_dtype=self.class_dtype
+        )
 
     @staticmethod
     def get_input_spec(
         batch_size: int = 1,
         num_channels: int = 3,
         height: int = 640,
         width: int = 640,
```

## qai_hub_models/models/yolov7/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: Yolo-v7
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.400111Z'
+    timestamp: '2024-04-23T18:42:33.954673Z'
   - torchscript_onnx_tflite:
       inference_time: 16244.0
       throughput: 61.56119182467373
       estimated_peak_memory_range:
         min: 40960
         max: 202538080
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.400202Z'
-  - torchscript_onnx_ort:
-      inference_time: 157265.0
-      throughput: 6.358693924267955
+    timestamp: '2024-04-23T18:42:33.954727Z'
+  - torchscript_onnx_tflite:
+      inference_time: 20857.0
+      throughput: 47.94553387351968
       estimated_peak_memory_range:
-        min: 33193984
-        max: 124688816
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 9539584
+        max: 12396608
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 292
         layers_on_gpu: 0
-        layers_on_cpu: 212
-        total_layers: 212
-      job_id: jogk78knp
+        layers_on_cpu: 21
+        total_layers: 313
+      job_id: jvgdekxz5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.400247Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.400256Z'
+    timestamp: '2024-04-23T18:42:33.954767Z'
```

## qai_hub_models/models/yolov7_quantized/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.yolov7_quantized import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.yolov7_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/yolov7_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "yolov7_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "yolov7_quantized",
             "Yolo-v7-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
@@ -196,14 +203,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/yolov7_quantized/model.py

```diff
@@ -22,15 +22,15 @@
 
 from qai_hub_models.models.yolov7.model import DEFAULT_WEIGHTS, YoloV7
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 from qai_hub_models.utils.quantization_aimet import tie_observers
 
 MODEL_ID = __name__.split(".")[-2]
-MODEL_ASSET_VERSION = 4
+MODEL_ASSET_VERSION = 6
 DEFAULT_ENCODINGS = "yolov7_quantized_encodings.json"
 
 
 class YoloV7Quantizable(AIMETQuantizableMixin, YoloV7):
     """Exportable Quantized YoloV7 bounding box detector, end-to-end."""
 
     def __init__(
@@ -51,14 +51,15 @@
     ) -> "YoloV7Quantizable":
         """Load YoloV7 from a weightfile created by the source YoloV7 repository."""
         fp16_model = YoloV7.from_pretrained(
             weights_name,
             include_postprocessing=include_postprocessing,
             split_output=True,
         )
+        fp16_model.class_dtype = torch.int8
 
         input_shape = cls.get_input_spec()["image"][0]
 
         model = prepare_model(fp16_model)
         equalize_model(model, input_shape)
 
         sim = QuantizationSimModel(
@@ -84,44 +85,10 @@
         return final_model
 
     def forward(self, image: torch.Tensor):
         """
         Run YoloV7Quantizable on `image`, and produce a
             predicted set of bounding boxes and associated class probabilities.
 
-        Parameters:
-            image: Pixel values pre-processed for encoder consumption.
-                   Range: float[0, 1]
-                   3-channel Color Space: BGR
-
-        Returns:
-            If self.include_postprocessing:
-                boxes: torch.Tensor
-                    Bounding box locations.  Shape [batch, num preds, 4] where 4 == (center_x, center_y, w, h)
-                scores: torch.Tensor
-                    class scores multiplied by confidence: Shape is [batch, num_preds]
-                class_idx: torch.tensor
-                    Shape is [batch, num_preds] where the last dim is the index of the most probable class of the prediction.
-
-
-            else if self.split_output:
-                output_xy: torch.Tensor
-                    Shape is [batch, num_preds, 2]
-                        where, 2 is [x_center, y_center] (box_coordinates)
-
-                output_wh: torch.Tensor
-                    Shape is [batch, num_preds, 2]
-                        where, 2 is [width, height] (box_size)
-
-                output_scores: torch.Tensor
-                    Shape is [batch, num_preds, j]
-                        where j is [confidence (1 element) , # of classes elements]
-
-
-            else:
-                detector_output: torch.Tensor
-                    Shape is [batch, num_preds, k]
-                        where, k = # of classes + 5
-                        k is structured as follows [box_coordinates (4) , conf (1) , # of classes]
-                        and box_coordinates are [x_center, y_center, w, h]
+        See YoloV7 model for details.
         """
         return self.model(image)
```

## qai_hub_models/models/yolov7_quantized/perf.yaml

```diff
@@ -69,15 +69,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T23:12:58.540077Z'
+    timestamp: '2024-04-23T18:42:33.972519Z'
   - torchscript_onnx_tflite:
       inference_time: 4059.0
       throughput: 246.3661000246366
       estimated_peak_memory_range:
         min: 40960
         max: 67566064
       primary_compute_unit: NPU
@@ -107,24 +107,54 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T23:12:58.540292Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.972581Z'
+  - torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1p80nqog
+      job_status: Failed
+    reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-16T23:12:58.540306Z'
-  - reference_device_info:
+    timestamp: '2024-04-23T18:42:33.972595Z'
+  - torchscript_onnx_qnn:
+      inference_time: 5978.0
+      throughput: 167.2800267648043
+      estimated_peak_memory_range:
+        min: 4939776
+        max: 15407880
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 220
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 220
+      job_id: j2p03wznp
+      job_status: Passed
+    reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T23:12:58.540318Z'
+    timestamp: '2024-04-23T18:42:33.972626Z'
```

## qai_hub_models/models/yolov7_quantized/test.py

```diff
@@ -23,15 +23,15 @@
 )
 
 
 @skip_clone_repo_check
 def test_task():
     image = load_image(IMAGE_ADDRESS)
     app = YoloV7DetectionApp(
-        YoloV7Quantizable.from_pretrained(), nms_score_threshold=0.4
+        YoloV7Quantizable.from_pretrained(), nms_score_threshold=0.5
     )
     boxes = app.predict_boxes_from_image(image, raw_output=True)[0][0].numpy()
     boxes_gt = load_numpy(GT_BOXES)
     boxes = sorted(boxes, key=lambda box: box[0])
     boxes_gt = sorted(boxes_gt, key=lambda box: box[0])
     assert len(boxes) == len(boxes_gt)
     ious = [get_iou(box, box_gt) for box, box_gt in zip(boxes, boxes_gt)]
```

## qai_hub_models/models/yolov8_det/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.yolov8_det import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.yolov8_det.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/yolov8_det/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "yolov8_det"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "yolov8_det",
             "YOLOv8-Detection",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,42 +112,43 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(
         model.to("cpu"), make_torch_inputs(input_spec), check_trace=False
     )
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -155,15 +163,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
@@ -189,14 +197,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/yolov8_det/model.py

```diff
@@ -203,8 +203,12 @@
         boxes = box_transform_xywh2xyxy_split_input(boxes[..., 0:2], boxes[..., 2:4])
     else:
         boxes = transform_box_layout_xywh2xyxy(boxes)
 
     # Get class ID of most likely score.
     scores, class_idx = torch.max(scores, -1, keepdim=False)
 
-    return boxes, scores, class_idx
+    # Quantized model runtime doesn't like int32 outputs, so cast class idx to int8.
+    # This is a no-op for coco models, but for datasets with >128 classes, this
+    # should be int32 for the unquantized model.
+    class_dtype = torch.int8 if use_quantized_postprocessing else torch.float32
+    return boxes, scores, class_idx.to(class_dtype)
```

## qai_hub_models/models/yolov8_det_quantized/conftest.py

```diff
@@ -1,29 +1,39 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.yolov8_det_quantized import Model
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 
-@pytest.fixture(autouse=True)
-@skip_clone_repo_check
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.yolov8_det_quantized.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        @skip_clone_repo_check
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/yolov8_det_quantized/export.py

```diff
@@ -33,14 +33,15 @@
     transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -61,14 +62,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -82,14 +85,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "yolov8_det_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "yolov8_det_quantized",
             "YOLOv8-Detection-Quantized",
             device,
             skip_profiling,
             skip_inferencing,
@@ -120,15 +127,15 @@
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         calibration_data=quant_calibration_data,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
@@ -136,15 +143,15 @@
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -162,15 +169,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
@@ -196,14 +203,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/yolov8_det_quantized/perf.yaml

```diff
@@ -4,17 +4,14 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
-  - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,16 +20,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
-  - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: YOLOv8-Detection-Quantized
   performance_metrics:
@@ -69,15 +64,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T23:12:41.254243Z'
+    timestamp: '2024-04-23T18:42:34.010775Z'
   - torchscript_onnx_tflite:
       inference_time: 1422.0
       throughput: 703.2348804500704
       estimated_peak_memory_range:
         min: 12288
         max: 49561728
       primary_compute_unit: NPU
@@ -107,24 +102,8 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T23:12:41.254553Z'
-  - reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T23:12:41.254570Z'
-  - reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs8550
-    timestamp: '2024-04-16T23:12:41.254585Z'
+    timestamp: '2024-04-23T18:42:34.010850Z'
```

## qai_hub_models/models/yolov8_seg/conftest.py

```diff
@@ -1,27 +1,37 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 # THIS FILE WAS AUTO-GENERATED. DO NOT EDIT MANUALLY.
 
 import inspect
-from unittest.mock import patch
 
 import pytest
 
 from qai_hub_models.models.yolov8_seg import Model
 
 
-@pytest.fixture(autouse=True)
-def mock_from_pretrained():
-    """
-    Model.from_pretrained() can be slow. Invoke it once and cache it so all invocations
-    across all tests return the cached instance of the model.
-    """
-    sig = inspect.signature(Model.from_pretrained)
-    mock = patch(
-        "qai_hub_models.models.yolov8_seg.Model.from_pretrained",
-        return_value=Model.from_pretrained(),
-    )
-    mock_obj = mock.start()
-    mock_obj.__signature__ = sig
+# Instantiate the model only once for all tests.
+# Mock from_pretrained to always return the initialized model.
+# This speeds up tests and limits memory leaks.
+@pytest.fixture(scope="module", autouse=True)
+def cached_from_pretrained():
+    with pytest.MonkeyPatch.context() as mp:
+        pretrained_cache = {}
+        from_pretrained = Model.from_pretrained
+        sig = inspect.signature(from_pretrained)
+
+        def _cached_from_pretrained(*args, **kwargs):
+            cache_key = str(args) + str(kwargs)
+            model = pretrained_cache.get(cache_key, None)
+            if model:
+                return model
+            else:
+                model = from_pretrained(*args, **kwargs)
+                pretrained_cache[cache_key] = model
+                return model
+
+        _cached_from_pretrained.__signature__ = sig
+
+        mp.setattr(Model, "from_pretrained", _cached_from_pretrained)
+        yield mp
```

## qai_hub_models/models/yolov8_seg/export.py

```diff
@@ -34,14 +34,15 @@
     export_without_hub_access,
     transpose_channel_first_to_last,
 )
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
+    chipset: Optional[str] = None,
     skip_profiling: bool = False,
     skip_inferencing: bool = False,
     skip_downloading: bool = False,
     skip_summary: bool = False,
     output_dir: Optional[str] = None,
     target_runtime: TargetRuntime = TargetRuntime.TFLITE,
     compile_options: str = "",
@@ -62,14 +63,16 @@
 
     Each of the last four steps can be optionally skipped using the input options.
 
     Parameters:
         device: Device for which to export the model.
             Full list of available devices can be found by running `hub.get_devices()`.
             Defaults to DEFAULT_DEVICE if not specified.
+        chipset: If set, will choose a random device with this chipset.
+            Overrides the `device` argument.
         skip_profiling: If set, skips profiling of compiled model on real devices.
         skip_inferencing: If set, skips computing on-device outputs from sample data.
         skip_downloading: If set, skips downloading of compiled model.
         skip_summary: If set, skips waiting for and summarizing results
             from profiling and inference.
         output_dir: Directory to store generated assets (e.g. compiled model).
             Defaults to `<cwd>/build/<model_name>`.
@@ -83,14 +86,18 @@
         A 3-tuple of:
             * A CompileJob object containing metadata about the compile job submitted to hub.
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
     model_name = "yolov8_seg"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
+    if chipset:
+        hub_device = hub.Device(attributes=f"chipset:{chipset}")
+    else:
+        hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "yolov8_seg",
             "YOLOv8-Segmentation",
             device,
             skip_profiling,
             skip_inferencing,
@@ -105,42 +112,43 @@
     # 1. Initialize PyTorch model
     model = Model.from_pretrained(**get_model_kwargs(Model, additional_model_kwargs))
     input_spec = model.get_input_spec(
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
+    model.eval()
     source_model = torch.jit.trace(
         model.to("cpu"), make_torch_inputs(input_spec), check_trace=False
     )
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime, compile_options + " --force_channel_last_input image"
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
-        device=hub.Device(device),
+        device=hub_device,
         name=model_name,
         options=model_compile_options,
     )
     compile_job = cast(hub.client.CompileJob, submitted_compile_job)
 
     # 3. Profile the model asset on real devices
     profile_job: Optional[hub.client.ProfileJob] = None
     if not skip_profiling:
         profile_options_all = model.get_hub_profile_options(
             target_runtime, profile_options
         )
         print(f"Profiling model {model_name} on a hosted device.")
         submitted_profile_job = hub.submit_profile_job(
             model=compile_job.get_target_model(),
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         profile_job = cast(hub.client.ProfileJob, submitted_profile_job)
 
     # 4. Run inference on-device with sample inputs
     inference_job: Optional[hub.client.InferenceJob] = None
@@ -155,15 +163,15 @@
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
             "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
-            device=hub.Device(device),
+            device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
         inference_job = cast(hub.client.InferenceJob, submitted_inference_job)
 
     # 5. Download the model asset to a local file
     if not skip_downloading:
```

## qai_hub_models/models/yolov8_seg/perf.yaml

```diff
@@ -4,17 +4,15 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
-  - QCS6490 (Proxy)
   - QCS8550 (Proxy)
-  - RB3 Gen 2 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
@@ -23,15 +21,14 @@
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
-  - Qcs6490
   - Qcs8550
   - Snapdragon® 8 Gen 1
   - Snapdragon® 8 Gen 2
   - Snapdragon® 8 Gen 3
   - Snapdragon® 888
 models:
 - name: YOLOv8-Segmentation
@@ -69,15 +66,15 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 2
-    timestamp: '2024-04-16T20:17:34.459189Z'
+    timestamp: '2024-04-23T18:42:34.024849Z'
   - torchscript_onnx_tflite:
       inference_time: 5210.0
       throughput: 191.93857965451056
       estimated_peak_memory_range:
         min: 40960
         max: 98992992
       primary_compute_unit: NPU
@@ -107,39 +104,31 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon® 8 Gen 3
-    timestamp: '2024-04-16T20:17:34.459265Z'
-  - torchscript_onnx_ort:
-      inference_time: 163872.0
-      throughput: 6.10232376488967
+    timestamp: '2024-04-23T18:42:34.024902Z'
+  - torchscript_onnx_tflite:
+      inference_time: 7217.0
+      throughput: 138.56172925038103
       estimated_peak_memory_range:
-        min: 49770496
-        max: 153912944
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 4579328
+        max: 18295080
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 337
         layers_on_gpu: 0
-        layers_on_cpu: 242
-        total_layers: 242
-      job_id: jopr8wm75
+        layers_on_cpu: 0
+        total_layers: 337
+      job_id: j0pxnq9l5
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
-      manufacturer: ''
-      chipset: Qcs6490
-    timestamp: '2024-04-16T20:17:34.459312Z'
-  - reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
-      manufacturer: ''
+      manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-16T20:17:34.459318Z'
+    timestamp: '2024-04-23T18:42:34.024944Z'
```

## qai_hub_models/utils/args.py

```diff
@@ -10,15 +10,15 @@
 import argparse
 import inspect
 import os
 import sys
 from functools import partial
 from importlib import import_module
 from pydoc import locate
-from typing import Any, List, Mapping, Optional, Type
+from typing import Any, List, Mapping, Optional, Set, Type
 
 import qai_hub as hub
 
 from qai_hub_models.models.protocols import (
     FromPrecompiledTypeVar,
     FromPretrainedProtocol,
     FromPretrainedTypeVar,
@@ -360,14 +360,25 @@
     """
     Create this model's input spec from an argparse namespace.
     Default behavior is to assume the CLI args have the same names as get_input_spec method args.
     """
     return model.get_input_spec(**get_input_spec_kwargs(model, vars(cli_args)))
 
 
+def get_qcom_chipsets() -> Set[str]:
+    return set(
+        [
+            attr[len("chipset:") :]
+            for dev in hub.get_devices()
+            for attr in dev.attributes
+            if attr.startswith("chipset:qualcomm")
+        ]
+    )
+
+
 def export_parser(
     model_cls: Type[FromPretrainedTypeVar] | Type[FromPrecompiledTypeVar],
     components: Optional[List[str]] = None,
     supports_qnn=True,
     supports_ort=True,
     exporting_compiled_model=False,
 ) -> argparse.ArgumentParser:
@@ -398,14 +409,22 @@
     parser.add_argument(
         "--device",
         type=str,
         default=DEFAULT_EXPORT_DEVICE,
         help="Device for which to export.",
     )
     parser.add_argument(
+        "--chipset",
+        type=str,
+        default=None,
+        choices=sorted(get_qcom_chipsets(), reverse=True),
+        help="If set, will choose a random device with this chipset. "
+        "Overrides whatever is set in --device.",
+    )
+    parser.add_argument(
         "--skip-profiling",
         action="store_true",
         help="If set, writes compiled model to local directory without profiling.",
     )
     parser.add_argument(
         "--skip-inferencing",
         action="store_true",
@@ -427,15 +446,24 @@
         type=str,
         default=None,
         help="Directory to store generated assets (e.g. compiled model). "
         "Defaults to `<cwd>/build/<model_name>`.",
     )
     if not exporting_compiled_model:
         # Default runtime for compiled model is fixed for given model
-        add_target_runtime_arg(parser, help="The runtime for which to export.")
+        available_runtimes = [TargetRuntime.TFLITE]
+        if supports_qnn:
+            available_runtimes.append(TargetRuntime.QNN)
+        if supports_ort:
+            available_runtimes.append(TargetRuntime.ORT)
+        add_target_runtime_arg(
+            parser,
+            available_target_runtimes=available_runtimes,
+            help="The runtime for which to export.",
+        )
         # No compilation for compiled models
         parser.add_argument(
             "--compile-options",
             type=str,
             default="",
             help="Additional options to pass when submitting the compile job.",
         )
```

## qai_hub_models/utils/base_model.py

```diff
@@ -34,33 +34,31 @@
 
 class HubModel(HubModelProtocol):
     """
     Base interface for AI Hub models.
     """
 
     def __init__(self):
-        # Change self.get_input_spec() to call _get_input_spec_for_instance() instead.
-        #
-        # _get_input_spec_for_instance() is an override that allows get_input_spec()
-        # to access instance variables. This may be used in case input shape is "hard-coded"
-        # based on parameters passed to the model upon initialization.
-        #
-        self.get_input_spec = self._get_input_spec_for_instance
+        # If a child class implements _get_input_spec_for_instance(),
+        # then calling `get_input_spec` on the instance will redirect to it.
+        if self._get_input_spec_for_instance.__module__ != __name__:
+            self.get_input_spec = self._get_input_spec_for_instance
 
     def _get_input_spec_for_instance(self, *args, **kwargs) -> InputSpec:
         """
         Get the input specifications for an instance of this model.
 
         Typically this will pre-fill inputs of get_input_spec
         with values determined by instance members of the model class.
 
-        The initializer for BaseModel will automatically override get_input_spec
-        with this function when the class is instantiated.
+        If this function is implemented by a child class, the initializer for BaseModel
+        will automatically override get_input_spec with this function
+        when the class is instantiated.
         """
-        return self.__class__.get_input_spec(*args, **kwargs)
+        raise NotImplementedError
 
     def sample_inputs(self, input_spec: InputSpec | None = None) -> SampleInputsType:
         """
         Returns a set of sample inputs for the model.
 
         For each input name in the model, a list of numpy arrays is provided.
         If the returned set is batch N, all input names must contain exactly N numpy arrays.
```

## qai_hub_models/utils/compare.py

```diff
@@ -48,21 +48,21 @@
     eps: float = 1e-5,
     eps2: float = 1e-10,
 ) -> float:
     """
     Computes the PSNR between two tensors.
     """
     if not isinstance(output_a, np.ndarray):
-        a = output_a.detach().numpy().flatten()
+        a = output_a.detach().float().numpy().flatten()
     else:
-        a = output_a.flatten()
+        a = output_a.flatten().astype(np.float32)
     if not isinstance(output_b, np.ndarray):
-        b = output_b.detach().numpy().flatten()
+        b = output_b.detach().float().numpy().flatten()
     else:
-        b = output_b.flatten()
+        b = output_b.flatten().astype(np.float32)
     max_b = np.abs(b).max()
     sumdeltasq = 0.0
     sumdeltasq = ((a - b) * (a - b)).sum()
     sumdeltasq /= b.size
     sumdeltasq = np.sqrt(sumdeltasq)
 
     return 20 * np.log10((max_b + eps) / (sumdeltasq + eps2))
```

## qai_hub_models/utils/scorecard/common.py

```diff
@@ -1,29 +1,18 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 import qai_hub as hub
 
-from qai_hub_models.models.common import TargetRuntime
-
-TEST_DEVICES_PER_RUNTIME = {
-    TargetRuntime.ORT: {
-        "s23",
-        "s24",
-    },
-    TargetRuntime.QNN: {"s23", "s24", "6490", "8550"},
-    TargetRuntime.TFLITE: {"s23", "s24", "6490", "8550"},
-}
-
-
 SCORECARD_DEVICE_NAME_TO_CHIPSET_NAME = {
     "s23": "qualcomm-snapdragon-8gen2",
     "s24": "qualcomm-snapdragon-8gen3",
     "6490": "qualcomm-qcs6490",
+    "8250": "qualcomm-qcs8250",
     "8550": "qualcomm-qcs8550",
 }
 
 
 SCORECARD_DEVICE_NAME_TO_CHIPSET = {
     device: f"chipset:{chipset}"
     for device, chipset in SCORECARD_DEVICE_NAME_TO_CHIPSET_NAME.items()
@@ -38,9 +27,10 @@
     raise ValueError(f"No device named {device_name}")
 
 
 REFERENCE_DEVICE_PER_SUPPORTED_CHIPSETS = {
     "qualcomm-snapdragon-8gen2": __get_device("Samsung Galaxy S23"),
     "qualcomm-snapdragon-8gen3": __get_device("Samsung Galaxy S24"),
     "qualcomm-qcs6490": __get_device("RB3 Gen 2 (Proxy)"),
+    "qualcomm-qcs8250": __get_device("RB5 (Proxy)"),
     "qualcomm-qcs8550": __get_device("QCS8550 (Proxy)"),
 }
```

## qai_hub_models/utils/scorecard/job_summary.py

```diff
@@ -188,30 +188,36 @@
             else:
                 components = list(model_code_gen.components.keys())
 
         for runtime in TargetRuntime:
             for device, chipset in SCORECARD_DEVICE_NAME_TO_CHIPSET_NAME.items():
                 run_dev = f"{runtime.name}-{device}"
                 if not components:
+                    if (job_id := job_ids.get(f"{model_id}_{run_dev}", None)) is None:
+                        continue
                     model_runs.append(
                         cls(
                             model_id=model_info.name,
-                            job_id=job_ids.get(f"{model_id}_{run_dev}", None),
+                            job_id=job_id,
                             runtime=runtime,
                             _chipset=chipset,
                         )
                     )
                 else:
                     for component in components:
+                        if (
+                            job_id := job_ids.get(
+                                f"{model_id}_{run_dev}_{component}", None
+                            )
+                        ) is None:
+                            continue
                         model_runs.append(
                             cls(
                                 model_id=component,
-                                job_id=job_ids.get(
-                                    f"{model_id}_{run_dev}_{component}", None
-                                ),
+                                job_id=job_id,
                                 runtime=runtime,
                                 _chipset=chipset,
                             )
                         )
 
         return model_runs
```

## Comparing `qai_hub_models-0.5.0.dist-info/LICENSE` & `qai_hub_models-0.5.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `qai_hub_models-0.5.0.dist-info/METADATA` & `qai_hub_models-0.5.1.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: qai-hub-models
-Version: 0.5.0
+Version: 0.5.1
 Summary: Models optimized for export to run on device.
 Home-page: https://github.com/quic/ai-hub-models
 Author: Qualcomm® Technologies, Inc.
 License: BSD-3
 Platform: UNKNOWN
 Requires-Python: >=3.8, <3.11
 Description-Content-Type: text/markdown
@@ -604,14 +604,16 @@
 | [SESR-M5](https://aihub.qualcomm.com/models/sesr_m5) | [qai_hub_models.models.sesr_m5](qai_hub_models/models/sesr_m5/README.md) | ✔️ | ✔️ | ✔️
 | [SESR-M5-Quantized](https://aihub.qualcomm.com/models/sesr_m5_quantized) | [qai_hub_models.models.sesr_m5_quantized](qai_hub_models/models/sesr_m5_quantized/README.md) | ✔️ | ✔️ | ✔️
 | [XLSR](https://aihub.qualcomm.com/models/xlsr) | [qai_hub_models.models.xlsr](qai_hub_models/models/xlsr/README.md) | ✔️ | ✔️ | ✔️
 | [XLSR-Quantized](https://aihub.qualcomm.com/models/xlsr_quantized) | [qai_hub_models.models.xlsr_quantized](qai_hub_models/models/xlsr_quantized/README.md) | ✔️ | ✔️ | ✔️
 | | | | |
 | **Semantic Segmentation**
 | [DDRNet23-Slim](https://aihub.qualcomm.com/models/ddrnet23_slim) | [qai_hub_models.models.ddrnet23_slim](qai_hub_models/models/ddrnet23_slim/README.md) | ✔️ | ✔️ | ✔️
+| [DeepLabV3-Plus-MobileNet](https://aihub.qualcomm.com/models/deeplabv3_plus_mobilenet) | [qai_hub_models.models.deeplabv3_plus_mobilenet](qai_hub_models/models/deeplabv3_plus_mobilenet/README.md) | ✔️ | ✔️ | ✔️
+| [DeepLabV3-Plus-MobileNet-Quantized](https://aihub.qualcomm.com/models/deeplabv3_plus_mobilenet_quantized) | [qai_hub_models.models.deeplabv3_plus_mobilenet_quantized](qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/README.md) | ✔️ | ✔️ | ✔️
 | [DeepLabV3-ResNet50](https://aihub.qualcomm.com/models/deeplabv3_resnet50) | [qai_hub_models.models.deeplabv3_resnet50](qai_hub_models/models/deeplabv3_resnet50/README.md) | ✔️ | ✔️ | ✔️
 | [FCN_ResNet50](https://aihub.qualcomm.com/models/fcn_resnet50) | [qai_hub_models.models.fcn_resnet50](qai_hub_models/models/fcn_resnet50/README.md) | ✔️ | ✔️ | ✔️
 | [FFNet-122NS-LowRes](https://aihub.qualcomm.com/models/ffnet_122ns_lowres) | [qai_hub_models.models.ffnet_122ns_lowres](qai_hub_models/models/ffnet_122ns_lowres/README.md) | ✔️ | ✔️ | ✔️
 | [FFNet-40S](https://aihub.qualcomm.com/models/ffnet_40s) | [qai_hub_models.models.ffnet_40s](qai_hub_models/models/ffnet_40s/README.md) | ✔️ | ✔️ | ✔️
 | [FFNet-40S-Quantized](https://aihub.qualcomm.com/models/ffnet_40s_quantized) | [qai_hub_models.models.ffnet_40s_quantized](qai_hub_models/models/ffnet_40s_quantized/README.md) | ✔️ | ✔️ | ✔️
 | [FFNet-54S](https://aihub.qualcomm.com/models/ffnet_54s) | [qai_hub_models.models.ffnet_54s](qai_hub_models/models/ffnet_54s/README.md) | ✔️ | ✔️ | ✔️
 | [FFNet-54S-Quantized](https://aihub.qualcomm.com/models/ffnet_54s_quantized) | [qai_hub_models.models.ffnet_54s_quantized](qai_hub_models/models/ffnet_54s_quantized/README.md) | ✔️ | ✔️ | ✔️
```

## Comparing `qai_hub_models-0.5.0.dist-info/RECORD` & `qai_hub_models-0.5.1.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 qai_hub_models/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/_version.py,sha256=f9kkIsFtA1v7fOa_sakYAhr5DF4PqxO6HGLGtVMNeLo,281
+qai_hub_models/_version.py,sha256=re3Skq9KHSFnKkF5HvubG0knuLXdhMdZAY5wfbcr3yQ,281
 qai_hub_models/asset_bases.yaml,sha256=53T7xh4bysMolqjwBOIxm4tm23szHhVQPgOKo7JzCvo,620
 qai_hub_models/conftest.py,sha256=STs4po9vrxUPOpDmX0XbVCVSIHtsbKou_Y11coTBAbU,734
 qai_hub_models/global_requirements.txt,sha256=y7Pp0d7YLb--E7J-8DQAPOG2Irq3K8oSPJqU0VhwR4M,913
 qai_hub_models/requirements-dev.txt,sha256=z81nhpASF1QUiIvaj3ZEXr75ifaWkSstpB-Y-3jOJuo,395
 qai_hub_models/requirements.txt,sha256=qu_-OPNrU_0-iG2lejY1zNn-4t0WbTOxw--z6IH7WQk,427
 qai_hub_models/datasets/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/datasets/bsd300.py,sha256=ndX_-l3KFRQeOPxsq8AjyIwDb0ozGYQpP7N84lpQwtk,4757
 qai_hub_models/datasets/coco.py,sha256=p1tVNIqajR6P1B3b7yNpztiVq_uo3KKBOuDzlCg0WNw,4109
 qai_hub_models/datasets/common.py,sha256=_MxFQMrogLDazscDyxu3NNid4j3IWK9jTl5kefWfgNA,1614
-qai_hub_models/datasets/imagenette.py,sha256=Rdz6vw6tL7ysw-7DSHZoW8u6jCBPA6PLOIyYKSun0Ks,3203
+qai_hub_models/datasets/imagenette.py,sha256=Iwu0UIRgnP6UhWGzHcoyq-qZLyIU0AQvcX9wKskPEEQ,3173
+qai_hub_models/datasets/pascal_voc.py,sha256=Z4au_liYBDNjTzh3_S9GkdLPulYe_vnRa8SL6emKIGg,2616
 qai_hub_models/evaluators/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/evaluators/base_evaluators.py,sha256=vFG_4HpZkx3bHBluZq17GvP7iBCWkGphy07__pkvaww,6212
 qai_hub_models/evaluators/classification_evaluator.py,sha256=ccjbu9vcVTn8UBB0iUM_fRxvTlbarwfFWlXIR90gFAU,1326
 qai_hub_models/evaluators/detection_evaluator.py,sha256=qQl-1WApD95Os0tYTVLpMsVNNT7ViqS4nvcNW7rAViw,3699
 qai_hub_models/evaluators/image_evaluator.py,sha256=nPMHIeyo7WreD6LEa-eqdHdvIZQA1NbPhYoVrOQw3DY,2434
 qai_hub_models/evaluators/superres_evaluator.py,sha256=7NwBcPFJCjCbnmVpjB0cctRPNc2Am6PCL7z8vYzcDUY,2181
 qai_hub_models/models/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
@@ -24,17 +25,18 @@
 qai_hub_models/models/_shared/cityscapes_segmentation/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/cityscapes_segmentation/app.py,sha256=2n2tPq3bmKmAudJaQGoewvOZAX_RybjjLJnBmvfWKEM,4346
 qai_hub_models/models/_shared/cityscapes_segmentation/demo.py,sha256=Hzjw4Bo2hKUbPkHQTk3-extZweVbAaL7bd7LAHeVy5w,2904
 qai_hub_models/models/_shared/cityscapes_segmentation/evaluator.py,sha256=25G3q93XoJhT9-1sKh6p0cstO1vhAI13hOR91IucKmE,890
 qai_hub_models/models/_shared/cityscapes_segmentation/model.py,sha256=z-F-WrxlqJDdv2TNXwJW73pKgx-S0fSR2gQK0BqOoBU,2888
 qai_hub_models/models/_shared/cityscapes_segmentation/patches/move_datasets.diff,sha256=y_7LNVu_y66oJzmjUps6PFe-1XRj0J5cf68-DJNeBcU,8565
 qai_hub_models/models/_shared/deeplab/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/models/_shared/deeplab/app.py,sha256=mQ00GgK-pwllOy4hqhEnvv6fbWPht5tghBSfic59Pjs,2651
-qai_hub_models/models/_shared/deeplab/demo.py,sha256=T98PVkEdE3enflfwJT_gKo4W0GnJ2ovCxLgrKgxpWho,2252
+qai_hub_models/models/_shared/deeplab/app.py,sha256=KbIYMuVoiYDEYwuhCeOY-ZnxQFu6VGNqb4bFOP4fD8M,2449
+qai_hub_models/models/_shared/deeplab/demo.py,sha256=qcH492vccd3MSJj4KSDkN-8_Z0unWUAvKYJBU5YrMH0,2256
 qai_hub_models/models/_shared/deeplab/evaluator.py,sha256=z_lorsF_6cW4G5bNBxG89HdWmeG53PyRk9fOuswxB0M,915
+qai_hub_models/models/_shared/deeplab/model.py,sha256=_ZFBMSGMHFDlBXD9NCztIsdlqhUObU2-QrjgNoHWqEc,2015
 qai_hub_models/models/_shared/detr/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/detr/app.py,sha256=EKqvk7pHd8_JVhc0BbESwLyZiVu_7poSf1Qgad2pbSM,4604
 qai_hub_models/models/_shared/detr/coco_label_map.py,sha256=QOEkaqmyW6mS8gXLxfUHJfCyWtCQ1lNzzLEap8zlW1c,3565
 qai_hub_models/models/_shared/detr/demo.py,sha256=TytpiefnNUXvlwezD9QkAe5Xt5aRzswJu9SWca4AwR8,2091
 qai_hub_models/models/_shared/detr/model.py,sha256=G0nk4_EnJDLINqr1jrbdW54EzH4-QoEKhCRqFvVViYs,2050
 qai_hub_models/models/_shared/fastsam/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/fastsam/app.py,sha256=VcGV3iloO_1t3g694bkV02PIPXIbnBaAoL6emPK-eRA,4883
@@ -72,826 +74,842 @@
 qai_hub_models/models/_shared/video_classifier/model.py,sha256=obQoYEt1AGWxrfm3tD12j1gmj6WlOGeBbATiXoEFTds,1969
 qai_hub_models/models/_shared/whisper/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/whisper/app.py,sha256=FG9-3wysMYdfh7tcH6miAO5xf3EayCqVbCzLsSP05XQ,10188
 qai_hub_models/models/_shared/whisper/demo.py,sha256=7zi12g3CskAHOndf2nZIl_QVFLYp0n-P3VtO-ylEECU,1302
 qai_hub_models/models/_shared/whisper/model.py,sha256=8NfDTyLWl2Hp2t8lqc2fbNos65RsnwHLWxVXcelUT_8,14119
 qai_hub_models/models/_shared/whisper/test_utils.py,sha256=MVNzlRySllNxozJt9_biaPIii2hLNJHNga7weTv1mdU,2848
 qai_hub_models/models/_shared/yolo/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/models/_shared/yolo/app.py,sha256=iuFEHq3aGXkI5yEAsuixRk2ntWAM8dVdNdun4fF7Mvs,7354
+qai_hub_models/models/_shared/yolo/app.py,sha256=SfL-yVTn7RWHPzRiX0dgfNGk84BKBcNOnIjIZ5b3XzI,7413
 qai_hub_models/models/_shared/yolo/demo.py,sha256=XBrIzWUujUBj6r4qPhra5mbUnZF5FZSs0MzWTIEeqz8,2432
-qai_hub_models/models/_shared/yolo/utils.py,sha256=4c_Psz-kVEhq-iczYlj5chV37RH77HLDdtkC9LJ2XL0,5643
+qai_hub_models/models/_shared/yolo/utils.py,sha256=EZWCEI-Z2oot3NyL9h3J_B_2GRSlShyqARfXEenPoHQ,6081
 qai_hub_models/models/aotgan/__init__.py,sha256=BaaLR3zQEAxIdLMsE_bj1ifMQKJs6_HBVeTvC601C8Q,450
-qai_hub_models/models/aotgan/conftest.py,sha256=CTqtFJRsqW03tZp4iC66bRJMqG8aDFWW5fmICXvQfyw,980
+qai_hub_models/models/aotgan/conftest.py,sha256=2KW4tZi0QOWX-_NVefoyLT0Mmcq8hsFa0md4R6eVMbc,1405
 qai_hub_models/models/aotgan/demo.py,sha256=pNMiZ4xPMXki0cep2i7KrSCYpbkFjVA7AjWJK3WLROw,597
-qai_hub_models/models/aotgan/export.py,sha256=HRVUYynOlKuoGa5cSUBxu5ilBFfX00mzOhuljik0uXk,8156
+qai_hub_models/models/aotgan/export.py,sha256=sX-ECB4WXo1aCQquwBjKjlllRTUx_vWiz59S8bVQyQE,8437
 qai_hub_models/models/aotgan/info.yaml,sha256=n2LVDkDfhrNh60R-XxQiZexVaNz3DHlmtJ7yuJ3MJ5U,1023
 qai_hub_models/models/aotgan/model.py,sha256=mj9Zn0MkmSh1Frsw9ESCB-pL1LjJoE_EHhNVPhfbqyo,4838
-qai_hub_models/models/aotgan/perf.yaml,sha256=s8lGifIN-sjniudD8y98pvt6tbOaIlRO3xMmEek3N8o,4412
+qai_hub_models/models/aotgan/perf.yaml,sha256=f8-Qu-mep_LzR0jOBXPWVDpYe-0JxvxKb9ioI3vdYHU,4545
 qai_hub_models/models/aotgan/test.py,sha256=sPojPTH2xmn5YoLMpTt8rurhiF1BDS1PMgRhXK01Fio,2006
 qai_hub_models/models/aotgan/patches/layer_norm.diff,sha256=NbhINLgR1N0wfrI3x89m7JheBbxT2MTNkEm9dpedBeY,532
-qai_hub_models/models/baichuan_7b_quantized/info.yaml,sha256=f4r1NJYFIzd7MJNcrOIkfg07OKkIjWFDFbViUT2umWU,1983
-qai_hub_models/models/baichuan_7b_quantized/perf.yaml,sha256=An29D-fEHO6H1ihUWR43hePTiHDuEP2VxUfHhloCkkk,2036
+qai_hub_models/models/baichuan_7b_quantized/info.yaml,sha256=dW4nlITktQn6wzRj8G5g8F3jzgWLtf9Azn8Tg5rcgcA,1987
+qai_hub_models/models/baichuan_7b_quantized/perf.yaml,sha256=CJPN5oLM6lBtEdScWIlo9aKCxb2sxYxH5OYC4zVGRzs,2038
 qai_hub_models/models/controlnet_quantized/__init__.py,sha256=aCoQX1Qz9SrPHhuSJr4vD7f9nJtkhCzAa9__HBvcg4g,559
 qai_hub_models/models/controlnet_quantized/app.py,sha256=5wCKVlYNyeZ_m1PfqCOQlIq80bToaYsTdxw2Fk4l05U,9705
 qai_hub_models/models/controlnet_quantized/demo.py,sha256=rPSPdSGbFMUJ9jdMj5nLEypRqsbmrMaitON-j6zgzds,6397
-qai_hub_models/models/controlnet_quantized/export.py,sha256=SfnuhRIGIfCTGSP6tUhhlQ3TRJYkPovKHNsI2rowUZ8,7622
+qai_hub_models/models/controlnet_quantized/export.py,sha256=rZ4tiyoA0qzpYkObTNmRMbw7-U67XDCqDCce6PGcF2c,7894
 qai_hub_models/models/controlnet_quantized/info.yaml,sha256=ivhi6oY9IC0gGtw9REooBoV7aJQqUIvHw-g9YfE-qVw,1329
 qai_hub_models/models/controlnet_quantized/model.py,sha256=8BO-diXqlxH6wuNRs7VeYUlSxhDGWYCemAT_XgEzVt8,5426
 qai_hub_models/models/controlnet_quantized/perf.yaml,sha256=a0C641UKT6i75a3Tu0x_zsyVtB07oWBzsM3P2PSftAc,8427
 qai_hub_models/models/controlnet_quantized/requirements.txt,sha256=HWz6kAx8f-AYWf5IWvv-q1IOznXS94gXTQrtKGY4L70,46
 qai_hub_models/models/controlnet_quantized/test.py,sha256=ilelb6gq5P7_1RveAB09vfppdhzrVbWpHYiB-twI55w,1556
 qai_hub_models/models/convnext_tiny/__init__.py,sha256=HCfwohVDc0VtmT7yLoa43BTiBAEkKXtFE9-jfRy3XrM,475
-qai_hub_models/models/convnext_tiny/conftest.py,sha256=TchLpc6aQTfmrRY5s315X9ne0k4BmKnQ35H-mWGiQj8,908
+qai_hub_models/models/convnext_tiny/conftest.py,sha256=ujfvC8Pi3rMwWoBaR_PHFxG--AQmLkiLr6EBNwGwuww,1318
 qai_hub_models/models/convnext_tiny/demo.py,sha256=22oIp3py29Uzbsh0ob2ak4ic4ZdetMiqwgyo3I2fqEY,543
-qai_hub_models/models/convnext_tiny/export.py,sha256=f6daP73JHRI7gBvlspn_eE8Kd86aEWLRt3hwyXyX8dY,7915
+qai_hub_models/models/convnext_tiny/export.py,sha256=oRwJYk21MkDOPCkeiQhDj931pHiyQNlIXRbF6fpoWkc,8196
 qai_hub_models/models/convnext_tiny/info.yaml,sha256=C017TbQv8k22-OVMYW8RnesPhOAtRXZVqKANe2fAP7s,1287
 qai_hub_models/models/convnext_tiny/model.py,sha256=TohaPxU5EWTTBKMEjZusg238O3MEaA9MNKH36v6SoFw,708
 qai_hub_models/models/convnext_tiny/perf.yaml,sha256=Fg5TvSc1qlKNVDJEGh8C2aABL4YgG955krRLgO5yg7M,2707
 qai_hub_models/models/convnext_tiny/test.py,sha256=cijDPmavZsw6N6okJvEeQFhiC8zoWymxjT1QvbPUNhg,857
 qai_hub_models/models/ddrnet23_slim/__init__.py,sha256=fz62JiNfyn7NSLR8F95JKyRtjB6CQFJSxx8Vt2YQwyc,398
 qai_hub_models/models/ddrnet23_slim/app.py,sha256=bUGwG2F79praX6iTM5vHKxUlVlPyrUVl4FzmX3U9uWk,3750
-qai_hub_models/models/ddrnet23_slim/conftest.py,sha256=KXIS06bjhHzQ0EeR6yV6tiET_eej7rF3unREuL8r_1I,994
+qai_hub_models/models/ddrnet23_slim/conftest.py,sha256=A1oi389WhdhvCaYKOZbMRpACx6hY6LhOD7YPfeDtazM,1412
 qai_hub_models/models/ddrnet23_slim/demo.py,sha256=DtX3jP0RGrtig0qNjNSrhuhYErtopHTmsVcArT_HDrg,2128
-qai_hub_models/models/ddrnet23_slim/export.py,sha256=qCMq8B7iRlRZqM2GwkpiA9lkuEPVgXAWj47y19ru_8M,8193
+qai_hub_models/models/ddrnet23_slim/export.py,sha256=aqXz3HeEggHYqgDScvJSwb1RSiipDrIK-tjL45WV2lI,8474
 qai_hub_models/models/ddrnet23_slim/info.yaml,sha256=J8BrgX-ROS7IN6jqb1Co6mC7mJxBO3VFVlCGHQyqIfc,1334
 qai_hub_models/models/ddrnet23_slim/model.py,sha256=o19xSxPaoEKCcYRotMWrqZyqPHI3iwigwU7OLXUKQ4M,3831
-qai_hub_models/models/ddrnet23_slim/perf.yaml,sha256=Sjaqe4RcbwHFSTBe0z365aClcGsVO2lY0mVRohqOexw,3617
+qai_hub_models/models/ddrnet23_slim/perf.yaml,sha256=sQJiUkGIDMJ6GgqrFdxOZEDmkyQwOfpX-ZDUqEpUqHw,3358
 qai_hub_models/models/ddrnet23_slim/test.py,sha256=Km5AirOt-YDL_qfBSKMuco1rFWR5-4MpK-o1GRBqPBY,1790
+qai_hub_models/models/deeplabv3_plus_mobilenet/__init__.py,sha256=58gIL-e--wJG349dRqjibNq1oNRWsJXgEvxdFXmshwc,455
+qai_hub_models/models/deeplabv3_plus_mobilenet/conftest.py,sha256=bBqQVhtUYgRY9UQqbX5bYZ2bMThL5NN741N7iPSVUdE,1423
+qai_hub_models/models/deeplabv3_plus_mobilenet/demo.py,sha256=VGfLkT14sw3VZa1h7zO8pCGeDvUz7H2zNQzGrmmRFc8,1091
+qai_hub_models/models/deeplabv3_plus_mobilenet/export.py,sha256=zaN3RtLVHt7EaBK9HR1-mYKGgGpQTYj36XGgapFfaPM,8498
+qai_hub_models/models/deeplabv3_plus_mobilenet/info.yaml,sha256=tOIqh_pZfUo_BSpIgVlLIo_cMJVmH6W2wM7rlrsZ-58,1266
+qai_hub_models/models/deeplabv3_plus_mobilenet/model.py,sha256=dKYL4xLKUadUbe7Lo2ehMUatVAm0-d0eowxSqSgnCrM,2098
+qai_hub_models/models/deeplabv3_plus_mobilenet/perf.yaml,sha256=qfnMaqfJDygw4qcMJXU_PBrnNzShz8NXE4IuTgc5w1o,3812
+qai_hub_models/models/deeplabv3_plus_mobilenet/test.py,sha256=FBAfk-9Og-uhCceKzyEE2mAHXiviSSORSr8OBGyr8uA,1862
+qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/__init__.py,sha256=6gI73njrwOCedV126NWRglDo8htjzBkCv-wafEOcm2s,466
+qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/conftest.py,sha256=9bwTSPjEbFS_ehNB1R9JrV4V-3SA15_6zO0ack3bm58,1433
+qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/demo.py,sha256=xNiFm6o-ExYHcIWhaKyykyoqGZCCbfMvSVOK0x1MKpA,1156
+qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/export.py,sha256=BDMPIbt6zl0SFExisffmuPUPZQvm5MnnGYLdgDG0-OU,8914
+qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/info.yaml,sha256=jzYZAMV9CZj3dzyi1FBgJdrzgeJht67WCYljOe5G_A8,1306
+qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/model.py,sha256=WExi05L2u2fQ1sJ1ifacq8yGeuGEsT9P1uPdM3NMX6s,2861
+qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/perf.yaml,sha256=mZlwe3z3zo9yhKsF77dXYBA0zZe3pJaSjo65j5UON28,5887
+qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/test.py,sha256=y_jG5Nc56h9W5HCWbVSV3xWG4HUi7iig2LqXxFNmkgs,2192
 qai_hub_models/models/deeplabv3_resnet50/__init__.py,sha256=RgC9JWAyPRQugvGjLMpeeWh68mvBeK7KxBnIvMtA_P8,451
-qai_hub_models/models/deeplabv3_resnet50/conftest.py,sha256=VRJr9ADQ7R9WB9DhDy3wP3qV8Mwc05W1eqYXUv7LTPc,1004
-qai_hub_models/models/deeplabv3_resnet50/demo.py,sha256=pxsw91eJcacHy5-N-4POOM_vv8er47uTUIpIGM91P1k,1026
-qai_hub_models/models/deeplabv3_resnet50/export.py,sha256=Wuwe7Zq8MSoDuoCaohLFIbiCGf1xMZiWRArCtK9I_T8,8068
-qai_hub_models/models/deeplabv3_resnet50/info.yaml,sha256=xJyhJArERFP4UdDYrFc1dc9NLLJqkeQTi4EM_vZFaxQ,1278
-qai_hub_models/models/deeplabv3_resnet50/model.py,sha256=NLurkhtOrORi268v1t9_IlBzSTk3Lyy3jdZ2wlDHJUU,2886
-qai_hub_models/models/deeplabv3_resnet50/perf.yaml,sha256=IZnHJ9cWuh6o2DUnpVRaTzPYWWzeu2wWv6Oh1raUBfs,4374
-qai_hub_models/models/deeplabv3_resnet50/test.py,sha256=yXJY6pGC_QkjLGavh40vSP4flUm33H83FhWUFXTLzxM,2086
+qai_hub_models/models/deeplabv3_resnet50/conftest.py,sha256=ZTzIgleUyd3NmjF3jHcTk9_augG4gmjytPgkzHeS3lo,1417
+qai_hub_models/models/deeplabv3_resnet50/demo.py,sha256=lMuWqSJPGTmg-nop-u9DtDlGAQl7H3-JIaPsKcQ2rzE,1077
+qai_hub_models/models/deeplabv3_resnet50/export.py,sha256=t5xm1C592lkYdYH22v0ZyvsgAPfJbBQTtQrvjTZ7eBk,8492
+qai_hub_models/models/deeplabv3_resnet50/info.yaml,sha256=f1lisHPDiRoqpQpWreOc6R4_p5v7MIYoKLpWmTf7ecY,1285
+qai_hub_models/models/deeplabv3_resnet50/model.py,sha256=_rRFyPK9tCpuY-LfIty6wreSQi0zKZkNxTqCclU8lKY,1581
+qai_hub_models/models/deeplabv3_resnet50/perf.yaml,sha256=Sy0BVOL9ExUmcWFiPsWwm_UbGAosHvd38kQqPhkia0w,3781
+qai_hub_models/models/deeplabv3_resnet50/test.py,sha256=e8d2z4KXbtOXYEHW-Qxp1v1Mvn9rE-VvwA-X4LZByjM,1766
 qai_hub_models/models/densenet121/__init__.py,sha256=TxCV8tdZZ-D4d15naxmzu-0l5CHW_N3BkJiIV5HRzbY,471
-qai_hub_models/models/densenet121/conftest.py,sha256=qid7VQx1XssPdfKlC2AkbzoNmJQAYpN8InrCjxQitbw,904
+qai_hub_models/models/densenet121/conftest.py,sha256=CXbss3wdhiBSf9-KJeBwORzGCZTZuPPIjwP35npouvc,1316
 qai_hub_models/models/densenet121/demo.py,sha256=kmSL8S4QnCR2AUdz-josBJzF3ehyQ5fOgHcsbgaIN2g,533
-qai_hub_models/models/densenet121/export.py,sha256=FrgJZ5oUwDfnRGuRYiyeHRd9l44-mRLl81Yl2I9wXGg,7888
+qai_hub_models/models/densenet121/export.py,sha256=gRRjsjzagElAkfC_2VyUJGl8MPXttmtXvf6cWwlZhOs,8169
 qai_hub_models/models/densenet121/info.yaml,sha256=RRS9Pk3XMI-kwohy4GZbzLePqaFD_03Iky3W4adFvX8,1310
 qai_hub_models/models/densenet121/model.py,sha256=Lp5noeTPApNrJL6z_VmsZfEHDuKCZNl_WUY-Lr7xatM,698
-qai_hub_models/models/densenet121/perf.yaml,sha256=lkpxXYeB7qYtmaI_zlITQEViR9i9sPoO8o9ZzTJs95g,4399
+qai_hub_models/models/densenet121/perf.yaml,sha256=yWrL0L1imY6MlzAnTRSocrd8VRw18-SKxY4B4oElLHM,4525
 qai_hub_models/models/densenet121/test.py,sha256=iCxswqgBQ_GXA2vfr-ShiukFZS2Gj38OV9zY6Jm3Gvk,841
 qai_hub_models/models/detr_resnet101/__init__.py,sha256=X6U_xy5Tp6diGFQfn7Na0XebaI6Ojc0wFv6xSoh_sKY,483
-qai_hub_models/models/detr_resnet101/conftest.py,sha256=Ug7EujulNyBsCJ7yesXxfa97ZC95Di_ej4vjjIrQAvI,910
+qai_hub_models/models/detr_resnet101/conftest.py,sha256=lbkM1UMAWwunLYYfxspPN3aPMAUrX0Mrre3ymvNAJRs,1319
 qai_hub_models/models/detr_resnet101/demo.py,sha256=9o_p8fmwrZycE5JQAsWCfskglZNfyrJOAVV49KJwChg,896
-qai_hub_models/models/detr_resnet101/export.py,sha256=20hH6wkFADuLHYxelRN7gcw6TBGJDcgQg_s3d3pSguk,7905
+qai_hub_models/models/detr_resnet101/export.py,sha256=la23kz4e0y0e0YURydoYT33bwXft1nK30AxZxhP3ORI,8186
 qai_hub_models/models/detr_resnet101/info.yaml,sha256=YP4G-_ATugwYJnXGz9uJ0ghrH38exu_7SQfOq-2_OLE,1188
 qai_hub_models/models/detr_resnet101/model.py,sha256=f6SJr1m53RdUirPG1GOVeLDluBBxLIsOV-z2hSgF20E,661
-qai_hub_models/models/detr_resnet101/perf.yaml,sha256=kgkOhp2A-T8SbT1eOMg_Nk13XbygaRaiSrIg_KztBi8,3658
+qai_hub_models/models/detr_resnet101/perf.yaml,sha256=v7u9nCgEGMPn4JAgTbBfcGaMsvm8QdOezdsAyXS7NX0,3401
 qai_hub_models/models/detr_resnet101/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet101/test.py,sha256=gDbI8asjHE5ONZLujGFXIcO_h-EC0OtEkg1t4SATMd8,1316
 qai_hub_models/models/detr_resnet101_dc5/__init__.py,sha256=u76QZ7QpcYASS-4FZptRsGd5jROIardr7dRIEktZ0Ds,490
-qai_hub_models/models/detr_resnet101_dc5/conftest.py,sha256=l4mWRDRbUasGSYnD18XlfKpEa4F9AYAXo54YBE821A8,918
+qai_hub_models/models/detr_resnet101_dc5/conftest.py,sha256=ljzCSez1M8ssW6cI8D8VKwpe2aQ6iNYIlcBBBq2E5Gk,1323
 qai_hub_models/models/detr_resnet101_dc5/demo.py,sha256=ie0VdX_40vFCnnIWRCnjxH9SRsxlGpGIJpvoteK6rNI,906
-qai_hub_models/models/detr_resnet101_dc5/export.py,sha256=Vo7EJDf9pRoMNM72GSWzwUWtE9Ipb4ZCTYUOZQx2QCM,7921
+qai_hub_models/models/detr_resnet101_dc5/export.py,sha256=RKaJQ4JkhhWxDQrcmJNX7C3Bt1_zW7qUdT-UVgEKZes,8202
 qai_hub_models/models/detr_resnet101_dc5/info.yaml,sha256=otqz2oAdErfBmbBgnQNQgRXlBMcw_nI-p_UTJr4nYsw,1215
 qai_hub_models/models/detr_resnet101_dc5/model.py,sha256=A4OU7F9N_JMElzRdKUH7VKkXmqlmvBxlZ71B6kN1Yyo,668
-qai_hub_models/models/detr_resnet101_dc5/perf.yaml,sha256=w3Pp_vEpOebwA1ke6dxiXTCeWKVU8iLNYcO963SUNk4,3677
+qai_hub_models/models/detr_resnet101_dc5/perf.yaml,sha256=yniqcl9xkno9ks4jOZTz9yHuw5bWVDB1E6XsqqtRNxc,3420
 qai_hub_models/models/detr_resnet101_dc5/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet101_dc5/test.py,sha256=nvO-ikZrOs6mMSkoDxxFwOSuwA64diJrxslDgMVpXpw,1373
 qai_hub_models/models/detr_resnet50/__init__.py,sha256=atV57qR8yJou6gYUi56f7UTiBsbDVyQAvbwgHFuXI0c,481
-qai_hub_models/models/detr_resnet50/conftest.py,sha256=zteaOt1YwLJ987gBK43ECIDTbvediAo2Jwg8uSitZlo,908
+qai_hub_models/models/detr_resnet50/conftest.py,sha256=c4IiCFPKy2a1FUt9aGI1z5FUxhgjs8eI59SMGpJ97ig,1318
 qai_hub_models/models/detr_resnet50/demo.py,sha256=X9w6CWF9U1rYaN3Xo1RIDLgxk4l-CP8Kdrbp-wYaWBQ,893
-qai_hub_models/models/detr_resnet50/export.py,sha256=UN230OEgT-Sv9r44fpLHPkcCqSaWCaLPKvmhI7zYbx0,7901
+qai_hub_models/models/detr_resnet50/export.py,sha256=aFW5Y40A7oZlOcj2EEXoGcVx0ba_x9AsV_YcfSsBjsA,8182
 qai_hub_models/models/detr_resnet50/info.yaml,sha256=Zgk7l86YchB6yOV1EHo072l0o8pOroZ79-VHvZbQhdQ,1185
 qai_hub_models/models/detr_resnet50/model.py,sha256=f7rWSFyK5_LybX4KkfnxZVlHpCCVk6hRfbx6CyEs7dg,659
-qai_hub_models/models/detr_resnet50/perf.yaml,sha256=WPguHdWhZPevfqo-G3mybOKOSg83zcMobG-mfKBlZRI,3662
+qai_hub_models/models/detr_resnet50/perf.yaml,sha256=q0rwAKFeJSULU8tEfGfNBBvjyQbtuxkWVhQ6GWjCboo,3406
 qai_hub_models/models/detr_resnet50/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet50/test.py,sha256=O0P_osmIHBpoaSgCSsyPgMrS0hEhyWYiQiGfhVZzdMg,1636
 qai_hub_models/models/detr_resnet50_dc5/__init__.py,sha256=EOUQu8Eybv_S6dv2MxT06c5fP8fb-zbNd-MGM6Yo4DI,488
-qai_hub_models/models/detr_resnet50_dc5/conftest.py,sha256=AUw92oGCmYP4yO7DR8rSvPDrEtnTiGvY9p2Kbpew_ec,916
+qai_hub_models/models/detr_resnet50_dc5/conftest.py,sha256=7V30k07eDrT8aRJX47OFbfbXxhUiMHFgcAK-f27MOTE,1322
 qai_hub_models/models/detr_resnet50_dc5/demo.py,sha256=BFw09UhIreglAFeM1lS9xnKYumTh4fnS3hgqp0io7Es,903
-qai_hub_models/models/detr_resnet50_dc5/export.py,sha256=sF6QZQLdns8vQTkEnrR1bd5pccZttziRrexO0NFi_Fc,7917
+qai_hub_models/models/detr_resnet50_dc5/export.py,sha256=iSAne6D4IGvR2JkCXgOe-TFuFFfDWBYyDlj40G5sTQY,8198
 qai_hub_models/models/detr_resnet50_dc5/info.yaml,sha256=QrFUKwhno5ciqd0fpkPTis2SvXi2Nmqg9DkZTk6PaUE,1212
 qai_hub_models/models/detr_resnet50_dc5/model.py,sha256=8bZ0js6wEEb64RPwCHfwJtt9OrgTyk2KaRaS_9QxIAo,666
-qai_hub_models/models/detr_resnet50_dc5/perf.yaml,sha256=AR3xhPXEgzRxTG1wgDGmWrXYg_Fb2DR_7xtyox2vsTg,3670
+qai_hub_models/models/detr_resnet50_dc5/perf.yaml,sha256=vjLSOax81ighjLerrlkXCmHgtsgSppU3o4f_ppdo5XE,3413
 qai_hub_models/models/detr_resnet50_dc5/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet50_dc5/test.py,sha256=xt60UK2gLEMyw460kHLE14y2zgzHhhaM3peD7PfIb7Y,1344
 qai_hub_models/models/efficientnet_b0/__init__.py,sha256=7iMYCbJmA6I13XjCyUEWsizqKWjp0PMumYfobTRAhww,477
-qai_hub_models/models/efficientnet_b0/conftest.py,sha256=LuC5mnIPILSq12OphTHYrPSmz95Be00kMBWYKhZNvL4,912
+qai_hub_models/models/efficientnet_b0/conftest.py,sha256=mmy-wvh_GtqMw-oXw2zUJhvVMpb1t52oYGYVKC4PEu0,1320
 qai_hub_models/models/efficientnet_b0/demo.py,sha256=aZVZDgd4XTbaL3_3NKFMSHP15gKWYwe2qX6DEwJRPuM,549
-qai_hub_models/models/efficientnet_b0/export.py,sha256=VOsj4Jwn6jCYO2YamVKQ82c77IMFQ3hTtPwgVqr2dEY,7903
+qai_hub_models/models/efficientnet_b0/export.py,sha256=QTZaeK00ekGRu9GG8FGqd2-q7ifctRotVjmUwy3DRsk,8184
 qai_hub_models/models/efficientnet_b0/info.yaml,sha256=qHfAm4KiPtXcdb3GmVePXiZZmsOE07Gbzl8c7Hg268c,1361
 qai_hub_models/models/efficientnet_b0/model.py,sha256=3miNRC3gRr5pfkWrwYdLFmx7Wjw0eI1CIKEOHLlB7jQ,714
-qai_hub_models/models/efficientnet_b0/perf.yaml,sha256=xM1-XIKcBxXPWN6zlx6MDkzmKpYOJAzpTuTqjppmGZE,4428
+qai_hub_models/models/efficientnet_b0/perf.yaml,sha256=xxJ7lRgM5BsXhM70_ah-BBTByiYJxnJqQVqf7pqmo74,4559
 qai_hub_models/models/efficientnet_b0/test.py,sha256=RyrRTanG4fCwWk8d_EaFn9j5GVqLdDcubsEzKBM16mI,867
 qai_hub_models/models/esrgan/__init__.py,sha256=BYOPJKlrdJCd_dkZlkI9_rvh_Ip3TtrD70sT4hk6mWM,463
-qai_hub_models/models/esrgan/conftest.py,sha256=I4EmCrb9_b9FAiH4tIM6V0jPq5dDgXcXT-xn-0uSaAA,980
+qai_hub_models/models/esrgan/conftest.py,sha256=nAb4KPJgxCpWfgWGOakDl35g6NO1NyqrfFjhsfoPCBU,1405
 qai_hub_models/models/esrgan/demo.py,sha256=QL3qaDz7O56O938cS7fMOPl_bb1gvcfJhP8onfAcMf4,939
-qai_hub_models/models/esrgan/export.py,sha256=vjeek7I4d4RvbA0YcTW7FWmv_8A7o5FkAInYRoxdkqM,8002
+qai_hub_models/models/esrgan/export.py,sha256=ZrIX3XdVuPrkugdkP753vKT192OpJsXt2JnaEbr6gSI,8283
 qai_hub_models/models/esrgan/info.yaml,sha256=uO-MC4Ex0o9haQh7iX2SBYKmtjEHkeko_xZmkGRAlPs,1117
 qai_hub_models/models/esrgan/model.py,sha256=zitbXArjzUEyP9_u5Z4ePgLFGYxG4DFKiWBRPdGy9Ow,3473
-qai_hub_models/models/esrgan/perf.yaml,sha256=_ZWbpDlr5JtouZu9zR44a-FSpbsEEpoHG2pOKHfrBKc,4460
+qai_hub_models/models/esrgan/perf.yaml,sha256=qPS8jr6-1auln7_o4msEQaXXigEWHS31rP9WA1Pw2ZM,4597
 qai_hub_models/models/esrgan/test.py,sha256=b4pQrPN0z66v__gKsgAcXqIfEViysY9yhstuOaiNXi0,1831
 qai_hub_models/models/facebook_denoiser/__init__.py,sha256=2p_IKIllP1Me0a2Ga3OfzoAWS-sarWBUoedNgVhknoY,418
 qai_hub_models/models/facebook_denoiser/app.py,sha256=FNIaoA5NYrjXxrOoL3kiDS7TI_MpuxKPldE3ru_tA_w,3207
-qai_hub_models/models/facebook_denoiser/conftest.py,sha256=p9w_wBB9s_W1GWPSOjsknAXZMvcnudvfhPawH2rAQKE,1002
+qai_hub_models/models/facebook_denoiser/conftest.py,sha256=I80EDdxb0xRVFjMoYWFb_mqBn7EuADI9KB6KugtSi1I,1416
 qai_hub_models/models/facebook_denoiser/demo.py,sha256=pwZ7sKCYgl2YcRWBD6L4WChECUCwu_OrUmMWcfoqQOc,3172
-qai_hub_models/models/facebook_denoiser/export.py,sha256=z2MgZsaxc7s-1Di66Bt8lqZcn1a3uehI3gQynrE0W0c,7670
+qai_hub_models/models/facebook_denoiser/export.py,sha256=TzP4EYf26f2Dg9533nH-hcYTmmP_YDY8XGQMPGdw0t4,7951
 qai_hub_models/models/facebook_denoiser/info.yaml,sha256=uBJBGRUnVPzhU6dK-LACvms0FCVSzJh2qYiLtv3JoGo,1070
 qai_hub_models/models/facebook_denoiser/model.py,sha256=_mrUIQsgrhGH_5HH103yFgEfpe1GEgU_FZe1qfbVTQo,2414
-qai_hub_models/models/facebook_denoiser/perf.yaml,sha256=JOfzNRAKBZU-9883_BqYC5ULqA5CztbB4ck9Ou_EBD0,3685
+qai_hub_models/models/facebook_denoiser/perf.yaml,sha256=dFSqSfm1uLyGsIMT_gf3-Mu3VEhORyX-zP674JsSB3A,3431
 qai_hub_models/models/facebook_denoiser/requirements.txt,sha256=o34BIQCgYdBbS6Yp3eup9VorcrIfdNiUoGtrK3AoiPg,74
 qai_hub_models/models/facebook_denoiser/test.py,sha256=kmnG_zoQOVGd45TqH-pJcRTDrHq_Y0TYymSPg2r7PdY,2492
 qai_hub_models/models/fastsam_s/__init__.py,sha256=t-LgQ3KECa94clPW6e9afDjg0_iE3_AT5wwGh9u0ViE,440
-qai_hub_models/models/fastsam_s/conftest.py,sha256=QfIFtpPs19JINAPuwmHizLtL7sOtHYdCCa58ZpcLlT4,900
+qai_hub_models/models/fastsam_s/conftest.py,sha256=_oEpAzGrHUH0toAcao-aw-DbKtK7HKL0_DXsTI8a_DM,1314
 qai_hub_models/models/fastsam_s/demo.py,sha256=Ja3h5Trq_ZHYvEpQLp6AVki87esrcrrymRv0RpNp35g,762
-qai_hub_models/models/fastsam_s/export.py,sha256=iGdi4Hd_-rAGxyRg35qXCrlt5fQJsfyUHWwSlCslszI,8264
+qai_hub_models/models/fastsam_s/export.py,sha256=UW1GJlZZeFGnWFMqJeL7pQz9WBDvIeeUeMLGlS7BSrc,8545
 qai_hub_models/models/fastsam_s/info.yaml,sha256=BGCkK4ixZsZXeXRq_6IUUBn-cc3mV-1Zcq73p3TjTlQ,1301
 qai_hub_models/models/fastsam_s/model.py,sha256=-ynkCngyRJIeb85n8GIsAmAWmAt_fiRcZ85pz1c0nPw,683
-qai_hub_models/models/fastsam_s/perf.yaml,sha256=n0WrFKadm4B-Lvfi2x9074gFXDzCEnnt8MCXcYunVOc,3660
+qai_hub_models/models/fastsam_s/perf.yaml,sha256=dQRFG2C71pbE11NYtxZezWIuEjNcA8QBTQrcYqhJ7pk,3402
 qai_hub_models/models/fastsam_s/requirements.txt,sha256=bzLR7n9PMXzABwKJJUEb6gOX23t_wZuYJkbyRPJjjQI,64
 qai_hub_models/models/fastsam_s/test.py,sha256=KJD_DBh8sto_x8A9VmEN-z2AyLBhTZyykZzGTv1KsMA,1332
 qai_hub_models/models/fastsam_x/__init__.py,sha256=1jZ4uim6OseYmBo5ppIM32GcilG-cEFb0CDmJAFvSPU,440
-qai_hub_models/models/fastsam_x/conftest.py,sha256=1GKSwPv29EJhCNXvUqDgVMSaRp4YDi9fvU70S6YXlC4,900
+qai_hub_models/models/fastsam_x/conftest.py,sha256=dsLBHKCGWnrbsTmDiP6YkPqWZHMjiVrHFpLSR4pxklE,1314
 qai_hub_models/models/fastsam_x/demo.py,sha256=7jJK0ZIIhiMh_XS_HOUmpek_XcKaeZ0TxQudsXzUBRc,762
-qai_hub_models/models/fastsam_x/export.py,sha256=pTsoCwNEN92apRgozJMXx_8HQLE__ajM5G7jcxbUqxc,8264
+qai_hub_models/models/fastsam_x/export.py,sha256=hyI6MXzsWuvS8-34RRUDXMi36pKXPrgemgetWJPrY-c,8545
 qai_hub_models/models/fastsam_x/info.yaml,sha256=Z3zYIUu7OAKfoOkoFd9j5jH-ZJII930iPflu_wFp12I,1300
 qai_hub_models/models/fastsam_x/model.py,sha256=15O2L72SANNVLxoWjNdugNV4hgUb0oZzdoxnNddQYM4,683
-qai_hub_models/models/fastsam_x/perf.yaml,sha256=7nu_SMQRgIpGnWQ5n8sHKLDxbsVVtZOx7AZfFnrXivg,3668
+qai_hub_models/models/fastsam_x/perf.yaml,sha256=V52hDrG37i-mOgxVjBmUamGKoH_QoU1Fi6fDwXzR_9M,3409
 qai_hub_models/models/fastsam_x/requirements.txt,sha256=bzLR7n9PMXzABwKJJUEb6gOX23t_wZuYJkbyRPJjjQI,64
 qai_hub_models/models/fastsam_x/test.py,sha256=QUaUSFuPrqESZSMF8R6c2zH2rGrvD7FdDXrcRjue8wk,1332
 qai_hub_models/models/fcn_resnet50/__init__.py,sha256=PIkLfZIDzf16OqQ6kptH6YVx_2MdAwWt3zLJJLtlbDc,410
 qai_hub_models/models/fcn_resnet50/app.py,sha256=U_aLzk8it6_-tC66l4Dz17-LgBJAfyIz9hPjKZGhO3M,2683
-qai_hub_models/models/fcn_resnet50/conftest.py,sha256=3dkT3E68bjtnzu_gZ1ILdnAsvbVM4LKqZxYbFBJ9sZs,992
+qai_hub_models/models/fcn_resnet50/conftest.py,sha256=Q7Dn2LQRlW19oWeM8xkEwJ9Ak43uYj8xlrIW_K_xNgM,1411
 qai_hub_models/models/fcn_resnet50/demo.py,sha256=aHhtDDnA4ESHE4JkkOs_7EJtsTKYAZxW8Btz9Wxhcbg,2315
-qai_hub_models/models/fcn_resnet50/export.py,sha256=ViyDzn-IyTEOXUE-yRTZ68RnTEysHlQtpyi1v29Cu3s,8169
+qai_hub_models/models/fcn_resnet50/export.py,sha256=wUdRw9tcYYggjLIWyUuzhPVXwKBR5t1ry86Y32R6fV8,8450
 qai_hub_models/models/fcn_resnet50/info.yaml,sha256=ny0XW7G2TGqi6uH9OAY4Gh8qzgd-QXNPqe8hSnayDLk,1241
 qai_hub_models/models/fcn_resnet50/model.py,sha256=LFsVZFks3DkPqsp62A5d6-ABQdVrIKlvBD6Sx2wrHpg,1989
-qai_hub_models/models/fcn_resnet50/perf.yaml,sha256=d2NIPrdgB13uQ5CSDsFfI6ZztZUazgHzLoyVgjfrNW4,4442
+qai_hub_models/models/fcn_resnet50/perf.yaml,sha256=zU9cHU_-bFye58uZbUDH2bt1aR3dCtmTuzMRGYmTSSg,4576
 qai_hub_models/models/fcn_resnet50/test.py,sha256=mIM8NwhGPlSf9KBX1QIWek3rTLoYC_bBvqMbZ0mrK70,1637
 qai_hub_models/models/ffnet_122ns_lowres/__init__.py,sha256=UoCIQtMaEcM3Dg5PhPZzDxbNdV4d3k_Rw0im4Jak6pY,487
-qai_hub_models/models/ffnet_122ns_lowres/conftest.py,sha256=tIRDyHxzyBoeWqcwso1mmJ0bIfpSpDFsQgmt5z74JaM,1004
+qai_hub_models/models/ffnet_122ns_lowres/conftest.py,sha256=ic0oXtut6oAfpsCALVx7KLlVIwWIxTIkUvOaw0Ef6A4,1417
 qai_hub_models/models/ffnet_122ns_lowres/demo.py,sha256=t2c7R8wXMYU_1XHw0y7W08uSWZFlG0ExvHW6RCDdaXg,607
-qai_hub_models/models/ffnet_122ns_lowres/export.py,sha256=WrmfZzJEnwZF2aS4TTJKrioTcw0qjX5ZUH8cX5m-3hk,8050
+qai_hub_models/models/ffnet_122ns_lowres/export.py,sha256=4ykCr0rSf-4k9klS5ZHv9iyE5Inu6-2WhpWsdMx3hcg,8331
 qai_hub_models/models/ffnet_122ns_lowres/info.yaml,sha256=V2mZJGqPO-kJJwTVNFzLJ_xmB661g--bhcRMwsGULE8,1322
 qai_hub_models/models/ffnet_122ns_lowres/model.py,sha256=Jxz7d6RB3rbuya27MvlQgDrjkovo-gejpCBhpQda8wc,648
-qai_hub_models/models/ffnet_122ns_lowres/perf.yaml,sha256=HM3BCmbR58A-VUom1govFY8oSlWGJc-BuxNE9C7-kWw,4455
+qai_hub_models/models/ffnet_122ns_lowres/perf.yaml,sha256=Yw18bbcdye-lAsmTNcWrEM3ntFXYTmoSptgVmpaWY8I,4585
 qai_hub_models/models/ffnet_122ns_lowres/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_122ns_lowres/test.py,sha256=izaltx2uBp4Sa9cxpa27oo8koqI9wMs4KykgoOYxxBc,804
 qai_hub_models/models/ffnet_40s/__init__.py,sha256=cur1hXK3ukU_DL_ujlWpJykMMaxCXzpfT6yqAoZX4w4,479
-qai_hub_models/models/ffnet_40s/conftest.py,sha256=lWSTh2N1EVBkAJSKfJIFfj0mYIT5Ul28GAHDyYqWqzw,986
+qai_hub_models/models/ffnet_40s/conftest.py,sha256=P4JxddjLlJCqqtwXq0Zla6_XedTzdjud5ZTdVj4q-fg,1408
 qai_hub_models/models/ffnet_40s/demo.py,sha256=e5examY5jBnCcFrA73SOXV4sdY0gkOF_YheXwgLQ7GA,582
-qai_hub_models/models/ffnet_40s/export.py,sha256=1iBaGDRuKjfRy7UaPaqENsDrHYp4ca6Ng69lIRKlVa8,8014
+qai_hub_models/models/ffnet_40s/export.py,sha256=iMjRCHvT90aqK44GvuNnVZ6PIuetym65gC8Pmfms_0g,8295
 qai_hub_models/models/ffnet_40s/info.yaml,sha256=DbHnioJHDD2KHUyYryhPGiss8VnBKHvKUaeM1C6QgzQ,1298
 qai_hub_models/models/ffnet_40s/model.py,sha256=vMqOf4XTdSWVlmM_72tBvjNtks874DkBM-aizeN5QEM,580
-qai_hub_models/models/ffnet_40s/perf.yaml,sha256=b45D_QRMIoavioJCgtSIdHXZV_ExbHpOMVZ1DZod-0k,4437
+qai_hub_models/models/ffnet_40s/perf.yaml,sha256=BPqjO4-jtVZdeDdfQgoHsK_fPRjJXA5ikD_RDh0ifsA,4571
 qai_hub_models/models/ffnet_40s/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_40s/test.py,sha256=Z-7FI6ko8Dw9B54eKzGZKYjPqJRT9iUumz3VaMMyzoE,746
 qai_hub_models/models/ffnet_40s_quantized/__init__.py,sha256=D9fLMk2pWGuTgkIU7jGbxtkoiWLnA7pajSv441yVE7w,490
-qai_hub_models/models/ffnet_40s_quantized/conftest.py,sha256=EVLBxZKszqVxHitcaZ0aGdA5rFfCuldQhbUdk6lEuQE,1006
+qai_hub_models/models/ffnet_40s_quantized/conftest.py,sha256=Xmad9xsA67ko118-nMnXSilR_VWkbo1ofbpJVxiGeDI,1418
 qai_hub_models/models/ffnet_40s_quantized/demo.py,sha256=a2SWiqFS6qv59AOAJgHvbBpRMMiFXGVI1v26YqxKZtU,627
-qai_hub_models/models/ffnet_40s_quantized/export.py,sha256=AlwF-TGMFAw4R0MeMDtQibHBs4X3boypf8jjA8zkIxk,8467
+qai_hub_models/models/ffnet_40s_quantized/export.py,sha256=Thd1hkMANXVT-iWb90LWofYbLZedLbB1i_h9nNzwQto,8731
 qai_hub_models/models/ffnet_40s_quantized/info.yaml,sha256=ZRi81cKXx7ih9hPKERPXwN6hIrpI4k0Q3ci8KfXsRJQ,1347
 qai_hub_models/models/ffnet_40s_quantized/model.py,sha256=_QVWRQb90OhW_0UCzzjy4-Hgc4V9DKRPrkvzhynHL9M,1169
-qai_hub_models/models/ffnet_40s_quantized/perf.yaml,sha256=7YMGTwnsglGH0wIb8nZRxEzRj9MU_j9EEwfwLNiAODg,3661
+qai_hub_models/models/ffnet_40s_quantized/perf.yaml,sha256=k_7IWVvg17PVOd4nOu82KsicRyoKBRKR6eo96Bvu8yo,5112
 qai_hub_models/models/ffnet_40s_quantized/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_40s_quantized/test.py,sha256=11tNLKpWi4Wz1D99W0NAW531ENyWDNYpfxi6sMfy-UI,840
 qai_hub_models/models/ffnet_54s/__init__.py,sha256=Zhyq437g7aiixQVmV1yF2iZSE_0L_2Hb53mZab_fH78,479
-qai_hub_models/models/ffnet_54s/conftest.py,sha256=h2ZefRpesghCUl0sgvcR8mJZpFr1sah79pGhIiY-Jq8,986
+qai_hub_models/models/ffnet_54s/conftest.py,sha256=wW6j8OlrEqTkSi66o7fpnDGk3zhC4uzaa6bO5WiVqk0,1408
 qai_hub_models/models/ffnet_54s/demo.py,sha256=6oplfV5nQdodhEYRApOupdZMZ6Jxk-yxUwJS-j1zYyo,582
-qai_hub_models/models/ffnet_54s/export.py,sha256=MMBp8EiSDSthJU9TBnTigX9-PBqlKAoyJ3y-0kHN4x0,8014
+qai_hub_models/models/ffnet_54s/export.py,sha256=PasZRSw4EtgKl74GwGf8iAl5y0YOpLrzmIt-IsALVo4,8295
 qai_hub_models/models/ffnet_54s/info.yaml,sha256=ipbP8LhRKP6hky2-CdAHgusN0zu4RcJLNkSj8UziC7M,1275
 qai_hub_models/models/ffnet_54s/model.py,sha256=uOu3W3LoWHUB4cUcHQDIRcn7d2INjhVn7nju2WxG7sc,580
-qai_hub_models/models/ffnet_54s/perf.yaml,sha256=IjSx2SoBjkFWNlTDZeaj_q1qkN-CLfs05qKGuFAQQpA,4454
+qai_hub_models/models/ffnet_54s/perf.yaml,sha256=f1fxwCDU_pZmMdoNCshXv1z2XSxIK__HaEAFsgO7Jqk,4591
 qai_hub_models/models/ffnet_54s/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_54s/test.py,sha256=wxVmLWpT7VdTph6c4ktD7yYNyCeB-ms2aYQyZnWNXII,746
 qai_hub_models/models/ffnet_54s_quantized/__init__.py,sha256=exZ2v9rO0y7x3f0-V2qTvVLoeOjbmqdSMpHDMI9-cfE,490
-qai_hub_models/models/ffnet_54s_quantized/conftest.py,sha256=S3PaKk_SCZzvbKNsn7XJU4MBvNosfxVtyKEtTGEEpEY,1006
+qai_hub_models/models/ffnet_54s_quantized/conftest.py,sha256=jA4965h2ygKxkMtNN__VJpB32QsEmcLaQvWO_cbjxH0,1418
 qai_hub_models/models/ffnet_54s_quantized/demo.py,sha256=QGw6h_1O-Ecp_yQJgAWqOgaGHPRQ2SvIrSrNf-_RdSY,627
-qai_hub_models/models/ffnet_54s_quantized/export.py,sha256=p8XeqNQl751nPnFo0f3Sr76yz4nzLXVIeZ2PdQwlrMw,8467
+qai_hub_models/models/ffnet_54s_quantized/export.py,sha256=O7gHcR75MdUFzgJsSxe21BsWHQk7Niw6YeJXJW5_sQA,8731
 qai_hub_models/models/ffnet_54s_quantized/info.yaml,sha256=whLq0baglC4ajzuhqqU7qvBh7cJBiK8yn61hUBalMOg,1347
 qai_hub_models/models/ffnet_54s_quantized/model.py,sha256=itN2MVSiS5LEUk9CcQfKFNM5JeCNrGQRwjeukp_7k4U,1156
-qai_hub_models/models/ffnet_54s_quantized/perf.yaml,sha256=VDHDfdQVuhEf5dqXGbHMbZd05qX-NC1FUGsgOhLR_mc,3668
+qai_hub_models/models/ffnet_54s_quantized/perf.yaml,sha256=bNKS2nI5AVG3h1dOKFrIqP2ngAHihkBcNKCMfE2FzY4,5127
 qai_hub_models/models/ffnet_54s_quantized/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_54s_quantized/test.py,sha256=QLlA7Q-IcJJZFYgXrFRyw_RyhULYRZkCcyXEpSeG1_M,840
 qai_hub_models/models/ffnet_78s/__init__.py,sha256=EZpm0M4BbQeAEyxW-qlt-qCFp0idi3Kbyo3qkhmtoIc,479
-qai_hub_models/models/ffnet_78s/conftest.py,sha256=YIp8y8gW0viYVUOAD3Hn0ZBGBAI7ANF5VQTKIjgdRb8,986
+qai_hub_models/models/ffnet_78s/conftest.py,sha256=-mp4Facal0kDqXF-BUr0qGcSfZyb26wKyxaiCwDgnG8,1408
 qai_hub_models/models/ffnet_78s/demo.py,sha256=4b0QiRJKM-uCuJbmhV5MlO9k8k9Y8nYHs3khMb8-IGs,582
-qai_hub_models/models/ffnet_78s/export.py,sha256=rCr-Ys2h1WKF0EZ0XBjKFGp2sD7Yh2XzG2oiNf9FOVk,8014
+qai_hub_models/models/ffnet_78s/export.py,sha256=2fNLZu77hDuUJyrKJi4WD-KXDi_ArXL2YbJHfVJ572g,8295
 qai_hub_models/models/ffnet_78s/info.yaml,sha256=eTVW7aCWkN36QgKXhmSHmuKgJE907R_6ZaFBHLLzY_w,1279
 qai_hub_models/models/ffnet_78s/model.py,sha256=zxXU_ZSi81vLKD92yok1BS3vgLgLkS8p2sfGJKZT0Os,580
-qai_hub_models/models/ffnet_78s/perf.yaml,sha256=DesrAjR9nzRAV279u17IittGCfmUMLAq696hGbpKMjI,4450
+qai_hub_models/models/ffnet_78s/perf.yaml,sha256=QhypgLe6AsSuZyzD74SGpZd541mvOWfdd5PLR9_KnBE,4585
 qai_hub_models/models/ffnet_78s/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_78s/test.py,sha256=rgdUjMRz67XdsnmHkyNH0ba3uvbXYyVfe5PfVnbekEs,746
 qai_hub_models/models/ffnet_78s_lowres/__init__.py,sha256=dGFCFlJOiHyZHYM-RC3aU-dT6dU1kGGAXxtoFtI1GPY,485
-qai_hub_models/models/ffnet_78s_lowres/conftest.py,sha256=ndvpiShsViE59iCFIxhwyxJuf7Q8b-L1xKz2Y67CYjM,1000
+qai_hub_models/models/ffnet_78s_lowres/conftest.py,sha256=eAzaEG2nMd34C7UpVOPQw5QjraxZnEwdZDjIPi3Ea0E,1415
 qai_hub_models/models/ffnet_78s_lowres/demo.py,sha256=DW_YMV_N9oqo3jn2VLiN6QjKDy2n70Y6UzTxABv7XAM,601
-qai_hub_models/models/ffnet_78s_lowres/export.py,sha256=SXpYonaWEA8QsVvmum1bOlUKSjjJQJOtZcTbq9jqbV8,8042
+qai_hub_models/models/ffnet_78s_lowres/export.py,sha256=NcYvFOX_XUTWTyuHvGnPqWSHGgDJucHseTNUL5hrRFQ,8323
 qai_hub_models/models/ffnet_78s_lowres/info.yaml,sha256=Mgrc9Bf84QxaywT0AIXKUx2zorycN8lQ2bCBl_VLN0g,1327
 qai_hub_models/models/ffnet_78s_lowres/model.py,sha256=ckAt5bunYsfv5pqH9jrwREUfZQJwaKtcgBCjOz-Vw3I,640
-qai_hub_models/models/ffnet_78s_lowres/perf.yaml,sha256=Ae4vtbFcRrNe-dTudIpjyQdwZ16AJPXJ6uc2kfeZZNQ,4446
+qai_hub_models/models/ffnet_78s_lowres/perf.yaml,sha256=Heg9ftF_lC8KNY0LC1VOxc9vdzoy9OAXL2A1zl-_lN4,4582
 qai_hub_models/models/ffnet_78s_lowres/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_78s_lowres/test.py,sha256=XuLMLq71pxQcI6SDVEqtwyl86MaLtxa9mtlLDfBgSSk,794
 qai_hub_models/models/ffnet_78s_quantized/__init__.py,sha256=byB0kzkpkVxI7x6LjOGtge9X58UkVzOkp_VjfUdJrVw,490
-qai_hub_models/models/ffnet_78s_quantized/conftest.py,sha256=NqWjPkw149TZigi4Ac5OkEHw5l7U_Q2yrQP26V31tTQ,1006
+qai_hub_models/models/ffnet_78s_quantized/conftest.py,sha256=8k14iOWqxpaGBYG4hUjBG36cdfgNwyiPUouglSjTAg8,1418
 qai_hub_models/models/ffnet_78s_quantized/demo.py,sha256=3mI25tVr9FGDR2y5_2PuYvKJ5vHCzE7Ogweps8oziAs,627
-qai_hub_models/models/ffnet_78s_quantized/export.py,sha256=NmxGcUmhUsgEX_zlNsHpXLoQln0FMNWKewd_VMt0jk8,8467
+qai_hub_models/models/ffnet_78s_quantized/export.py,sha256=cO0rzDxZDBpoglUl6jNErVSwXg9uEgRoqeAZwilkYkQ,8731
 qai_hub_models/models/ffnet_78s_quantized/info.yaml,sha256=WdgiQxWS3hXRs4A3xeP-Mis44omsqUNaQPhIHFYJ2r0,1347
 qai_hub_models/models/ffnet_78s_quantized/model.py,sha256=gilgE2OHPv4mCaKEr4AAcFyFzAdUGq92_RIvhbcqNTA,1156
-qai_hub_models/models/ffnet_78s_quantized/perf.yaml,sha256=xjkWYipRMwl8WYylYq4_qiKEjx2TRaO4wDAdc4_aw1o,3666
+qai_hub_models/models/ffnet_78s_quantized/perf.yaml,sha256=GT8Tmamo9KO069jdvpjMOvY9ljeKn0PVvngIr_KOoFc,5123
 qai_hub_models/models/ffnet_78s_quantized/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_78s_quantized/test.py,sha256=f63BCpe22Yh5AtLy3HmZ6M37zu3G7kr18SmevBwh-VI,840
 qai_hub_models/models/googlenet/__init__.py,sha256=yMXfFedJcmupf20Fk8xXuArMIPqOdIvxB2S_AJ5j1Pk,472
-qai_hub_models/models/googlenet/conftest.py,sha256=UplfK65r7WZ8QtJVmulnfe65DHdtm4rqUOvQrm6xYOI,900
+qai_hub_models/models/googlenet/conftest.py,sha256=KG0q37N6PuXolmfutSCwyqkepcjCMYtTgssfBCFJ90Y,1314
 qai_hub_models/models/googlenet/demo.py,sha256=Owxr4VAkRm-resJoBLK_1ZcAEnI2IwjHZ6Hgqs7zYXU,533
-qai_hub_models/models/googlenet/export.py,sha256=29CZslvFCuCXsnN17D1EKFzhx7m7-YLs0ngbmzxHNrg,7879
+qai_hub_models/models/googlenet/export.py,sha256=CVvEHTccEr0lCTr2ICPHp2tCHbttIr-q_64MbhL6nhY,8160
 qai_hub_models/models/googlenet/info.yaml,sha256=6Pb1-Auz6VGYb2FEYhCoKR6a7sJxkm-6eJ9KPoGz1_c,1295
 qai_hub_models/models/googlenet/model.py,sha256=7tVmhQtiQZeWGgeYTPVvwKr4ustB0tHoYafJyXZvVKQ,743
-qai_hub_models/models/googlenet/perf.yaml,sha256=XvTbWus0mzD6oJ6N_jChuLhBXnOLZf5CJHo7eZYex8I,4409
+qai_hub_models/models/googlenet/perf.yaml,sha256=UfJjBv4W_ilVFz1n0dWbCnXNL3J8QmejS3uH4nr1aow,4547
 qai_hub_models/models/googlenet/test.py,sha256=gJcMSpz8KmfhCwa9E0d6iTnlOB40dHwsr8Qp9B4QdAQ,840
 qai_hub_models/models/googlenet_quantized/__init__.py,sha256=96ENoBCt6lWCfOBrvFQJIE9xh-sA3IDtfUXmdIWVhqg,573
-qai_hub_models/models/googlenet_quantized/conftest.py,sha256=_RiShp_5sPn93L1eJbupV6yu71OnAibnKfND2LD8Qos,920
+qai_hub_models/models/googlenet_quantized/conftest.py,sha256=8bXw-7g42_83uQXzGwuYHomFIudS1YRmWbCfg6OEKpU,1324
 qai_hub_models/models/googlenet_quantized/demo.py,sha256=FT-qPTxt9WnT7-wPR0jitsw0LnhvkbHlRCfJE_jiB-Y,578
-qai_hub_models/models/googlenet_quantized/export.py,sha256=eF4ic913Mj9jzn3GdNGRhzDBNJ90r2OY7X9Hb1ANE4M,8359
+qai_hub_models/models/googlenet_quantized/export.py,sha256=8QJIrEd2YxeDxlKbGaKRC9zrJ1bCcGxJ-zp8sEdwZio,8623
 qai_hub_models/models/googlenet_quantized/info.yaml,sha256=VmjYexBXTnmukRqI8u0IEJYGZxMbKxze2l-Q3ROPyC0,1325
 qai_hub_models/models/googlenet_quantized/model.py,sha256=vGyWrxc-hmzDhR2HSVudaBPosPrPdodjZ6SqUUa7qIk,4298
-qai_hub_models/models/googlenet_quantized/perf.yaml,sha256=KPgmV0yHb3oCahOUXs77v5gtBpHLu_1nJ-0najh8g5k,4423
+qai_hub_models/models/googlenet_quantized/perf.yaml,sha256=qHTkMbg7k1vpKMyFsQua_tCmu2YsHV0KB8TD4ZPssXQ,6623
 qai_hub_models/models/googlenet_quantized/test.py,sha256=ox8Mz4egNJmOQzfD-gNNXu9k5zHs7ZHKn1rV3xlNNd0,885
 qai_hub_models/models/hrnet_pose/__init__.py,sha256=_r8rdSD0i5yJRlSct5_EQpCV-hEExVZYmI_mHV5VkWQ,430
-qai_hub_models/models/hrnet_pose/app.py,sha256=kioFIQB9kvspm1dtEbaOBn9o0R1XBu7bORye6rhQP8c,8134
-qai_hub_models/models/hrnet_pose/conftest.py,sha256=okrEhLkU2Hnfm3idY3JDH2Ly2oSxHDppfHofxH3E4r4,988
+qai_hub_models/models/hrnet_pose/app.py,sha256=3Ui8V3bt6Fu5dfB0KPdo9fAVLqsltG8ZUqS9p15PXOs,8458
+qai_hub_models/models/hrnet_pose/conftest.py,sha256=5ksauTz7qnrrGVO3Tu2c4Kp3Z7Xyn07nSNwRmoUC-ZI,1409
 qai_hub_models/models/hrnet_pose/demo.py,sha256=-XbTGmCKHJXe-qoj3Abrm81VZr1FGiJQ-Za6FBVFBww,1739
-qai_hub_models/models/hrnet_pose/export.py,sha256=3e6QFsKCsF5Thvcywll162yPLnIWafsi0421bg5yGSI,8160
+qai_hub_models/models/hrnet_pose/export.py,sha256=x5WN5qZ47yI9CdqkR7Rqm7wMe4Bk378Uvg9u2FSY2mU,8441
 qai_hub_models/models/hrnet_pose/info.yaml,sha256=QdjXu2cv2b6EpWTS92p4iSu8-D2YmcRv59Zdxae7-yA,1195
-qai_hub_models/models/hrnet_pose/model.py,sha256=b5b4NUBfnMaqre3KXue5reT01t4RAF9VHv8Jvh6ixBA,2801
-qai_hub_models/models/hrnet_pose/perf.yaml,sha256=jqJVyiZ_zZjyfPej3PW9SglplPQxH10nqYvrlGDY7cg,4426
+qai_hub_models/models/hrnet_pose/model.py,sha256=eYdiilsjMO8kuvH-FGuompMoMqHg2Ad-dT1Go75OLuc,3360
+qai_hub_models/models/hrnet_pose/perf.yaml,sha256=ODYLJANa5ue8ZWYSz6AakPq8LNRhygePZwJR3tHOWdo,4557
 qai_hub_models/models/hrnet_pose/requirements.txt,sha256=2U8K4vxuQ1x6tB35TVOiLW4UA0gN77o0ZvKJ5baKAgo,51
 qai_hub_models/models/hrnet_pose/test.py,sha256=LHAh3hxZfE1aeQt5Cwvv4SF3w6X3Dxz4tly_UxZpr84,1420
 qai_hub_models/models/huggingface_wavlm_base_plus/__init__.py,sha256=muVZdjzxn0rzXTLE50i1-birAkTSWucNt1VWyCoUwJ8,434
 qai_hub_models/models/huggingface_wavlm_base_plus/app.py,sha256=QlmoRNNkpzNGI-EzErNXsR8S-ygiC09QdAiS7ggnQs8,2133
-qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py,sha256=ocbQcBdTUBTANkvZvPi8ENwUJLvjr5mrpd5-EjB77ys,1022
+qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py,sha256=h1bUo9zjWU7pN-OKUKPU_rQYSivMqTKWyhNK-yA_ovM,1426
 qai_hub_models/models/huggingface_wavlm_base_plus/demo.py,sha256=NtjLWFe0oiJgcm10f5ws6a4SIx-7RNbRLL-Q3pB3csY,1517
-qai_hub_models/models/huggingface_wavlm_base_plus/export.py,sha256=W9Mc2PVm2RlSySza9709N54UQ91aKQgybR6UJWmQx8Q,7567
+qai_hub_models/models/huggingface_wavlm_base_plus/export.py,sha256=Vu0wDt9lhCrUBA6KH78Sa7LDW8m2E0nx8-dNVMknO9k,7848
 qai_hub_models/models/huggingface_wavlm_base_plus/info.yaml,sha256=fJzxA3WwfNBQnWZq65pR90IGEHzlm5hxoJHvzEhVN1E,1248
 qai_hub_models/models/huggingface_wavlm_base_plus/model.py,sha256=BjWZCqVFXNJaINoT1San9dY2aZOAFbZjJ_WKQLV-CGg,7587
-qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml,sha256=NFtS1iNP2pn-Jv1YZtN2ENKIPupl0VUB8LEZzyoO-6k,3698
+qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml,sha256=ESPCK_Shr8yhKwJG_IQE47GkmhNYGqOQ0NjxgZbPAgc,3446
 qai_hub_models/models/huggingface_wavlm_base_plus/requirements.txt,sha256=L9-RRcRpUwd13vGIthoV8IQaEALYng-Hd7RhR1hdpSE,72
 qai_hub_models/models/huggingface_wavlm_base_plus/test.py,sha256=irHDKl6YChuYRBq8zcNwe5p7CLd9ghY7yicSfHEtBZw,2560
 qai_hub_models/models/inception_v3/__init__.py,sha256=7VMcF8m79rmZ-UZk265C_tIMIHJev0qZFBz5cNzSd98,477
-qai_hub_models/models/inception_v3/conftest.py,sha256=ZGh1bPXQZGdeJVZ8WT8jd1GD5BSWkUoc6cMtxrNLUbQ,906
+qai_hub_models/models/inception_v3/conftest.py,sha256=qX9wf2lRJjAxyJTWsg-DpbRCnH1dCaD5rAP4Sppbj_E,1317
 qai_hub_models/models/inception_v3/demo.py,sha256=f9m6H1zPOHEGK1fi4PLDUYPb8TYoa8a4MqbjRWZBX1o,546
-qai_hub_models/models/inception_v3/export.py,sha256=ISZkP0_TbAFsH7ZSQUmYgT8KVnnuy9-I-KB13sKxrq0,7891
+qai_hub_models/models/inception_v3/export.py,sha256=Acz_grkLrrrcU5gjrBQSxzW99Frz9Topdn2udEzOsI8,8172
 qai_hub_models/models/inception_v3/info.yaml,sha256=XoUgkOsDw_vFkVOOMARDWbBGFdcsRt2EcGqIqJZC_BU,1359
 qai_hub_models/models/inception_v3/model.py,sha256=PKOe7hJigPXwymvjHC8Q_mwVFj5LTp96cIPA6hoaDto,756
-qai_hub_models/models/inception_v3/perf.yaml,sha256=ECL2nbDbmwY9D3WRLc5RdRRnQfqP1jNGGhkW4OVLaB8,4432
+qai_hub_models/models/inception_v3/perf.yaml,sha256=CEDhNficaLvvMUkoodHNcXhpHxRmYFKYsL4nmf4cVJ0,4565
 qai_hub_models/models/inception_v3/test.py,sha256=H4y4OTCinQAYkJvaceCOenfWzwjPqpWb2kk_hvSm2WM,861
 qai_hub_models/models/inception_v3_quantized/__init__.py,sha256=LYwSg6XsJc3GL5Bvm8AjHw5I2aCmG_kmpyQQRxxYlsM,584
-qai_hub_models/models/inception_v3_quantized/conftest.py,sha256=EihlfDsg84Ir_sALoRHtMG_fr5DDc9SZ6a5z65RRhPA,926
+qai_hub_models/models/inception_v3_quantized/conftest.py,sha256=R8oCGdmloUxRZ8TmFvJYpqW7Q4-sLSChRl5c1udk_jQ,1327
 qai_hub_models/models/inception_v3_quantized/demo.py,sha256=HT8ebEriXX7NZpj9rdy6CKBWSdpM10pxniEvhwVFe10,591
-qai_hub_models/models/inception_v3_quantized/export.py,sha256=cD_bkS8q1V1BDMVCWVS5Vh3BLnYnT89aCW-CWxwzRKc,8392
+qai_hub_models/models/inception_v3_quantized/export.py,sha256=6lG_w1N3fx3CJoVKHftKVi6e_WC045ZqZpgXHSvtkko,8636
 qai_hub_models/models/inception_v3_quantized/info.yaml,sha256=FsSC1SKNT3quSNQaELEGwfybCmlNi1L2xiRI2oV08CQ,1556
 qai_hub_models/models/inception_v3_quantized/model.py,sha256=3eASb8PKybBcQ67prsdQYAddsmwdsYDIiJmWGKOuoT0,7762
-qai_hub_models/models/inception_v3_quantized/perf.yaml,sha256=xitxNrgeKqnUmEUGRtAQWnq8EPA6OmP_IP53Xv59JRs,3656
+qai_hub_models/models/inception_v3_quantized/perf.yaml,sha256=ynfTPM08cL93a4WWfyyV72MbdbRPUNo4m5gm9HcvR_w,5106
 qai_hub_models/models/inception_v3_quantized/test.py,sha256=Xf0YrWDwy01ulM-o4IuAOQotc6hOU0Dn2IaV7h8SrA4,901
 qai_hub_models/models/lama_dilated/__init__.py,sha256=XfDJqXXsCsai11UnjLNnECFN6WquE9I55knTyDl-R7E,455
-qai_hub_models/models/lama_dilated/conftest.py,sha256=bmSz9TVTOdlUdepfKx2xni7IWweTBQ07M2oGwg5So5g,992
+qai_hub_models/models/lama_dilated/conftest.py,sha256=6Rizw6XyqvqEZrgrfy7xnOgKVOhDsdKOhjgtMxVkqVU,1411
 qai_hub_models/models/lama_dilated/demo.py,sha256=J8E3pR2GQTpmpqFfqPiQOfKUcywmDMPAzyUqIj2wMZY,911
-qai_hub_models/models/lama_dilated/export.py,sha256=JxVF14yOa09nmTuXQIhLPj7CvLpyVSKmiCU2bNby7NI,8179
+qai_hub_models/models/lama_dilated/export.py,sha256=HGxcqO52GxFK7sf1P8fWoweZOXrNdxjidP1Kc-LNhSY,8460
 qai_hub_models/models/lama_dilated/info.yaml,sha256=whgpaBOBqJnbIc6ErqRS4PTlaeBIkoJBEtrfscPeC9U,1082
 qai_hub_models/models/lama_dilated/model.py,sha256=uJp3NNmPQuzBXZ_gcgk1pK4DDkG7JmO_R-KlpbfpLy0,4977
-qai_hub_models/models/lama_dilated/perf.yaml,sha256=UZPQeiNTuGGNYH705cbJe6vLN0C0DwAuFtvE8HvBi7w,4407
+qai_hub_models/models/lama_dilated/perf.yaml,sha256=GoUbF8Rg10nX67oU03ibSWVY--uTJkY8QDXZwQuCYc0,4545
 qai_hub_models/models/lama_dilated/requirements.txt,sha256=897JpKdjy0fMli83RtBp6iSBaWYRHXNDK0oMaO5aMBo,171
 qai_hub_models/models/lama_dilated/test.py,sha256=g1YxEMQjYA65848I8YM0lxylfy4f0LGSJ38pJXrPOYM,2074
 qai_hub_models/models/litehrnet/__init__.py,sha256=lboiEAPcBxZN_ao_VVtBVJl-B9EvrrwXcxK02PiZbLY,404
 qai_hub_models/models/litehrnet/app.py,sha256=FEwh94V2uJrp6bofDJTP_p53fR85Ir-nCiacakJ90BI,3986
-qai_hub_models/models/litehrnet/conftest.py,sha256=1YOPJdOwkP8ABG_u8tUuTh38BpKMFagNrY9-_t5qpGA,900
+qai_hub_models/models/litehrnet/conftest.py,sha256=4YLcFq1IKhh66JyVpAwE7o1rsjIkW0JZJVVb7HGPW7U,1314
 qai_hub_models/models/litehrnet/demo.py,sha256=N3NDwdOwfdjFANzufoZq2mBE0yk4vTZZFQCl_fgk9Ds,2072
-qai_hub_models/models/litehrnet/export.py,sha256=hDups2agm9CsHmE51yhKzWTP8xzzPb_D_ogEW5MnkYM,7638
+qai_hub_models/models/litehrnet/export.py,sha256=QQ721z16MzMP8DfZdNaL8BWEGfbMY08GyGujziS35X4,7919
 qai_hub_models/models/litehrnet/info.yaml,sha256=xfMHffAMgyP3Z0UT9aBd5_C5yF4QYifTOvXd4-yGEDs,1112
 qai_hub_models/models/litehrnet/model.py,sha256=XELmt0HE-7-ZHQ7VOZAJNdAWBqR2Cg-zK_0ifR4UxGc,3788
-qai_hub_models/models/litehrnet/perf.yaml,sha256=c91sbiVaenoxuRQUYQjXQGqcGUdRPd4_zbxZGvsNCVQ,3620
+qai_hub_models/models/litehrnet/perf.yaml,sha256=ITkeSH4dj8dF9EHb_pVidiAOWhWdqyNloao3qQm2wLY,3366
 qai_hub_models/models/litehrnet/requirements.txt,sha256=CFGaEErf8SbXCYCCjg58TvJZIExg-yQiluC2q78zZoY,39
 qai_hub_models/models/litehrnet/test.py,sha256=FStsyYV21iqYE_DTsLCDyFBISas7lTXpvHrP86Xxc3M,1714
-qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml,sha256=rB9tYtIqi_cMMnWDorvulawmTGH7ATbTbWGQm8mvU_Q,1993
-qai_hub_models/models/llama_v2_7b_chat_quantized/perf.yaml,sha256=vS9DRf4_wVb93PCBo07qhZyOK37y-mcsER-FMIs4agU,2037
+qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml,sha256=F6lfh0zDgvKfB_1WU1uIKJcq8B22zPHtgbt6cNBnpdU,1997
+qai_hub_models/models/llama_v2_7b_chat_quantized/perf.yaml,sha256=c2QHcYkaKgtRAYZzmxq3dU_AzAA2UiAaj948gNGZVRk,2039
 qai_hub_models/models/mediapipe_face/__init__.py,sha256=24U-bkhm4gXXnVLowI7uytJShrDYSq5KbKWGxkK-xxM,412
 qai_hub_models/models/mediapipe_face/app.py,sha256=eug0kpc1nRgnnhO94P242qu8x-P15nmK9YDCRVi_4Ug,2099
-qai_hub_models/models/mediapipe_face/conftest.py,sha256=ry4doRmYtz__yWJtbGWXiuWExUELQfB4bsCJo33IB4I,996
+qai_hub_models/models/mediapipe_face/conftest.py,sha256=_mXXKud8SCZkaVm_5kok-6u4Jya-s_0eH2vhQUYKBEE,1413
 qai_hub_models/models/mediapipe_face/demo.py,sha256=k43EC7_YMXA4Wk41JwLaCtUc_1HSQFEyZ5UY317fWG8,2862
-qai_hub_models/models/mediapipe_face/export.py,sha256=TkGCrVAO3QQkHARQUXUXlIwLSWFoRyaVKT-NhXnACEM,9755
+qai_hub_models/models/mediapipe_face/export.py,sha256=jQBrhnGnhLybLzd_19MrGqJ4XvRLXMxd6E2oaLHydeE,10089
 qai_hub_models/models/mediapipe_face/info.yaml,sha256=QX3h5VsxsrDt0X6b0QpMelIhilsHJr6MatZewTwF2N4,1469
 qai_hub_models/models/mediapipe_face/model.py,sha256=WurHj_hb5bf8eOyryuBUAKzOC_mZkRgeM7fPpvERoss,7669
-qai_hub_models/models/mediapipe_face/perf.yaml,sha256=dVMvzpzGPSJXNJqc58kS6OObimMC2nlHFqNbDSUBuEg,8096
+qai_hub_models/models/mediapipe_face/perf.yaml,sha256=pZvWZNNxET6iCXpFU6YzbZfnEqUB46CfGOUDddF6VCs,8427
 qai_hub_models/models/mediapipe_face/test.py,sha256=eAgJ0ZjbMNL1bRBF6Ma-mVQ0hBMTQvbUEWO6PH8Gr4I,1441
 qai_hub_models/models/mediapipe_hand/__init__.py,sha256=XWXPWZ12S5pgPBmT3fwgql_JF_6Y_dY699q8nntteVQ,412
 qai_hub_models/models/mediapipe_hand/app.py,sha256=4-UayauaiZDVytVsclGrBnqvGjVQNf0PPCUwYVk-KLQ,9819
-qai_hub_models/models/mediapipe_hand/conftest.py,sha256=3U8iT-R0-Esc0SZ5VhldxgBJVijB8jvkz-UklSDBuaI,996
+qai_hub_models/models/mediapipe_hand/conftest.py,sha256=sroHsUr1p6RTvvcxk45k6rP03ylslgLjVQHlh0SdigE,1413
 qai_hub_models/models/mediapipe_hand/demo.py,sha256=u8ej7boLAgN-0O--m_lXFTVXb-4WW3qU4WKR5sijEJo,2832
-qai_hub_models/models/mediapipe_hand/export.py,sha256=p3n9sw0U5MSClD3IDR-kqxk-411Oht1H03AS_kmkSoY,9755
+qai_hub_models/models/mediapipe_hand/export.py,sha256=YUgyHcmmSKF7-VPIZgRF0GUBps2F9N-rgYUdDHHL2AU,10044
 qai_hub_models/models/mediapipe_hand/info.yaml,sha256=oSKmBY6rTjq_Jj8ToPWD7eIu6cxkVpB1RGuQ23rb7WU,1374
 qai_hub_models/models/mediapipe_hand/model.py,sha256=Mjz_ttgFHlNUsWIKbcIGEU3j6TMiOLHZvmnF0eY1df4,6017
-qai_hub_models/models/mediapipe_hand/perf.yaml,sha256=NzNrlHtSw3wCEtm07qYiKehx8CDItBRmAaise9VnLhE,8118
+qai_hub_models/models/mediapipe_hand/perf.yaml,sha256=Rn6sFtHD12hzLzjnYI9UC3z-fO22iclDwW3RAMWhG1A,8437
 qai_hub_models/models/mediapipe_hand/test.py,sha256=y-8RhCNppYbhLY_znVZKYVNwElIaJPdSkXEfBoyfGcM,1442
 qai_hub_models/models/mediapipe_pose/__init__.py,sha256=PE4aVOSnDXWvU9YVcKjBKUyO8926RMaeRXyD1chVXpw,412
 qai_hub_models/models/mediapipe_pose/app.py,sha256=djV3gcRkcvw2qYHBXqOjxY_2DxNahekk4BZ4Iax6uW4,4430
-qai_hub_models/models/mediapipe_pose/conftest.py,sha256=IFu2JzH2c_3PyIFy4vfzRxw76NdrBoVbHM0y2OP4lg8,996
+qai_hub_models/models/mediapipe_pose/conftest.py,sha256=-8fIHIDPA5Q-p0t45CuiVNtzL_uZH6XzxAeqLg9Dl6g,1413
 qai_hub_models/models/mediapipe_pose/demo.py,sha256=hvy2P0bX0dZ9cU0NrxSN8MjJqfat2M4GHlXjKcvixO4,2889
-qai_hub_models/models/mediapipe_pose/export.py,sha256=1SkXwMYtcqYpP-2Dw5KNyR30vzZPAhgApm-xae3GZJ4,9756
+qai_hub_models/models/mediapipe_pose/export.py,sha256=tuAmOy84LiAWQogoLm-wvCcU5I-btwGdHtqSnFNj8HU,10045
 qai_hub_models/models/mediapipe_pose/info.yaml,sha256=D2BVSmo_qZbGJIbONvBpj2JJcJFGtQA78VBMyInXkIA,1387
 qai_hub_models/models/mediapipe_pose/model.py,sha256=oPrNq_v0zKYU0OL84GwAKt9yUsdAaMsH4lDNsPB0k1Y,5801
-qai_hub_models/models/mediapipe_pose/perf.yaml,sha256=jPwkV3799gcekIB_4msqZe9kQwZSUfSmjrhWVZXaUnE,8103
+qai_hub_models/models/mediapipe_pose/perf.yaml,sha256=PM8KocYcEbCQKaWaVkUeApZlOVk5XPeLo7P5AA0GL7E,8435
 qai_hub_models/models/mediapipe_pose/test.py,sha256=rsTIYChYPqBL1lI9cl4y_QR6jCmRhb4FgAjcwhLSGA4,1443
 qai_hub_models/models/mediapipe_selfie/__init__.py,sha256=TrPNyvFmctjh5bMloBxJAIFZTgojNj16AOnPBH6T8R4,362
 qai_hub_models/models/mediapipe_selfie/app.py,sha256=nWD63c4A6_M0gwC8P9n0dTlzsXL45V9Ep8I6hy3Ytd8,1411
-qai_hub_models/models/mediapipe_selfie/conftest.py,sha256=yQNsBhlD07U83dK1jFhIrNRophSpKrLBg0chdGqaskQ,914
+qai_hub_models/models/mediapipe_selfie/conftest.py,sha256=wTMdTYYTOWWEK0OA96C0WeDhQZ47Pv6VE8krG2UAPNA,1321
 qai_hub_models/models/mediapipe_selfie/demo.py,sha256=DULze0M58GkHmBIpIEoB6So-EsgYoqHBC3CQspRSR4Y,2674
-qai_hub_models/models/mediapipe_selfie/export.py,sha256=VqUG4XMoRgNCeSzHPLvZS3HBPK_K5OkUgXZuzG_NJbM,8198
+qai_hub_models/models/mediapipe_selfie/export.py,sha256=1JM1ciGZUPsBEZqWZiYPlDJ1I9FtPdIpAiBU-J3mKy8,8479
 qai_hub_models/models/mediapipe_selfie/info.yaml,sha256=gEUPVKMaB0_qlPbEo1zM8uQ3dqAPK6YKQWHUNNOFdMo,1455
 qai_hub_models/models/mediapipe_selfie/model.py,sha256=COQHgx4VM0auz9h_etmnh1l3nQGU2BzmhP4qoqdIR_M,12352
-qai_hub_models/models/mediapipe_selfie/perf.yaml,sha256=-nH9E6BfcLhKE9ixp51vn2vzgE7VkKEQjxHldWVv2sI,4449
+qai_hub_models/models/mediapipe_selfie/perf.yaml,sha256=G2MkBZukovkgjWP8CNQxuLoUw-zPTlQf_OgzrI_rXMw,4583
 qai_hub_models/models/mediapipe_selfie/requirements.txt,sha256=jjmBqpB3WUtMLBjVgPXNNN5yslq_EXOWdM9WMOES_a4,15
 qai_hub_models/models/mediapipe_selfie/test.py,sha256=5VWzDmnNS9jz90tZhqTPIIFaBUeHQ8GYGJRnmpvIavs,1396
 qai_hub_models/models/mediapipe_selfie/utils.py,sha256=-Fh52J_5r4Ty0YZiKtwSnP4yi93K3kpBc3-Y9jPCEUk,2492
 qai_hub_models/models/mnasnet05/__init__.py,sha256=d3xeB69OYbrgNbOBMVMVsEqwClH2MMhlJqhfSUIx8UY,472
-qai_hub_models/models/mnasnet05/conftest.py,sha256=GsM0RlGz2wmj_Dc0_NR6-IQYJZHWHQOX7OIIGL-EMKE,900
+qai_hub_models/models/mnasnet05/conftest.py,sha256=NXm_I3IX1pUwEMMPte8jmUXWFNkhtetc0uaAfzwfxMA,1314
 qai_hub_models/models/mnasnet05/demo.py,sha256=Fgz0-kQofu_pf78mBfAA0kcenEwRIYtoLuaofrbcfYA,533
-qai_hub_models/models/mnasnet05/export.py,sha256=e7ZUgRTMEUpSn_dtHm64jFB1fSxaeBwf_ra-KBavDS4,7879
+qai_hub_models/models/mnasnet05/export.py,sha256=SldbNVN6E5WZjjwM7CEjIS2t6fQvdy71YwpXJWFgwfw,8160
 qai_hub_models/models/mnasnet05/info.yaml,sha256=A7bKsnRyqJJ6Jnx8SMQmn6EBaOMvHwXW08xOCLdT60o,1333
 qai_hub_models/models/mnasnet05/model.py,sha256=kkgjfQ0BbYuE_ls2Yh1zHIqncG678OyyvGTJGJemIjo,699
-qai_hub_models/models/mnasnet05/perf.yaml,sha256=z2ayms16d2uJUMuvwzJ2WvCJxdt764AVxgeBTLiINXE,4393
+qai_hub_models/models/mnasnet05/perf.yaml,sha256=FWn6SwNKfAYmibBks0Le1giaWV1CgZdIhAwr6dQLIL0,4528
 qai_hub_models/models/mnasnet05/test.py,sha256=Vcc7zhvyUHQXDj3mcFZetvZmV0y36askMPKqPZRJ1i8,882
 qai_hub_models/models/mobilenet_v2/__init__.py,sha256=EF3fi_jeNIAmKhFrVJdKaZjazdFlsYXrW1TZT3fDaxY,474
-qai_hub_models/models/mobilenet_v2/conftest.py,sha256=9Ig_No-mHV_fvhQrUbqXkmiSUqTkX0u3LizNSIyWA-M,992
+qai_hub_models/models/mobilenet_v2/conftest.py,sha256=r_wUwYWkmuceVH50ftHmgbdfXepuJRneZfQ96zEpjgI,1411
 qai_hub_models/models/mobilenet_v2/demo.py,sha256=GR2uHGNaKtqt-bIO7rjjgR2V0fdziHAQWwDZs7FvZV4,540
-qai_hub_models/models/mobilenet_v2/export.py,sha256=ItKh11DZbK3kP7ylpeAUeN3tP7YSbQSx5D3oU3GCY88,7891
+qai_hub_models/models/mobilenet_v2/export.py,sha256=vnkg1FacVBIg4GEdKLf1OPS0ZvyiM4FhjYiudjXRPnE,8172
 qai_hub_models/models/mobilenet_v2/info.yaml,sha256=irPP5qDlcSbPbjkqJSO4k_iA159GEVB1u4pIruYAlyc,1380
 qai_hub_models/models/mobilenet_v2/model.py,sha256=Tn-8Pf3YJEf0dXEOgoO2XYWquN8AtKSCYOtE1XJaHNk,2457
-qai_hub_models/models/mobilenet_v2/perf.yaml,sha256=jIjqvXPXPDdUg_YoEe7KiYqtaioqe5WjWPkSfLsDGlw,4419
+qai_hub_models/models/mobilenet_v2/perf.yaml,sha256=MddUazC5xUNu3Sjo0gsbBE6A46EisCjrZ8Af2_gy6kY,4553
 qai_hub_models/models/mobilenet_v2/test.py,sha256=Kjj6s-5jkKoqRaV7GkE9zf7js34k37uBSluNMFM3V-4,1091
 qai_hub_models/models/mobilenet_v2_quantized/__init__.py,sha256=MQOqYDYL-abmQsTEsRl7dBpFWu0Vd2mAW64gj02npN4,485
-qai_hub_models/models/mobilenet_v2_quantized/conftest.py,sha256=RJjrm508tof2lfHqqFOspR8XOXQCgcvNwdV_52z2fG0,1012
+qai_hub_models/models/mobilenet_v2_quantized/conftest.py,sha256=QjSfo5VicitRAzkuz14oKseEztLYNb6rPN4bFaxG75M,1421
 qai_hub_models/models/mobilenet_v2_quantized/demo.py,sha256=oxbLdz1mYSbr0GA4W6nRtFvnAp9Muma8F3eTP2VEKNo,585
-qai_hub_models/models/mobilenet_v2_quantized/export.py,sha256=PVcqu3hLtl38h7BwBdKbYOnfn3d_qwiI_PXs0L8aBx0,8372
+qai_hub_models/models/mobilenet_v2_quantized/export.py,sha256=HVIE43btnCjjEhiTFDI3vpg713c69a0xAVvsdztg-Fc,8636
 qai_hub_models/models/mobilenet_v2_quantized/info.yaml,sha256=B8RS3tsewoOtaOVFYWCp6aVqKJ2VIiCK3GJHfz8ghPo,1362
 qai_hub_models/models/mobilenet_v2_quantized/model.py,sha256=gye1E7azV4sqwZTh4113gmdytvv-SHf1sPOgSmFny88,3429
-qai_hub_models/models/mobilenet_v2_quantized/perf.yaml,sha256=U6IdxP8CdDwDRlSxIdsqMj27gnPXnzjsZyjq18Q8CUo,4423
+qai_hub_models/models/mobilenet_v2_quantized/perf.yaml,sha256=bHG3p_hnr273plVtUWv6aAVfxQ0vJHvx_4G883euPdE,6626
 qai_hub_models/models/mobilenet_v2_quantized/test.py,sha256=h54ZCxWmJA87qmOXeFpYZl8_3wLUQgDAJgowNlk_QhM,1002
 qai_hub_models/models/mobilenet_v3_large/__init__.py,sha256=7a-2Ng3rVspm46Uf4wyI8WfezV5IVaKm-oCfdkM0JVQ,479
-qai_hub_models/models/mobilenet_v3_large/conftest.py,sha256=7WUkyKyBfQAAP3zBreDoZREblvLdbRPRSD3r8sTvLS8,918
+qai_hub_models/models/mobilenet_v3_large/conftest.py,sha256=rk5NFEOGN4kuCK49JsWa8iDOysqt1fVh2s9gQE0kXYc,1323
 qai_hub_models/models/mobilenet_v3_large/demo.py,sha256=4KxbUpanGuUgvVcRk-YTTf90uQlEsMaBhbpjmXYEPUI,556
-qai_hub_models/models/mobilenet_v3_large/export.py,sha256=D4pRUuMYudoy7jVLy3vrpf1RYkg8PuhnubHYp7K3pGQ,7935
+qai_hub_models/models/mobilenet_v3_large/export.py,sha256=4HzSxL1HlZ7xz9ovkU_Ncv1ZSkYeeh_R1wx6FD0s9ZY,8216
 qai_hub_models/models/mobilenet_v3_large/info.yaml,sha256=CbC2G_Zw1YshI-danUA20dWHRyzP2ZFsqHnVZU1nBUc,1340
 qai_hub_models/models/mobilenet_v3_large/model.py,sha256=AjaEjXhIddKyIg6FSzzGloeZibD96zC7HORYX7MZOqw,721
-qai_hub_models/models/mobilenet_v3_large/perf.yaml,sha256=Oq_QLXAlXczdRPxeQPSALsu62Z1xxrUuCDBvm8zhrcs,3644
+qai_hub_models/models/mobilenet_v3_large/perf.yaml,sha256=V3tX_Km-DZZnq3EuFk-IhS1dd3IS03ZVUbfAogIzlqo,3392
 qai_hub_models/models/mobilenet_v3_large/test.py,sha256=BKu3TBqXd6EelITtwPlPq58e20I530caSxC4vahg_MA,879
 qai_hub_models/models/mobilenet_v3_large_quantized/__init__.py,sha256=HWjXMytQDRVZh4KUJTPN8z_oCl6vbY5dtI1lC6C79ZI,607
-qai_hub_models/models/mobilenet_v3_large_quantized/conftest.py,sha256=1IVT8o6gQfThxEvgdGOrW8W1mKk348WM5HyMcWvoDB4,938
+qai_hub_models/models/mobilenet_v3_large_quantized/conftest.py,sha256=V-2BAa7ZQPyxO1Nmgch5prnoJPmMlKeNeeVGPAD9OXY,1333
 qai_hub_models/models/mobilenet_v3_large_quantized/demo.py,sha256=XKC0CItIVpPeV60QD8VR7zAKBasKdjjcLZGuutvTm3s,748
-qai_hub_models/models/mobilenet_v3_large_quantized/export.py,sha256=e2NPb_LGjV0kprYLmO8mB6JKMboeqPV452-eiBTJICs,8104
+qai_hub_models/models/mobilenet_v3_large_quantized/export.py,sha256=f-X87IauD1Roih9AG7UIatzAXhoDvlGLyEZp99bEpf4,8368
 qai_hub_models/models/mobilenet_v3_large_quantized/info.yaml,sha256=w72_OYX5316wOqxjPIrMG1CD4q9mqAsRbE1tnsS7IOM,1374
 qai_hub_models/models/mobilenet_v3_large_quantized/model.py,sha256=4H8KV9f1ju-Wv77NqjmXTJaI2FXffV6sYLMDDgxi7rw,3075
-qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml,sha256=f9IjMlTA7j6tTsq8aiJddtcXXTe2lEsi2eJoCudYMTo,3673
+qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml,sha256=O3MlwhBUrVeNwYNW1qyFyiM_lfUhCfm4Ns3VDNEp61k,5123
 qai_hub_models/models/mobilenet_v3_large_quantized/test.py,sha256=ruAffWPJDAifH_miRT6hNoOnaNY7tJpSXZ42U8zM9Yc,917
 qai_hub_models/models/mobilenet_v3_small/__init__.py,sha256=RhAKXSevAm64F3DtEwC1whT3RiawVPUuZRpHxupCwvY,479
-qai_hub_models/models/mobilenet_v3_small/conftest.py,sha256=DTQPnX2ZJUOcENlj0fQVSFzB3sMIE1DYANKbJQtv24A,918
+qai_hub_models/models/mobilenet_v3_small/conftest.py,sha256=-bA5nmhYo-6fvaMmH5_LgV-SXzc1sYWrg2zwyWuQEro,1323
 qai_hub_models/models/mobilenet_v3_small/demo.py,sha256=f2RqGEK_t9UJ2MNkIYTPnAkeNhOk5ORtBgORks8pQzE,556
-qai_hub_models/models/mobilenet_v3_small/export.py,sha256=D2OinHQ0I3p10v50ZlUAgu0BjbBAGIlo72v_ukqLSy4,7935
+qai_hub_models/models/mobilenet_v3_small/export.py,sha256=2O1IaPXT4r6O3FBtrID9KVcYFrqKQR1R7A-frc0pV9Y,8216
 qai_hub_models/models/mobilenet_v3_small/info.yaml,sha256=1UbgDdCaGry4M8mc17lsEQHN0c7dzg1OYujzQncLDGw,1338
 qai_hub_models/models/mobilenet_v3_small/model.py,sha256=AMSPekj2w7Odv-HX_YUPs5AIFiDMDmbaq-77eRM2MtU,721
-qai_hub_models/models/mobilenet_v3_small/perf.yaml,sha256=x5OtkJw9NRxNoqlnj5u57RcGfA-dHynSRgqWdVL2UEs,3648
+qai_hub_models/models/mobilenet_v3_small/perf.yaml,sha256=14u3wQu306ykCqpCTMnIghT9t0TPb7uRSElNIcPUyQc,3396
 qai_hub_models/models/mobilenet_v3_small/test.py,sha256=PkDBlLPWfV8ktLcict2uFWM7pn2nl8B_NtKHKIp9njM,879
 qai_hub_models/models/openai_clip/__init__.py,sha256=zmmQCL2hQl0hA_RIPa0pQrKUYOXbMBQWmaYYlSy-XrU,394
 qai_hub_models/models/openai_clip/app.py,sha256=hgOce5Y7aS2Y_V-EltMyp6CkLRXQDL0PTqelz-pBf9Y,3958
-qai_hub_models/models/openai_clip/conftest.py,sha256=bev7VH9Y--iOeI7Tps4PHX2JWWR9Frz5O5EUljDm-KU,990
+qai_hub_models/models/openai_clip/conftest.py,sha256=rJ4rxdfv_hfnNfwrOCcUuxDfRdSzAkckAFSEH0f_tQU,1410
 qai_hub_models/models/openai_clip/demo.py,sha256=3zSDWd1n8MHfreShD06TLyTTivGTmydTLeiA1UUJy9Y,3404
-qai_hub_models/models/openai_clip/export.py,sha256=qoOJ0JWBCvcgTigEWnjK-icB555TK5YTMHhUC2Qzlzk,9711
+qai_hub_models/models/openai_clip/export.py,sha256=elz1iWJqlsmLQgoIWdgKhH75eHXdmmkySXk0TLpSBdg,10000
 qai_hub_models/models/openai_clip/info.yaml,sha256=tZQO7SbgP6M2JFAeoGOzl6WnuLhG_lyfWbiCc9WtpMM,1494
 qai_hub_models/models/openai_clip/model.py,sha256=KSRCG8Z9_KE_PyGUoTpzMSH41s0xvXEEnbGrg3p0hLk,5374
-qai_hub_models/models/openai_clip/perf.yaml,sha256=2rHERgIgF94xREfgTdwrxdYwgXN6NEQZswpoXDkdN2w,6529
+qai_hub_models/models/openai_clip/perf.yaml,sha256=gHAIb_FzMc4vpys4IsNmtCe13wihaSl0kJhoswKVk8w,6070
 qai_hub_models/models/openai_clip/requirements.txt,sha256=qCIDOVe-LijMtnGXDgJtHPUP8Yig6otJASVYYaxE4JQ,29
 qai_hub_models/models/openai_clip/test.py,sha256=PV-_Wn-pUdg4M0DbbemnPREGDskjQnv8yqm19L9M2L4,2118
 qai_hub_models/models/openpose/__init__.py,sha256=DJnYvOA5-007mXB0GUl1TnKyBSNqFsQYlpGCpixb9Jk,402
 qai_hub_models/models/openpose/app.py,sha256=MrS_HKYzz8-ylXJH9_x2jEJg1H0ZZDBaYaWDOmme-Uc,12008
-qai_hub_models/models/openpose/conftest.py,sha256=uswkPSIX2k0yHNaMLKqgk_HpeW9UZGI8w7bwOXwOPEg,984
+qai_hub_models/models/openpose/conftest.py,sha256=EHKFXp3E3-vyR4Og9fU3JhPrjOpnLCse4_zaK8iJZTQ,1407
 qai_hub_models/models/openpose/demo.py,sha256=rPuIuHf7S_ncKOIFWoit8rR1jFG4uvHrAZ_NviMthuI,2053
-qai_hub_models/models/openpose/export.py,sha256=MYIXfRyGnSKCvHExllyEk6kxQfSgNWsNK2ZoEOEJ-d8,8171
+qai_hub_models/models/openpose/export.py,sha256=iMH_Z3RM9t0Q8WPrU9yCP3xT9qqhiZWP122qv9e7Kco,8452
 qai_hub_models/models/openpose/info.yaml,sha256=zZAZK46yA0iB2H7zccZHf0cxuQjdMdO3osgi1K-hRZo,1246
 qai_hub_models/models/openpose/model.py,sha256=iWpglU2znltOYKlQ9w-1xBt9yrrySJW5YhNBAH6lMkw,5084
-qai_hub_models/models/openpose/perf.yaml,sha256=fm9TK-qERIdJiyXK1WBy4oDiI1NmJhNQQiPz1_j162g,4438
+qai_hub_models/models/openpose/perf.yaml,sha256=mHhw8IhKgQAk81GfaVDKpWPWARDrPG8k4dqQqFBpzW8,4572
 qai_hub_models/models/openpose/requirements.txt,sha256=0xg0sn4HQZDSpN03SNwngv61Ry0AXQZvwPHrXtSuR00,31
 qai_hub_models/models/openpose/test.py,sha256=JhSAWSbHck2xqktdTa_2RAaBdoJOL86g6jLigVNHrgU,1321
 qai_hub_models/models/quicksrnetlarge/__init__.py,sha256=n0s0M56rmewoor90_VOznUSWRXF8hBUbBkwMNjKg1xQ,472
-qai_hub_models/models/quicksrnetlarge/conftest.py,sha256=iWxFgo0BdeV06oH0bb6TXfxNRFKEzhd0rnVXdKxwtt8,998
+qai_hub_models/models/quicksrnetlarge/conftest.py,sha256=cgAWpB2Y43uSdUEMn91Tbx0WVdMcJkC6sr-LDbhaJx8,1414
 qai_hub_models/models/quicksrnetlarge/demo.py,sha256=IgRtHWqQVLGsuNXJTb9kwaWkiR9xuz36Qnhc9Z34jKA,972
-qai_hub_models/models/quicksrnetlarge/export.py,sha256=efeI70rfwFR4UM1E0ilNnlPm-j1efd7jRGhc5bLLl0U,8181
+qai_hub_models/models/quicksrnetlarge/export.py,sha256=9gKWn-8mXf7ySkKe9ZjeS8GdguiqMyYImtB4fpCetzc,8462
 qai_hub_models/models/quicksrnetlarge/info.yaml,sha256=jwBhAffyc2KTYL7kvsETHA6a9gvrc0XEZ2vEOgaRqdU,1248
 qai_hub_models/models/quicksrnetlarge/model.py,sha256=DZpev_PEQicNemo1YZgQ1PXNZG0QgRuvQ53H2DNdHWc,3170
-qai_hub_models/models/quicksrnetlarge/perf.yaml,sha256=IJ0zShR56DYddJfytvd8K9rO0325urx2vGj9imUzA1E,4422
+qai_hub_models/models/quicksrnetlarge/perf.yaml,sha256=W_RXXBBTB3aSqyvq15h5wnryYB9qRmHGtor4a8Dw1Y8,4555
 qai_hub_models/models/quicksrnetlarge/test.py,sha256=HICEbmnkI-D2g1MYjiUGkzMcl1Sg9NjC9JS7vttnveU,1422
 qai_hub_models/models/quicksrnetlarge_quantized/__init__.py,sha256=kpW-qloH72jOP0aPE4r_0ZBcV8MackQ1Ro4yiunY9sM,483
-qai_hub_models/models/quicksrnetlarge_quantized/conftest.py,sha256=1u1yRi5Zc2bXLUeu0YbqtA2gSs0-fAjxVI1_DoHQZHQ,1018
+qai_hub_models/models/quicksrnetlarge_quantized/conftest.py,sha256=gBUr78hNbQe_2YCqX0AMIpjSTka3cYhWFL4pO0MLFqU,1424
 qai_hub_models/models/quicksrnetlarge_quantized/demo.py,sha256=gScOAfsMlVs9d-GBMW8zHfPeP7CJiFawQldlU1CVuvQ,891
-qai_hub_models/models/quicksrnetlarge_quantized/export.py,sha256=2ROI_sG-o2-DwWp9TQTBvvgjYBb76bGLBtm2lWkWOPA,8634
+qai_hub_models/models/quicksrnetlarge_quantized/export.py,sha256=xw3tIDNA4aU2-G9TdTD6Ip1RtuM2eEjZHGj8-u5CcoY,8898
 qai_hub_models/models/quicksrnetlarge_quantized/info.yaml,sha256=qcahBBa33N55My5C2mINys5t3l41qQOFE3h-nLFR-ec,1274
 qai_hub_models/models/quicksrnetlarge_quantized/model.py,sha256=XeQCDOktDx1m2s56AlOEGwcsfUCkvI6i_MNU5VWPErI,4587
-qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml,sha256=GXbB2BbYjKKb-3XrH6yItbrIILU-wfyNGM4Mtd4Ne1Y,2485
+qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml,sha256=Tcn_-tngDBRtdecap65fjdmDO3olrkmjBvFiLXQlot4,3932
 qai_hub_models/models/quicksrnetlarge_quantized/test.py,sha256=C7ytsb9iOxrsxGz__BL5XrGU2xBqYUwM9A1XOa6--Zc,2921
 qai_hub_models/models/quicksrnetmedium/__init__.py,sha256=GTyPcoDHE8Hz702v_Bk__uhwi2N0E2KqyXxPQ5wpY54,473
-qai_hub_models/models/quicksrnetmedium/conftest.py,sha256=MdNMSQH_3HKfqfc7I5-GSX-g6W1FEcduPwsA9LlIFN0,1000
+qai_hub_models/models/quicksrnetmedium/conftest.py,sha256=DB4QeL-aok28nxmwvITQPtS7TpJrYinykdGhQTR7XHs,1415
 qai_hub_models/models/quicksrnetmedium/demo.py,sha256=EPLslU6kBps_nPCOUFcfVYWE66rGNIRGx9zC8gAoOes,976
-qai_hub_models/models/quicksrnetmedium/export.py,sha256=VECraeXBVPG7If_YJHHqRmJsloUXKrbZ1Ay-ilVS51o,8185
+qai_hub_models/models/quicksrnetmedium/export.py,sha256=UqUMI5pyU3rndWHruZ8y601RY2FGnmTALBFBv6xcW2E,8466
 qai_hub_models/models/quicksrnetmedium/info.yaml,sha256=qF6K6GLHopGKnuT0H3OBAwFMssi5KAt-4LW2ijIHWd8,1242
 qai_hub_models/models/quicksrnetmedium/model.py,sha256=ChFo9Fr8BHeFB7b_l6FfiWwE85fFZMMlmh_AxTxoCE8,3177
-qai_hub_models/models/quicksrnetmedium/perf.yaml,sha256=-ceHyhIYMOGsnqb0UVIKD6o_73U2TR8PE4-rk3egNFs,4419
+qai_hub_models/models/quicksrnetmedium/perf.yaml,sha256=6UxYjjldX9TaPmSy2LhowneUmlZqbfnhgLy5lHU11Ws,4554
 qai_hub_models/models/quicksrnetmedium/test.py,sha256=7MmkvCPJul33giNuZK8iAyRN11ci7Zw4bOq8LcjW824,1428
 qai_hub_models/models/quicksrnetmedium_quantized/__init__.py,sha256=1S_5eD4OACNMsJuLjPa1A7qoFokADw1fpuU3MtBkx6M,484
-qai_hub_models/models/quicksrnetmedium_quantized/conftest.py,sha256=LiJ2kOvsSB_BFQQ25Ly95FMt-g0mufGFrkfSFGzOeDI,1020
+qai_hub_models/models/quicksrnetmedium_quantized/conftest.py,sha256=k5oeKWvIP5oH9yEMhmhVmTJxhZ6KgCfvY8jueF5UWYE,1425
 qai_hub_models/models/quicksrnetmedium_quantized/demo.py,sha256=6AGfl5ee02A7ZnM08cIJDOG14YEpQ1gWhJvPqBNVHEE,900
-qai_hub_models/models/quicksrnetmedium_quantized/export.py,sha256=dNztMKK0HIntu1TQeLIZdtPyoBWZQYYo5N7vmw0A0LU,8638
+qai_hub_models/models/quicksrnetmedium_quantized/export.py,sha256=zF_-spk5ATOfz_5QeXzNiDv3Uxk-98z7REjhJIc_fEQ,8902
 qai_hub_models/models/quicksrnetmedium_quantized/info.yaml,sha256=oas-ZZ1q7bu5_jE1tiAPIAvsbbCRii6V-9piQUVh1Tw,1282
 qai_hub_models/models/quicksrnetmedium_quantized/model.py,sha256=QtXYgZiBkUH4YXV1w-1jFmvDVYFLafAIlnxmcHYI19M,4597
-qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml,sha256=4rFMOZpQn67w-CmzO9vqRzSsTdnlLim4oz202L7JEIU,2488
+qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml,sha256=Wzi1GCUXD1CG18gjdehngpm1T1MVcq3f_Cw6wbrShqg,3935
 qai_hub_models/models/quicksrnetmedium_quantized/test.py,sha256=g10Mox6qmH2HVihYkCXfOeBEo6CVGDA5CEXsuMIG1Es,2912
 qai_hub_models/models/quicksrnetsmall/__init__.py,sha256=x3REqFLXfs8YWq2ld1olPTKLwdQ5aRavepT-WIj9f2Y,472
-qai_hub_models/models/quicksrnetsmall/conftest.py,sha256=YMJStKlUSpTK_8ui_UTEcGyKfUhdfvrtDiVXzDIE4zs,998
+qai_hub_models/models/quicksrnetsmall/conftest.py,sha256=Bzd6X75IGiPiue1s_4U6xM1w0HKvXh3aNDw6GVoxtB8,1414
 qai_hub_models/models/quicksrnetsmall/demo.py,sha256=y1Z_GTvbWhOaJ7iyVMqTYaUxxePBhUI0J0bguRaEUB4,972
-qai_hub_models/models/quicksrnetsmall/export.py,sha256=SXxv1GFYjXTv-Pr2jhM1ul-Kwdam4B6I8UuSe861ZX4,8181
+qai_hub_models/models/quicksrnetsmall/export.py,sha256=-FcI0gtqUe0YnKaZuXQ9_jDlvk95dH9CKuu6f1shphk,8462
 qai_hub_models/models/quicksrnetsmall/info.yaml,sha256=cPEJEcn1_ctwMF7nHQ9-AeUsPaq9mhjhfb4Gm77Vgp8,1238
 qai_hub_models/models/quicksrnetsmall/model.py,sha256=6cufAzO8C3wItx4iQpkXrexkV5W9nUsi8z9OUcIhogA,3170
-qai_hub_models/models/quicksrnetsmall/perf.yaml,sha256=8aFUfFjy9001OLHu1LucJFTNyULO0LU6MpDtoM1neIo,4413
+qai_hub_models/models/quicksrnetsmall/perf.yaml,sha256=CDccV19aNgO_3cubSoakWcw7pZKKGp5bd0_OuNW_X0A,4546
 qai_hub_models/models/quicksrnetsmall/test.py,sha256=d3OvtLPqk_3uzP2cSQMa6G4MJu9kx27aXOLBaeCzdPc,1422
 qai_hub_models/models/quicksrnetsmall_quantized/__init__.py,sha256=DYwTcgaBjIFARfMJ64zY-VPAT1DWz0kdcMunIa0uNuM,483
-qai_hub_models/models/quicksrnetsmall_quantized/conftest.py,sha256=7_mIH306wEY00_0C8UembQHI5xWJzrtVm7IytauIq2E,1018
+qai_hub_models/models/quicksrnetsmall_quantized/conftest.py,sha256=k3SFCtby_o6U06FOTiUoXQJLdFffSkkwN7dG9eVZHKA,1424
 qai_hub_models/models/quicksrnetsmall_quantized/demo.py,sha256=mYIQyNQniXfQEVkmli0ft40hNwiXkeXkjUJWhT08YvE,891
-qai_hub_models/models/quicksrnetsmall_quantized/export.py,sha256=3V954w9CEcbWVnnQhrIdmEJpKg83VCw9R0DIcCPj4ik,8634
+qai_hub_models/models/quicksrnetsmall_quantized/export.py,sha256=gmy33X5dO7mNfr9QzcVby-C3ldgJV5pgVAWigaa_ETU,8898
 qai_hub_models/models/quicksrnetsmall_quantized/info.yaml,sha256=OiqbT9kVtT8AxmvJ0QTU3i-kZwOaoDj2bkc9F7iS4mk,1278
 qai_hub_models/models/quicksrnetsmall_quantized/model.py,sha256=GfLJVnZxxDgWFcGmhHJbEdX28SYgW_QoqlBCdbBZroc,4577
-qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml,sha256=FX8_2CbArdyk6ffp6Q6y22axVM6QXslC5oaczPH_GpI,2484
+qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml,sha256=CnIxFzRzXG71MU-nQlopoGiiRRJAn_9BA1TpBlJM3Sc,3932
 qai_hub_models/models/quicksrnetsmall_quantized/test.py,sha256=bQdBBfrTuY5mN24JOeq8QYVz95uoQkw3NxFG9cUuVn4,2859
 qai_hub_models/models/real_esrgan_general_x4v3/__init__.py,sha256=-eD72P0MMcOI5S_6ZbVDAtQ5MAsZFiAZ2-Rs5wkfGCc,481
-qai_hub_models/models/real_esrgan_general_x4v3/conftest.py,sha256=m9br30O2D9u9reTypWuBFzIA2xqeANk3siuNzQmFcpg,1016
+qai_hub_models/models/real_esrgan_general_x4v3/conftest.py,sha256=hJY_booqt2Gd904osWkT08IKF_vSn7Q1bo8EtZGR8eI,1423
 qai_hub_models/models/real_esrgan_general_x4v3/demo.py,sha256=mdJUw7jmSDTPXDVyGHAOMgI-RUQdcjhvdLqju0Zeskg,1280
-qai_hub_models/models/real_esrgan_general_x4v3/export.py,sha256=O2aDAesmqpbnb3WKVg9XOcrAVdyP1IE4z3WTRVHkBNU,8217
+qai_hub_models/models/real_esrgan_general_x4v3/export.py,sha256=denqqxbWKnPw2LKWU92CsuG7VRKbgn03wTTCEHInuOM,8498
 qai_hub_models/models/real_esrgan_general_x4v3/info.yaml,sha256=pKlB9DD4Qw5WDJgKW9xuxvccY-48a3vXY14Cp5M7RQ4,1206
-qai_hub_models/models/real_esrgan_general_x4v3/model.py,sha256=X0osnTyA3iHcWX-TlmCu6LN3-FUeLlPalUP6oSTk-nw,5226
-qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml,sha256=3LaXDJp2bFwGQsdemG1AUUf-7pENs-IS6gre6DBOWac,4443
+qai_hub_models/models/real_esrgan_general_x4v3/model.py,sha256=hBBSs9w-CVg2ueIi_BxNKgaqCrI5YRf3ABM0HJK2gfw,5435
+qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml,sha256=S0EOLjf4SX4qFsLvC5YG0ZBO0yGp8VL8BKCveATmERA,4580
 qai_hub_models/models/real_esrgan_general_x4v3/requirements.txt,sha256=eivyaYj7iqy9Z98WwPoKDVihhM_HDbjOBu49PUryfJI,44
 qai_hub_models/models/real_esrgan_general_x4v3/test.py,sha256=CfOp0wKNA7sXZihxigo9inRQH-AQLkdzeg_o5KMKZIg,1480
 qai_hub_models/models/real_esrgan_x4plus/__init__.py,sha256=9EdQhD9AYsj6uAL3JlTGCNhMZc-2MiyfpZDJEmjspMQ,475
-qai_hub_models/models/real_esrgan_x4plus/conftest.py,sha256=_RYIUePYZKehGxjei0LK5feLM4VHgE5zSBg8-UaHupg,1004
+qai_hub_models/models/real_esrgan_x4plus/conftest.py,sha256=hIJbBMJWuYVBnLbn6XUc64sHl34twzivNigkOQj2WvA,1417
 qai_hub_models/models/real_esrgan_x4plus/demo.py,sha256=_X0Jc79lhoHqB01NfCidG-TFBMaDtez-4Ipc5DCCL8Y,1256
-qai_hub_models/models/real_esrgan_x4plus/export.py,sha256=JL-SphhOUikqPMy3_4OnAGZMxhbymSdNEeMbMZS_2zI,7654
+qai_hub_models/models/real_esrgan_x4plus/export.py,sha256=XNyE1x5ss124oVcDa379Ep06kwWvfB2QjJuFJ1qs9Wc,7935
 qai_hub_models/models/real_esrgan_x4plus/info.yaml,sha256=xP8_0t2kPCJmXXY0M1oTjK5mPYGNaWMpgmRZ9rXNfUk,1330
 qai_hub_models/models/real_esrgan_x4plus/model.py,sha256=aH0QcIXpz336Nw7ca7BUPzMtuVFRvaFNOlun9pFVhQc,4443
-qai_hub_models/models/real_esrgan_x4plus/perf.yaml,sha256=C_CXBnfPgh79_2-IaCqKKzCzQCYy8WRhkxkjjhHQB7w,3673
+qai_hub_models/models/real_esrgan_x4plus/perf.yaml,sha256=FtcUnJVTFvIQYcqXd7FpeJ99oT9vL4m3FCfM53GyYYQ,3412
 qai_hub_models/models/real_esrgan_x4plus/requirements.txt,sha256=eivyaYj7iqy9Z98WwPoKDVihhM_HDbjOBu49PUryfJI,44
 qai_hub_models/models/real_esrgan_x4plus/test.py,sha256=amJIYWs1sw7ndZgviSvm9X4uZSUH8TAz9Sa_9lyCTUA,1440
 qai_hub_models/models/regnet/__init__.py,sha256=eVvJ3D2GfGdcBriF_ihwwMVGDFu5OtZSuPz61h-uKsM,469
-qai_hub_models/models/regnet/conftest.py,sha256=-vgdZlzN3w_54HqHgY2-ZoFjgFbg2SVY2p5tjgHcpgc,894
+qai_hub_models/models/regnet/conftest.py,sha256=Cv_og-XOwnHDzfkA7dhoVsxEcrknTDFBtWNduCktnPE,1311
 qai_hub_models/models/regnet/demo.py,sha256=QTBBLpnIfWQLVnJOzzj1-pW5QWbpl68HJQ55_rtMAZs,524
-qai_hub_models/models/regnet/export.py,sha256=lHifWnGm7fCfxHFKr9pdTfBh5b4uqfMu7CYEHZIwSv8,7867
+qai_hub_models/models/regnet/export.py,sha256=Vw0mzJDRryIyZ-R2MI2ef9uxjTcEJM0hgpmThZ1ejhA,8148
 qai_hub_models/models/regnet/info.yaml,sha256=hdUbHswd8D4ya7xX4_YPl4ZOT1ItebCTvId0VEh-mBg,1291
 qai_hub_models/models/regnet/model.py,sha256=ZAd-Ymzr5ak1Fe3oMS5a2DFvwnOWEKYJZaEP2tu-fQo,635
-qai_hub_models/models/regnet/perf.yaml,sha256=vW_0MKs0lH0SWE9riPNw6MckDrb-aCDXAHqh35pwo5o,4424
+qai_hub_models/models/regnet/perf.yaml,sha256=_DtbeTSVawkgu0VMbR-qA8cJX8vKDok5TK9kQvF3xt0,4558
 qai_hub_models/models/regnet/test.py,sha256=zZ4cDdGDWjkZ15L8zTOn770-LYfeTyRPaR-J2OA-RnI,984
 qai_hub_models/models/resnet101/__init__.py,sha256=giKR_QG6BGLXFiSVr_sRAJ-h2b2Ea6L8rhZgxEoJ7jY,472
-qai_hub_models/models/resnet101/conftest.py,sha256=VimFHSIxjCAi71NH3R9cpUbU_j-RbjdPoKAMmsqMPo4,900
+qai_hub_models/models/resnet101/conftest.py,sha256=an87gaJVW9hYWAb78cfkimhVyJkmWtLtBZcoTtM3838,1314
 qai_hub_models/models/resnet101/demo.py,sha256=RYdwNDEnxQpmUsUPP4qvYiGYJar-1WfLBbO5cVMx6N8,533
-qai_hub_models/models/resnet101/export.py,sha256=OTX5fEdD8f3QTWQo-bTq44gE50m6MTNgyrzVGy6fgJ4,7879
+qai_hub_models/models/resnet101/export.py,sha256=CPHqkl4h37aNobFM2oK3NrvYpM-Xf0FVSHRb6zJ9Wzw,8160
 qai_hub_models/models/resnet101/info.yaml,sha256=fbGPKTdV8UBcjMHR0aVK6amiizctrlbiQFaDSLrotcw,1312
 qai_hub_models/models/resnet101/model.py,sha256=Pjnm6vSdazT6uhrfn0Hx7hLU-U2rF2KiTMa9jWlmm7o,609
-qai_hub_models/models/resnet101/perf.yaml,sha256=HfiWuZSZ6QemWmHRweJohaIseB5xm56s10t9xNDcuLM,4436
+qai_hub_models/models/resnet101/perf.yaml,sha256=-01MYMB5_KCkmihHLhqNy-UZTK7dV3DaRUV-VLnfNcs,4570
 qai_hub_models/models/resnet101/test.py,sha256=Gr4R3pO8Be43sRBm3KEc5NaJ3fHofnMMWfKTJ3K9lW0,961
 qai_hub_models/models/resnet101_quantized/__init__.py,sha256=aJsoYEIOw9cKKPYWV-wtGWvN45BTKvqZVoN31Eag4TA,483
-qai_hub_models/models/resnet101_quantized/conftest.py,sha256=mefhOjTyfCv-IbnJ3BrNlNuTtcGzY1UsmcSmPB2iuSY,920
+qai_hub_models/models/resnet101_quantized/conftest.py,sha256=snEUk5hF5Digs9PrP8fE4lL-I-mttGaoOCMmFwAHHms,1324
 qai_hub_models/models/resnet101_quantized/demo.py,sha256=to8ShYfeBVmzd0OkyV4SxeflUXeedhqmrc6dwbnanfQ,578
-qai_hub_models/models/resnet101_quantized/export.py,sha256=qQ-nXnNW9ZXmaiGhXVWeDC1xN4BXnLTxGWRMPl5hQEQ,8359
+qai_hub_models/models/resnet101_quantized/export.py,sha256=FyABn-zxrzf4eaOqXpI_k2HFD9_8M5fLeCbfREoMk-c,8623
 qai_hub_models/models/resnet101_quantized/info.yaml,sha256=28u2ptmsdw9O6ZAljymiAaTOZxV6DDD-_eXsXYZx3X8,1346
 qai_hub_models/models/resnet101_quantized/model.py,sha256=7QEdqZJ4vJ1w51aJK4O11WvuJZuXcyBkFXpx5Blmi_4,3210
-qai_hub_models/models/resnet101_quantized/perf.yaml,sha256=k2bOHxTaniI2Rc7BHhMzEJszgS27tI-mDnX_822FPyQ,4437
+qai_hub_models/models/resnet101_quantized/perf.yaml,sha256=_HEIS4QwfrG4kUhcNCSlqhnR4ucs_liNHRu45g1rEB8,6649
 qai_hub_models/models/resnet101_quantized/test.py,sha256=C-8zqBiwN-nCgJb1I9RYfMykLPSKJIWW_t_Pbh97mJM,921
 qai_hub_models/models/resnet18/__init__.py,sha256=YXCoY4ADkV9rxnJQmx6th3bEwYaxE4UF6ECJMkuN4gY,471
-qai_hub_models/models/resnet18/conftest.py,sha256=TAcwnazTwiIPxByxU625YwIZjDiA8-I5GUd-p6NHSZE,898
+qai_hub_models/models/resnet18/conftest.py,sha256=it7mAYPB7-SDG_KypQElINnLd14JXA1S_bWW80XYuto,1313
 qai_hub_models/models/resnet18/demo.py,sha256=e_s-tuUKno5C8PmEh8AISsqywzoNtYUQVFJjyaJRLYU,530
-qai_hub_models/models/resnet18/export.py,sha256=f7YafLDSqMCLK6jyz7XZGK4viTQOzRgHc2aruSHkn1M,7875
+qai_hub_models/models/resnet18/export.py,sha256=OPAmZyZJH19KE3waNduxC7y2jrJRxn_k575r7kRWNIM,8156
 qai_hub_models/models/resnet18/info.yaml,sha256=EApwQW1yExiQzFT8qWYraKeeULj1aF-_j15imZQshuU,1310
 qai_hub_models/models/resnet18/model.py,sha256=gl5xAhxkR3kMkbQ5DQXSZ42eGFE0uLN2gNmrianZWEY,607
-qai_hub_models/models/resnet18/perf.yaml,sha256=aPH4wF1_-PiidGxIbqm_Auqhhk12_o1-UTpUO3KJKnk,4408
+qai_hub_models/models/resnet18/perf.yaml,sha256=lf6DXi_bZmuGSLPWphOXqJkT5QaVeaL-HNc0gUo_Egw,4539
 qai_hub_models/models/resnet18/test.py,sha256=raI736nKtyy7_kNAmWEci7wQYPqPQ_ajP0h7MTfpdW4,955
 qai_hub_models/models/resnet18_quantized/__init__.py,sha256=6MfeTP9-civpAklCTt8I8kwEj4qfK2bpGpBFzi7Gp9Q,482
-qai_hub_models/models/resnet18_quantized/conftest.py,sha256=ea1HuqnhttyIgGi7uDAJiEcgDDY8x5GKooAx9E_v6u8,918
+qai_hub_models/models/resnet18_quantized/conftest.py,sha256=K1SyZCqYud_LeDwOtoo0V07J4hJAUehFN9WspAhNOQo,1323
 qai_hub_models/models/resnet18_quantized/demo.py,sha256=J0yoGrm2LP5iUCCMXqc-paqC0VJZPP_Zi2heqCd_FGk,562
-qai_hub_models/models/resnet18_quantized/export.py,sha256=HFkos23ACf2up_oSihWvWBBxNM2k1utUbHQxloMNgTc,8355
+qai_hub_models/models/resnet18_quantized/export.py,sha256=M_bPCh1zlyHw1Ceebl5m54PnXhmY9SVsGo3Cl4pGydE,8619
 qai_hub_models/models/resnet18_quantized/info.yaml,sha256=EF_rGgvR-mbYSxraghrHtbP_TYxrI6cLx4xG2PuRPmE,1343
 qai_hub_models/models/resnet18_quantized/model.py,sha256=ayCXNSVAyPBqDnxYyE-UNG77dsNGhXuS6nBH-yNP7uo,3012
-qai_hub_models/models/resnet18_quantized/perf.yaml,sha256=6M3yivo4QBRpqGcZmoyhn-6OQB3WAunbktoN4Ua2exU,4411
+qai_hub_models/models/resnet18_quantized/perf.yaml,sha256=PArdCZ3z3i5Cjrxe_NfAGyxLDqvR0c7BOD9zYPrYe4M,6614
 qai_hub_models/models/resnet18_quantized/test.py,sha256=gB-2uNRrC-phYHX6dwCn6cVSAxpVq41FFX_UwPwb5qk,917
 qai_hub_models/models/resnet50/__init__.py,sha256=i6A3by1VSJeHInRvYv3jO-x3QRBzywCjnlKuG6ALGA8,471
-qai_hub_models/models/resnet50/conftest.py,sha256=iPKAMySlPbpydU4N2jijq0pXJkaMkHyk_7-Lkf_5CDg,898
+qai_hub_models/models/resnet50/conftest.py,sha256=0FMjxPcc8P38c9qy2GZFP6OtPr5cpGtvCnnA19DusgU,1313
 qai_hub_models/models/resnet50/demo.py,sha256=Ad2nWpI-kmP5M1sGhFAqSZFZ1Mz0Lov4q0ilsJIsdBg,530
-qai_hub_models/models/resnet50/export.py,sha256=aVxIA-zx0o5awp4QrqHXr_E18in5XvJCFURibYKjDNY,7875
+qai_hub_models/models/resnet50/export.py,sha256=PHrEv8xlw4QdxE-vHyK1WswIpf8icAfGmqm0DMkY2tM,8156
 qai_hub_models/models/resnet50/info.yaml,sha256=mF1FdZhk7GG8vBDs0sW6l0VWW9jBxyZAkyS6SdaAF7M,1303
 qai_hub_models/models/resnet50/model.py,sha256=3Uz5cop4u1xHyZuR9ss0Lg6cSxWyV_xuJcgFENbiHlY,607
-qai_hub_models/models/resnet50/perf.yaml,sha256=elVeynu-xaYdqtMqauK-QkyBjEEfDBsyofyvx4ttjQg,4418
+qai_hub_models/models/resnet50/perf.yaml,sha256=FZmELMtiKEx4G08Xkdw98VWOh2Y_n0c1sN-_a7AUx64,4553
 qai_hub_models/models/resnet50/test.py,sha256=7iQrq9aU53jUwGBRxj6dRXH9SwvFqTwegbMMxTmO5Fc,955
 qai_hub_models/models/resnext101/__init__.py,sha256=KsXu0bXmQcD_vUB9tOkoYL8cPsCuexuQfSA_ZoSEtxc,473
-qai_hub_models/models/resnext101/conftest.py,sha256=BXLqYikBB_tiff9HrRGb3fxh-eNFPa-3tQTjSYoiJOk,902
+qai_hub_models/models/resnext101/conftest.py,sha256=QBDbnLIcZmM38CpOpAycixR5lvhAJ4cmg0PpQiofl-8,1315
 qai_hub_models/models/resnext101/demo.py,sha256=W_MBGy--3Gyo6IkASUus4IeC2OJo6w7_vh8AN--pHj4,536
-qai_hub_models/models/resnext101/export.py,sha256=l_qAa4fRfY4FM9B-7Zdid8YA4Dy4lYYZJUt-uAy945U,7883
+qai_hub_models/models/resnext101/export.py,sha256=VTd20lCOqzNc-xm048r25mjGQKHEdSJhxuBf0thb20w,8164
 qai_hub_models/models/resnext101/info.yaml,sha256=QQQ7IKSLieegTlThhnySi_BdYTnXcrBzSt-FNmAfspo,1324
 qai_hub_models/models/resnext101/model.py,sha256=dD8WyCTD397PbDRLg_EoNX-hcQekN2P7S8OrzHRz7SU,617
-qai_hub_models/models/resnext101/perf.yaml,sha256=yttbnVtzcAI_7IieLVt-Dru33eQO6Hz4U3OD2DEPP4A,4436
+qai_hub_models/models/resnext101/perf.yaml,sha256=YDFYgDtCC-LrIuKYQOFNJ4TqQJ7uP9ntfhWrCEu5-Fw,4568
 qai_hub_models/models/resnext101/test.py,sha256=Gf470ZYn46oHSNisYxPiShq74rXykhqvriYbnNRnwoY,897
 qai_hub_models/models/resnext101_quantized/__init__.py,sha256=Sr23eMhxryEjt6hRjftWbdgS9YcDk1b4OshOi7irTY8,484
-qai_hub_models/models/resnext101_quantized/conftest.py,sha256=Rx15T1AFt5AsA-bJpv8roVtFGJ8b32phEzVV9NKFH_0,922
+qai_hub_models/models/resnext101_quantized/conftest.py,sha256=Zj1RXi90rgF6NokRUCy5f6ific7tmppJ-L-INSWepoc,1325
 qai_hub_models/models/resnext101_quantized/demo.py,sha256=Po6sLvHLkvqNm9tZ-vAeM-A-t_FLKaE18gTnJh6oSuo,581
-qai_hub_models/models/resnext101_quantized/export.py,sha256=1CxhCRqEJ1pWuBoTLhwLui8GH-_HMkyGQbLUH6x8cZg,8383
+qai_hub_models/models/resnext101_quantized/export.py,sha256=LOIaRxka7taUtpxiTdniot7o2-mA502aD9uk8IPVE1o,8647
 qai_hub_models/models/resnext101_quantized/info.yaml,sha256=Zlrngu5Y1UGGPhy_4bgpaE86oIzeB5NIeiAslZh8cto,1365
 qai_hub_models/models/resnext101_quantized/model.py,sha256=BQAwwRR0Z2ix9UGwm2DtMZYUohwaSXrQyZ8Da298LYg,3017
-qai_hub_models/models/resnext101_quantized/perf.yaml,sha256=DBVxBbb7aw2aHJpNIo_N5tdFQh8v1n-37m0YyBfca3Y,3658
+qai_hub_models/models/resnext101_quantized/perf.yaml,sha256=HLTteNi_dXEF1x_QSINZa12b6rD80HnGjgsgLBAKeUs,5115
 qai_hub_models/models/resnext101_quantized/test.py,sha256=jw2RahxL2zJq95ittlG6mBH1HKfCe-5MLpuL6f_9NOw,925
 qai_hub_models/models/resnext50/__init__.py,sha256=mvGLhLT1_Hj0eQ5sERqw2qBaaHTc8q9nGaoIMc_zVwE,472
-qai_hub_models/models/resnext50/conftest.py,sha256=0sAqTtCuUs2CUoo9HvOj8bjynDaA_fEmmf0BtVaD4CI,900
+qai_hub_models/models/resnext50/conftest.py,sha256=xg4KNyUABmbqbR54NpX-c22OYxK-J4KSahzIw4zqmyY,1314
 qai_hub_models/models/resnext50/demo.py,sha256=layKRO8OVlf7zDRarcUKuBwziX6mh0TahnxW_B4hOR8,533
-qai_hub_models/models/resnext50/export.py,sha256=mPGxM0Rl9-ULtL3LSWUEMi1RhjfymRmFRwYlCAdAup0,7879
+qai_hub_models/models/resnext50/export.py,sha256=tjwa-TPeonJXejIx_x3otlbotm9ZL6eQgw-y7LyiC58,8160
 qai_hub_models/models/resnext50/info.yaml,sha256=x36YHWLVNgYYWVx4Y9GrJYKmwbgCmIXcUrVuX5Mv_XI,1322
 qai_hub_models/models/resnext50/model.py,sha256=g_2NntA_nWKxlgNZJiLaSpk-VcUO4S4EBtK1fLqvFH0,704
-qai_hub_models/models/resnext50/perf.yaml,sha256=bH8XaVF6wYrTB1RJcZUqAhkio6VCSjAt2VwmsN9Xedg,4421
+qai_hub_models/models/resnext50/perf.yaml,sha256=f1xiIcuMRavdZrfMLLJ-YKKpLwY08wcUwHNQzMS2-xo,4554
 qai_hub_models/models/resnext50/test.py,sha256=mO0iDd4c_7F_QjcOypdCsexc47QbdJpKFKFU5Mbwipk,840
 qai_hub_models/models/resnext50_quantized/__init__.py,sha256=4Oqop6yfnVXt2Zh9RoVJLqXYnXkriZIMSa90EcFh08Q,483
-qai_hub_models/models/resnext50_quantized/conftest.py,sha256=1qc-CpEvyxHlJpvGG3Y3EOiAwPpciMRJ80TBZw1WvkM,920
+qai_hub_models/models/resnext50_quantized/conftest.py,sha256=cKaMIuZ_Lxy91femyBDaT2Vnx9Y229XaDJSsohDgl_E,1324
 qai_hub_models/models/resnext50_quantized/demo.py,sha256=sl3AOEAMwvqU2v3_7mpy3yUD-8j1JpcL_WCTpkel_8w,578
-qai_hub_models/models/resnext50_quantized/export.py,sha256=PW3ER7OQgO5yQbZQAANFVLWCpM9LS7QJGtwWhBZWkHE,8379
+qai_hub_models/models/resnext50_quantized/export.py,sha256=Qc3JwIJ4tlJXf5oAN9QFnHVK9ZRg4rRaQLkKCGkJMQ4,8643
 qai_hub_models/models/resnext50_quantized/info.yaml,sha256=cC9rkqVl4QXSu_EjJRQY97AAqh8FsBuez-Rf3XSWae0,1362
 qai_hub_models/models/resnext50_quantized/model.py,sha256=0mq9UHGSzZfPTnAkiEZmPVs1HCZizJ7PPiIoLiN9jN0,3008
-qai_hub_models/models/resnext50_quantized/perf.yaml,sha256=k41HuhIPjpxVryN2K_LcQ-tEE4qhE5oIe3HrYaKm1_w,3642
+qai_hub_models/models/resnext50_quantized/perf.yaml,sha256=dvQcvRhmF_9jtoOT_QamNLyEST4-dK7e29IGoc7YhYM,5084
 qai_hub_models/models/resnext50_quantized/test.py,sha256=zmx2xRiJL2_B6zZwUmhMWzSm7cmcbCGyay7dB9O1XKY,921
 qai_hub_models/models/sam/__init__.py,sha256=z2l9N1qWNXdFDly6YvRC6MBOTANKL28klDYmF2qF7Qs,404
 qai_hub_models/models/sam/app.py,sha256=jCDEbspglkuXi5SS86A96Ek0SaqO-xhHWwHPh_3N8LY,5101
-qai_hub_models/models/sam/conftest.py,sha256=N3pExY_BC9_0ScpQHjvoeLczPt37cjKhr0_hHCrKvTE,1015
+qai_hub_models/models/sam/conftest.py,sha256=LTnCPPC4ahdYfXzeghcDWpzA5Dr0IgkPvv6UPrjGaPE,1402
 qai_hub_models/models/sam/demo.py,sha256=G-Fv5x7usY6c1YeSdRzKBpw4KhC-9akcxINsspJBiHg,3088
-qai_hub_models/models/sam/export.py,sha256=tJ0yT8gyuzIp5Siu5DE5mkXjxTqbLH0vmyzZttuscVg,10152
+qai_hub_models/models/sam/export.py,sha256=6yNX5zuoAHMBrMgEnwIuvl1jI1Wub8adKgPqUfQyTfY,10468
 qai_hub_models/models/sam/info.yaml,sha256=b0Dez8dcQ4IAQs0xAHWsYkAvolTKi_hbFSRJvr19f4s,1391
 qai_hub_models/models/sam/model.py,sha256=qaW2mQiobMnmQ-SDme_JYPdbzUKHMqjeu4B7CJTEkx4,12000
-qai_hub_models/models/sam/perf.yaml,sha256=kAt2A5IbOlokGFQbmc5oMaUKmbN7etljkcNjzQ-PPuk,3667
+qai_hub_models/models/sam/perf.yaml,sha256=6hgw3pxu-Su8e5iJKjqtMAyWaRQTTbR1A_UMGJUWdVA,3411
 qai_hub_models/models/sam/requirements.txt,sha256=vtppKtEacR8giw86K6Z90D_Xko9hityPiHQx7PxkAgo,37
 qai_hub_models/models/sam/test.py,sha256=cmynvGV9g_6A0etO-ORNxj16gyCF97MVYxZQK0CNzIM,3062
 qai_hub_models/models/sam/utils.py,sha256=5Zc1YnUm0HXsJ_T-R_Ge0CjMARxWxHA-ITG424uOCic,826
 qai_hub_models/models/sesr_m5/__init__.py,sha256=kS2GuYY43QHK5jRUcssK2u92IM8LMLy3SIbV5YCEsio,464
-qai_hub_models/models/sesr_m5/conftest.py,sha256=JEkN7NNT2lRIY3iUTgwrB0i9d3bt44vaQymqR-XTQEE,982
+qai_hub_models/models/sesr_m5/conftest.py,sha256=flksqnAEfg8jkxYZ1tam9lAn80Cjis6id5VbWIwiEdU,1406
 qai_hub_models/models/sesr_m5/demo.py,sha256=uQqrkMLYcKnXc6_uDW8Niuj1YQBiMya3sFZwMGVXlOk,923
-qai_hub_models/models/sesr_m5/export.py,sha256=O8dg1F-ODACIO48vA5qHfBUAoWegV_ZzUDEWq_XnkOs,8006
+qai_hub_models/models/sesr_m5/export.py,sha256=hAv-JZzAo-UrVrHq93WqeQZLK0OHcFyibJP29cbtvSA,8287
 qai_hub_models/models/sesr_m5/info.yaml,sha256=xDzw26L21cJQ14jXZFrODO_klhrPTV5L5ALknfT4MnU,1104
 qai_hub_models/models/sesr_m5/model.py,sha256=c9-Q3psWvkFyFKgaOfWN5rhJpsvMZOKIUvapZhv6JFs,2984
-qai_hub_models/models/sesr_m5/perf.yaml,sha256=2_TsvCuPoKczV4eukSBpOIdAdd-jQ4T9_QwRSFY0MD0,4418
+qai_hub_models/models/sesr_m5/perf.yaml,sha256=FMyaSEBP7eMuC87kDtYuI5A2UX5dZUO0F_hflAqczwY,4547
 qai_hub_models/models/sesr_m5/test.py,sha256=hH27IYEOa4fPbfgHeQYuiUhHdPvMHX50DdW-LY7btng,1471
 qai_hub_models/models/sesr_m5_quantized/__init__.py,sha256=PZUoiiSQjJOpVhj2TNp_ixSeoy6RwK34nb45232QDsM,475
-qai_hub_models/models/sesr_m5_quantized/conftest.py,sha256=xMRNy_u80PgkbU3pst6kWGzPZbZ2giFl7TVWN3nZjGY,1002
+qai_hub_models/models/sesr_m5_quantized/conftest.py,sha256=hY20QoeSDSpnl1h-O3nQGc7MDj0rKk4XtPPEHPiS2Xk,1416
 qai_hub_models/models/sesr_m5_quantized/demo.py,sha256=OD2D8ExW3h3SYF2_KX_7Qv_GQ8ylmjui2tmpWsqyAqg,990
-qai_hub_models/models/sesr_m5_quantized/export.py,sha256=4RcsQryaa2A4t8kmJ_Zg6DcR3XNsjoRQ68DBJY8aX1U,8167
+qai_hub_models/models/sesr_m5_quantized/export.py,sha256=fvERC-GQAWIOoqHawayYgrE40loc3BuHci9z03gjfsY,8431
 qai_hub_models/models/sesr_m5_quantized/info.yaml,sha256=J1XebgMlJ73n3is32JI0bduz8pFhlImgxaxr6khZfvU,1151
 qai_hub_models/models/sesr_m5_quantized/model.py,sha256=6O-GFAI1vPgyqLIRuMt1MWPQ8xAqfGvTBk4ohFy3w0Q,4269
-qai_hub_models/models/sesr_m5_quantized/perf.yaml,sha256=HXsffl70zulWA-ekgcQ13r1kuBfAh53qxiNjkYepw6o,2477
+qai_hub_models/models/sesr_m5_quantized/perf.yaml,sha256=4FoWtASZBBY5XkHhJ8DroR6ki5PchJDxUya3J4pnYVM,3925
 qai_hub_models/models/sesr_m5_quantized/test.py,sha256=3EXTzeXWm0qVHMZmGJeiy7OPzAafb2b-ORgUMtQ5KiI,2927
 qai_hub_models/models/shufflenet_v2/__init__.py,sha256=V__9vnDnnEeNjHp4_cliHUq8Ea7djP5ppdXt1VH3MI4,475
-qai_hub_models/models/shufflenet_v2/conftest.py,sha256=S-Wr9fqouHAdPNoxjj-iswGeA67W97fiVVBPuHrviO0,908
+qai_hub_models/models/shufflenet_v2/conftest.py,sha256=_HeqWMK0_TRVqWHvS7pDERferegiiH1N-YxpfbFzj0c,1318
 qai_hub_models/models/shufflenet_v2/demo.py,sha256=-yorlVXF_fAdjOp_xfBQeZ63w2hU952DdIcWeLJDMJU,543
-qai_hub_models/models/shufflenet_v2/export.py,sha256=_05PPcIRFYUXYhA5T545JHg4u4D1GEH8lRnNa5IsagI,7895
+qai_hub_models/models/shufflenet_v2/export.py,sha256=TysuC1uW0qOWpxhJbYftSyUf5xiVA2NZig4-WWBgcII,8176
 qai_hub_models/models/shufflenet_v2/info.yaml,sha256=5myQBF6qAlWS_h8a9emJbmux2ZpQIgsf3o7ZERL-nVE,1353
 qai_hub_models/models/shufflenet_v2/model.py,sha256=TLi9PPf3FIDi5l-pgCkmJGRA8TduOSRpsB3CbNhtVX4,713
-qai_hub_models/models/shufflenet_v2/perf.yaml,sha256=lu3g2QXIIA0xC1q-rMIe_Y8uhWgKC2e-95r65ZheXZA,4427
+qai_hub_models/models/shufflenet_v2/perf.yaml,sha256=cqlZ3rvK-NnFSyxvgJmRiuEyboA3NjnZzB_LwN5-tto,4561
 qai_hub_models/models/shufflenet_v2/test.py,sha256=11Yio-WBjhooUiLFMX7XC70gjCjq-MeiJr8zM854Rhw,857
 qai_hub_models/models/shufflenet_v2_quantized/__init__.py,sha256=CjZcBsRqc8WZXtl4c5QvzSJtfCx0TgArcKOymMp9yw8,584
-qai_hub_models/models/shufflenet_v2_quantized/conftest.py,sha256=0rOP4nvtJHwwfEDsihwwJX6K-R55LH1jddejqRLBVzQ,928
+qai_hub_models/models/shufflenet_v2_quantized/conftest.py,sha256=0RqB7hStDPCSLtKJh8GX33EfKeVUJJsNZ7pLMfPew1k,1328
 qai_hub_models/models/shufflenet_v2_quantized/demo.py,sha256=DFJLJ83mzCD7bDyJ9KVFnO8c9rHSYygU6BlzoSEE-OA,588
-qai_hub_models/models/shufflenet_v2_quantized/export.py,sha256=6PwD_86NPYN42RmJTVVeQk3WyOdR7b1-GMaZSyM3diM,8375
+qai_hub_models/models/shufflenet_v2_quantized/export.py,sha256=_HbdfqhIGzonvIaNItGe4vrtSwHokcN5XTnitaWbWvY,8639
 qai_hub_models/models/shufflenet_v2_quantized/info.yaml,sha256=fFtlMqu2yN4_tu5ZZtayMfrNCYxxHuDjXQvOy9k6tw4,1383
 qai_hub_models/models/shufflenet_v2_quantized/model.py,sha256=q8sHReFVrVZwHRn_iD8fZjNYt5jb4AehzQpcoeLEo2Y,5989
-qai_hub_models/models/shufflenet_v2_quantized/perf.yaml,sha256=OIr3nTS4PYKyDnt3oGPMyzFKqNR7YewFreiYRQS17hw,3269
+qai_hub_models/models/shufflenet_v2_quantized/perf.yaml,sha256=94hF7BlzH7Nl53cYx21nPOnFxPVxSk6wFaBkOJsoVtY,5481
 qai_hub_models/models/shufflenet_v2_quantized/test.py,sha256=_U1q8NxyW4nqeb-jfVwqNcISHQpMO9mvVSK7QQK5Hro,899
 qai_hub_models/models/sinet/__init__.py,sha256=WIFJwnJg_47VGh96lUb-Ned-MCqHd9KL_OHzc2GU8Qc,396
 qai_hub_models/models/sinet/app.py,sha256=1w0xj_VwsDGJWRitmBq-iBWjDNI2dB-MOKHqCe7_pr8,3793
-qai_hub_models/models/sinet/conftest.py,sha256=W0lfLUURU_TSqs5jdAP8Nf5igo5mxegZjfzKfTH0L0A,978
+qai_hub_models/models/sinet/conftest.py,sha256=hl1kupzGIfejOOwWhudHdebFh9fARIpjvXtOAGTtQrY,1404
 qai_hub_models/models/sinet/demo.py,sha256=1H922wHmgorXtZKVNVWn30PHA9Xz7NPPkM5wR77vVkA,1657
-qai_hub_models/models/sinet/export.py,sha256=yWEsq9ELCfZJJxs6x-kiyyXDDJMpjc3HHp5tBkoKIwo,8141
+qai_hub_models/models/sinet/export.py,sha256=q3BJPljYVnEM0AbRNEiOnflZ6qjjrHIFNuL0rXKCkOo,8422
 qai_hub_models/models/sinet/info.yaml,sha256=L-fe7Oh7vvQkdYXQ_xaMDXBqtusxxXtBjD2or5fa_8w,1260
 qai_hub_models/models/sinet/model.py,sha256=kTdRqRgfCXsIUcxUwSsIoqsePRM0o2UNKRajxDIK9Og,4770
-qai_hub_models/models/sinet/perf.yaml,sha256=9aVjP6tE75u8Eve3EcWm1cOqgL97HsX_zcbj8KLIRhU,4384
+qai_hub_models/models/sinet/perf.yaml,sha256=UxYqzdyQSfIpKFz2o5-M7QOGGVRPua1EksLHAtBQ_vk,4518
 qai_hub_models/models/sinet/test.py,sha256=3WEZ8v6IIXSA2f_wfWWIeIJaeFCc1h_layHLP2LEf9E,1355
 qai_hub_models/models/squeezenet1_1/__init__.py,sha256=wWegyK2XfBcw4YceoPl-TZKn1uoRNNveDHXHOzA_tGQ,473
-qai_hub_models/models/squeezenet1_1/conftest.py,sha256=b4i337aB6olxjqcXCfDnZ8Pg5eeb_GuFiGyv4fmrb4Q,908
+qai_hub_models/models/squeezenet1_1/conftest.py,sha256=1_BoD4N0lI_Fj9Xt2wQ1SqHdp-L_GVDYc8JFT4PjaFY,1318
 qai_hub_models/models/squeezenet1_1/demo.py,sha256=v2sws22uWCnGNOPIUPT_NolPTvTDLH-phqz8IYG6bVg,539
-qai_hub_models/models/squeezenet1_1/export.py,sha256=y2_Td7tSSCP7hbgfGhwIIRgom8JGLL3WB5YaJMLyoPA,7896
+qai_hub_models/models/squeezenet1_1/export.py,sha256=cjiINdF6EctJADM9JI3RM-tewwH-6ud2dJDYb61zbgI,8177
 qai_hub_models/models/squeezenet1_1/info.yaml,sha256=fhuQnYnNgu3GELnXi1xO655VmJ4EgaHkWn013HHiD1c,1325
 qai_hub_models/models/squeezenet1_1/model.py,sha256=mfmXr4T7EmhLa_86TXHOwv3XvlZMEsYAuiQmn68mt4c,696
-qai_hub_models/models/squeezenet1_1/perf.yaml,sha256=aB_XbNjF6H99zhyEdOSZKRKP2MFKw_4IId-LmWO1Wrw,4420
+qai_hub_models/models/squeezenet1_1/perf.yaml,sha256=YD5LRgpgd7H1SXNnZl5DFHyUnnSKubpNhqeh9aeIpCU,4551
 qai_hub_models/models/squeezenet1_1/test.py,sha256=ytrCi-ZAAYWJ2rTV-6k_yZhYZkfrQRWD9FFoo8vUHEM,851
 qai_hub_models/models/squeezenet1_1_quantized/__init__.py,sha256=lQk7lSxtBwX8Y2q5w0xHIfz9bhvub3pkezT88Tore-U,582
-qai_hub_models/models/squeezenet1_1_quantized/conftest.py,sha256=8ivtHd9JOpw8ulknqKw8M8L_8T7EjlCToPhWrzvz_-M,928
+qai_hub_models/models/squeezenet1_1_quantized/conftest.py,sha256=-XWcvLJnZN6qOqPjP04YkDW8-5wVmUHD151lLtRU7Zc,1328
 qai_hub_models/models/squeezenet1_1_quantized/demo.py,sha256=khZa64iQFJ9MIwUA5pjiAiq8bOQwXiFxAzNsVLObTgw,584
-qai_hub_models/models/squeezenet1_1_quantized/export.py,sha256=SCohi0Bc_3yg-SReP_641UIDkPtWJViewHxagTq2QwM,8328
+qai_hub_models/models/squeezenet1_1_quantized/export.py,sha256=brrHFKlW16krmEwhEe5mgrlfJ69n-wZcZjncgC4XrUM,8592
 qai_hub_models/models/squeezenet1_1_quantized/info.yaml,sha256=ORH-gyADqkVAWBPF08xspyFovFxhdm6LHlMmRztfK5o,1358
 qai_hub_models/models/squeezenet1_1_quantized/model.py,sha256=FXk13i2nFxCAy5H3NDXn0MCvJ4Tr2UJ7Rstqvbyh3a8,3023
-qai_hub_models/models/squeezenet1_1_quantized/perf.yaml,sha256=gcRoy0g_mFoXLOrMr6DxAur1sH9KftMLGcvuSWqeikk,4420
+qai_hub_models/models/squeezenet1_1_quantized/perf.yaml,sha256=9nA5iKB1U2ACGoatk0hyoXLglfjoklIhE48VgFVsj3o,6622
 qai_hub_models/models/squeezenet1_1_quantized/test.py,sha256=5e9_c2yGKzkl1OaYkwWXBijYxQijS-bKt7Q10Oo7P3M,895
 qai_hub_models/models/stable_diffusion_quantized/__init__.py,sha256=vtVBYmQud6hMNb2GOWcqEiUr6ue5QU-Jftnss61bUoc,540
 qai_hub_models/models/stable_diffusion_quantized/app.py,sha256=o9Ao1qdv1trphz35GuPUjCi3leh29KoyyYGd3A7LdA8,7966
 qai_hub_models/models/stable_diffusion_quantized/demo.py,sha256=barv_jVC8nGiXsdL6SS8oDbMRSk5Bwm1gfw20St2d_Y,5765
-qai_hub_models/models/stable_diffusion_quantized/export.py,sha256=iZR_G93977K1mActHLw4DQv227VT4qJuAIqtLePEe7s,7432
+qai_hub_models/models/stable_diffusion_quantized/export.py,sha256=zC4U4Lp8yiIKuiJaOzRCARY8gz_-Er8xQA3l0UbrQFU,7704
 qai_hub_models/models/stable_diffusion_quantized/info.yaml,sha256=bnzWKCd80EDo1AOmlfnTDXrAAl0VXbO8CI0VjmdATyg,1355
 qai_hub_models/models/stable_diffusion_quantized/model.py,sha256=Wx2-g4gqcUMQ3hjM9Ah2WZiOuQ85qkVpNqd2P1Mj_aE,3604
 qai_hub_models/models/stable_diffusion_quantized/perf.yaml,sha256=EJT82CEPbK653yyloO5L_exjE2HXoIdU0utrOZ0yHNw,6384
 qai_hub_models/models/stable_diffusion_quantized/requirements.txt,sha256=HWz6kAx8f-AYWf5IWvv-q1IOznXS94gXTQrtKGY4L70,46
 qai_hub_models/models/stable_diffusion_quantized/test.py,sha256=IIEtaueWT-pakv6P0_Uy10jPdKE0suxjwUHJPpij2VA,1599
 qai_hub_models/models/stylegan2/__init__.py,sha256=DEYHc9DKA6dqX9iajm9yRNNFLLVZzwzyo4jepbBghDg,404
 qai_hub_models/models/stylegan2/app.py,sha256=lif1hpxwzpEWKHJUWJn_b8U_UdNQuqOecVqAZxqHGDI,4155
-qai_hub_models/models/stylegan2/conftest.py,sha256=_Fs_nAyb2o6qYDWyfWJ6-5ePdIJ4Wp9iw7QNnBIMTxs,986
+qai_hub_models/models/stylegan2/conftest.py,sha256=iynzCBgS9hzFA4G9nr3Eu6x0jdC46CXjkb4XlO8XMlY,1408
 qai_hub_models/models/stylegan2/demo.py,sha256=FzMK1wvc9Qsf46XK2HqFdzCIel_q5zjOGCoBjlA2BuE,2847
-qai_hub_models/models/stylegan2/export.py,sha256=JhYvexMPMhgdmVwzIxKJqbvm_4pz4M9XNhlQy9G8DmQ,7762
+qai_hub_models/models/stylegan2/export.py,sha256=gq-J8BClePpemvI_um2LFItsmmv-gkbSEAN3sf15SAo,8043
 qai_hub_models/models/stylegan2/info.yaml,sha256=V9PJJsIuSz0pAWl_YfsVUO9-INABRs-aFTpxfEK3jGk,1084
 qai_hub_models/models/stylegan2/model.py,sha256=GxCrDsAd8ovfqMNsYKuTz8VBOqbv8WlUYdXM_0xk_VY,8406
-qai_hub_models/models/stylegan2/perf.yaml,sha256=p86BTKQPBnfuLk-gBxORC2dsztadR-uit3kZz9_THRg,3607
+qai_hub_models/models/stylegan2/perf.yaml,sha256=-OYHtOuxDSl45xZLNJSn9jeNlbTzxEiTWhoP_HzePRs,3385
 qai_hub_models/models/stylegan2/requirements.txt,sha256=qOlq51yRk-GCNSKWjcoQno6BFnDOutA_gOCsMRiEm50,11
 qai_hub_models/models/stylegan2/test.py,sha256=yv5SE96wBsLm-6YNITvLtP0MQUMEUce-66yyvAbtMfY,2497
 qai_hub_models/models/swin_base/__init__.py,sha256=p2IuNVS1xnxD94TMxel73mdH8VFqkLGalELLTDN2rQk,471
-qai_hub_models/models/swin_base/conftest.py,sha256=z-BksOnLptPDz-ycIZ51L4fQ8MpocZdGgxrfavgBa6E,900
+qai_hub_models/models/swin_base/conftest.py,sha256=g07LL7AVF9Y0ug2jkLKRRyvgc7fJoWvAHlj7lVhsdV8,1314
 qai_hub_models/models/swin_base/demo.py,sha256=YsZ3rMrIIZI14QSkHgvlkzk4H4Wb9eVxnR7Ne7ohA5M,531
-qai_hub_models/models/swin_base/export.py,sha256=ytWuoTSFsDlnQ_JR3QpXcDtTBUCHeMLWsMrZAzC4iyk,7899
+qai_hub_models/models/swin_base/export.py,sha256=q7IGC3PW0q5AH0Hpq3k3OhkjdqFxpB5RkxQ8LWMFIlo,8180
 qai_hub_models/models/swin_base/info.yaml,sha256=3LfGk69zgLbjR5j35drDhHRZ8F0ZkA6wuZlAB_85JEE,1383
 qai_hub_models/models/swin_base/model.py,sha256=l_TKswq9uU6F5XSqWxgffQrdGD7yPuqvoAWFl1hBM-g,1241
-qai_hub_models/models/swin_base/perf.yaml,sha256=FZVlCXy5uwDekFbIywfbNOtkyJm2lnP6m44zxDNT_Ns,3665
+qai_hub_models/models/swin_base/perf.yaml,sha256=jwuSKz3_Qo0ZyZVGwE_lRmMxGm9cVFqeN5SWGAp59bU,3405
 qai_hub_models/models/swin_base/test.py,sha256=u4-I37mNGQRA9fPOUkcP6RsIWC82mEDlaHO-gBy_45k,1358
 qai_hub_models/models/swin_small/__init__.py,sha256=HOXEswRvtQUl3Mx-uVvmr9RMaEZ5pYSkMG-TiNK-EZU,472
-qai_hub_models/models/swin_small/conftest.py,sha256=cYsqkK7BsAUKBiFxOZjDxTLioNWz_ZyNHB1SmoKATwU,902
+qai_hub_models/models/swin_small/conftest.py,sha256=cGLnQTo-GX4paLhN3y83S2qAqulC4svgSpQJs3KOZOQ,1315
 qai_hub_models/models/swin_small/demo.py,sha256=QpJfG2oCPFgZVNW2OZxOSmQCFBXmsCbQSTrAi-xipp0,534
-qai_hub_models/models/swin_small/export.py,sha256=X2j_A7RciaSBX8JYu7VkgpqUkWvQW8vELpvlE-xpNLY,7903
+qai_hub_models/models/swin_small/export.py,sha256=1fdpZrIMiwrQt8ws6ErU6FgHUP5ka2EMIjxf-0D4o24,8184
 qai_hub_models/models/swin_small/info.yaml,sha256=F5iz-3X5s-d6JW-HeCHW4AkSRPcd6QSUTS0Rt_E9ObA,1378
 qai_hub_models/models/swin_small/model.py,sha256=1G-ENU7Vf5cbpwH_cqzHLG7qhTgkDPp85ZVcxydBglg,1242
-qai_hub_models/models/swin_small/perf.yaml,sha256=6ENxh11jHEPLWbOXqZPUPi-wfLR878Kg7F3VSQFLtho,3663
+qai_hub_models/models/swin_small/perf.yaml,sha256=2eQoBGfEq69y4vNwWQG_4_gTcm2LF6xnw76rPT0wvQM,3404
 qai_hub_models/models/swin_small/test.py,sha256=6AzCPufcvpD7II5eHrzckEA-T_jaKHSNVWpQmgBPjAA,1364
 qai_hub_models/models/swin_tiny/__init__.py,sha256=KMabHS9sJvTh7SR1iY71UsqXV8UXvWCCYc8rL-Ifbi0,471
-qai_hub_models/models/swin_tiny/conftest.py,sha256=65Suo7s0xbEchjCA2gJwXjpgl323EqwTS-zbTr35Gts,900
+qai_hub_models/models/swin_tiny/conftest.py,sha256=-7XOvcvqTNJBC1lF325iC7vhykLVdXuOV9DiGA59mV0,1314
 qai_hub_models/models/swin_tiny/demo.py,sha256=sgvM7gVMjsiMPGwZRGg0MBn7MUdD6PoNj-WD1ejr70M,531
-qai_hub_models/models/swin_tiny/export.py,sha256=WWAyYC8tDoXIDYWXOyikvUHhZTLbV4c_dU6S357mhhc,7899
+qai_hub_models/models/swin_tiny/export.py,sha256=_gLaa_v79vzyyyMW3Wh0aIx22j4Zd5T_NAIXm3nWE_c,8180
 qai_hub_models/models/swin_tiny/info.yaml,sha256=xFXYmV7ol4weKYVxrdW2RleeH603mzkJSV1fYQ9XPlM,1376
 qai_hub_models/models/swin_tiny/model.py,sha256=_dgBGuAk4ZWHYPKWLqeHyU605TkcgWOgfQQlDJyBw1c,1241
-qai_hub_models/models/swin_tiny/perf.yaml,sha256=URY5rdUA0oGbkTZPErmMINseboL-7uUhqRtygZkYqck,3657
+qai_hub_models/models/swin_tiny/perf.yaml,sha256=seU2s-neq4KxWmxkHFOaOWgx1rFE7Zaom5AhdMTOqTs,3398
 qai_hub_models/models/swin_tiny/test.py,sha256=pyRcuX_EtrOTbrjGtVXD-vqiW2s_SAFiJSN8SSCqrAI,1476
 qai_hub_models/models/trocr/__init__.py,sha256=wXm8GYsyaT3M3HQlZczeJy3t5Qifdcarms23LJcFT4E,396
 qai_hub_models/models/trocr/app.py,sha256=kiE57iafy8ZZ7_uxo8SR2xs9-LFzaBiNEAjaVzrXbHg,10207
-qai_hub_models/models/trocr/conftest.py,sha256=XNJ0HVijnxOX9IdS20HKaTnvTHHik01zVBkxJjC01Fw,892
+qai_hub_models/models/trocr/conftest.py,sha256=UMnuW9OGM1Zf3xgCWXrLxkWbudICtfJkg_n9780jCZg,1310
 qai_hub_models/models/trocr/demo.py,sha256=aFQgalfyjuUsZvP5VT7TshR70MCW6UUGfQd8A3nJVag,1779
-qai_hub_models/models/trocr/export.py,sha256=sHJI3nIVxtA6rRcEYmXdFsQfRsbPt7sRGBxOuroGL4o,9655
+qai_hub_models/models/trocr/export.py,sha256=c3xMa8M0gwXcoe_gDraO8FkZnV-GEacEBKayVSY4Hus,9944
 qai_hub_models/models/trocr/info.yaml,sha256=XYdHrSlr6IJkxEIvt9trKhBdhW-HSvPA_CFjW-viEcs,1370
 qai_hub_models/models/trocr/model.py,sha256=VF0yXAP60MolsUgkBz1nV-D0X4to_qrNLApcx-TYLnM,10482
-qai_hub_models/models/trocr/perf.yaml,sha256=BbV1WfRWHwZnSIZc_FAt4RUDfEclqh1wZJiZPFGCE_k,6534
+qai_hub_models/models/trocr/perf.yaml,sha256=szBEQsEWW84Z3eQCc4hJZj7158ypy2JjC0GZmPF4uug,6080
 qai_hub_models/models/trocr/requirements.txt,sha256=cnbvoRqx9v7r3NmR_f70Cgvd_PGeI7GWEBGEY_vOTyw,42
 qai_hub_models/models/trocr/test.py,sha256=OPMndfm1qj4PKF6YmEyHkbjKi_hpXh6zx6D57XDwNvM,2357
 qai_hub_models/models/unet_segmentation/__init__.py,sha256=eCv56Tgc7rcutJMfHTjnd3W1aUffmcuJtgjSURzbOGc,348
 qai_hub_models/models/unet_segmentation/app.py,sha256=DdrUoO1W-Jk99IuBc-JNHgGNlBXQj-p1f0h_xyxiQ1w,1305
-qai_hub_models/models/unet_segmentation/conftest.py,sha256=HowREPtuUkvswrlS6mbbFWG90INafrk26pQ_iYauKhA,916
+qai_hub_models/models/unet_segmentation/conftest.py,sha256=o9t9jgaZIDwQ5GOGEJzu7IxvJBUcG8QvUrZkGJtBgdQ,1322
 qai_hub_models/models/unet_segmentation/demo.py,sha256=6WhmNetb5POKcNluaRClyIsyPWHDGRSn1-p6v82yQUE,2509
-qai_hub_models/models/unet_segmentation/export.py,sha256=xyoj8lMrCLi2z-B-B51Znxs4Uo8Npqn5X8zhKBpKIRw,8189
+qai_hub_models/models/unet_segmentation/export.py,sha256=n7wUCeNcoi6kGUhcOpsCmVRIXnCznEYMdxvj3ExxRQQ,8470
 qai_hub_models/models/unet_segmentation/info.yaml,sha256=XeH1Ge0eqcX-iNM7iZOczv3o_TYjmdXqYlD2qzzt690,1310
 qai_hub_models/models/unet_segmentation/model.py,sha256=RpV7TfLdmaQNQuyX0FNy7KvJOlBXNyZMuQN70LLaR-g,2666
-qai_hub_models/models/unet_segmentation/perf.yaml,sha256=G_mW_zYejWaAF24qI0HVdH1voVYAcN863D9LVF2uQBk,4464
+qai_hub_models/models/unet_segmentation/perf.yaml,sha256=ywrvCp5xOuI2lRC7m-cnSiqwYWHuZD5WGtsy8TQMSBM,4595
 qai_hub_models/models/unet_segmentation/test.py,sha256=tMInbwc85HhxL0n8tKutOUcc2xXe0GlcEYhGhkynjPg,1215
 qai_hub_models/models/vit/__init__.py,sha256=sb8lq6b6jX0ld11rHa7wllm3un53paM2DsdWQkwsfCQ,466
-qai_hub_models/models/vit/conftest.py,sha256=PHteQUGOejp1CN_9W3Q22b9nLRfIKPgPz82DxpMAF-w,888
+qai_hub_models/models/vit/conftest.py,sha256=Uoqu964BWITybAfHp4NzwAr--MuncmyWLd07BxRjMa0,1308
 qai_hub_models/models/vit/demo.py,sha256=WJKe7oH0jOK6__GnIWESX-a2v7LhjX0C6A6vUZvrmZM,515
-qai_hub_models/models/vit/export.py,sha256=uEPCWq3Bkc4OGGg0urRB8UHyLFGnPi0UJG9Fq27piJY,7908
+qai_hub_models/models/vit/export.py,sha256=grf_TbilWo1LPOref6BVjrKToJ9eJDADMktoRJ_QjZA,8189
 qai_hub_models/models/vit/info.yaml,sha256=FDb8b6QwZlnXV4D53BxZecQ_AUunJMqgfOMRi-ArmHE,1342
 qai_hub_models/models/vit/model.py,sha256=kIDZHjG_XZs2afBUm5MbB-tv1H3q9Mc7hLzwJ43ITgY,685
-qai_hub_models/models/vit/perf.yaml,sha256=aRvkbJ8dbozQc6N4OkhAHiq8IqrDjSWJ2KwC9JOv1ds,3652
+qai_hub_models/models/vit/perf.yaml,sha256=bDq72pXxRjnJnMUAiULm7okuFJW-6LCJvuna9zSi9wU,3394
 qai_hub_models/models/vit/test.py,sha256=2rVAdhoCo3hpKuMinwO_pU_N27Sj7KD_iG2kWeOFpcU,807
 qai_hub_models/models/whisper_base_en/__init__.py,sha256=gPx7T0jgsv-7IFKaRf-Mm-D99qIGGDdvEZngoTC9hL8,444
-qai_hub_models/models/whisper_base_en/conftest.py,sha256=q6Y1lNVS205ze1xXYUV9Bp-Cz8lEPd-Qvs9ma2sKhbY,912
+qai_hub_models/models/whisper_base_en/conftest.py,sha256=AIIagCL25iXni4px4vYYX2pjPpvrofzcSz6187mHui8,1320
 qai_hub_models/models/whisper_base_en/demo.py,sha256=2Ud55e8xLWbFjwnZBHAdV2xWmRCXUnFl9CylXMLADWE,483
-qai_hub_models/models/whisper_base_en/export.py,sha256=c3vnRuPS1ZAQcZE_Dsz_4Fy2BRwJq3qqh0tg8EU-gfM,9707
+qai_hub_models/models/whisper_base_en/export.py,sha256=gzYGNeyRdXUUtwrlM5fPGaYo5GDm3Zy62niBZb5igLA,9996
 qai_hub_models/models/whisper_base_en/info.yaml,sha256=h0b_rJzb3Me1kvHEPgRFCvxVHjdaWNUpd_t9be6KpdY,1849
 qai_hub_models/models/whisper_base_en/model.py,sha256=HPsCIIHuJvVb62t0wYBofNUr6ehqIXrdMYSSrKH65YA,558
-qai_hub_models/models/whisper_base_en/perf.yaml,sha256=CjFsMiiu2H9m-iNbew-8Rr6U8QKYuvqEqDePt0jQraM,6521
+qai_hub_models/models/whisper_base_en/perf.yaml,sha256=jRuFAQS78mKmLd0HiduuAdttru6e7uBgY0ZAIXA0Zok,6064
 qai_hub_models/models/whisper_base_en/requirements.txt,sha256=6mPzVdJaLJE0PzWpF06bHbvLYjjD6uqQgJK9df11eQk,31
 qai_hub_models/models/whisper_base_en/test.py,sha256=YbVbfTk_wimEZ6dNVV5vPC1u6BpWlbCBzZlaSVnlDpM,696
 qai_hub_models/models/whisper_small_en/__init__.py,sha256=AdhsuSgpnju4sajPAn1AD0Ej3eQ67mW802ehQeWftu0,445
-qai_hub_models/models/whisper_small_en/conftest.py,sha256=8f6D1l9G-ADheY0grBH4sALosG5qlvQD-zdkJMjnDwg,914
+qai_hub_models/models/whisper_small_en/conftest.py,sha256=iRKz8OphA0wtmFOs_uqmHYCPthiY7WGgj5wV13dVHak,1321
 qai_hub_models/models/whisper_small_en/demo.py,sha256=Qcz9_UlcZMjkRkob6yEHpH-vreKiEPf2Mz0-IGEsGNA,486
-qai_hub_models/models/whisper_small_en/export.py,sha256=ORS-Pkjg7S9Ga5-Y5GUQJO9swJHXMIycYdvCcsl-Rbo,9711
+qai_hub_models/models/whisper_small_en/export.py,sha256=5NhaQckv8xC-FcxoZsMWg1sKgNnWYT_gNtLjvIoogcI,10000
 qai_hub_models/models/whisper_small_en/info.yaml,sha256=d2gUKgMzU0fHZ8v1Uo0PVchcMFny9O9XV2B7_gS3p0c,1848
 qai_hub_models/models/whisper_small_en/model.py,sha256=pzVmdcKIbKzQDdJfmSpSqrr2nqUXmEctuVNYaxIpB_Q,560
-qai_hub_models/models/whisper_small_en/perf.yaml,sha256=JIkaLtaWR63F7ylP1d8RHVO1o0pdw4cBJ_1IcrzCpyI,6509
+qai_hub_models/models/whisper_small_en/perf.yaml,sha256=WeXhaEPc59KV3-kikoYznx3EymfWSpYfNxuzk-LJgOE,6078
 qai_hub_models/models/whisper_small_en/requirements.txt,sha256=5VmKKgsGZpToTOQ9qGw40spPnoGgu1RiCV1LlwT8PZE,38
 qai_hub_models/models/whisper_small_en/test.py,sha256=YbVbfTk_wimEZ6dNVV5vPC1u6BpWlbCBzZlaSVnlDpM,696
 qai_hub_models/models/whisper_tiny_en/__init__.py,sha256=TmMgJVcrcymvkH9ZV4B1C4ap5GOxKq77dR7BWJsQtd4,444
-qai_hub_models/models/whisper_tiny_en/conftest.py,sha256=vyPQf2Y2EV4ivkHQ9DyI9fM73pSE_9fRPsmPVycpWEM,912
+qai_hub_models/models/whisper_tiny_en/conftest.py,sha256=MeXHufPXmi_-5qysf0l1Q0lY_l39hJ9cAdsvE_9bnGc,1320
 qai_hub_models/models/whisper_tiny_en/demo.py,sha256=gvvmZMuOmNLc3eybBOyMcghS4tmMTXec9Q7C0f2Idv0,483
-qai_hub_models/models/whisper_tiny_en/export.py,sha256=czs9SEg-QllerrKX3IgOvh_oYzl0Gcg4FgOK0SYp4vo,9707
+qai_hub_models/models/whisper_tiny_en/export.py,sha256=1qhqnpJ6kYSpErIZwk9Dsbs9yVQtg0njlDky04whlKU,9996
 qai_hub_models/models/whisper_tiny_en/info.yaml,sha256=r2AoylFZ7T77Mgi8-YSzF8zhHQFGI72-g127nS1Lw8w,1849
 qai_hub_models/models/whisper_tiny_en/model.py,sha256=osptTKHSqPTiEdh4aypQ2WNjEaHnv6yNFY5JXgGjnqg,558
-qai_hub_models/models/whisper_tiny_en/perf.yaml,sha256=nXXkIhxOIHHWRuuR1F3XXejN9BAHniAgPvkDlfCJRaM,6514
+qai_hub_models/models/whisper_tiny_en/perf.yaml,sha256=K-fOSHskOx1Rpkv2tAMllWtbNPbdisuPX_9Rkf8bva0,6057
 qai_hub_models/models/whisper_tiny_en/requirements.txt,sha256=6mPzVdJaLJE0PzWpF06bHbvLYjjD6uqQgJK9df11eQk,31
 qai_hub_models/models/whisper_tiny_en/test.py,sha256=YbVbfTk_wimEZ6dNVV5vPC1u6BpWlbCBzZlaSVnlDpM,696
 qai_hub_models/models/wideresnet50/__init__.py,sha256=2gofgGLnNTfjSpI_9iWz-W1gmfqP32BJ8e7hR2OmyhE,475
-qai_hub_models/models/wideresnet50/conftest.py,sha256=B0lQTCB6j31cczALY5G2gJQ4gk4WF52v13TAs79OKGc,906
+qai_hub_models/models/wideresnet50/conftest.py,sha256=rkl85btEPOePl1IWtCr-ASgm1n2ceLg-uQ8vk13zJXI,1317
 qai_hub_models/models/wideresnet50/demo.py,sha256=kPn94bMa4KF6Q4-yzgeDIr8UfaxAsSGTblk3xG4m9ns,542
-qai_hub_models/models/wideresnet50/export.py,sha256=HQM7vWGFMKXbJxGnyGZDis9d8P-GUJt8zoewYcvH-wc,7891
+qai_hub_models/models/wideresnet50/export.py,sha256=75ouW_cjJXWTZQK1IhX6pL5eE1wx81UzGRZJBHk7Cao,8172
 qai_hub_models/models/wideresnet50/info.yaml,sha256=MAq6BAeuSZXhglFGyKF_GWmzrYFqLbZFBOxpLaQ1w3A,1298
 qai_hub_models/models/wideresnet50/model.py,sha256=JkEEOEgGikkD_gPlOZDhI9bnZU-pPfxFj8UifbjP4KQ,710
-qai_hub_models/models/wideresnet50/perf.yaml,sha256=Ifd8dg-GYqDtuQyqsCedYlDdt-oXRrF9ko3NH2Ghqws,4431
+qai_hub_models/models/wideresnet50/perf.yaml,sha256=RLTuihLss5P5L-JYr11pKW3_blyGSFGKDa0FrLcCy1o,4567
 qai_hub_models/models/wideresnet50/test.py,sha256=4pY8YCYKR4_rURwM3byuRUanThobgwlhlVMG44bnwHA,855
 qai_hub_models/models/wideresnet50_quantized/__init__.py,sha256=OVOoOn80LrgiO61p2JZTOPk4wd6MBIpV78Qoa5ei6Jw,582
-qai_hub_models/models/wideresnet50_quantized/conftest.py,sha256=-kQ3XSeDBJDT_BD_F7Wcv0vtTjOuj-yAqdG3AGHqYdk,926
+qai_hub_models/models/wideresnet50_quantized/conftest.py,sha256=Zx0Dx2rMLGHf77PHhmcaGXO8n-mTNtXasa1_jrIC9nc,1327
 qai_hub_models/models/wideresnet50_quantized/demo.py,sha256=WTRZHUaAnWHPKjZvyvDF8XZU1dGrqfjoyjxYSGhLaos,587
-qai_hub_models/models/wideresnet50_quantized/export.py,sha256=A5Om4QPVJIFMeZtOfMOmvKH6qxpyaK94qtOzwFKbThU,8324
+qai_hub_models/models/wideresnet50_quantized/export.py,sha256=MZfVj1Qz1vP2_FX2OEWr5NlaQnXyYQomYdamTJZzMWU,8588
 qai_hub_models/models/wideresnet50_quantized/info.yaml,sha256=gP6lIjMqJDuVtTo8IqaoPNc2upCrOyBKfXBucpG-Mw0,1333
 qai_hub_models/models/wideresnet50_quantized/model.py,sha256=8_KCwK6ydEW0fpAvBGfZlyqygntXmPP-5TavgJIE7WQ,3214
-qai_hub_models/models/wideresnet50_quantized/perf.yaml,sha256=VuC6HxovfmGnXd6oOyck-o43yAZR6ci1_Q4VCo9jdVA,4425
+qai_hub_models/models/wideresnet50_quantized/perf.yaml,sha256=EQAnd0mkztow2NsF6M2jphkvCYFVW5dBzjXQeHlqJUc,6629
 qai_hub_models/models/wideresnet50_quantized/test.py,sha256=YJwhgaYpuN_9QXUb-o-vzZJnzKXR_x2iT0Xnqg82-xk,932
 qai_hub_models/models/xlsr/__init__.py,sha256=Xj4k054juNe3bPUFmKqSI-I5zJ7qJLcNhSoUEW8Qwgs,461
-qai_hub_models/models/xlsr/conftest.py,sha256=l87rPtEBFtemMnNBdUE0BahCIx6b6IM3PrOkFlorElY,976
+qai_hub_models/models/xlsr/conftest.py,sha256=EQcp6Xa_gttR4T6pr2HxqvkSxg11k2BHCVr6SUejr4g,1403
 qai_hub_models/models/xlsr/demo.py,sha256=xBY0HiM5h0F6F5LSCMbRG6hiKsINFANd_ifc4NIrFyQ,742
-qai_hub_models/models/xlsr/export.py,sha256=EtqBZx-M-YqFMwVv4ImrltEA2MeNTjQdGss5mvaOL9Y,7994
+qai_hub_models/models/xlsr/export.py,sha256=uGlZwbt0BBel1wsMLwRUrG4djuyzNqzfi8zYiPZQVRg,8275
 qai_hub_models/models/xlsr/info.yaml,sha256=2rij6izH-44dhEtjANktxfoMRctZpcEyvq2583qHVV0,1156
 qai_hub_models/models/xlsr/model.py,sha256=OaTY4JT5HHVVLXJaePfqgtC604FhAZzddfQTMx82y1w,3403
-qai_hub_models/models/xlsr/perf.yaml,sha256=cU7adOaB7m-AvrcR5eR3Qq5TAxEZ5gjpDGyvifFEL20,4411
+qai_hub_models/models/xlsr/perf.yaml,sha256=pkUsHPitSIvfTHsEiy_-Mz90FTgPM30U_kR6ssfzgts,4545
 qai_hub_models/models/xlsr/test.py,sha256=Qyu0UUHhC2zr6cN-3u44Ag66nyD7Dod5o17Le3XRnfI,1402
 qai_hub_models/models/xlsr_quantized/__init__.py,sha256=T-8JP8m6jhMk9yhGMGibcnc_uDL-LMR_IHSc5zCqD_E,472
-qai_hub_models/models/xlsr_quantized/conftest.py,sha256=TKSaSw43SGIfjzAdvROUYLHyQDKnOs7xnpjtAtRjssQ,996
+qai_hub_models/models/xlsr_quantized/conftest.py,sha256=lOttq-aUmsLNE6-sp_15A--e2a99Kpc2K_3UYftuoHk,1413
 qai_hub_models/models/xlsr_quantized/demo.py,sha256=mWpJz3Ignt_53oi5Gn757OxdgTQoVWuz1KXESXaOXyk,956
-qai_hub_models/models/xlsr_quantized/export.py,sha256=p42FnLp0aR7DK6JeDMUZdXUndXa7l6cisUKAfF3UA2c,8447
+qai_hub_models/models/xlsr_quantized/export.py,sha256=Jju7SKqyVrqejFMU_j6jvRCXB4mq1DjdxanQWtEpRQs,8711
 qai_hub_models/models/xlsr_quantized/info.yaml,sha256=6qK2zWzJLt9NrUj4EIX0gGSc_RfbExbFDVbmmDoB-Ts,1191
 qai_hub_models/models/xlsr_quantized/model.py,sha256=1gGN57aBaLu0sxnE5GFDYZ2TsvBIzydv1mEhgVB-zmo,3915
-qai_hub_models/models/xlsr_quantized/perf.yaml,sha256=io21-9U8F_SJXuP8wqCXCsSzLVwwVR1xVXJQkH7D5a0,2474
+qai_hub_models/models/xlsr_quantized/perf.yaml,sha256=bmOIaQbM_zqxgr2GubAeQkg4jJsIVW4C-eXT-Cqc1PU,3919
 qai_hub_models/models/xlsr_quantized/test.py,sha256=b5hK1dbE8zYxn0fePctYKEbEQfIJBTETUjBZblBG5TY,1607
 qai_hub_models/models/yolov6/__init__.py,sha256=Nj-zEnhLQO70kQCq81cFp9dgULeIydtL993JsUebRVs,436
 qai_hub_models/models/yolov6/app.py,sha256=vZ9FeEPzCDxyetqe9PRRung35W7VlB2agaRnXPJ3a1I,1071
-qai_hub_models/models/yolov6/conftest.py,sha256=W5Pcs3mpUtY08eLXsS9v5o0fyC8afky6QtrAhfUrcTU,980
+qai_hub_models/models/yolov6/conftest.py,sha256=IEuRYdXu-l03Qj2P856JzCzdqwkGglnJ3WUm2R15JAA,1405
 qai_hub_models/models/yolov6/demo.py,sha256=53T1nvu3uhTeAjz1OQC8UuOgdHqSYW0lLigjTGj6UD0,1027
-qai_hub_models/models/yolov6/export.py,sha256=gIV7YEiohUf3DaD4_3iKS4nR8tUdfO40nhE8nOWgXQ8,7897
+qai_hub_models/models/yolov6/export.py,sha256=QdSC04hrbeM7g_m1Wnr25CDW_qJDNHW1vDYrfGu870A,8178
 qai_hub_models/models/yolov6/info.yaml,sha256=lhJNwEPUVxBxRgiHqrAz4lV4Sr48haa7dASp65oXZYE,1168
 qai_hub_models/models/yolov6/model.py,sha256=M5LrnuwB1SE4sOQ-mXTeEtnEBbKCcyCNqiazGavHS5I,4686
-qai_hub_models/models/yolov6/perf.yaml,sha256=5mvAbuYgGjXvIDzMZF-Sz9Aped8FqZXz_vtjDR1zKZg,4431
+qai_hub_models/models/yolov6/perf.yaml,sha256=ZdjTaEIbTQwnLhDqU_9z9Yd4MIVGcTAcs_a1ONpzpRw,4574
 qai_hub_models/models/yolov6/test.py,sha256=LaGoMWkMYM-w4WgX8gomuU_3nWa4f-H8mDxNmjk22A0,1845
 qai_hub_models/models/yolov7/__init__.py,sha256=Eou1LnQavGFVBbp1fNjErCYGt5uD82Jy-ID8GRjZb50,436
 qai_hub_models/models/yolov7/app.py,sha256=C_u1KX19yUvyWPCijwIawFhWPWWwIDY84PoIofAYQT4,2033
-qai_hub_models/models/yolov7/conftest.py,sha256=khUND0iZbc1v_tWA_f5fXSdBb6xegL8RB1gyhNJ6-1U,980
+qai_hub_models/models/yolov7/conftest.py,sha256=7v6cG4X5mCFo7jU6pL3uhGQncRwaUWV5r1F0QowDvDU,1405
 qai_hub_models/models/yolov7/demo.py,sha256=d9tyOftqvwQdh_xP8Bo4xDAeoWx4mynA41caE18xKLY,909
-qai_hub_models/models/yolov7/export.py,sha256=3UcQoGig-Qb4LlUtSzX91iy9_goiPZ18lCGDHg89Ql8,7917
+qai_hub_models/models/yolov7/export.py,sha256=QJjnAbrpU2FxuWLOcnJfWIxdS2N5DbhlfkBxbBQLims,8198
 qai_hub_models/models/yolov7/info.yaml,sha256=IafIJ3NrcVkjlgn9mn1OT44HhnS-Cle3Tq_8vAjX-fA,1131
-qai_hub_models/models/yolov7/model.py,sha256=6JOXwTJKHIF_S1ckCpV1m9Uy06nwJoFj-hy2b2bdN1c,11831
-qai_hub_models/models/yolov7/perf.yaml,sha256=5WCUeT5clxoTLQfGfq6KcGkaQLARnP-UmemotvRs8Bg,3664
+qai_hub_models/models/yolov7/model.py,sha256=WReHjQa9--_co8yv9TLQrZALcS3XqY6oWj_Xc5jNrw0,11972
+qai_hub_models/models/yolov7/perf.yaml,sha256=895i61OrTu24omgWHQqov3OhiYVoxFk_xs9SpuP6Jl4,3408
 qai_hub_models/models/yolov7/requirements.txt,sha256=PW4Lx0JxDAgGAISG4XX8_qTk1AAh9xaF5VUU9on5LG4,83
 qai_hub_models/models/yolov7/test.py,sha256=AtgMa81Mg20TrZaKK1aVSkZuj7jf0sjMXZ-kda5sfE4,2322
 qai_hub_models/models/yolov7_quantized/__init__.py,sha256=NceTvKTE95x6nsrxuUev_YhyWyBdFWfF_BfGT3H9ajU,447
-qai_hub_models/models/yolov7_quantized/conftest.py,sha256=OcFsNtcxY6tyuJbTDxYk-QcrEHouv33riqLTsWChZhU,1000
+qai_hub_models/models/yolov7_quantized/conftest.py,sha256=JEUUBieJJGJZMFLu_QIQiCOV7L244KyNZlcl6sxiIEE,1415
 qai_hub_models/models/yolov7_quantized/demo.py,sha256=ksBNS182rXuOGjOKclcsnKfN8wqmL9P1IWHO7YCXHHg,853
-qai_hub_models/models/yolov7_quantized/export.py,sha256=4rwoYoBHEb7sPhc5MmivsmWQ0scz6y5y6-UV5IvPhmM,8330
+qai_hub_models/models/yolov7_quantized/export.py,sha256=XRr6tlXRGjRcGmEKLQqMCLeX8DVW3mGvAjwvDT56IHM,8614
 qai_hub_models/models/yolov7_quantized/info.yaml,sha256=DxF2q2V6Hy3wV1IUvmUhowiciPxKfeEyqypOCoBqIIk,1318
-qai_hub_models/models/yolov7_quantized/model.py,sha256=0Pkn-GXEQ0Q0OyHu3V6ucZpowFuHUh7ZXxP24XvhR7A,4728
-qai_hub_models/models/yolov7_quantized/perf.yaml,sha256=ZDaPqg66bpQa2DnpFRZIOArG-mDFfS8h6Sq78hlGEJc,3270
+qai_hub_models/models/yolov7_quantized/model.py,sha256=Uai9cHpJ1qzIm3_CGa5fVpjanCXOBV2wLS769NbNLT8,3251
+qai_hub_models/models/yolov7_quantized/perf.yaml,sha256=EzBPG6MD-V5Y4QhBkTZsfHaPN85599QMshm-i3mVWqs,4045
 qai_hub_models/models/yolov7_quantized/requirements.txt,sha256=PW4Lx0JxDAgGAISG4XX8_qTk1AAh9xaF5VUU9on5LG4,83
-qai_hub_models/models/yolov7_quantized/test.py,sha256=siyZWwe4gssoTj12kyVRMr6OWOrpz_X3Ae828ZpAHEw,1558
+qai_hub_models/models/yolov7_quantized/test.py,sha256=YHD_l2dPCxYrfVwHdGAzatCBKpLmizd3PMILop4PnRI,1558
 qai_hub_models/models/yolov8_det/__init__.py,sha256=Nb2Zbj0lG9sZBbpuQNXewrVWXEWCWVeP4lceLzYEhnw,415
 qai_hub_models/models/yolov8_det/app.py,sha256=av7rEkUBYQeL6bO24j_8C_RjLGzR-iO9QG4a_tQ89yo,892
-qai_hub_models/models/yolov8_det/conftest.py,sha256=iYtPWbHBhsOP75MK1q_7MXLETanrZM8HuRe9Go2sBG8,988
+qai_hub_models/models/yolov8_det/conftest.py,sha256=xwU-0TlimV953NUiCY6kybxhYJDidMtsT5U9RU0QESI,1409
 qai_hub_models/models/yolov8_det/demo.py,sha256=wQWSWP6jzBUI9jUo5yA8P2Of-JE9IXI1DguJ2mg8du0,926
-qai_hub_models/models/yolov8_det/export.py,sha256=S4fhP3pbFH5HXgxNxb0urRBnbQLDhZYjtCgULhAaJd8,7951
+qai_hub_models/models/yolov8_det/export.py,sha256=Dz5c2iKzU1OcybgD4-eOyCPnXYa12Ul2VPsOnMK8g-A,8252
 qai_hub_models/models/yolov8_det/info.yaml,sha256=nkmJtXLXqc8u7zPVbu0F0ztsfGtrQ7dPtVPqJaGWkO8,1171
-qai_hub_models/models/yolov8_det/model.py,sha256=AzwFCVZkftf5VCD0znp_h1HKPh62_GSA7HTsCI--Rq4,7721
+qai_hub_models/models/yolov8_det/model.py,sha256=SFBzfbrmsk9o9QOp_X8QCiz-QQobW8H3ei6m2riAI6g,8031
 qai_hub_models/models/yolov8_det/perf.yaml,sha256=OFGlCVDTzvn_qreH2Uhdwz3v4DxLM_5tmwfOVBUHpeI,2768
 qai_hub_models/models/yolov8_det/requirements.txt,sha256=KeSSTmOmqqXkmr85OvV2DHCUUVVwe3wB7bLEQFKdwfg,100
 qai_hub_models/models/yolov8_det/test.py,sha256=3FiaunOU6sy0X_MKDxlV2-sOGaJ0qZUV26WCDZmcQrE,2270
 qai_hub_models/models/yolov8_det_quantized/__init__.py,sha256=bQ-K9zPqN3IIaEPpmzX0zB0JGpk1ggt_C2IpOa4sQlw,459
-qai_hub_models/models/yolov8_det_quantized/conftest.py,sha256=h1stkj98LYCn2O9GnD-qS683tdhGyDfLjPWVEks0ORQ,1008
+qai_hub_models/models/yolov8_det_quantized/conftest.py,sha256=Mh8heAplzMXMsYTug5yM6pftIHMoEDYwtKa_LFBDjLo,1419
 qai_hub_models/models/yolov8_det_quantized/demo.py,sha256=ekFtmhgdxaaaqJSNC11Rz-6HgIO9yvVlPfdjQZnBuMs,804
-qai_hub_models/models/yolov8_det_quantized/export.py,sha256=JIKrDKUEV0GsaBJO14sq-fmLCXUaA4kmZmvZfJ7-42Y,8351
+qai_hub_models/models/yolov8_det_quantized/export.py,sha256=e9dzpurKzg__uaIDIuog4Mkdg0HXIeD6wzl8-QEhR18,8635
 qai_hub_models/models/yolov8_det_quantized/info.yaml,sha256=9b68CjV_PSrw7K5Z65IXvvPkyH3FhERnkblNJEU-Tm8,1354
 qai_hub_models/models/yolov8_det_quantized/model.py,sha256=BeEaUPdbgZEZjWlQrBnxtKzUKXatByqjDFV-_RmhVvs,3540
-qai_hub_models/models/yolov8_det_quantized/perf.yaml,sha256=XEpQPHaRzp5WR_ADQQrAiXm0QHDCadFrG-v_AG9SE6Y,3279
+qai_hub_models/models/yolov8_det_quantized/perf.yaml,sha256=DDIFMDp0hoasej6Gt35PeW_ylIP0suVKeKdtAc9NaZ8,2777
 qai_hub_models/models/yolov8_det_quantized/requirements.txt,sha256=KeSSTmOmqqXkmr85OvV2DHCUUVVwe3wB7bLEQFKdwfg,100
 qai_hub_models/models/yolov8_det_quantized/test.py,sha256=IFXvXCBWHsuleQUM3C8LaeTtpTg3mq7aTd6AD0quNCk,1542
 qai_hub_models/models/yolov8_seg/__init__.py,sha256=lX4Am1C4ATTR-7lkE-t0OObLD1MzUuPH8EgaNNZRKcY,419
 qai_hub_models/models/yolov8_seg/app.py,sha256=J0ULAGgwVPwk3qA_YbJ8QI9HSaK2z4ziPFifs4SigZ0,7698
-qai_hub_models/models/yolov8_seg/conftest.py,sha256=rmpvVr-xnXSfAptDun-JRUkbpRNUNYGTAqO6kAWCy5I,902
+qai_hub_models/models/yolov8_seg/conftest.py,sha256=xL3qFsShwnb9F_FNBleCnPHKweYOl6azYL_fyhaFcGA,1315
 qai_hub_models/models/yolov8_seg/demo.py,sha256=vSnKZpPUHS1JoFvo7BHU3Ob3LRUyu4AUVimU5JeikjU,3155
-qai_hub_models/models/yolov8_seg/export.py,sha256=70_xLG5FO6PL0AKRl_UFbAE8ymujl0F-hWkWkWsQcG8,7974
+qai_hub_models/models/yolov8_seg/export.py,sha256=qJgfgCq91z4vo_MGJ6NylprBIAo0BuIn2Z6BG2F4EEI,8255
 qai_hub_models/models/yolov8_seg/info.yaml,sha256=pNHALG4CJQq4sYjwljtyRttWOqx8bUWBYkSrV3R335Y,1287
 qai_hub_models/models/yolov8_seg/model.py,sha256=Ge6Jt8U-Ibc55bgJqWi5B15h6VNf3fP6jLOrZBeDx-4,4663
-qai_hub_models/models/yolov8_seg/perf.yaml,sha256=1FpCyF9bexHtrfHK1vuvLNDRiz31nckhG1ffB3Tfuw4,3665
+qai_hub_models/models/yolov8_seg/perf.yaml,sha256=IC5-KuQKt2RaqK3leQgODc_WZPhu-tVD_5f43aXghnk,3409
 qai_hub_models/models/yolov8_seg/requirements.txt,sha256=bzLR7n9PMXzABwKJJUEb6gOX23t_wZuYJkbyRPJjjQI,64
 qai_hub_models/models/yolov8_seg/test.py,sha256=pagCgV1nOZpFXyx8Z34mi6b4okh7hMIPxbGzhNzT6iY,2536
 qai_hub_models/test/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/test/test_async_compile_jobs.py,sha256=YpUEjSSeDKtLUYeglO0OxIH1ph5z88Lmz1W0VIAl9xw,1043
 qai_hub_models/test/e2e/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/test/e2e/test_aimet_compile.py,sha256=q4dXkf3-pvNK_mR5OmgQv0ZzAFPfPRyOB1rC9ZXqTT4,1661
 qai_hub_models/test/test_utils/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/test/test_utils/perf.yaml,sha256=XjLXpqrOwleDdnxWXkgyZZtkRHTq6UoLkaKIAVdTl-U,1493
 qai_hub_models/test/test_utils/test_info_specs.py,sha256=zSIdFoI8SOrcy8MmwC2j7vMAzZFZCBJ-rWvXpMZteLE,3229
 qai_hub_models/test/test_utils/test_perf_summary.py,sha256=BRlkxFyjaq5wqTA14rHE5xbbsDwt6qJ2yp1s82f5bek,6525
 qai_hub_models/test/test_utils/test_qai_hub_helpers.py,sha256=0pwOk90MDeUJjubQaMDC8NSTtcX_PABa6mdLZJdpGWU,3295
 qai_hub_models/utils/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/utils/args.py,sha256=H3jQYkCF7ZngVE_d-JJGfKVf91KjLmsaweCSpgO5KXg,16452
+qai_hub_models/utils/args.py,sha256=3Y36PdzNBMo4V1rPTysk7ShOtB50tuObn8aGuOHINlo,17286
 qai_hub_models/utils/asset_loaders.py,sha256=ZFkgBC9vCBumh4t8-RGaayQW9MFODdDibaCjHJM641Y,34094
-qai_hub_models/utils/base_model.py,sha256=6nrJ9qY8L5NHuMEpcDfOholAOPiaLqxttA2z48HCvaY,6052
+qai_hub_models/utils/base_model.py,sha256=u7JEUyXKcay0qSVKclIdDqj79K46t0ustgtl_Yialac,5946
 qai_hub_models/utils/bounding_box_processing.py,sha256=Y7-situFnXBfbr2_hbKX1VTyr7u9mZDfuymEF0Ki82k,9261
 qai_hub_models/utils/camera_capture.py,sha256=9W3v2XIEUB2lRuO6DYtLLZs-aaRMwh29Vu7eeQN3M_Q,1771
-qai_hub_models/utils/compare.py,sha256=C-Oj8YSdi5OiLpet-Quxhiaica4KrGnCgimfo4rzeBk,5222
+qai_hub_models/utils/compare.py,sha256=3Da26Q7DYYI6AGA1C9boVS-rnrwxOLuD-lMGOB54UAY,5276
 qai_hub_models/utils/config_loaders.py,sha256=vTnqqrNtu1kn1zASMErFHD60hrtFDwFe4nhy6gNbXKE,32743
 qai_hub_models/utils/display.py,sha256=YlvgoKyKVckfSJfbIbWC110LyQ2ziqK1ZKZUXeggU7c,3066
 qai_hub_models/utils/draw.py,sha256=o5N3H1-QKXKA7gIKaQ7tclI2gjRPiPyvIo-CJqbL7Cs,6403
 qai_hub_models/utils/huggingface.py,sha256=OSLKW14892oyrAYn3ovL1d-TUVHDja-BVylnlnDQnSI,1549
 qai_hub_models/utils/image_processing.py,sha256=hYp8LJ0xoCRAzVs0QXafG7fr3aWmPEaBHTDLzJ8wc7w,13246
 qai_hub_models/utils/inference.py,sha256=hRR0jwhIoam_ydKri15bkbkyE2QwfT-adWSIucKTTjQ,12482
 qai_hub_models/utils/input_spec.py,sha256=3PW9fB0UufkPWiSoH_QPzzFix0Bv_SOYp75IOlAeGRc,1308
@@ -909,16 +927,16 @@
 qai_hub_models/utils/aimet/config_loader.py,sha256=KxZzYmw4550LnQnlJQclgulV2MHNJhxKkHYQDXu3bKU,876
 qai_hub_models/utils/aimet/default_config.json,sha256=uq3OIQ03ON9IHyRbNvLybGMPA4mpCXsE8yE_n57PqkI,1233
 qai_hub_models/utils/aimet/default_config_legacy_v1.json,sha256=2YxxGBslxx_u-bG8n9UGFzc12_13zS6DEQ3cJD5KxH8,946
 qai_hub_models/utils/aimet/default_config_legacy_v2.json,sha256=Beb1iFG4Uu_iCF5aoWNEZ_E4-rqfGbD_qtE5NEDnOKc,955
 qai_hub_models/utils/aimet/default_config_per_channel_qnn.json,sha256=G08RroTHx9H9VrlAcCya4gJ2B9tl1c23NN_SH8N8_Yk,919
 qai_hub_models/utils/aimet/repo.py,sha256=d33BJ9T8pUXA_lOE9uwJrdUn-tH_a2wetk82CewnsAI,1187
 qai_hub_models/utils/scorecard/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/utils/scorecard/common.py,sha256=w2l4xccVavD3lLJ7-1fLBTCcUDk7JDW01tMi3dFxsBs,1468
-qai_hub_models/utils/scorecard/job_summary.py,sha256=r8dZXCfo5VMKNa5KgC6loxq1uyaY0chLfFGpFFhaiq0,12269
+qai_hub_models/utils/scorecard/common.py,sha256=aCh3eEgl6ArFWNtd3f-d7eGqBt3cxOsC3J-_uDZTkM0,1289
+qai_hub_models/utils/scorecard/job_summary.py,sha256=evdsH80qe_c0tZKdAa-ZPMgo9Okx2LzbX314pBXeWjI,12491
 qai_hub_models/utils/scorecard/model_card.py,sha256=Src_R8XdlkoQDcXKlpX7LLIMy3sWpTiwgzcX5EWNyUI,13385
 qai_hub_models/utils/scorecard/perf_summary.py,sha256=nrAKooK5FxonON_qbe45nDw933r0s0rFTHdvNmccxGA,11415
-qai_hub_models-0.5.0.dist-info/LICENSE,sha256=i2rmENXGu1jwHqNMu7arhPkIcgMnWTcOyMyXktqe5PA,1481
-qai_hub_models-0.5.0.dist-info/METADATA,sha256=kyKcL3WSvXkr9LBnSOHQ9dfacY64azQsqrpYLIONRsE,42594
-qai_hub_models-0.5.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-qai_hub_models-0.5.0.dist-info/top_level.txt,sha256=p1WCkillFWC1qnvse7gwhc-dqH0dNRTpd4Xe-wqn4IY,15
-qai_hub_models-0.5.0.dist-info/RECORD,,
+qai_hub_models-0.5.1.dist-info/LICENSE,sha256=i2rmENXGu1jwHqNMu7arhPkIcgMnWTcOyMyXktqe5PA,1481
+qai_hub_models-0.5.1.dist-info/METADATA,sha256=IKtHVehOHzum57df78rKjL0DNsi84jHza2G_o0OBxw4,43084
+qai_hub_models-0.5.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+qai_hub_models-0.5.1.dist-info/top_level.txt,sha256=p1WCkillFWC1qnvse7gwhc-dqH0dNRTpd4Xe-wqn4IY,15
+qai_hub_models-0.5.1.dist-info/RECORD,,
```

