# Comparing `tmp/flax-0.8.2.tar.gz` & `tmp/flax-0.8.3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "flax-0.8.2.tar", last modified: Thu Mar 14 11:34:59 2024, max compression
+gzip compressed data, was "flax-0.8.3.tar", last modified: Tue Apr 30 09:57:11 2024, max compression
```

## Comparing `flax-0.8.2.tar` & `flax-0.8.3.tar`

### file list

```diff
@@ -1,536 +1,559 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.346439 flax-0.8.2/
--rw-r--r--   0 runner    (1001) docker     (127)       54 2024-03-14 11:34:46.000000 flax-0.8.2/.git-blame-ignore-revs
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.270439 flax-0.8.2/.github/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.270439 flax-0.8.2/.github/ISSUE_TEMPLATE/
--rw-r--r--   0 runner    (1001) docker     (127)      792 2024-03-14 11:34:46.000000 flax-0.8.2/.github/ISSUE_TEMPLATE/bug_report.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.270439 flax-0.8.2/.github/analytics/
--rw-r--r--   0 runner    (1001) docker     (127)      544 2024-03-14 11:34:46.000000 flax-0.8.2/.github/analytics/README.md
--rw-r--r--   0 runner    (1001) docker     (127)    13817 2024-03-14 11:34:46.000000 flax-0.8.2/.github/analytics/get_repo_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)     1677 2024-03-14 11:34:46.000000 flax-0.8.2/.github/analytics/issue_activity_since_date.gql
--rw-r--r--   0 runner    (1001) docker     (127)     2016 2024-03-14 11:34:46.000000 flax-0.8.2/.github/analytics/pr_data_query.gql
--rw-r--r--   0 runner    (1001) docker     (127)       34 2024-03-14 11:34:46.000000 flax-0.8.2/.github/analytics/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-03-14 11:34:46.000000 flax-0.8.2/.github/pull_request_template.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.270439 flax-0.8.2/.github/workflows/
--rw-r--r--   0 runner    (1001) docker     (127)     6228 2024-03-14 11:34:46.000000 flax-0.8.2/.github/workflows/build.yml
--rw-r--r--   0 runner    (1001) docker     (127)      834 2024-03-14 11:34:46.000000 flax-0.8.2/.github/workflows/pythonpublish.yml
--rw-r--r--   0 runner    (1001) docker     (127)      185 2024-03-14 11:34:46.000000 flax-0.8.2/.gitignore
--rw-r--r--   0 runner    (1001) docker     (127)     1214 2024-03-14 11:34:46.000000 flax-0.8.2/.pre-commit-config.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      562 2024-03-14 11:34:46.000000 flax-0.8.2/.readthedocs.yml
--rw-r--r--   0 runner    (1001) docker     (127)      293 2024-03-14 11:34:46.000000 flax-0.8.2/AUTHORS
--rw-r--r--   0 runner    (1001) docker     (127)    24302 2024-03-14 11:34:46.000000 flax-0.8.2/CHANGELOG.md
--rw-r--r--   0 runner    (1001) docker     (127)    11309 2024-03-14 11:34:46.000000 flax-0.8.2/LICENSE
--rw-r--r--   0 runner    (1001) docker     (127)    10159 2024-03-14 11:34:59.346439 flax-0.8.2/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)     7910 2024-03-14 11:34:46.000000 flax-0.8.2/README.md
--rw-r--r--   0 runner    (1001) docker     (127)      110 2024-03-14 11:34:46.000000 flax-0.8.2/contributing.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.270439 flax-0.8.2/dev/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.270439 flax-0.8.2/dev/.devcontainer/
--rw-r--r--   0 runner    (1001) docker     (127)     2643 2024-03-14 11:34:46.000000 flax-0.8.2/dev/.devcontainer/Dockerfile
--rw-r--r--   0 runner    (1001) docker     (127)     1806 2024-03-14 11:34:46.000000 flax-0.8.2/dev/.devcontainer/devcontainer.json
--rw-r--r--   0 runner    (1001) docker     (127)      814 2024-03-14 11:34:46.000000 flax-0.8.2/dev/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     4581 2024-03-14 11:34:46.000000 flax-0.8.2/dev/update_requirements.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.274439 flax-0.8.2/docs/
--rw-r--r--   0 runner    (1001) docker     (127)       18 2024-03-14 11:34:46.000000 flax-0.8.2/docs/.gitignore
--rw-r--r--   0 runner    (1001) docker     (127)      634 2024-03-14 11:34:46.000000 flax-0.8.2/docs/Makefile
--rw-r--r--   0 runner    (1001) docker     (127)     5359 2024-03-14 11:34:46.000000 flax-0.8.2/docs/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.274439 flax-0.8.2/docs/_ext/
--rw-r--r--   0 runner    (1001) docker     (127)     4255 2024-03-14 11:34:46.000000 flax-0.8.2/docs/_ext/codediff.py
--rw-r--r--   0 runner    (1001) docker     (127)     3392 2024-03-14 11:34:46.000000 flax-0.8.2/docs/_ext/codediff_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2288 2024-03-14 11:34:46.000000 flax-0.8.2/docs/_ext/flax_module.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.258439 flax-0.8.2/docs/_static/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.274439 flax-0.8.2/docs/_static/css/
--rw-r--r--   0 runner    (1001) docker     (127)      309 2024-03-14 11:34:46.000000 flax-0.8.2/docs/_static/css/flax_theme.css
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.258439 flax-0.8.2/docs/_templates/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.274439 flax-0.8.2/docs/_templates/autosummary/
--rw-r--r--   0 runner    (1001) docker     (127)      674 2024-03-14 11:34:46.000000 flax-0.8.2/docs/_templates/autosummary/flax_module.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.274439 flax-0.8.2/docs/api_reference/
--rw-r--r--   0 runner    (1001) docker     (127)      190 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.config.rst
--rw-r--r--   0 runner    (1001) docker     (127)      321 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.core.frozen_dict.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1679 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.cursor.rst
--rw-r--r--   0 runner    (1001) docker     (127)      158 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.errors.rst
--rw-r--r--   0 runner    (1001) docker     (127)      365 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.jax_utils.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.278439 flax-0.8.2/docs/api_reference/flax.linen/
--rw-r--r--   0 runner    (1001) docker     (127)     1143 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/activation_functions.rst
--rw-r--r--   0 runner    (1001) docker     (127)      192 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/decorators.rst
--rw-r--r--   0 runner    (1001) docker     (127)      350 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)      232 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/init_apply.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1153 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/initializers.rst
--rw-r--r--   0 runner    (1001) docker     (127)      162 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/inspection.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2955 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/layers.rst
--rw-r--r--   0 runner    (1001) docker     (127)      509 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/module.rst
--rw-r--r--   0 runner    (1001) docker     (127)      295 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/profiling.rst
--rw-r--r--   0 runner    (1001) docker     (127)      931 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/spmd.rst
--rw-r--r--   0 runner    (1001) docker     (127)      590 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/transformations.rst
--rw-r--r--   0 runner    (1001) docker     (127)      213 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.linen/variable.rst
--rw-r--r--   0 runner    (1001) docker     (127)      480 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.serialization.rst
--rw-r--r--   0 runner    (1001) docker     (127)      161 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.struct.rst
--rw-r--r--   0 runner    (1001) docker     (127)      275 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.traceback_util.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1212 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.training.rst
--rw-r--r--   0 runner    (1001) docker     (127)      798 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/flax.traverse_util.rst
--rw-r--r--   0 runner    (1001) docker     (127)      265 2024-03-14 11:34:46.000000 flax-0.8.2/docs/api_reference/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     5673 2024-03-14 11:34:46.000000 flax-0.8.2/docs/conf.py
--rw-r--r--   0 runner    (1001) docker     (127)     6623 2024-03-14 11:34:46.000000 flax-0.8.2/docs/conf_sphinx_patch.py
--rw-r--r--   0 runner    (1001) docker     (127)    11924 2024-03-14 11:34:46.000000 flax-0.8.2/docs/contributing.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.278439 flax-0.8.2/docs/developer_notes/
--rw-r--r--   0 runner    (1001) docker     (127)      153 2024-03-14 11:34:46.000000 flax-0.8.2/docs/developer_notes/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)    18081 2024-03-14 11:34:46.000000 flax-0.8.2/docs/developer_notes/lift.md
--rw-r--r--   0 runner    (1001) docker     (127)    22000 2024-03-14 11:34:46.000000 flax-0.8.2/docs/developer_notes/module_lifecycle.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.278439 flax-0.8.2/docs/examples/
--rw-r--r--   0 runner    (1001) docker     (127)     4692 2024-03-14 11:34:46.000000 flax-0.8.2/docs/examples/community_examples.rst
--rw-r--r--   0 runner    (1001) docker     (127)     4422 2024-03-14 11:34:46.000000 flax-0.8.2/docs/examples/core_examples.rst
--rw-r--r--   0 runner    (1001) docker     (127)    22579 2024-03-14 11:34:46.000000 flax-0.8.2/docs/examples/google_research_examples.rst
--rw-r--r--   0 runner    (1001) docker     (127)      148 2024-03-14 11:34:46.000000 flax-0.8.2/docs/examples/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2028 2024-03-14 11:34:46.000000 flax-0.8.2/docs/examples/repositories_that_use_flax.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.278439 flax-0.8.2/docs/experimental/
--rw-r--r--   0 runner    (1001) docker     (127)       70 2024-03-14 11:34:46.000000 flax-0.8.2/docs/experimental/index.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.278439 flax-0.8.2/docs/experimental/nnx/
--rw-r--r--   0 runner    (1001) docker     (127)     3227 2024-03-14 11:34:46.000000 flax-0.8.2/docs/experimental/nnx/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)   146435 2024-03-14 11:34:46.000000 flax-0.8.2/docs/experimental/nnx/mnist_tutorial.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    11869 2024-03-14 11:34:46.000000 flax-0.8.2/docs/experimental/nnx/mnist_tutorial.md
--rw-r--r--   0 runner    (1001) docker     (127)    18157 2024-03-14 11:34:46.000000 flax-0.8.2/docs/experimental/nnx/nnx_basics.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    10103 2024-03-14 11:34:46.000000 flax-0.8.2/docs/experimental/nnx/nnx_basics.md
--rw-r--r--   0 runner    (1001) docker     (127)     4134 2024-03-14 11:34:46.000000 flax-0.8.2/docs/faq.rst
--rw-r--r--   0 runner    (1001) docker     (127)    20991 2024-03-14 11:34:46.000000 flax-0.8.2/docs/flax.png
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.282439 flax-0.8.2/docs/flip/
--rw-r--r--   0 runner    (1001) docker     (127)      648 2024-03-14 11:34:46.000000 flax-0.8.2/docs/flip/0000-template.md
--rw-r--r--   0 runner    (1001) docker     (127)    17256 2024-03-14 11:34:46.000000 flax-0.8.2/docs/flip/1009-optimizer-api.md
--rw-r--r--   0 runner    (1001) docker     (127)     8189 2024-03-14 11:34:46.000000 flax-0.8.2/docs/flip/1777-default-dtype.md
--rw-r--r--   0 runner    (1001) docker     (127)    11758 2024-03-14 11:34:46.000000 flax-0.8.2/docs/flip/2396-rnn.md
--rw-r--r--   0 runner    (1001) docker     (127)    10424 2024-03-14 11:34:46.000000 flax-0.8.2/docs/flip/2434-general-metadata.md
--rw-r--r--   0 runner    (1001) docker     (127)     4099 2024-03-14 11:34:46.000000 flax-0.8.2/docs/flip/2974-kw-only-dataclasses.md
--rw-r--r--   0 runner    (1001) docker     (127)     1404 2024-03-14 11:34:46.000000 flax-0.8.2/docs/flip/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     6679 2024-03-14 11:34:46.000000 flax-0.8.2/docs/glossary.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.282439 flax-0.8.2/docs/guides/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.282439 flax-0.8.2/docs/guides/converting_and_upgrading/
--rw-r--r--   0 runner    (1001) docker     (127)    10527 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst
--rw-r--r--   0 runner    (1001) docker     (127)    26573 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/converting_and_upgrading/haiku_migration_guide.rst
--rw-r--r--   0 runner    (1001) docker     (127)      255 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/converting_and_upgrading/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)    17314 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (127)    10433 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/converting_and_upgrading/optax_update_guide.rst
--rw-r--r--   0 runner    (1001) docker     (127)     9051 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (127)     4119 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (127)     6093 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.282439 flax-0.8.2/docs/guides/data_preprocessing/
--rw-r--r--   0 runner    (1001) docker     (127)     6994 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/data_preprocessing/full_eval.rst
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/data_preprocessing/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     8230 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/data_preprocessing/loading_datasets.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     5149 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/data_preprocessing/loading_datasets.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.286439 flax-0.8.2/docs/guides/flax_fundamentals/
--rw-r--r--   0 runner    (1001) docker     (127)     4087 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/flax_fundamentals/arguments.md
--rw-r--r--   0 runner    (1001) docker     (127)    38158 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/flax_fundamentals/flax_basics.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    20797 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/flax_fundamentals/flax_basics.md
--rw-r--r--   0 runner    (1001) docker     (127)      214 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/flax_fundamentals/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)    68242 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/flax_fundamentals/rng_guide.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    29429 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/flax_fundamentals/rng_guide.md
--rw-r--r--   0 runner    (1001) docker     (127)     3125 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/flax_fundamentals/setup_or_nncompact.rst
--rw-r--r--   0 runner    (1001) docker     (127)     5886 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/flax_fundamentals/state_params.rst
--rw-r--r--   0 runner    (1001) docker     (127)     7240 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/flax_sharp_bits.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     5808 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/flax_sharp_bits.md
--rw-r--r--   0 runner    (1001) docker     (127)      252 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/index.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.286439 flax-0.8.2/docs/guides/model_inspection/
--rw-r--r--   0 runner    (1001) docker     (127)    12699 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/model_inspection/extracting_intermediates.rst
--rw-r--r--   0 runner    (1001) docker     (127)      110 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/model_inspection/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     7138 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/model_inspection/model_surgery.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     4368 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/model_inspection/model_surgery.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.286439 flax-0.8.2/docs/guides/parallel_training/
--rw-r--r--   0 runner    (1001) docker     (127)    10480 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/parallel_training/ensembling.rst
--rw-r--r--   0 runner    (1001) docker     (127)    62621 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/parallel_training/flax_on_pjit.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    23623 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/parallel_training/flax_on_pjit.md
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/parallel_training/index.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.286439 flax-0.8.2/docs/guides/training_techniques/
--rw-r--r--   0 runner    (1001) docker     (127)     9141 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/training_techniques/batch_norm.rst
--rw-r--r--   0 runner    (1001) docker     (127)    11028 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/training_techniques/dropout.rst
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/training_techniques/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     8171 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/training_techniques/lr_schedule.rst
--rw-r--r--   0 runner    (1001) docker     (127)    11290 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/training_techniques/transfer_learning.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     7843 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/training_techniques/transfer_learning.md
--rw-r--r--   0 runner    (1001) docker     (127)    53426 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/training_techniques/use_checkpointing.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    25768 2024-03-14 11:34:46.000000 flax-0.8.2/docs/guides/training_techniques/use_checkpointing.md
--rw-r--r--   0 runner    (1001) docker     (127)     8649 2024-03-14 11:34:46.000000 flax-0.8.2/docs/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)    39597 2024-03-14 11:34:46.000000 flax-0.8.2/docs/linen_intro.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    20900 2024-03-14 11:34:46.000000 flax-0.8.2/docs/linen_intro.md
--rw-r--r--   0 runner    (1001) docker     (127)     7604 2024-03-14 11:34:46.000000 flax-0.8.2/docs/philosophy.md
--rw-r--r--   0 runner    (1001) docker     (127)   101054 2024-03-14 11:34:46.000000 flax-0.8.2/docs/quick_start.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    14177 2024-03-14 11:34:46.000000 flax-0.8.2/docs/quick_start.md
--rw-r--r--   0 runner    (1001) docker     (127)      673 2024-03-14 11:34:46.000000 flax-0.8.2/docs/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.286439 flax-0.8.2/examples/
--rw-r--r--   0 runner    (1001) docker     (127)      743 2024-03-14 11:34:46.000000 flax-0.8.2/examples/README.md
--rw-r--r--   0 runner    (1001) docker     (127)      582 2024-03-14 11:34:46.000000 flax-0.8.2/examples/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.286439 flax-0.8.2/examples/cloud/
--rw-r--r--   0 runner    (1001) docker     (127)     4635 2024-03-14 11:34:46.000000 flax-0.8.2/examples/cloud/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     9222 2024-03-14 11:34:46.000000 flax-0.8.2/examples/cloud/launch_gce.py
--rw-r--r--   0 runner    (1001) docker     (127)     1650 2024-03-14 11:34:46.000000 flax-0.8.2/examples/cloud/startup_script.sh
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.290439 flax-0.8.2/examples/imagenet/
--rw-r--r--   0 runner    (1001) docker     (127)     9944 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.290439 flax-0.8.2/examples/imagenet/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     2192 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)     1105 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/configs/fake_data_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)     1670 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/configs/tpu.py
--rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/configs/v100_x8.py
--rw-r--r--   0 runner    (1001) docker     (127)     1088 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/configs/v100_x8_mixed_precision.py
--rw-r--r--   0 runner    (1001) docker     (127)   293668 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/imagenet.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     3334 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/imagenet_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)     2190 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/imagenet_fake_data_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)     8124 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     2125 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/main.py
--rw-r--r--   0 runner    (1001) docker     (127)     4346 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     1933 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/models_test.py
--rw-r--r--   0 runner    (1001) docker     (127)      341 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)    12863 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     3049 2024-03-14 11:34:46.000000 flax-0.8.2/examples/imagenet/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.290439 flax-0.8.2/examples/linen_design_test/
--rw-r--r--   0 runner    (1001) docker     (127)     6242 2024-03-14 11:34:46.000000 flax-0.8.2/examples/linen_design_test/attention_simple.py
--rw-r--r--   0 runner    (1001) docker     (127)     3145 2024-03-14 11:34:46.000000 flax-0.8.2/examples/linen_design_test/autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (127)     1246 2024-03-14 11:34:46.000000 flax-0.8.2/examples/linen_design_test/dense.py
--rw-r--r--   0 runner    (1001) docker     (127)     1304 2024-03-14 11:34:46.000000 flax-0.8.2/examples/linen_design_test/linear_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)     2303 2024-03-14 11:34:46.000000 flax-0.8.2/examples/linen_design_test/mlp_explicit.py
--rw-r--r--   0 runner    (1001) docker     (127)     1880 2024-03-14 11:34:46.000000 flax-0.8.2/examples/linen_design_test/mlp_inline.py
--rw-r--r--   0 runner    (1001) docker     (127)     1891 2024-03-14 11:34:46.000000 flax-0.8.2/examples/linen_design_test/mlp_lazy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.294439 flax-0.8.2/examples/lm1b/
--rw-r--r--   0 runner    (1001) docker     (127)     3320 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.294439 flax-0.8.2/examples/lm1b/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     5011 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)    12500 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     3355 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2190 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/main.py
--rw-r--r--   0 runner    (1001) docker     (127)    13273 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/models.py
--rw-r--r--   0 runner    (1001) docker     (127)      347 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     4843 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/temperature_sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1453 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/temperature_sampler_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     5313 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    20414 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     1994 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/train_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     5793 2024-03-14 11:34:46.000000 flax-0.8.2/examples/lm1b/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.294439 flax-0.8.2/examples/mnist/
--rw-r--r--   0 runner    (1001) docker     (127)     1741 2024-03-14 11:34:46.000000 flax-0.8.2/examples/mnist/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.294439 flax-0.8.2/examples/mnist/configs/
--rw-r--r--   0 runner    (1001) docker     (127)      912 2024-03-14 11:34:46.000000 flax-0.8.2/examples/mnist/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)     2121 2024-03-14 11:34:46.000000 flax-0.8.2/examples/mnist/main.py
--rw-r--r--   0 runner    (1001) docker     (127)    98260 2024-03-14 11:34:46.000000 flax-0.8.2/examples/mnist/mnist.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     2366 2024-03-14 11:34:46.000000 flax-0.8.2/examples/mnist/mnist_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)      298 2024-03-14 11:34:46.000000 flax-0.8.2/examples/mnist/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     5231 2024-03-14 11:34:46.000000 flax-0.8.2/examples/mnist/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     2262 2024-03-14 11:34:46.000000 flax-0.8.2/examples/mnist/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.294439 flax-0.8.2/examples/nlp_seq/
--rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-03-14 11:34:46.000000 flax-0.8.2/examples/nlp_seq/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     7884 2024-03-14 11:34:46.000000 flax-0.8.2/examples/nlp_seq/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     4073 2024-03-14 11:34:46.000000 flax-0.8.2/examples/nlp_seq/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     6464 2024-03-14 11:34:46.000000 flax-0.8.2/examples/nlp_seq/models.py
--rw-r--r--   0 runner    (1001) docker     (127)       60 2024-03-14 11:34:46.000000 flax-0.8.2/examples/nlp_seq/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)    14100 2024-03-14 11:34:46.000000 flax-0.8.2/examples/nlp_seq/train.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.298439 flax-0.8.2/examples/ogbg_molpcba/
--rw-r--r--   0 runner    (1001) docker     (127)     4486 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.298439 flax-0.8.2/examples/ogbg_molpcba/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     1520 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)     1551 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/configs/default_graph_net.py
--rw-r--r--   0 runner    (1001) docker     (127)     1946 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/configs/hparam_sweep.py
--rw-r--r--   0 runner    (1001) docker     (127)     1405 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/configs/test.py
--rw-r--r--   0 runner    (1001) docker     (127)     8133 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     2571 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2198 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/main.py
--rw-r--r--   0 runner    (1001) docker     (127)     7068 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     5179 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/models_test.py
--rw-r--r--   0 runner    (1001) docker     (127)  1110530 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/ogbg_molpcba.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     4769 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)      329 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)    13697 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/train.py
--rw-r--r--   0 runner    (1001) docker     (127)    12431 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ogbg_molpcba/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.302439 flax-0.8.2/examples/ppo/
--rw-r--r--   0 runner    (1001) docker     (127)     2501 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ppo/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     2607 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ppo/agent.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.302439 flax-0.8.2/examples/ppo/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     1955 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ppo/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)     2460 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ppo/env_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2346 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ppo/models.py
--rw-r--r--   0 runner    (1001) docker     (127)    13152 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ppo/ppo_lib.py
--rw-r--r--   0 runner    (1001) docker     (127)     5286 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ppo/ppo_lib_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     1529 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ppo/ppo_main.py
--rw-r--r--   0 runner    (1001) docker     (127)      192 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ppo/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     8930 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ppo/seed_rl_atari_preprocessing.py
--rw-r--r--   0 runner    (1001) docker     (127)     1897 2024-03-14 11:34:46.000000 flax-0.8.2/examples/ppo/test_episodes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.302439 flax-0.8.2/examples/seq2seq/
--rw-r--r--   0 runner    (1001) docker     (127)      913 2024-03-14 11:34:46.000000 flax-0.8.2/examples/seq2seq/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-03-14 11:34:46.000000 flax-0.8.2/examples/seq2seq/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     4395 2024-03-14 11:34:46.000000 flax-0.8.2/examples/seq2seq/models.py
--rw-r--r--   0 runner    (1001) docker     (127)       65 2024-03-14 11:34:46.000000 flax-0.8.2/examples/seq2seq/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)    24740 2024-03-14 11:34:46.000000 flax-0.8.2/examples/seq2seq/seq2seq.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     6844 2024-03-14 11:34:46.000000 flax-0.8.2/examples/seq2seq/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     3201 2024-03-14 11:34:46.000000 flax-0.8.2/examples/seq2seq/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.306439 flax-0.8.2/examples/sst2/
--rw-r--r--   0 runner    (1001) docker     (127)     1893 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/README.md
--rwxr-xr-x   0 runner    (1001) docker     (127)     2028 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/build_vocabulary.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.306439 flax-0.8.2/examples/sst2/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     1226 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/configs/default.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     9834 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     3522 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2118 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/main.py
--rw-r--r--   0 runner    (1001) docker     (127)    14403 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     3526 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/models_test.py
--rw-r--r--   0 runner    (1001) docker     (127)      156 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     8607 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/sst2.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     9335 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     2123 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/train_test.py
--rw-r--r--   0 runner    (1001) docker     (127)   117898 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/vocab.txt
--rwxr-xr-x   0 runner    (1001) docker     (127)     4407 2024-03-14 11:34:46.000000 flax-0.8.2/examples/sst2/vocabulary.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.306439 flax-0.8.2/examples/vae/
--rw-r--r--   0 runner    (1001) docker     (127)     1132 2024-03-14 11:34:46.000000 flax-0.8.2/examples/vae/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.306439 flax-0.8.2/examples/vae/configs/
--rw-r--r--   0 runner    (1001) docker     (127)      883 2024-03-14 11:34:46.000000 flax-0.8.2/examples/vae/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)     1458 2024-03-14 11:34:46.000000 flax-0.8.2/examples/vae/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     1814 2024-03-14 11:34:46.000000 flax-0.8.2/examples/vae/main.py
--rw-r--r--   0 runner    (1001) docker     (127)     1777 2024-03-14 11:34:46.000000 flax-0.8.2/examples/vae/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     2152 2024-03-14 11:34:46.000000 flax-0.8.2/examples/vae/reconstruction.png
--rw-r--r--   0 runner    (1001) docker     (127)      114 2024-03-14 11:34:46.000000 flax-0.8.2/examples/vae/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.306439 flax-0.8.2/examples/vae/results/
--rw-r--r--   0 runner    (1001) docker     (127)        6 2024-03-14 11:34:46.000000 flax-0.8.2/examples/vae/results/.gitignore
--rw-r--r--   0 runner    (1001) docker     (127)    43139 2024-03-14 11:34:46.000000 flax-0.8.2/examples/vae/sample.png
--rw-r--r--   0 runner    (1001) docker     (127)     4596 2024-03-14 11:34:46.000000 flax-0.8.2/examples/vae/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     3580 2024-03-14 11:34:46.000000 flax-0.8.2/examples/vae/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.310439 flax-0.8.2/examples/wmt/
--rw-r--r--   0 runner    (1001) docker     (127)     6106 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     7270 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/bleu.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.310439 flax-0.8.2/examples/wmt/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     3482 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)    14745 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/decode.py
--rw-r--r--   0 runner    (1001) docker     (127)    12910 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     3318 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2166 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/main.py
--rw-r--r--   0 runner    (1001) docker     (127)    18604 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/models.py
--rw-r--r--   0 runner    (1001) docker     (127)      398 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     5314 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    23409 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     1995 2024-03-14 11:34:46.000000 flax-0.8.2/examples/wmt/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.310439 flax-0.8.2/flax/
--rw-r--r--   0 runner    (1001) docker     (127)     1135 2024-03-14 11:34:46.000000 flax-0.8.2/flax/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5990 2024-03-14 11:34:46.000000 flax-0.8.2/flax/configurations.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.314439 flax-0.8.2/flax/core/
--rw-r--r--   0 runner    (1001) docker     (127)     1467 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5359 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/axes_scan.py
--rw-r--r--   0 runner    (1001) docker     (127)    10482 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/flax_functional_engine.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     9974 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/frozen_dict.py
--rw-r--r--   0 runner    (1001) docker     (127)    61453 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/lift.py
--rw-r--r--   0 runner    (1001) docker     (127)    11745 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/meta.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.314439 flax-0.8.2/flax/core/nn/
--rw-r--r--   0 runner    (1001) docker     (127)     1795 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/nn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    17932 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/nn/attention.py
--rw-r--r--   0 runner    (1001) docker     (127)    12040 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/nn/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     6897 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/nn/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1488 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/nn/stochastic.py
--rw-r--r--   0 runner    (1001) docker     (127)     2546 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/partial_eval.py
--rw-r--r--   0 runner    (1001) docker     (127)    38092 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/scope.py
--rw-r--r--   0 runner    (1001) docker     (127)     1054 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/tracers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1558 2024-03-14 11:34:46.000000 flax-0.8.2/flax/core/variables.py
--rw-r--r--   0 runner    (1001) docker     (127)    26339 2024-03-14 11:34:46.000000 flax-0.8.2/flax/cursor.py
--rw-r--r--   0 runner    (1001) docker     (127)    30713 2024-03-14 11:34:46.000000 flax-0.8.2/flax/errors.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.314439 flax-0.8.2/flax/experimental/
--rw-r--r--   0 runner    (1001) docker     (127)      582 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.314439 flax-0.8.2/flax/experimental/nnx/
--rw-r--r--   0 runner    (1001) docker     (127)     1831 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/.gitignore
--rw-r--r--   0 runner    (1001) docker     (127)    21176 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     4909 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.314439 flax-0.8.2/flax/experimental/nnx/docs/
--rw-r--r--   0 runner    (1001) docker     (127)      350 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/docs/blog.md
--rw-r--r--   0 runner    (1001) docker     (127)    10272 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/docs/demo.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     4450 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/docs/demo.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.314439 flax-0.8.2/flax/experimental/nnx/docs/images/
--rw-r--r--   0 runner    (1001) docker     (127)   304812 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/docs/images/stateful-transforms.png
--rw-r--r--   0 runner    (1001) docker     (127)    61270 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/docs/quick_start.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    16288 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/docs/tiny_nnx.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    26897 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/docs/why.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    15217 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/docs/why.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.266439 flax-0.8.2/flax/experimental/nnx/examples/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.318439 flax-0.8.2/flax/experimental/nnx/examples/lm1b/
--rw-r--r--   0 runner    (1001) docker     (127)     3267 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.318439 flax-0.8.2/flax/experimental/nnx/examples/lm1b/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     5096 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)    12344 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     3293 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2143 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/main.py
--rw-r--r--   0 runner    (1001) docker     (127)    15055 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     9517 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/models_test.py
--rw-r--r--   0 runner    (1001) docker     (127)      347 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     4799 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/temperature_sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1448 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/temperature_sampler_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     5263 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    20465 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     2026 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/train_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     5015 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/lm1b/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.322439 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/
--rw-r--r--   0 runner    (1001) docker     (127)    13748 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/00_demo.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     2814 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/01_functional_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     2571 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/02_lifted_transforms.py
--rw-r--r--   0 runner    (1001) docker     (127)     2872 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/03_train_state.py
--rw-r--r--   0 runner    (1001) docker     (127)     5704 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/05_vae.py
--rw-r--r--   0 runner    (1001) docker     (127)     2785 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/06_scan_over_layers.py
--rw-r--r--   0 runner    (1001) docker     (127)    12131 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/07_transformer.py
--rw-r--r--   0 runner    (1001) docker     (127)     1936 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/08_save_load_checkpoints.py
--rw-r--r--   0 runner    (1001) docker     (127)     1940 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/09_parameter_surgery.py
--rw-r--r--   0 runner    (1001) docker     (127)    11038 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/10_quantization.py
--rw-r--r--   0 runner    (1001) docker     (127)       35 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/examples/toy_examples/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.322439 flax-0.8.2/flax/experimental/nnx/ideas/
--rw-r--r--   0 runner    (1001) docker     (127)     5520 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/ideas/shape_inference.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.322439 flax-0.8.2/flax/experimental/nnx/nnx/
--rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3134 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/compatibility.py
--rw-r--r--   0 runner    (1001) docker     (127)      626 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/errors.py
--rw-r--r--   0 runner    (1001) docker     (127)     2631 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/filterlib.py
--rw-r--r--   0 runner    (1001) docker     (127)    25695 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/graph_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5598 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)     2326 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/ids.py
--rw-r--r--   0 runner    (1001) docker     (127)    17574 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/module.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.326439 flax-0.8.2/flax/experimental/nnx/nnx/nn/
--rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/nn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1219 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/nn/activations.py
--rw-r--r--   0 runner    (1001) docker     (127)    25288 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/nn/attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     3318 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/nn/dtypes.py
--rw-r--r--   0 runner    (1001) docker     (127)     2527 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/nn/initializers.py
--rw-r--r--   0 runner    (1001) docker     (127)    22682 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/nn/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)    18368 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/nn/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     2800 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/nn/stochastic.py
--rw-r--r--   0 runner    (1001) docker     (127)     2562 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/proxy_caller.py
--rw-r--r--   0 runner    (1001) docker     (127)     8721 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/pytreelib.py
--rw-r--r--   0 runner    (1001) docker     (127)     2781 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/reprlib.py
--rw-r--r--   0 runner    (1001) docker     (127)     7296 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/rnglib.py
--rw-r--r--   0 runner    (1001) docker     (127)     6410 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/spmd.py
--rw-r--r--   0 runner    (1001) docker     (127)     7556 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/state.py
--rw-r--r--   0 runner    (1001) docker     (127)     1728 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/tracers.py
--rw-r--r--   0 runner    (1001) docker     (127)    44735 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/transforms.py
--rw-r--r--   0 runner    (1001) docker     (127)    15128 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/nnx/variables.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.326439 flax-0.8.2/flax/experimental/nnx/scripts/
--rw-r--r--   0 runner    (1001) docker     (127)       17 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/scripts/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)      285 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/scripts/run-all-examples.bash
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.326439 flax-0.8.2/flax/experimental/nnx/tests/
--rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.326439 flax-0.8.2/flax/experimental/nnx/tests/nn/
--rw-r--r--   0 runner    (1001) docker     (127)     4937 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/nn/test_attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     3003 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/nn/test_conv.py
--rw-r--r--   0 runner    (1001) docker     (127)     2077 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/nn/test_embed.py
--rw-r--r--   0 runner    (1001) docker     (127)     2643 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/nn/test_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     7537 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/nn/test_normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1204 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_compatibility.py
--rw-r--r--   0 runner    (1001) docker     (127)     1636 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_containers.py
--rw-r--r--   0 runner    (1001) docker     (127)     7433 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_graph_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2078 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)      929 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_ids.py
--rw-r--r--   0 runner    (1001) docker     (127)     7368 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_integration.py
--rw-r--r--   0 runner    (1001) docker     (127)    15907 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     4232 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_partitioning.py
--rw-r--r--   0 runner    (1001) docker     (127)     6641 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_pytree.py
--rw-r--r--   0 runner    (1001) docker     (127)     4699 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_rngs.py
--rw-r--r--   0 runner    (1001) docker     (127)     2184 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_spmd.py
--rw-r--r--   0 runner    (1001) docker     (127)     2077 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_state.py
--rw-r--r--   0 runner    (1001) docker     (127)    18175 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_transforms.py
--rw-r--r--   0 runner    (1001) docker     (127)      897 2024-03-14 11:34:46.000000 flax-0.8.2/flax/experimental/nnx/tests/test_variable.py
--rw-r--r--   0 runner    (1001) docker     (127)     1752 2024-03-14 11:34:46.000000 flax-0.8.2/flax/ids.py
--rw-r--r--   0 runner    (1001) docker     (127)     5309 2024-03-14 11:34:46.000000 flax-0.8.2/flax/io.py
--rw-r--r--   0 runner    (1001) docker     (127)    11534 2024-03-14 11:34:46.000000 flax-0.8.2/flax/jax_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.330439 flax-0.8.2/flax/linen/
--rw-r--r--   0 runner    (1001) docker     (127)     2191 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     5253 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4101 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/activation.py
--rw-r--r--   0 runner    (1001) docker     (127)    31104 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     4172 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/batch_apply.py
--rw-r--r--   0 runner    (1001) docker     (127)     3846 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/combinators.py
--rw-r--r--   0 runner    (1001) docker     (127)     3860 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/dtypes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.330439 flax-0.8.2/flax/linen/experimental/
--rw-r--r--   0 runner    (1001) docker     (127)    11235 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/experimental/layers_with_named_axes.py
--rw-r--r--   0 runner    (1001) docker     (127)     9324 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/fp8_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)     2676 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/initializers.py
--rw-r--r--   0 runner    (1001) docker     (127)     8828 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/kw_only_dataclasses.py
--rw-r--r--   0 runner    (1001) docker     (127)    44278 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)   109421 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/module.py
--rw-r--r--   0 runner    (1001) docker     (127)    48880 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)    19421 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/partitioning.py
--rw-r--r--   0 runner    (1001) docker     (127)     5554 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)    45635 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/recurrent.py
--rw-r--r--   0 runner    (1001) docker     (127)    11192 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/spmd.py
--rw-r--r--   0 runner    (1001) docker     (127)     3619 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/stochastic.py
--rw-r--r--   0 runner    (1001) docker     (127)    26065 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/summary.py
--rw-r--r--   0 runner    (1001) docker     (127)    79055 2024-03-14 11:34:46.000000 flax-0.8.2/flax/linen/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.330439 flax-0.8.2/flax/metrics/
--rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-03-14 11:34:46.000000 flax-0.8.2/flax/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7792 2024-03-14 11:34:46.000000 flax-0.8.2/flax/metrics/tensorboard.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.334439 flax-0.8.2/flax/oss/
--rw-r--r--   0 runner    (1001) docker     (127)      443 2024-03-14 11:34:46.000000 flax-0.8.2/flax/oss/ .git-blame-ignore-revs
--rw-r--r--   0 runner    (1001) docker     (127)       58 2024-03-14 11:34:46.000000 flax-0.8.2/flax/py.typed
--rw-r--r--   0 runner    (1001) docker     (127)    14315 2024-03-14 11:34:46.000000 flax-0.8.2/flax/serialization.py
--rw-r--r--   0 runner    (1001) docker     (127)     7886 2024-03-14 11:34:46.000000 flax-0.8.2/flax/struct.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.334439 flax-0.8.2/flax/testing/
--rw-r--r--   0 runner    (1001) docker     (127)      647 2024-03-14 11:34:46.000000 flax-0.8.2/flax/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9320 2024-03-14 11:34:46.000000 flax-0.8.2/flax/testing/benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)     1990 2024-03-14 11:34:46.000000 flax-0.8.2/flax/traceback_util.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.334439 flax-0.8.2/flax/training/
--rw-r--r--   0 runner    (1001) docker     (127)      613 2024-03-14 11:34:46.000000 flax-0.8.2/flax/training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    43940 2024-03-14 11:34:46.000000 flax-0.8.2/flax/training/checkpoints.py
--rw-r--r--   0 runner    (1001) docker     (127)     3690 2024-03-14 11:34:46.000000 flax-0.8.2/flax/training/common_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6056 2024-03-14 11:34:46.000000 flax-0.8.2/flax/training/dynamic_scale.py
--rw-r--r--   0 runner    (1001) docker     (127)     3194 2024-03-14 11:34:46.000000 flax-0.8.2/flax/training/early_stopping.py
--rw-r--r--   0 runner    (1001) docker     (127)     7419 2024-03-14 11:34:46.000000 flax-0.8.2/flax/training/lr_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     3685 2024-03-14 11:34:46.000000 flax-0.8.2/flax/training/orbax_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2978 2024-03-14 11:34:46.000000 flax-0.8.2/flax/training/prefetch_iterator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4731 2024-03-14 11:34:46.000000 flax-0.8.2/flax/training/train_state.py
--rw-r--r--   0 runner    (1001) docker     (127)    13746 2024-03-14 11:34:46.000000 flax-0.8.2/flax/traverse_util.py
--rw-r--r--   0 runner    (1001) docker     (127)     2560 2024-03-14 11:34:46.000000 flax-0.8.2/flax/typing.py
--rw-r--r--   0 runner    (1001) docker     (127)      650 2024-03-14 11:34:46.000000 flax-0.8.2/flax/version.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.342439 flax-0.8.2/flax.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)    10159 2024-03-14 11:34:59.000000 flax-0.8.2/flax.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    15643 2024-03-14 11:34:59.000000 flax-0.8.2/flax.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-03-14 11:34:59.000000 flax-0.8.2/flax.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)      552 2024-03-14 11:34:59.000000 flax-0.8.2/flax.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)        5 2024-03-14 11:34:59.000000 flax-0.8.2/flax.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.334439 flax-0.8.2/images/
--rw-r--r--   0 runner    (1001) docker     (127)    80407 2024-03-14 11:34:46.000000 flax-0.8.2/images/flax_logo.png
--rw-r--r--   0 runner    (1001) docker     (127)     3862 2024-03-14 11:34:46.000000 flax-0.8.2/images/flax_logo.svg
--rw-r--r--   0 runner    (1001) docker     (127)    15137 2024-03-14 11:34:46.000000 flax-0.8.2/images/flax_logo_250px.png
--rw-r--r--   0 runner    (1001) docker     (127)    29095 2024-03-14 11:34:46.000000 flax-0.8.2/images/flax_logo_500px.png
--rw-r--r--   0 runner    (1001) docker     (127)    14116 2024-03-14 11:34:46.000000 flax-0.8.2/pylintrc
--rw-r--r--   0 runner    (1001) docker     (127)     5586 2024-03-14 11:34:46.000000 flax-0.8.2/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-03-14 11:34:59.346439 flax-0.8.2/setup.cfg
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.338439 flax-0.8.2/tests/
--rw-r--r--   0 runner    (1001) docker     (127)    17456 2024-03-14 11:34:46.000000 flax-0.8.2/tests/checkpoints_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     1733 2024-03-14 11:34:46.000000 flax-0.8.2/tests/colab_tpu_jax_version.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     1672 2024-03-14 11:34:46.000000 flax-0.8.2/tests/configurations_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.338439 flax-0.8.2/tests/core/
--rw-r--r--   0 runner    (1001) docker     (127)     5262 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/core_frozen_dict_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     8399 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/core_lift_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     6750 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/core_meta_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     9633 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/core_scope_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.338439 flax-0.8.2/tests/core/design/
--rw-r--r--   0 runner    (1001) docker     (127)     4795 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/design/core_attention_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     4537 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/design/core_auto_encoder_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2857 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/design/core_big_resnets_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2500 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/design/core_custom_vjp_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     4565 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/design/core_dense_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2621 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/design/core_flow_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     4653 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/design/core_resnet_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2572 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/design/core_scan_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2209 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/design/core_tied_autoencoder_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2612 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/design/core_vmap_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2515 2024-03-14 11:34:46.000000 flax-0.8.2/tests/core/design/core_weight_std_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    25755 2024-03-14 11:34:46.000000 flax-0.8.2/tests/cursor_test.py
--rw-r--r--   0 runner    (1001) docker     (127)      887 2024-03-14 11:34:46.000000 flax-0.8.2/tests/download_dataset_metadata.sh
--rw-r--r--   0 runner    (1001) docker     (127)     2950 2024-03-14 11:34:46.000000 flax-0.8.2/tests/early_stopping_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     1686 2024-03-14 11:34:46.000000 flax-0.8.2/tests/import_test.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     8291 2024-03-14 11:34:46.000000 flax-0.8.2/tests/io_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     3486 2024-03-14 11:34:46.000000 flax-0.8.2/tests/jax_utils_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-14 11:34:59.342439 flax-0.8.2/tests/linen/
--rw-r--r--   0 runner    (1001) docker     (127)     1974 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/initializers_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     4357 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/kw_only_dataclasses_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2594 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/linen_activation_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    17146 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/linen_attention_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2999 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/linen_batch_apply_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     5702 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/linen_combinators_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     1502 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/linen_dtypes_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    41550 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/linen_linear_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     6137 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/linen_meta_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    86606 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/linen_module_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    16205 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/linen_recurrent_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    43532 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/linen_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    66213 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/linen_transforms_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    17745 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/partitioning_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    22947 2024-03-14 11:34:46.000000 flax-0.8.2/tests/linen/summary_test.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4165 2024-03-14 11:34:46.000000 flax-0.8.2/tests/run_all_tests.sh
--rw-r--r--   0 runner    (1001) docker     (127)    16224 2024-03-14 11:34:46.000000 flax-0.8.2/tests/serialization_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2884 2024-03-14 11:34:46.000000 flax-0.8.2/tests/struct_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    14326 2024-03-14 11:34:46.000000 flax-0.8.2/tests/tensorboard_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     6014 2024-03-14 11:34:46.000000 flax-0.8.2/tests/traceback_util_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    10620 2024-03-14 11:34:46.000000 flax-0.8.2/tests/traverse_util_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.858113 flax-0.8.3/
+-rw-r--r--   0 runner    (1001) docker     (127)       54 2024-04-30 09:57:01.000000 flax-0.8.3/.git-blame-ignore-revs
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.770113 flax-0.8.3/.github/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.770113 flax-0.8.3/.github/ISSUE_TEMPLATE/
+-rw-r--r--   0 runner    (1001) docker     (127)      792 2024-04-30 09:57:01.000000 flax-0.8.3/.github/ISSUE_TEMPLATE/bug_report.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.774113 flax-0.8.3/.github/analytics/
+-rw-r--r--   0 runner    (1001) docker     (127)      544 2024-04-30 09:57:01.000000 flax-0.8.3/.github/analytics/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)    13817 2024-04-30 09:57:01.000000 flax-0.8.3/.github/analytics/get_repo_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1677 2024-04-30 09:57:01.000000 flax-0.8.3/.github/analytics/issue_activity_since_date.gql
+-rw-r--r--   0 runner    (1001) docker     (127)     2016 2024-04-30 09:57:01.000000 flax-0.8.3/.github/analytics/pr_data_query.gql
+-rw-r--r--   0 runner    (1001) docker     (127)       34 2024-04-30 09:57:01.000000 flax-0.8.3/.github/analytics/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-04-30 09:57:01.000000 flax-0.8.3/.github/pull_request_template.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.774113 flax-0.8.3/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (127)     6228 2024-04-30 09:57:01.000000 flax-0.8.3/.github/workflows/build.yml
+-rw-r--r--   0 runner    (1001) docker     (127)      834 2024-04-30 09:57:01.000000 flax-0.8.3/.github/workflows/pythonpublish.yml
+-rw-r--r--   0 runner    (1001) docker     (127)      185 2024-04-30 09:57:01.000000 flax-0.8.3/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)     1214 2024-04-30 09:57:01.000000 flax-0.8.3/.pre-commit-config.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      563 2024-04-30 09:57:01.000000 flax-0.8.3/.readthedocs.yml
+-rw-r--r--   0 runner    (1001) docker     (127)      293 2024-04-30 09:57:01.000000 flax-0.8.3/AUTHORS
+-rw-r--r--   0 runner    (1001) docker     (127)    28201 2024-04-30 09:57:01.000000 flax-0.8.3/CHANGELOG.md
+-rw-r--r--   0 runner    (1001) docker     (127)    11309 2024-04-30 09:57:01.000000 flax-0.8.3/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (127)    10230 2024-04-30 09:57:11.858113 flax-0.8.3/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     7910 2024-04-30 09:57:01.000000 flax-0.8.3/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)      110 2024-04-30 09:57:01.000000 flax-0.8.3/contributing.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.774113 flax-0.8.3/dev/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.774113 flax-0.8.3/dev/.devcontainer/
+-rw-r--r--   0 runner    (1001) docker     (127)     2643 2024-04-30 09:57:01.000000 flax-0.8.3/dev/.devcontainer/Dockerfile
+-rw-r--r--   0 runner    (1001) docker     (127)     1806 2024-04-30 09:57:01.000000 flax-0.8.3/dev/.devcontainer/devcontainer.json
+-rw-r--r--   0 runner    (1001) docker     (127)      814 2024-04-30 09:57:01.000000 flax-0.8.3/dev/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     4581 2024-04-30 09:57:01.000000 flax-0.8.3/dev/update_requirements.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.778113 flax-0.8.3/docs/
+-rw-r--r--   0 runner    (1001) docker     (127)       18 2024-04-30 09:57:01.000000 flax-0.8.3/docs/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)      634 2024-04-30 09:57:01.000000 flax-0.8.3/docs/Makefile
+-rw-r--r--   0 runner    (1001) docker     (127)     5359 2024-04-30 09:57:01.000000 flax-0.8.3/docs/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.778113 flax-0.8.3/docs/_ext/
+-rw-r--r--   0 runner    (1001) docker     (127)     4255 2024-04-30 09:57:01.000000 flax-0.8.3/docs/_ext/codediff.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3392 2024-04-30 09:57:01.000000 flax-0.8.3/docs/_ext/codediff_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2288 2024-04-30 09:57:01.000000 flax-0.8.3/docs/_ext/flax_module.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.762112 flax-0.8.3/docs/_static/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.778113 flax-0.8.3/docs/_static/css/
+-rw-r--r--   0 runner    (1001) docker     (127)      309 2024-04-30 09:57:01.000000 flax-0.8.3/docs/_static/css/flax_theme.css
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.762112 flax-0.8.3/docs/_templates/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.778113 flax-0.8.3/docs/_templates/autosummary/
+-rw-r--r--   0 runner    (1001) docker     (127)      674 2024-04-30 09:57:01.000000 flax-0.8.3/docs/_templates/autosummary/flax_module.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.778113 flax-0.8.3/docs/api_reference/
+-rw-r--r--   0 runner    (1001) docker     (127)      190 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.config.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      321 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.core.frozen_dict.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     1679 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.cursor.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      158 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.errors.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.782113 flax-0.8.3/docs/api_reference/flax.experimental.nnx/
+-rw-r--r--   0 runner    (1001) docker     (127)      257 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/helpers.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      308 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      204 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/module.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.782113 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)      762 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/activations.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      308 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/attention.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      268 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      778 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/initializers.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      319 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/linear.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      231 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/normalization.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/stochastic.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      184 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/rnglib.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      263 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/spmd.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.782113 flax-0.8.3/docs/api_reference/flax.experimental.nnx/training/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/training/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      279 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/training/metrics.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      174 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/training/optimizer.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      398 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/transforms.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      411 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/variables.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      145 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/visualization.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      365 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.jax_utils.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.786113 flax-0.8.3/docs/api_reference/flax.linen/
+-rw-r--r--   0 runner    (1001) docker     (127)      827 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/activation_functions.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      117 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/decorators.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      350 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      141 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/init_apply.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      756 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/initializers.rst
+-rw-r--r--   0 runner    (1001) docker     (127)       94 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/inspection.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2360 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/layers.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      509 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/module.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      176 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/profiling.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      587 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/spmd.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      412 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/transformations.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      116 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/variable.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      480 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.serialization.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      161 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.struct.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      275 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.traceback_util.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     1212 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.training.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      798 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.traverse_util.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      296 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5741 2024-04-30 09:57:01.000000 flax-0.8.3/docs/conf.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6623 2024-04-30 09:57:01.000000 flax-0.8.3/docs/conf_sphinx_patch.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11946 2024-04-30 09:57:01.000000 flax-0.8.3/docs/contributing.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.786113 flax-0.8.3/docs/developer_notes/
+-rw-r--r--   0 runner    (1001) docker     (127)      153 2024-04-30 09:57:01.000000 flax-0.8.3/docs/developer_notes/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    18081 2024-04-30 09:57:01.000000 flax-0.8.3/docs/developer_notes/lift.md
+-rw-r--r--   0 runner    (1001) docker     (127)    22000 2024-04-30 09:57:01.000000 flax-0.8.3/docs/developer_notes/module_lifecycle.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.786113 flax-0.8.3/docs/examples/
+-rw-r--r--   0 runner    (1001) docker     (127)     4692 2024-04-30 09:57:01.000000 flax-0.8.3/docs/examples/community_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     4422 2024-04-30 09:57:01.000000 flax-0.8.3/docs/examples/core_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    22579 2024-04-30 09:57:01.000000 flax-0.8.3/docs/examples/google_research_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      148 2024-04-30 09:57:01.000000 flax-0.8.3/docs/examples/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2028 2024-04-30 09:57:01.000000 flax-0.8.3/docs/examples/repositories_that_use_flax.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.786113 flax-0.8.3/docs/experimental/
+-rw-r--r--   0 runner    (1001) docker     (127)       70 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.790113 flax-0.8.3/docs/experimental/nnx/
+-rw-r--r--   0 runner    (1001) docker     (127)     3953 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)   317520 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/mnist_tutorial.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    10359 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/mnist_tutorial.md
+-rw-r--r--   0 runner    (1001) docker     (127)   194440 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/nnx_basics.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    13494 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/nnx_basics.md
+-rw-r--r--   0 runner    (1001) docker     (127)     4423 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/transforms.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     4134 2024-04-30 09:57:01.000000 flax-0.8.3/docs/faq.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    20991 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flax.png
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.790113 flax-0.8.3/docs/flip/
+-rw-r--r--   0 runner    (1001) docker     (127)      648 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/0000-template.md
+-rw-r--r--   0 runner    (1001) docker     (127)    17256 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/1009-optimizer-api.md
+-rw-r--r--   0 runner    (1001) docker     (127)     8189 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/1777-default-dtype.md
+-rw-r--r--   0 runner    (1001) docker     (127)    11758 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/2396-rnn.md
+-rw-r--r--   0 runner    (1001) docker     (127)    10424 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/2434-general-metadata.md
+-rw-r--r--   0 runner    (1001) docker     (127)     4099 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/2974-kw-only-dataclasses.md
+-rw-r--r--   0 runner    (1001) docker     (127)     4068 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/3099-rnnbase-refactor.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1404 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     6679 2024-04-30 09:57:01.000000 flax-0.8.3/docs/glossary.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.790113 flax-0.8.3/docs/guides/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.790113 flax-0.8.3/docs/guides/converting_and_upgrading/
+-rw-r--r--   0 runner    (1001) docker     (127)    10603 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    26613 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/haiku_migration_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      255 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    17314 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    10607 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/optax_update_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     9051 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     4119 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     6093 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.794113 flax-0.8.3/docs/guides/data_preprocessing/
+-rw-r--r--   0 runner    (1001) docker     (127)     6994 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/data_preprocessing/full_eval.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/data_preprocessing/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     8230 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/data_preprocessing/loading_datasets.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     5149 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/data_preprocessing/loading_datasets.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.794113 flax-0.8.3/docs/guides/flax_fundamentals/
+-rw-r--r--   0 runner    (1001) docker     (127)     4087 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/arguments.md
+-rw-r--r--   0 runner    (1001) docker     (127)    38158 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/flax_basics.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    20797 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/flax_basics.md
+-rw-r--r--   0 runner    (1001) docker     (127)      214 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    68258 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/rng_guide.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    29445 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/rng_guide.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3125 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/setup_or_nncompact.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5886 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/state_params.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     7226 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_sharp_bits.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     5794 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_sharp_bits.md
+-rw-r--r--   0 runner    (1001) docker     (127)      252 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.794113 flax-0.8.3/docs/guides/model_inspection/
+-rw-r--r--   0 runner    (1001) docker     (127)    12699 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/model_inspection/extracting_intermediates.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      110 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/model_inspection/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     7138 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/model_inspection/model_surgery.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     4368 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/model_inspection/model_surgery.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.798113 flax-0.8.3/docs/guides/parallel_training/
+-rw-r--r--   0 runner    (1001) docker     (127)    10480 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/parallel_training/ensembling.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    62518 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/parallel_training/flax_on_pjit.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    24602 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/parallel_training/flax_on_pjit.md
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/parallel_training/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.798113 flax-0.8.3/docs/guides/training_techniques/
+-rw-r--r--   0 runner    (1001) docker     (127)     9141 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/batch_norm.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    11028 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/dropout.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     8171 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/lr_schedule.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    11290 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/transfer_learning.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     7843 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/transfer_learning.md
+-rw-r--r--   0 runner    (1001) docker     (127)    53437 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/use_checkpointing.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    25779 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/use_checkpointing.md
+-rw-r--r--   0 runner    (1001) docker     (127)     8649 2024-04-30 09:57:01.000000 flax-0.8.3/docs/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    39597 2024-04-30 09:57:01.000000 flax-0.8.3/docs/linen_intro.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    20900 2024-04-30 09:57:01.000000 flax-0.8.3/docs/linen_intro.md
+-rw-r--r--   0 runner    (1001) docker     (127)     7604 2024-04-30 09:57:01.000000 flax-0.8.3/docs/philosophy.md
+-rw-r--r--   0 runner    (1001) docker     (127)   101054 2024-04-30 09:57:01.000000 flax-0.8.3/docs/quick_start.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    14177 2024-04-30 09:57:01.000000 flax-0.8.3/docs/quick_start.md
+-rw-r--r--   0 runner    (1001) docker     (127)      705 2024-04-30 09:57:01.000000 flax-0.8.3/docs/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      174 2024-04-30 09:57:01.000000 flax-0.8.3/docs/robots.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.798113 flax-0.8.3/examples/
+-rw-r--r--   0 runner    (1001) docker     (127)      743 2024-04-30 09:57:01.000000 flax-0.8.3/examples/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)      582 2024-04-30 09:57:01.000000 flax-0.8.3/examples/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.798113 flax-0.8.3/examples/cloud/
+-rw-r--r--   0 runner    (1001) docker     (127)     4635 2024-04-30 09:57:01.000000 flax-0.8.3/examples/cloud/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     9222 2024-04-30 09:57:01.000000 flax-0.8.3/examples/cloud/launch_gce.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1650 2024-04-30 09:57:01.000000 flax-0.8.3/examples/cloud/startup_script.sh
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.802113 flax-0.8.3/examples/imagenet/
+-rw-r--r--   0 runner    (1001) docker     (127)     9944 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.802113 flax-0.8.3/examples/imagenet/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     2192 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1105 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/configs/fake_data_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1670 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/configs/tpu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/configs/v100_x8.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1088 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/configs/v100_x8_mixed_precision.py
+-rw-r--r--   0 runner    (1001) docker     (127)   293668 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/imagenet.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     3334 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/imagenet_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2190 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/imagenet_fake_data_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8124 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2125 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4346 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1933 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)      341 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)    12863 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3049 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.802113 flax-0.8.3/examples/linen_design_test/
+-rw-r--r--   0 runner    (1001) docker     (127)     6242 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/attention_simple.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3145 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1246 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/dense.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1304 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/linear_regression.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2303 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/mlp_explicit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1880 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/mlp_inline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1891 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/mlp_lazy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.806113 flax-0.8.3/examples/lm1b/
+-rw-r--r--   0 runner    (1001) docker     (127)     3320 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.806113 flax-0.8.3/examples/lm1b/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     5011 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12500 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3355 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2190 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13273 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)      347 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     4843 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/temperature_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1453 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/temperature_sampler_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5313 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20414 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1994 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/train_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5793 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.806113 flax-0.8.3/examples/mnist/
+-rw-r--r--   0 runner    (1001) docker     (127)     1741 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.806113 flax-0.8.3/examples/mnist/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)      912 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2121 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    98260 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/mnist.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     2366 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/mnist_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)      298 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     5231 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2262 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.806113 flax-0.8.3/examples/nlp_seq/
+-rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     7884 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4073 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6464 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)       60 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)    14100 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/train.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.810113 flax-0.8.3/examples/ogbg_molpcba/
+-rw-r--r--   0 runner    (1001) docker     (127)     4486 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.810113 flax-0.8.3/examples/ogbg_molpcba/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     1520 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1551 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/configs/default_graph_net.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1946 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/configs/hparam_sweep.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1405 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/configs/test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8133 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2571 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2198 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7068 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5179 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)  1110530 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/ogbg_molpcba.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     4769 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)      329 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)    13697 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12431 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.814113 flax-0.8.3/examples/ppo/
+-rw-r--r--   0 runner    (1001) docker     (127)     2501 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2607 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/agent.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.814113 flax-0.8.3/examples/ppo/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     1955 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2460 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/env_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2346 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13152 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/ppo_lib.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5286 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/ppo_lib_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1529 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/ppo_main.py
+-rw-r--r--   0 runner    (1001) docker     (127)      192 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     8930 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/seed_rl_atari_preprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1897 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/test_episodes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.814113 flax-0.8.3/examples/seq2seq/
+-rw-r--r--   0 runner    (1001) docker     (127)      913 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4395 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)       65 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)    24740 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/seq2seq.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     6844 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3201 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.818113 flax-0.8.3/examples/sst2/
+-rw-r--r--   0 runner    (1001) docker     (127)     1893 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/README.md
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2028 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/build_vocabulary.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.818113 flax-0.8.3/examples/sst2/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     1226 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/configs/default.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     9834 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3522 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2118 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14403 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3526 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)      156 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     8607 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/sst2.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     9335 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2123 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/train_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)   117898 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/vocab.txt
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4407 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/vocabulary.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.818113 flax-0.8.3/examples/vae/
+-rw-r--r--   0 runner    (1001) docker     (127)     1132 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.818113 flax-0.8.3/examples/vae/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)      883 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1458 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1814 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1777 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2152 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/reconstruction.png
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.818113 flax-0.8.3/examples/vae/results/
+-rw-r--r--   0 runner    (1001) docker     (127)        6 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/results/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)    43139 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/sample.png
+-rw-r--r--   0 runner    (1001) docker     (127)     4596 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3580 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.822113 flax-0.8.3/examples/wmt/
+-rw-r--r--   0 runner    (1001) docker     (127)     6106 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     7270 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/bleu.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.822113 flax-0.8.3/examples/wmt/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     3482 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14745 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/decode.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12910 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3318 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2166 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18604 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)      398 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     5314 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23409 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1995 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.822113 flax-0.8.3/flax/
+-rw-r--r--   0 runner    (1001) docker     (127)     1135 2024-04-30 09:57:01.000000 flax-0.8.3/flax/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5990 2024-04-30 09:57:01.000000 flax-0.8.3/flax/configurations.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/core/
+-rw-r--r--   0 runner    (1001) docker     (127)     1467 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5763 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/axes_scan.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10482 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/flax_functional_engine.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     9974 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/frozen_dict.py
+-rw-r--r--   0 runner    (1001) docker     (127)    61688 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/lift.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11725 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/meta.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/core/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)     1795 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/nn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17932 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/nn/attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12040 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/nn/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6897 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/nn/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1488 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/nn/stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2546 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/partial_eval.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38092 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/scope.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1054 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/tracers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1558 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/variables.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26399 2024-04-30 09:57:01.000000 flax-0.8.3/flax/cursor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30713 2024-04-30 09:57:01.000000 flax-0.8.3/flax/errors.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/experimental/
+-rw-r--r--   0 runner    (1001) docker     (127)      582 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/experimental/nnx/
+-rw-r--r--   0 runner    (1001) docker     (127)     1831 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)     3678 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     5733 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/experimental/nnx/docs/
+-rw-r--r--   0 runner    (1001) docker     (127)      350 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/blog.md
+-rw-r--r--   0 runner    (1001) docker     (127)    10253 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/demo.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     4271 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/demo.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/experimental/nnx/docs/images/
+-rw-r--r--   0 runner    (1001) docker     (127)   304812 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/images/stateful-transforms.png
+-rw-r--r--   0 runner    (1001) docker     (127)    61295 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/quick_start.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    16318 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/tiny_nnx.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    26920 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/why.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    15238 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/why.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.766113 flax-0.8.3/flax/experimental/nnx/examples/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.830113 flax-0.8.3/flax/experimental/nnx/examples/lm1b/
+-rw-r--r--   0 runner    (1001) docker     (127)     3267 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.830113 flax-0.8.3/flax/experimental/nnx/examples/lm1b/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     5096 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12344 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3293 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2143 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15055 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9662 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)      347 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     4799 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/temperature_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1448 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/temperature_sampler_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5263 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20551 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2026 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/train_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5036 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.830113 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/
+-rw-r--r--   0 runner    (1001) docker     (127)     2868 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/01_functional_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2593 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/02_lifted_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5208 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/05_vae.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1865 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/06_scan_over_layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1946 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/08_save_load_checkpoints.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1969 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/09_parameter_surgery.py
+-rw-r--r--   0 runner    (1001) docker     (127)       35 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.834113 flax-0.8.3/flax/experimental/nnx/nnx/
+-rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3187 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/compatibility.py
+-rw-r--r--   0 runner    (1001) docker     (127)      626 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/errors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2913 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/filterlib.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37769 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6528 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2326 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/ids.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12260 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/module.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.838113 flax-0.8.3/flax/experimental/nnx/nnx/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1219 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/activations.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25351 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3318 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/dtypes.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2596 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/initializers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28389 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18356 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3520 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2562 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/proxy_caller.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3200 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/reprlib.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8995 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/rnglib.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6373 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/spmd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7560 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/state.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1728 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/tracers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.838113 flax-0.8.3/flax/experimental/nnx/nnx/training/
+-rw-r--r--   0 runner    (1001) docker     (127)     5291 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/training/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4861 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/training/optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    55991 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22081 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/variables.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3508 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/visualization.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.838113 flax-0.8.3/flax/experimental/nnx/scripts/
+-rw-r--r--   0 runner    (1001) docker     (127)       17 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/scripts/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      285 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/scripts/run-all-examples.bash
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.838113 flax-0.8.3/flax/experimental/nnx/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.842113 flax-0.8.3/flax/experimental/nnx/tests/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)     4930 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3069 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2127 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_embed.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4290 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7537 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2609 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1204 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_compatibility.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1636 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_containers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11196 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_graph_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2003 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)      929 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_ids.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7462 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_integration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2351 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16827 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4064 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4268 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_partitioning.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6492 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_rngs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2780 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_spmd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2078 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_state.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28817 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1836 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_variable.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1752 2024-04-30 09:57:01.000000 flax-0.8.3/flax/ids.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5309 2024-04-30 09:57:01.000000 flax-0.8.3/flax/io.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11534 2024-04-30 09:57:01.000000 flax-0.8.3/flax/jax_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.842113 flax-0.8.3/flax/linen/
+-rw-r--r--   0 runner    (1001) docker     (127)     2191 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     5259 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4101 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/activation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32287 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4172 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/batch_apply.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3846 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/combinators.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3860 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/dtypes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.842113 flax-0.8.3/flax/linen/experimental/
+-rw-r--r--   0 runner    (1001) docker     (127)    11235 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/experimental/layers_with_named_axes.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10062 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/fp8_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2676 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/initializers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8828 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/kw_only_dataclasses.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44217 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)   113027 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/module.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49760 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19421 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/partitioning.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5567 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45995 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/recurrent.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11187 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/spmd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3634 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26100 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/summary.py
+-rw-r--r--   0 runner    (1001) docker     (127)    79289 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.846113 flax-0.8.3/flax/metrics/
+-rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-04-30 09:57:01.000000 flax-0.8.3/flax/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7792 2024-04-30 09:57:01.000000 flax-0.8.3/flax/metrics/tensorboard.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.846113 flax-0.8.3/flax/oss/
+-rw-r--r--   0 runner    (1001) docker     (127)      443 2024-04-30 09:57:01.000000 flax-0.8.3/flax/oss/ .git-blame-ignore-revs
+-rw-r--r--   0 runner    (1001) docker     (127)       58 2024-04-30 09:57:01.000000 flax-0.8.3/flax/py.typed
+-rw-r--r--   0 runner    (1001) docker     (127)    14315 2024-04-30 09:57:01.000000 flax-0.8.3/flax/serialization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8275 2024-04-30 09:57:01.000000 flax-0.8.3/flax/struct.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.846113 flax-0.8.3/flax/testing/
+-rw-r--r--   0 runner    (1001) docker     (127)      647 2024-04-30 09:57:01.000000 flax-0.8.3/flax/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9320 2024-04-30 09:57:01.000000 flax-0.8.3/flax/testing/benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1990 2024-04-30 09:57:01.000000 flax-0.8.3/flax/traceback_util.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.846113 flax-0.8.3/flax/training/
+-rw-r--r--   0 runner    (1001) docker     (127)      613 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    43835 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/checkpoints.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3690 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/common_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6056 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/dynamic_scale.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3194 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/early_stopping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7419 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/lr_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3685 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/orbax_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2978 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/prefetch_iterator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4767 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/train_state.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13746 2024-04-30 09:57:01.000000 flax-0.8.3/flax/traverse_util.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2819 2024-04-30 09:57:01.000000 flax-0.8.3/flax/typing.py
+-rw-r--r--   0 runner    (1001) docker     (127)      650 2024-04-30 09:57:01.000000 flax-0.8.3/flax/version.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.854113 flax-0.8.3/flax.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    10230 2024-04-30 09:57:11.000000 flax-0.8.3/flax.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    16651 2024-04-30 09:57:11.000000 flax-0.8.3/flax.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-30 09:57:11.000000 flax-0.8.3/flax.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      595 2024-04-30 09:57:11.000000 flax-0.8.3/flax.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        5 2024-04-30 09:57:11.000000 flax-0.8.3/flax.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.846113 flax-0.8.3/images/
+-rw-r--r--   0 runner    (1001) docker     (127)    80407 2024-04-30 09:57:01.000000 flax-0.8.3/images/flax_logo.png
+-rw-r--r--   0 runner    (1001) docker     (127)     3862 2024-04-30 09:57:01.000000 flax-0.8.3/images/flax_logo.svg
+-rw-r--r--   0 runner    (1001) docker     (127)    15137 2024-04-30 09:57:01.000000 flax-0.8.3/images/flax_logo_250px.png
+-rw-r--r--   0 runner    (1001) docker     (127)    29095 2024-04-30 09:57:01.000000 flax-0.8.3/images/flax_logo_500px.png
+-rw-r--r--   0 runner    (1001) docker     (127)    14116 2024-04-30 09:57:01.000000 flax-0.8.3/pylintrc
+-rw-r--r--   0 runner    (1001) docker     (127)     5837 2024-04-30 09:57:01.000000 flax-0.8.3/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-04-30 09:57:11.858113 flax-0.8.3/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.850113 flax-0.8.3/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)    17456 2024-04-30 09:57:01.000000 flax-0.8.3/tests/checkpoints_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1733 2024-04-30 09:57:01.000000 flax-0.8.3/tests/colab_tpu_jax_version.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     1672 2024-04-30 09:57:01.000000 flax-0.8.3/tests/configurations_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.850113 flax-0.8.3/tests/core/
+-rw-r--r--   0 runner    (1001) docker     (127)     5262 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/core_frozen_dict_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8608 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/core_lift_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6750 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/core_meta_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9633 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/core_scope_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.850113 flax-0.8.3/tests/core/design/
+-rw-r--r--   0 runner    (1001) docker     (127)     4795 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_attention_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4537 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_auto_encoder_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2857 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_big_resnets_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2500 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_custom_vjp_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4565 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_dense_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2621 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_flow_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4653 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_resnet_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2572 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_scan_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2209 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_tied_autoencoder_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2612 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_vmap_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2515 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_weight_std_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25755 2024-04-30 09:57:01.000000 flax-0.8.3/tests/cursor_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)      887 2024-04-30 09:57:01.000000 flax-0.8.3/tests/download_dataset_metadata.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     2950 2024-04-30 09:57:01.000000 flax-0.8.3/tests/early_stopping_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1686 2024-04-30 09:57:01.000000 flax-0.8.3/tests/import_test.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     8291 2024-04-30 09:57:01.000000 flax-0.8.3/tests/io_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3486 2024-04-30 09:57:01.000000 flax-0.8.3/tests/jax_utils_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.854113 flax-0.8.3/tests/linen/
+-rw-r--r--   0 runner    (1001) docker     (127)     1974 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/initializers_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4357 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/kw_only_dataclasses_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2594 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_activation_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18253 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_attention_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2999 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_batch_apply_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5702 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_combinators_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1502 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_dtypes_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41550 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_linear_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6137 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_meta_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    86829 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_module_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16205 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_recurrent_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    46386 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    66422 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_transforms_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17745 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/partitioning_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24643 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/summary_test.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4523 2024-04-30 09:57:01.000000 flax-0.8.3/tests/run_all_tests.sh
+-rw-r--r--   0 runner    (1001) docker     (127)    16224 2024-04-30 09:57:01.000000 flax-0.8.3/tests/serialization_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4597 2024-04-30 09:57:01.000000 flax-0.8.3/tests/struct_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14326 2024-04-30 09:57:01.000000 flax-0.8.3/tests/tensorboard_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6014 2024-04-30 09:57:01.000000 flax-0.8.3/tests/traceback_util_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10620 2024-04-30 09:57:01.000000 flax-0.8.3/tests/traverse_util_test.py
```

### Comparing `flax-0.8.2/.github/ISSUE_TEMPLATE/bug_report.md` & `flax-0.8.3/.github/ISSUE_TEMPLATE/bug_report.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/.github/analytics/README.md` & `flax-0.8.3/.github/analytics/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/.github/analytics/get_repo_metrics.py` & `flax-0.8.3/.github/analytics/get_repo_metrics.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/.github/analytics/issue_activity_since_date.gql` & `flax-0.8.3/.github/analytics/issue_activity_since_date.gql`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/.github/analytics/pr_data_query.gql` & `flax-0.8.3/.github/analytics/pr_data_query.gql`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/.github/pull_request_template.md` & `flax-0.8.3/.github/pull_request_template.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/.github/workflows/build.yml` & `flax-0.8.3/.github/workflows/build.yml`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/.github/workflows/pythonpublish.yml` & `flax-0.8.3/.github/workflows/pythonpublish.yml`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/.pre-commit-config.yaml` & `flax-0.8.3/.pre-commit-config.yaml`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/.readthedocs.yml` & `flax-0.8.3/.readthedocs.yml`

 * *Files 1% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 
 # Required
 version: 2
 
 build:
   os: ubuntu-22.04
   tools:
-    python: "3.9"
+    python: "3.10"
 
 # Build documentation in the docs/ directory with Sphinx
 sphinx:
   configuration: docs/conf.py
 
 # Optionally build your docs in additional formats such as PDF and ePub
 formats:
```

### Comparing `flax-0.8.2/CHANGELOG.md` & `flax-0.8.3/CHANGELOG.md`

 * *Files 24% similar despite different names*

```diff
@@ -20,14 +20,58 @@
 -
 -
 -
 -
 -
 -
 
+0.8.2
+-----
+- fixed rng guide outputs by @chiamp in https://github.com/google/flax/pull/3685
+- enforce mask kwarg in norm layers by @chiamp in https://github.com/google/flax/pull/3663
+- added kwargs to self.param and self.variable by @chiamp in https://github.com/google/flax/pull/3675
+- added nnx normalization tests by @chiamp in https://github.com/google/flax/pull/3689
+- added NNX init_cache docstring example by @chiamp in https://github.com/google/flax/pull/3688
+- added nnx attention equivalence test by @chiamp in https://github.com/google/flax/pull/3687
+- Fix bug that assumed frozen-dict keys were strings. by @copybara-service in https://github.com/google/flax/pull/3692
+- added nnx rmsnorm by @chiamp in https://github.com/google/flax/pull/3691
+- updated nnx compute_stats by @chiamp in https://github.com/google/flax/pull/3693
+- fixed intercept_methods docstring by @chiamp in https://github.com/google/flax/pull/3694
+- [nnx] Add Sphinx Docs by @cgarciae in https://github.com/google/flax/pull/3678
+- Fix pointless docstring example of nn.checkpoint / nn.remat. by @levskaya in https://github.com/google/flax/pull/3703
+- added default params rng to .apply by @chiamp in https://github.com/google/flax/pull/3698
+- [nnx] add partial_init by @cgarciae in https://github.com/google/flax/pull/3674
+- make make_rng default to 'params' by @chiamp in https://github.com/google/flax/pull/3699
+- Add SimpleCell. by @carlosgmartin in https://github.com/google/flax/pull/3697
+- fix Module.module_paths docstring by @cgarciae in https://github.com/google/flax/pull/3709
+- Guarantee the latest JAX version on CI by @cgarciae in https://github.com/google/flax/pull/3705
+- Replace deprecated API `jax.tree_map` by @copybara-service in https://github.com/google/flax/pull/3715
+- Use `jax.tree_util.tree_map` instead of deprecated `jax.tree_map`. by @copybara-service in https://github.com/google/flax/pull/3714
+- [nnx] simplify readme by @cgarciae in https://github.com/google/flax/pull/3707
+- [nnx] add demo.ipynb by @cgarciae in https://github.com/google/flax/pull/3680
+- Fix Tabulate's compute_flops by @cgarciae in https://github.com/google/flax/pull/3721
+- [nnx] simplify TraceState by @cgarciae in https://github.com/google/flax/pull/3724
+- Add broadcast of `strides` and `kernel_dilation` to `nn.ConvTranspose` by @IvyZX in https://github.com/google/flax/pull/3731
+- [nnx] Fix State.__sub__ by @cgarciae in https://github.com/google/flax/pull/3704
+- [nnx] always fold_in on fork + new ForkedKeys return type by @cgarciae in https://github.com/google/flax/pull/3722
+- [nnx] explicit Variables by @cgarciae in https://github.com/google/flax/pull/3720
+- Improves fingerprint definition for Modules in nn.jit. by @copybara-service in https://github.com/google/flax/pull/3736
+- Flax: avoid key reuse in tests by @copybara-service in https://github.com/google/flax/pull/3740
+- added Einsum layer by @chiamp in https://github.com/google/flax/pull/3710
+- nn.jit: automatic fingerprint definition for dataclass attributes by @cgarciae in https://github.com/google/flax/pull/3737
+- [NVIDIA] Use custom grad accumulation for FP8 params by @kaixih in https://github.com/google/flax/pull/3623
+- removed nnx dataclass by @chiamp in https://github.com/google/flax/pull/3742
+- [nnx] cleanup graph_utils by @cgarciae in https://github.com/google/flax/pull/3728
+- Fix doctest and unbreak head by @IvyZX in https://github.com/google/flax/pull/3753
+- [nnx] add pytree support by @cgarciae in https://github.com/google/flax/pull/3732
+- fixed intercept_methods docstring by @chiamp in https://github.com/google/flax/pull/3752
+- Add ConvLSTMCell to docs. by @carlosgmartin in https://github.com/google/flax/pull/3712
+- [nnx] remove flagslib by @cgarciae in https://github.com/google/flax/pull/3733
+- Fix tests after applying JAX key-reuse checker. See: by @copybara-service in https://github.com/google/flax/pull/3748
+
 0.8.1
 -----
 - Added default collection in `make_rng`.
 - Added `InstanceNorm` and renamed `channel_axes` to `feature_axes`.
 - Added norm equivalence tests.
 - Added `Module.module_paths` and doc.
 - make `Sequential.__call__` compact.
```

### Comparing `flax-0.8.2/LICENSE` & `flax-0.8.3/LICENSE`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/PKG-INFO` & `flax-0.8.3/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flax
-Version: 0.8.2
+Version: 0.8.3
 Summary: Flax: A neural network library for JAX designed for flexibility
 Author-email: Flax team <flax-dev@google.com>
 Project-URL: homepage, https://github.com/google/flax
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
@@ -47,14 +47,15 @@
 Requires-Dist: sentencepiece; extra == "testing"
 Requires-Dist: tensorflow_text>=2.11.0; extra == "testing"
 Requires-Dist: tensorflow_datasets; extra == "testing"
 Requires-Dist: tensorflow; extra == "testing"
 Requires-Dist: torch; extra == "testing"
 Requires-Dist: nbstripout; extra == "testing"
 Requires-Dist: black[jupyter]==23.7.0; extra == "testing"
+Requires-Dist: penzai; python_version >= "3.10" and extra == "testing"
 
 <div align="center">
 <img src="https://raw.githubusercontent.com/google/flax/main/images/flax_logo_250px.png" alt="logo"></img>
 </div>
 
 # Flax: A neural network library and ecosystem for JAX designed for flexibility
 
@@ -247,15 +248,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.8.1},
+  version = {0.8.2},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.8.2/README.md` & `flax-0.8.3/README.md`

 * *Files 0% similar despite different names*

```diff
@@ -193,15 +193,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.8.1},
+  version = {0.8.2},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.8.2/dev/.devcontainer/Dockerfile` & `flax-0.8.3/dev/.devcontainer/Dockerfile`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/dev/.devcontainer/devcontainer.json` & `flax-0.8.3/dev/.devcontainer/devcontainer.json`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/dev/README.md` & `flax-0.8.3/dev/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/dev/update_requirements.py` & `flax-0.8.3/dev/update_requirements.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/Makefile` & `flax-0.8.3/docs/Makefile`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/README.md` & `flax-0.8.3/docs/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/_ext/codediff.py` & `flax-0.8.3/docs/_ext/codediff.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/_ext/codediff_test.py` & `flax-0.8.3/docs/_ext/codediff_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/_ext/flax_module.py` & `flax-0.8.3/docs/_ext/flax_module.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/_templates/autosummary/flax_module.rst` & `flax-0.8.3/docs/_templates/autosummary/flax_module.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/api_reference/flax.cursor.rst` & `flax-0.8.3/docs/api_reference/flax.cursor.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/api_reference/flax.linen/initializers.rst` & `flax-0.8.3/docs/api_reference/flax.linen/initializers.rst`

 * *Files 27% similar despite different names*

```diff
@@ -16,41 +16,12 @@
 .. autofunction:: lecun_uniform
 .. autofunction:: normal
 .. autofunction:: truncated_normal
 .. autofunction:: ones
 .. autofunction:: ones_init
 .. autofunction:: orthogonal
 .. autofunction:: uniform
-.. autofunction:: standardize
 .. autofunction:: variance_scaling
 .. autofunction:: xavier_normal
 .. autofunction:: xavier_uniform
 .. autofunction:: zeros
 .. autofunction:: zeros_init
-
-**Summary**
-
-.. autosummary::
-  :toctree: _autosummary
-
-  constant
-  delta_orthogonal
-  glorot_normal
-  glorot_uniform
-  he_normal
-  he_uniform
-  kaiming_normal
-  kaiming_uniform
-  lecun_normal
-  lecun_uniform
-  normal
-  truncated_normal
-  ones
-  ones_init
-  orthogonal
-  uniform
-  standardize
-  variance_scaling
-  xavier_normal
-  xavier_uniform
-  zeros
-  zeros_init
```

### Comparing `flax-0.8.2/docs/api_reference/flax.linen/layers.rst` & `flax-0.8.3/docs/api_reference/flax.linen/layers.rst`

 * *Files 21% similar despite different names*

```diff
@@ -148,52 +148,7 @@
 
 BatchApply
 ------------------------
 
 .. flax_module::
   :module: flax.linen
   :class: BatchApply
-
-
-**Summary**
-
-.. autosummary::
-  :toctree: _autosummary
-  :template: flax_module
-
-  Dense
-  DenseGeneral
-  Conv
-  ConvTranspose
-  ConvLocal
-  Einsum
-  Embed
-  BatchNorm
-  LayerNorm
-  GroupNorm
-  RMSNorm
-  SpectralNorm
-  WeightNorm
-  Sequential
-  Dropout
-  MultiHeadDotProductAttention
-  MultiHeadAttention
-  SelfAttention
-  RNNCellBase
-  LSTMCell
-  OptimizedLSTMCell
-  SimpleCell
-  GRUCell
-  RNN
-  Bidirectional
-  BatchApply
-
-.. autosummary::
-  :toctree: _autosummary
-
-  max_pool
-  avg_pool
-  pool
-  dot_product_attention_weights
-  dot_product_attention
-  make_attention_mask
-  make_causal_mask
```

### Comparing `flax-0.8.2/docs/api_reference/flax.linen/spmd.rst` & `flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/initializers.rst`

 * *Files 25% similar despite different names*

```diff
@@ -1,40 +1,27 @@
+Initializers
+------------------------
 
-SPMD
-----------------------
-
-.. automodule:: flax.linen.spmd
-.. currentmodule:: flax.linen
-
-.. autofunction:: Partitioned
-.. autofunction:: with_partitioning
-.. autofunction:: get_partition_spec
-.. autofunction:: get_sharding
-.. autofunction:: LogicallyPartitioned
-.. autofunction:: logical_axis_rules
-.. autofunction:: set_logical_axis_rules
-.. autofunction:: get_logical_axis_rules
-.. autofunction:: logical_to_mesh_axes
-.. autofunction:: logical_to_mesh
-.. autofunction:: logical_to_mesh_sharding
-.. autofunction:: with_logical_constraint
-.. autofunction:: with_logical_partitioning
-
-**Summary**
-
-.. autosummary::
-  :toctree: _autosummary
-
-  Partitioned
-  with_partitioning
-  get_partition_spec
-  get_sharding
-  LogicallyPartitioned
-  logical_axis_rules
-  set_logical_axis_rules
-  get_logical_axis_rules
-  logical_to_mesh_axes
-  logical_to_mesh
-  logical_to_mesh_sharding
-  with_logical_constraint
-  with_logical_partitioning
+.. automodule:: flax.experimental.nnx.initializers
+.. currentmodule:: flax.experimental.nnx.initializers
 
+.. autofunction:: constant
+.. autofunction:: delta_orthogonal
+.. autofunction:: glorot_normal
+.. autofunction:: glorot_uniform
+.. autofunction:: he_normal
+.. autofunction:: he_uniform
+.. autofunction:: kaiming_normal
+.. autofunction:: kaiming_uniform
+.. autofunction:: lecun_normal
+.. autofunction:: lecun_uniform
+.. autofunction:: normal
+.. autofunction:: truncated_normal
+.. autofunction:: ones
+.. autofunction:: ones_init
+.. autofunction:: orthogonal
+.. autofunction:: uniform
+.. autofunction:: variance_scaling
+.. autofunction:: xavier_normal
+.. autofunction:: xavier_uniform
+.. autofunction:: zeros
+.. autofunction:: zeros_init
```

### Comparing `flax-0.8.2/docs/api_reference/flax.training.rst` & `flax-0.8.3/docs/api_reference/flax.training.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/api_reference/flax.traverse_util.rst` & `flax-0.8.3/docs/api_reference/flax.traverse_util.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/conf.py` & `flax-0.8.3/docs/conf.py`

 * *Files 1% similar despite different names*

```diff
@@ -104,14 +104,16 @@
 html_title = ''
 
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
 # so a file named 'default.css' will overwrite the builtin 'default.css'.
 html_static_path = ['_static']
 
+html_extra_path = ['robots.txt']
+
 html_theme_options = {
   'repository_url': 'https://github.com/google/flax',
   'use_repository_button': True,  # add a 'link to repository' button
   'use_issues_button': False,  # add an 'Open an Issue' button
   'path_to_docs': (
     'docs'
   ),  # used to compute the path to launch notebooks in colab
@@ -145,14 +147,15 @@
 # types, even if the parameters aren't explicitly documented.
 always_document_param_types = True
 
 # -- doctest configuration -------------------------------------------------
 doctest_global_setup = """
 import jax
 import jax.numpy as jnp
+from flax.experimental import nnx
 
 import logging as slog
 from absl import logging as alog
 
 # Avoid certain absl logging messages to break doctest
 filtered_message = [
   'SaveArgs.aggregate is deprecated',
```

### Comparing `flax-0.8.2/docs/conf_sphinx_patch.py` & `flax-0.8.3/docs/conf_sphinx_patch.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/contributing.md` & `flax-0.8.3/docs/contributing.md`

 * *Files 1% similar despite different names*

```diff
@@ -103,14 +103,15 @@
    # or use `git add .` to add all changed files
    git commit -m "Your commit message"
    ```
 
    Then sync your code with the main repository:
 
    ```bash
+   git fetch upstream
    git rebase upstream/main
    ```
 
 10. Finally, push your commit on your `my_development_branch`, and create a remote
    branch in your fork that you can use to create a pull request from:
 
    ```bash
```

### Comparing `flax-0.8.2/docs/developer_notes/lift.md` & `flax-0.8.3/docs/developer_notes/lift.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/developer_notes/module_lifecycle.rst` & `flax-0.8.3/docs/developer_notes/module_lifecycle.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/examples/community_examples.rst` & `flax-0.8.3/docs/examples/community_examples.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/examples/core_examples.rst` & `flax-0.8.3/docs/examples/core_examples.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/examples/google_research_examples.rst` & `flax-0.8.3/docs/examples/google_research_examples.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/examples/repositories_that_use_flax.rst` & `flax-0.8.3/docs/examples/repositories_that_use_flax.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/experimental/nnx/index.rst` & `flax-0.8.3/docs/experimental/nnx/index.rst`

 * *Files 15% similar despite different names*

```diff
@@ -64,51 +64,62 @@
          :class-title: sd-fs-5
 
          .. div:: sd-font-normal
 
             NNX prioritizes simplicity for common use cases, building upon lessons learned from Linen
             to provide a streamlined experience.
 
-
-Installation
-^^^^^^^^^^^^
-NNX is under active development, we recommend using the latest version from Flax's GitHub repository:
-
-.. code-block:: bash
-
-   pip install git+https://github.com/google/flax.git
-
-
 Basic usage
 ^^^^^^^^^^^^
 
 .. testsetup::
 
    import jax
    import jax.numpy as jnp
 
 .. testcode::
 
    from flax.experimental import nnx
+   import optax
+
+
+   class Model(nnx.Module):
+     def __init__(self, din, dmid, dout, rngs: nnx.Rngs):
+       self.linear = nnx.Linear(din, dmid, rngs=rngs)
+       self.bn = nnx.BatchNorm(dmid, rngs=rngs)
+       self.dropout = nnx.Dropout(0.2, rngs=rngs)
+       self.linear_out = nnx.Linear(dmid, dout, rngs=rngs)
+
+     def __call__(self, x):
+       x = nnx.relu(self.dropout(self.bn(self.linear(x))))
+       return self.linear_out(x)
 
-   class Linear(nnx.Module):
-     def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):
-       key = rngs() # get a unique random key
-       self.w = nnx.Param(jax.random.uniform(key, (din, dout)))
-       self.b = nnx.Param(jnp.zeros((dout,))) # initialize parameters
-       self.din, self.dout = din, dout
+   model = Model(2, 64, 3, rngs=nnx.Rngs(0))  # eager initialization
+   optimizer = nnx.Optimizer(model, optax.adam(1e-3))  # reference sharing
 
-     def __call__(self, x: jax.Array):
-       return x @ self.w.value + self.b.value
+   @nnx.jit # automatic state management
+   def train_step(model, optimizer, x, y):
+     def loss_fn(model):
+       y_pred = model(x)  # call methods directly
+       return ((y_pred - y) ** 2).mean()
 
-   rngs = nnx.Rngs(0) # explicit RNG handling
-   model = Linear(din=2, dout=3, rngs=rngs) # initialize the model
+     loss, grads = nnx.value_and_grad(loss_fn)(model)
+     optimizer.update(grads)  # inplace updates
+
+     return loss
+
+
+Installation
+^^^^^^^^^^^^
+NNX is under active development, we recommend using the latest version from Flax's GitHub repository:
+
+.. code-block:: bash
+
+   pip install git+https://github.com/google/flax.git
 
-   x = jnp.empty((1, 2)) # generate random data
-   y = model(x) # forward pass
 
 ----
 
 Learn more
 ^^^^^^^^^^
 
 .. grid::
@@ -123,16 +134,31 @@
    .. grid-item::
       :columns: 6 6 6 4
 
       .. card:: :material-regular:`library_books;2em` MNIST Tutorial
          :class-card: sd-text-black sd-bg-light
          :link: mnist_tutorial.html
 
+   .. grid-item::
+      :columns: 6 6 6 4
+
+      .. card:: :material-regular:`sync_alt;2em` NNX vs JAX Transformations
+         :class-card: sd-text-black sd-bg-light
+         :link: transforms.html
+
+   .. grid-item::
+      :columns: 6 6 6 4
+
+      .. card:: :material-regular:`menu_book;2em` API reference
+         :class-card: sd-text-black sd-bg-light
+         :link: ../../api_reference/index.html
+
 
 ----
 
 .. toctree::
    :hidden:
    :maxdepth: 1
 
    nnx_basics
-   mnist_tutorial
+   mnist_tutorial
+   transforms
```

### Comparing `flax-0.8.2/docs/experimental/nnx/mnist_tutorial.md` & `flax-0.8.3/docs/experimental/nnx/mnist_tutorial.md`

 * *Files 18% similar despite different names*

```diff
@@ -24,297 +24,264 @@
 ## 1. Install NNX
 
 Since NNX is under active development, we recommend using the latest version from the Flax GitHub repository:
 
 ```{code-cell} ipython3
 :tags: [skip-execution]
 
-!pip install git+https://github.com/google/flax.git
+# !pip install git+https://github.com/google/flax.git
 ```
 
 ## 2. Load the MNIST Dataset
 
-We'll use TensorFlow Datasets (TFDS) for loading and preparing the MNIST dataset:
+First, the MNIST dataset is loaded and prepared for training and testing using 
+Tensorflow Datasets. Image values are normalized, the data is shuffled and divided 
+into batches, and samples are prefetched to enhance performance.
 
 ```{code-cell} ipython3
 import tensorflow_datasets as tfds  # TFDS for MNIST
-import tensorflow as tf             # TensorFlow operations
+import tensorflow as tf  # TensorFlow operations
 
-tf.random.set_seed(0) # set random seed for reproducibility
+tf.random.set_seed(0)  # set random seed for reproducibility
 
 num_epochs = 10
 batch_size = 32
 
 train_ds: tf.data.Dataset = tfds.load('mnist', split='train')
 test_ds: tf.data.Dataset = tfds.load('mnist', split='test')
 
-train_ds = train_ds.map(lambda sample: {
-  'image': tf.cast(sample['image'],tf.float32) / 255,
-  'label': sample['label']}) # normalize train set
-test_ds = test_ds.map(lambda sample: {
-  'image': tf.cast(sample['image'], tf.float32) / 255,
-  'label': sample['label']}) # normalize test set
-
-train_ds = train_ds.repeat(num_epochs).shuffle(1024) # create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from
-train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(1) # group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency
-test_ds = test_ds.shuffle(1024) # create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from
-test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1) # group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency
+train_ds = train_ds.map(
+  lambda sample: {
+    'image': tf.cast(sample['image'], tf.float32) / 255,
+    'label': sample['label'],
+  }
+)  # normalize train set
+test_ds = test_ds.map(
+  lambda sample: {
+    'image': tf.cast(sample['image'], tf.float32) / 255,
+    'label': sample['label'],
+  }
+)  # normalize test set
+
+# create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from
+train_ds = train_ds.repeat(num_epochs).shuffle(1024)
+# group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency
+train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(1)
+# create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from
+test_ds = test_ds.shuffle(1024)
+# group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency
+test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)
 ```
 
 ## 3. Define the Network with NNX
 
 Create a convolutional neural network with NNX by subclassing `nnx.Module`.
 
 ```{code-cell} ipython3
 from flax.experimental import nnx  # NNX API
+from functools import partial
 
 class CNN(nnx.Module):
   """A simple CNN model."""
 
   def __init__(self, *, rngs: nnx.Rngs):
-    self.conv1 = nnx.Conv(in_features=1, out_features=32, kernel_size=(3, 3), rngs=rngs)
-    self.conv2 = nnx.Conv(in_features=32, out_features=64, kernel_size=(3, 3), rngs=rngs)
-    self.linear1 = nnx.Linear(in_features=3136, out_features=256, rngs=rngs)
-    self.linear2 = nnx.Linear(in_features=256, out_features=10, rngs=rngs)
+    self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), rngs=rngs)
+    self.conv2 = nnx.Conv(32, 64, kernel_size=(3, 3), rngs=rngs)
+    self.avg_pool = partial(nnx.avg_pool, window_shape=(2, 2), strides=(2, 2))
+    self.linear1 = nnx.Linear(3136, 256, rngs=rngs)
+    self.linear2 = nnx.Linear(256, 10, rngs=rngs)
 
   def __call__(self, x):
-    x = self.conv1(x)
-    x = nnx.relu(x)
-    x = nnx.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
-    x = self.conv2(x)
-    x = nnx.relu(x)
-    x = nnx.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
-    x = x.reshape((x.shape[0], -1))  # flatten
-    x = self.linear1(x)
-    x = nnx.relu(x)
+    x = self.avg_pool(nnx.relu(self.conv1(x)))
+    x = self.avg_pool(nnx.relu(self.conv2(x)))
+    x = x.reshape(x.shape[0], -1)  # flatten
+    x = nnx.relu(self.linear1(x))
     x = self.linear2(x)
     return x
-  
-model = CNN(rngs=nnx.Rngs(0))
 
-print(f'model = {model}'[:500] + '\n...\n')  # print a part of the model
-print(f'{model.conv1.kernel.shape = }') # inspect the shape of the kernel of the first convolutional layer
+model = CNN(rngs=nnx.Rngs(0))
+nnx.display(model)
 ```
 
 ### Run model
 
 Let's put our model to the test!  We'll perform a forward pass with arbitrary data and print the results.
 
 ```{code-cell} ipython3
 :outputId: 2c580f41-bf5d-40ec-f1cf-ab7f319a84da
 
-import jax
 import jax.numpy as jnp  # JAX NumPy
 
 y = model(jnp.ones((1, 28, 28, 1)))
-y
-```
-
-## 4. Define Metrics
-
-To track our model's performance, we'll use the [clu](https://github.com/google/CommonLoopUtils) library. If you haven't already, install it with:
-
-```{code-cell} ipython3
-!pip install -q clu
-```
-
-Let's create a compound metric using clu.metrics.Collection.  This will include both an Accuracy metric for tracking how well our model classifies images, and an Average metric to monitor the average loss over each training epoch.
-
-```{code-cell} ipython3
-from clu import metrics
-from flax import struct   # Flax pytree dataclasses
-
-@struct.dataclass
-class Metrics(metrics.Collection):
-  accuracy: metrics.Accuracy
-  loss: metrics.Average.from_output('loss')
+nnx.display(y)
 ```
 
-## 5. Create the `TrainState`
+## 4. Create Optimizer and Metrics
 
-In Flax, a common practice is to use a dataclass to encapsulate the training state, including the step number, parameters, and optimizer state. The [`flax.training.train_state.TrainState`](https://flax.readthedocs.io/en/latest/flax.training.html#train-state) class is ideal for basic use cases, simplifying the process by allowing you to pass a single argument to functions like `train_step`.
+In NNX, we create an `Optimizer` object to manage the model's parameters and apply gradients during training. `Optimizer` receives the model parameters and an `optax` optimizer that will define the update rules. Additionally, we'll define a `MultiMetric` object to keep track of the `Accuracy` and the `Average` loss.
 
 ```{code-cell} ipython3
-from flax.training import train_state  # Useful dataclass to keep train state
-import optax  
-
-params, static = model.split(nnx.Param)
-
-class TrainState(train_state.TrainState):
-  static: nnx.GraphDef[CNN]
-  metrics: Metrics
+import optax
 
 learning_rate = 0.005
 momentum = 0.9
 
-tx = optax.adamw(learning_rate, momentum)
-state = TrainState.create(
-  apply_fn=None, params=params, tx=tx,
-  static=static, metrics=Metrics.empty()
+optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum))
+metrics = nnx.MultiMetric(
+  accuracy=nnx.metrics.Accuracy(), 
+  loss=nnx.metrics.Average('loss'),
 )
+
+nnx.display(optimizer)
 ```
 
-Since `TrainState` is a JAX pytree, `Module.split` splits the model into `State` and `GraphDef` pytree objects (representing parameters and the graph definition). A custom `TrainState` type holds the static `GraphDef` and metrics.  We use `optax` to create an optimizer (`adamw`) and initialize the `TrainState`.
+## 5. Training step
 
-+++
+We define a loss function using cross entropy loss (see more details in [`optax.softmax_cross_entropy_with_integer_labels()`](https://optax.readthedocs.io/en/latest/api/losses.html#optax.softmax_cross_entropy_with_integer_labels)) that our model will optimize over. In addition to the loss, the logits are also outputted since they will be used to calculate the accuracy metric during training and testing.
 
-## 6. Training step
+```{code-cell} ipython3
+def loss_fn(model: CNN, batch):
+  logits = model(batch['image'])
+  loss = optax.softmax_cross_entropy_with_integer_labels(
+    logits=logits, labels=batch['label']
+  ).mean()
+  return loss, logits
+```
 
-This function takes the `state` and a data `batch` and does the following:
+Next, we create the training step function. This function takes the `model` and a data `batch` and does the following:
 
-* Reconstructs the model with `static.merge` on the `params`.
-* Runs the neural network on the input image batch.
-* Calculates cross-entropy loss using 
-  [optax.softmax_cross_entropy_with_integer_labels()](https://optax.readthedocs.io/en/latest/api.html#optax.softmax_cross_entropy_with_integer_labels). Integer labels eliminate the need for one-hot encoding.
-* Computes the loss function's gradient with `jax.grad`.
-* Updates model parameters by applying the gradient pytree to the optimizer.
+* Computes the loss, logits and gradients with respect to the loss function using `nnx.value_and_grad`.
+* Updates training accuracy using the loss, logits, and batch labels.
+* Updates model parameters via the optimizer by applying the gradient updates.
 
 ```{code-cell} ipython3
-@jax.jit
-def train_step(state: TrainState, batch):
+@nnx.jit
+def train_step(model: CNN, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):
   """Train for a single step."""
-  def loss_fn(params):
-    model = state.static.merge(params)
-    logits = model(batch['image'])
-    loss = optax.softmax_cross_entropy_with_integer_labels(
-        logits=logits, labels=batch['label']).mean()
-    return loss
-  grad_fn = jax.grad(loss_fn)
-  grads = grad_fn(state.params)
-  state = state.apply_gradients(grads=grads)
-  return state
+  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)
+  (loss, logits), grads = grad_fn(model, batch)
+  metrics.update(loss=loss, logits=logits, labels=batch['label'])
+  optimizer.update(grads)
 ```
 
-The [@jax.jit](https://jax.readthedocs.io/en/latest/jax.html#jax.jit) decorator
-traces the `train_step` function for just-in-time compilation with 
+The [`nnx.jit`](https://flax.readthedocs.io/en/latest/api_reference/flax.experimental.nnx/transforms.html#flax.experimental.nnx.jit) decorator traces the `train_step` function for just-in-time compilation with 
 [XLA](https://www.tensorflow.org/xla), optimizing performance on 
-hardware accelerators.
+hardware accelerators. `nnx.jit` is similar to [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit),
+except it can transforms functions that contain NNX objects as inputs and outputs.
 
-## 7. Metric Computation
+## 6. Evaluation step
 
-Create a separate function to calculate loss and accuracy metrics. Loss is determined using the `optax.softmax_cross_entropy_with_integer_labels` function, and accuracy is computed using `clu.metrics`.
+Create a separate function to calculate loss and accuracy metrics for the test batch, since this will be outside the `train_step` function. Loss is determined using the `optax.softmax_cross_entropy_with_integer_labels` function, since we're reusing the loss function defined earlier.
 
 ```{code-cell} ipython3
-@jax.jit
-def compute_metrics(*, state: TrainState, batch):
-  model = state.static.merge(state.params)
-  logits = model(batch['image'])
-  loss = optax.softmax_cross_entropy_with_integer_labels(
-        logits=logits, labels=batch['label']).mean()
-  metric_updates = state.metrics.single_from_model_output(
-    logits=logits, labels=batch['label'], loss=loss)
-  metrics = state.metrics.merge(metric_updates)
-  state = state.replace(metrics=metrics)
-  return state
+@nnx.jit
+def eval_step(model: CNN, metrics: nnx.MultiMetric, batch):
+  loss, logits = loss_fn(model, batch)
+  metrics.update(loss=loss, logits=logits, labels=batch['label'])
 ```
 
-## 9. Seed randomness
+## 7. Seed randomness
 
 For reproducible dataset shuffling (using `tf.data.Dataset.shuffle`), set the TF random seed.
 
 ```{code-cell} ipython3
 tf.random.set_seed(0)
 ```
 
-## 10. Train and Evaluate
+## 8. Train and Evaluate
 
-**Dataset Preparation:** create a "shuffled" dataset
-- Repeat the dataset for the desired number of training epochs.
-- Establish a 1024-sample buffer (holding the dataset's initial 1024 samples).
-  Randomly draw batches from this buffer.
-- As samples are drawn, replenish the buffer with subsequent dataset samples.
-
-**Training Loop:** Iterate through epochs
-- Sample batches randomly from the dataset.
-- Execute an optimization step for each training batch.
-- Calculate mean training metrics across batches within the epoch.
-- With updated parameters, compute metrics on the test set.
-- Log train and test metrics for visualization.
-
-After 10 training and testing epochs, your model should reach approximately 99% accuracy.
+Now we train a model using batches of data for 10 epochs, evaluate its performance 
+on the test set after each epoch, and log the training and testing metrics (loss and
+accuracy) throughout the process. Typically this leads to a model with around 99% accuracy.
 
 ```{code-cell} ipython3
 :outputId: 258a2c76-2c8f-4a9e-d48b-dde57c342a87
 
 num_steps_per_epoch = train_ds.cardinality().numpy() // num_epochs
 
 metrics_history = {
   'train_loss': [],
   'train_accuracy': [],
   'test_loss': [],
-  'test_accuracy': []
+  'test_accuracy': [],
 }
 
-for step,batch in enumerate(train_ds.as_numpy_iterator()):
-  # Run optimization steps over training batches and compute batch metrics
-  state = train_step(state, batch) # get updated train state (which contains the updated parameters)
-  state = compute_metrics(state=state, batch=batch) # aggregate batch metrics
-
-  if (step+1) % num_steps_per_epoch == 0: # one training epoch has passed
-    for metric,value in state.metrics.compute().items(): # compute metrics
-      metrics_history[f'train_{metric}'].append(value) # record metrics
-    state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch
+for step, batch in enumerate(train_ds.as_numpy_iterator()):
+  # Run the optimization for one step and make a stateful update to the following:
+  # - the train state's model parameters
+  # - the optimizer state
+  # - the training loss and accuracy batch metrics
+  train_step(model, optimizer, metrics, batch)
+
+  if (step + 1) % num_steps_per_epoch == 0:  # one training epoch has passed
+    # Log training metrics
+    for metric, value in metrics.compute().items():  # compute metrics
+      metrics_history[f'train_{metric}'].append(value)  # record metrics
+    metrics.reset()  # reset metrics for test set
 
     # Compute metrics on the test set after each training epoch
-    test_state = state
     for test_batch in test_ds.as_numpy_iterator():
-      test_state = compute_metrics(state=test_state, batch=test_batch)
+      eval_step(model, metrics, test_batch)
 
-    for metric,value in test_state.metrics.compute().items():
+    # Log test metrics
+    for metric, value in metrics.compute().items():
       metrics_history[f'test_{metric}'].append(value)
+    metrics.reset()  # reset metrics for next training epoch
 
-    print(f"train epoch: {(step+1) // num_steps_per_epoch}, "
-          f"loss: {metrics_history['train_loss'][-1]}, "
-          f"accuracy: {metrics_history['train_accuracy'][-1] * 100}")
-    print(f"test epoch: {(step+1) // num_steps_per_epoch}, "
-          f"loss: {metrics_history['test_loss'][-1]}, "
-          f"accuracy: {metrics_history['test_accuracy'][-1] * 100}")
+    print(
+      f"train epoch: {(step+1) // num_steps_per_epoch}, "
+      f"loss: {metrics_history['train_loss'][-1]}, "
+      f"accuracy: {metrics_history['train_accuracy'][-1] * 100}"
+    )
+    print(
+      f"test epoch: {(step+1) // num_steps_per_epoch}, "
+      f"loss: {metrics_history['test_loss'][-1]}, "
+      f"accuracy: {metrics_history['test_accuracy'][-1] * 100}"
+    )
 ```
 
-## 11. Visualize Metrics
+## 9. Visualize Metrics
 
 Use Matplotlib to create plots for loss and accuracy.
 
 ```{code-cell} ipython3
 :outputId: 431a2fcd-44fa-4202-f55a-906555f060ac
 
 import matplotlib.pyplot as plt  # Visualization
 
 # Plot loss and accuracy in subplots
 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
 ax1.set_title('Loss')
 ax2.set_title('Accuracy')
-for dataset in ('train','test'):
+for dataset in ('train', 'test'):
   ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')
   ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')
 ax1.legend()
 ax2.legend()
 plt.show()
-plt.clf()
 ```
 
-## 12. Perform inference on test set
+## 10. Perform inference on test set
 
 Define a jitted inference function, `pred_step`, to generate predictions on the test set using the learned model parameters. This will enable you to visualize test images alongside their predicted labels for a qualitative assessment of model performance.
 
 ```{code-cell} ipython3
-@jax.jit
-def pred_step(state: TrainState, batch):
-  model = state.static.merge(state.params)
-  logits = model(test_batch['image'])
+@nnx.jit
+def pred_step(model: CNN, batch):
+  logits = model(batch['image'])
   return logits.argmax(axis=1)
 ```
 
 ```{code-cell} ipython3
 :outputId: 1db5a01c-9d70-4f7d-8c0d-0a3ad8252d3e
 
 test_batch = test_ds.as_numpy_iterator().next()
-pred = pred_step(state, test_batch)
+pred = pred_step(model, test_batch)
 
 fig, axs = plt.subplots(5, 5, figsize=(12, 12))
 for i, ax in enumerate(axs.flatten()):
-    ax.imshow(test_batch['image'][i, ..., 0], cmap='gray')
-    ax.set_title(f"label={pred[i]}")
-    ax.axis('off')
+  ax.imshow(test_batch['image'][i, ..., 0], cmap='gray')
+  ax.set_title(f'label={pred[i]}')
+  ax.axis('off')
 ```
 
 Congratulations! You made it to the end of the annotated MNIST example.
```

### Comparing `flax-0.8.2/docs/experimental/nnx/nnx_basics.ipynb` & `flax-0.8.3/flax/experimental/nnx/docs/tiny_nnx.ipynb`

 * *Files 27% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.8396685346448531%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(0, '# Tiny NNX\\n'), (1, '[![Open In "*

 * *            "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cgarciae/nnx/blob/main/docs/tiny_nnx.ipynb)\\n'), "*

 * *            '(3, "A pedagogical implementation of NNX\'s core APIs.\\n"), (4, \'\\n\'), (5, \'## '*

 * *            "Core APIs')], delete: [6, 5, 4, 3, 2, 0]}, 'attachments': OrderedDict()}, 1: "*

 * *            "{'source': {insert: [(0, 'import dataclasses\\n'), (1, 'im […]*

```diff
@@ -1,580 +1,466 @@
 {
     "cells": [
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "# NNX Basics\n",
+                "# Tiny NNX\n",
+                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cgarciae/nnx/blob/main/docs/tiny_nnx.ipynb)\n",
                 "\n",
-                "NNX is a **N**eural **N**etworks JA**X** library that embraces Python's object-oriented programming \n",
-                "model to provide an intuitive and highly simplified user experience. It aims to empower users\n",
-                "by making Modules very easy to integrate with any JAX API, it achieves this through a very small set\n",
-                "of primitives known as the [Functional API](#the-functional-api). NNX is specifically designed to support \n",
-                "all the patterns that allowed Linen to scale to large code bases building upon a much simpler foundation."
+                "A pedagogical implementation of NNX's core APIs.\n",
+                "\n",
+                "## Core APIs"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [],
             "source": [
-                "from flax.experimental import nnx\n",
+                "import dataclasses\n",
+                "import hashlib\n",
+                "import typing as tp\n",
+                "\n",
                 "import jax\n",
-                "import jax.numpy as jnp"
+                "import jax.numpy as jnp\n",
+                "from jax import random\n",
+                "\n",
+                "A = tp.TypeVar(\"A\")\n",
+                "M = tp.TypeVar(\"M\", bound=\"Module\")\n",
+                "Sharding = tp.Tuple[tp.Optional[str], ...]\n",
+                "Array = jax.Array\n",
+                "\n",
+                "\n",
+                "class Variable(tp.Generic[A]):\n",
+                "\n",
+                "  def __init__(\n",
+                "      self,\n",
+                "      value: A,\n",
+                "      *,\n",
+                "      sharding: tp.Optional[Sharding] = None,\n",
+                "  ):\n",
+                "    self.value = value\n",
+                "    self.sharding = sharding\n",
+                "\n",
+                "  def __repr__(self) -> str:\n",
+                "    return (\n",
+                "        f\"{type(self).__name__}(value={self.value}, sharding={self.sharding})\"\n",
+                "    )\n",
+                "\n",
+                "  def __init_subclass__(cls):\n",
+                "    super().__init_subclass__()\n",
+                "    jax.tree_util.register_pytree_node(\n",
+                "        cls,\n",
+                "        lambda x: ((x.value,), (x.sharding,)),\n",
+                "        lambda metadata, value: cls(value[0], sharding=metadata[0]),\n",
+                "    )\n",
+                "\n",
+                "\n",
+                "class State(dict[str, Variable[tp.Any]]):\n",
+                "\n",
+                "  def extract(self, variable_type: tp.Type[Variable]) -> \"State\":\n",
+                "    return State(\n",
+                "        {\n",
+                "            path: variable\n",
+                "            for path, variable in self.items()\n",
+                "            if isinstance(variable, variable_type)\n",
+                "        }\n",
+                "    )\n",
+                "\n",
+                "  def __repr__(self) -> str:\n",
+                "    elems = \",\\n  \".join(\n",
+                "        f\"'{path}': {variable}\".replace(\"\\n\", \"\\n    \")\n",
+                "        for path, variable in self.items()\n",
+                "    )\n",
+                "    return f\"State({{\\n  {elems}\\n}})\"\n",
+                "\n",
+                "\n",
+                "jax.tree_util.register_pytree_node(\n",
+                "    State,\n",
+                "    # in reality, values and paths should be sorted by path\n",
+                "    lambda x: (tuple(x.values()), tuple(x.keys())),\n",
+                "    lambda paths, values: State(dict(zip(paths, values))),\n",
+                ")\n",
+                "\n",
+                "\n",
+                "@dataclasses.dataclass\n",
+                "class GraphDef(tp.Generic[M]):\n",
+                "  type: tp.Type[M]\n",
+                "  index: int\n",
+                "  submodules: dict[str, tp.Union[\"GraphDef[Module]\", int]]\n",
+                "  static_fields: dict[str, tp.Any]\n",
+                "\n",
+                "  def merge(self, state: State) -> M:\n",
+                "    module = GraphDef._build_module_recursive(self, {})\n",
+                "    module.update(state)\n",
+                "    return module\n",
+                "\n",
+                "  @staticmethod\n",
+                "  def _build_module_recursive(\n",
+                "      graphdef: tp.Union[\"GraphDef[M]\", int],\n",
+                "      index_to_module: dict[int, \"Module\"],\n",
+                "  ) -> M:\n",
+                "    if isinstance(graphdef, int):\n",
+                "      return index_to_module[graphdef] # type: ignore\n",
+                "\n",
+                "    assert graphdef.index not in index_to_module\n",
+                "\n",
+                "    # add a dummy module to the index to avoid infinite recursion\n",
+                "    module = object.__new__(graphdef.type)\n",
+                "    index_to_module[graphdef.index] = module\n",
+                "\n",
+                "    submodules = {\n",
+                "        name: GraphDef._build_module_recursive(submodule, index_to_module)\n",
+                "        for name, submodule in graphdef.submodules.items()\n",
+                "    }\n",
+                "    vars(module).update(graphdef.static_fields)\n",
+                "    vars(module).update(submodules)\n",
+                "    return module\n",
+                "\n",
+                "  def apply(\n",
+                "      self, state: State\n",
+                "  ) -> tp.Callable[..., tuple[tp.Any, tuple[State, \"GraphDef[M]\"]]]:\n",
+                "    def _apply(*args, **kwargs):\n",
+                "      module = self.merge(state)\n",
+                "      out = module(*args, **kwargs)  # type: ignore\n",
+                "      return out, module.split()\n",
+                "\n",
+                "    return _apply\n",
+                "\n",
+                "\n",
+                "class Module:\n",
+                "\n",
+                "  def split(self: M) -> tp.Tuple[State, GraphDef[M]]:\n",
+                "    state = State()\n",
+                "    graphdef = Module._partition_recursive(\n",
+                "        module=self, module_id_to_index={}, path_parts=(), state=state\n",
+                "    )\n",
+                "    assert isinstance(graphdef, GraphDef)\n",
+                "    return state, graphdef\n",
+                "\n",
+                "  @staticmethod\n",
+                "  def _partition_recursive(\n",
+                "      module: M,\n",
+                "      module_id_to_index: dict[int, int],\n",
+                "      path_parts: tp.Tuple[str, ...],\n",
+                "      state: State,\n",
+                "  ) -> tp.Union[GraphDef[M], int]:\n",
+                "    if id(module) in module_id_to_index:\n",
+                "      return module_id_to_index[id(module)]\n",
+                "\n",
+                "    index = len(module_id_to_index)\n",
+                "    module_id_to_index[id(module)] = index\n",
+                "\n",
+                "    submodules = {}\n",
+                "    static_fields = {}\n",
+                "\n",
+                "    # iterate fields sorted by name to ensure deterministic order\n",
+                "    for name, value in sorted(vars(module).items(), key=lambda x: x[0]):\n",
+                "      value_path = (*path_parts, name)\n",
+                "      # if value is a Module, recurse\n",
+                "      if isinstance(value, Module):\n",
+                "        submoduledef = Module._partition_recursive(\n",
+                "            value, module_id_to_index, value_path, state\n",
+                "        )\n",
+                "        submodules[name] = submoduledef\n",
+                "      # if value is a Variable, add to state\n",
+                "      elif isinstance(value, Variable):\n",
+                "        state[\"/\".join(value_path)] = value\n",
+                "      else:  # otherwise, add to graphdef fields\n",
+                "        static_fields[name] = value\n",
+                "\n",
+                "    return GraphDef(\n",
+                "        type=type(module),\n",
+                "        index=index,\n",
+                "        submodules=submodules,\n",
+                "        static_fields=static_fields,\n",
+                "    )\n",
+                "\n",
+                "  def update(self, state: State) -> None:\n",
+                "    for path, value in state.items():\n",
+                "      path_parts = path.split(\"/\")\n",
+                "      Module._set_value_at_path(self, path_parts, value)\n",
+                "\n",
+                "  @staticmethod\n",
+                "  def _set_value_at_path(\n",
+                "      module: \"Module\", path_parts: tp.Sequence[str], value: Variable[tp.Any]\n",
+                "  ) -> None:\n",
+                "    if len(path_parts) == 1:\n",
+                "      setattr(module, path_parts[0], value)\n",
+                "    else:\n",
+                "      Module._set_value_at_path(\n",
+                "          getattr(module, path_parts[0]), path_parts[1:], value\n",
+                "      )\n",
+                "\n",
+                "\n",
+                "@dataclasses.dataclass\n",
+                "class Rngs:\n",
+                "  key: jax.Array\n",
+                "  count: int = 0\n",
+                "  count_path: tuple[int, ...] = ()\n",
+                "\n",
+                "  def fork(self) -> \"Rngs\":\n",
+                "    \"\"\"Forks the context, guaranteeing that all the random numbers generated\n",
+                "    will be different from the ones generated in the original context. Fork is\n",
+                "    used to create a new Rngs that can be passed to a JAX transform\"\"\"\n",
+                "    count_path = self.count_path + (self.count,)\n",
+                "    self.count += 1\n",
+                "    return Rngs(self.key, count_path=count_path)\n",
+                "\n",
+                "  def make_rng(self) -> jax.Array:\n",
+                "    fold_data = self._stable_hash(self.count_path + (self.count,))\n",
+                "    self.count += 1\n",
+                "    return random.fold_in(self.key, fold_data)  # type: ignore\n",
+                "\n",
+                "  @staticmethod\n",
+                "  def _stable_hash(data: tuple[int, ...]) -> int:\n",
+                "    hash_str = \" \".join(str(x) for x in data)\n",
+                "    _hash = hashlib.blake2s(hash_str.encode())\n",
+                "    hash_bytes = _hash.digest()\n",
+                "    # uint32 is represented as 4 bytes in big endian\n",
+                "    return int.from_bytes(hash_bytes[:4], byteorder=\"big\")\n",
+                "\n",
+                "\n",
+                "# in the real NNX Rngs is not a pytree, instead\n",
+                "# it has a split/merge API similar to Module\n",
+                "# but for simplicity we use a pytree here\n",
+                "jax.tree_util.register_pytree_node(\n",
+                "    Rngs,\n",
+                "    lambda x: ((x.key,), (x.count, x.count_path)),\n",
+                "    lambda metadata, value: Rngs(value[0], *metadata),\n",
+                ")"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## The Module System\n",
-                "To begin lets see how to create a `Linear` Module using NNX. The main noticeable\n",
-                "different between Module systems like Haiku or Linen and NNX is that in NNX everything is\n",
-                "**explicit**. This means amongst other things that 1) the Module itself holds the state \n",
-                "(e.g. parameters) directly, 2) the RNG state is threaded by the user, and 3) all shape information\n",
-                "must be provided on initialization (no shape inference)."
+                "## Basic Layers"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
             "metadata": {},
             "outputs": [],
             "source": [
-                "class Linear(nnx.Module):\n",
-                "  def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n",
-                "    key = rngs.params()\n",
-                "    self.w = nnx.Param(jax.random.uniform(key, (din, dout)))\n",
-                "    self.b = nnx.Param(jnp.zeros((dout,)))\n",
-                "    self.din, self.dout = din, dout\n",
+                "class Param(Variable[A]):\n",
+                "  pass\n",
                 "\n",
-                "  def __call__(self, x: jax.Array):\n",
-                "    return x @ self.w.value + self.b.value"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "As shown above dynamic state is stored in `nnx.Variable`s such as `nnx.Param`,\n",
-                "and static state (all types not handled by NNX) such as integers or strings \n",
-                "are stored directly. RNG keys can be requested from the `nnx.Rngs` object\n",
-                "by calling `rngs.<stream_name>()` where the stream name show match on of the names provided to the `Rngs` constructor (shown below).\n",
-                "\n",
-                "To actually initialize a Module is very easy: simply call the constructor. All of the\n",
-                "parameters of a Module will be created right then and there, and are immediately available\n",
-                "for inspection."
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 3,
-            "metadata": {},
-            "outputs": [
-                {
-                    "name": "stdout",
-                    "output_type": "stream",
-                    "text": [
-                        "model = Linear(\n",
-                        "  din=2,\n",
-                        "  dout=3\n",
-                        ")\n",
-                        "model.w.value = Array([[0.19007349, 0.31424356, 0.3686391 ],\n",
-                        "       [0.7862853 , 0.03352201, 0.50682676]], dtype=float32)\n",
-                        "model.b.value = Array([0., 0., 0.], dtype=float32)\n"
-                    ]
-                }
-            ],
-            "source": [
-                "model = Linear(din=2, dout=3, rngs=nnx.Rngs(params=0))\n",
                 "\n",
-                "print(f'{model = }')\n",
-                "print(f'{model.w.value = }')\n",
-                "print(f'{model.b.value = }')"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "This is very handy for debugging as it allows accessing the entire structure or\n",
-                "modify it. Similarly, computation can be ran directly."
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 4,
-            "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "Array([[0.9763588 , 0.34776556, 0.87546587]], dtype=float32)"
-                        ]
-                    },
-                    "execution_count": 4,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
-            "source": [
-                "x = jnp.ones((1, 2))\n",
+                "class BatchStat(Variable[A]):\n",
+                "  pass\n",
                 "\n",
-                "model(x)"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "Since Modules hold their own state there is no need for a separate `apply` method."
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "### Stateful Computation\n",
                 "\n",
-                "When implementing layers like Batch Normalization or Multi Head Attention with \n",
-                "autoregressive decoding you often need to store and update state inside a Module \n",
-                "during the forward pass. The way to do this in NNX is simply to store the state \n",
-                "inside a `Variable` and update it in-place when need it."
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 8,
-            "metadata": {},
-            "outputs": [
-                {
-                    "name": "stdout",
-                    "output_type": "stream",
-                    "text": [
-                        "counter.count.value = 0\n",
-                        "counter.count.value = 1\n"
-                    ]
-                }
-            ],
-            "source": [
-                "class Counter(nnx.Module):\n",
-                "  def __init__(self):\n",
-                "    self.count = nnx.Variable(0)\n",
-                "\n",
-                "  def __call__(self):\n",
-                "    self.count.value += 1\n",
-                "\n",
-                "counter = Counter()\n",
-                "print(f'{counter.count.value = }')\n",
-                "counter()\n",
-                "print(f'{counter.count.value = }')"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "**This looks too easy, what is the catch?** \n",
-                "JAX frameworks have avoided mutable references until now. The key innovations which \n",
-                "allows their usage in NNX is that 1) there is a clear boundary between reference \n",
-                "semantics and value semantics, defined by [The Functional API](#the-functional-api),\n",
-                "and 2) there are guards in place to avoid updating NNX objects from a `MainTrace`, \n",
-                "thus preventing tracer leakage."
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "### Nested Modules\n",
+                "class Linear(Module):\n",
                 "\n",
-                "As expected, Modules can used to compose other Modules in a nested\n",
-                "structure, this includes standard Modules such as `nnx.Linear`,\n",
-                "`nnx.Conv`, etc, or any custom Module created by users. Modules can \n",
-                "be assigned as attributes of a Module, but shown by `MLP.blocks` in the\n",
-                "example below, they can also be stored in attributes of type `list`, `dict`, `tuple`, \n",
-                "or nested structues of the previous."
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 5,
-            "metadata": {},
-            "outputs": [
-                {
-                    "name": "stdout",
-                    "output_type": "stream",
-                    "text": [
-                        "model = MLP(\n",
-                        "  blocks=[Block(\n",
-                        "      linear=Linear(\n",
-                        "            in_features=2,\n",
-                        "            out_features=2,\n",
-                        "            use_bias=True,\n",
-                        "            dtype=None,\n",
-                        "            param_dtype=<class 'jax.numpy.float32'>,\n",
-                        "            precision=None,\n",
-                        "            kernel_init=<function variance_scaling.<locals>.init at 0x169773f70>,\n",
-                        "            bias_init=<function zeros at 0x1353b8ca0>,\n",
-                        "            dot_general=<function dot_general at 0x126dc5700>\n",
-                        "          ),\n",
-                        "      bn=BatchNorm(\n",
-                        "            num_features=2,\n",
-                        "  ...\n"
-                    ]
-                }
-            ],
-            "source": [
-                "class Block(nnx.Module):\n",
-                "  def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
-                "    self.linear = nnx.Linear(dim, dim, rngs=rngs)\n",
-                "    self.bn = nnx.BatchNorm(dim, use_running_average=True, rngs=rngs)\n",
-                "\n",
-                "  def __call__(self, x: jax.Array):\n",
-                "    return nnx.relu(self.bn(self.linear(x)))\n",
-                "  \n",
-                "class MLP(nnx.Module):\n",
-                "  def __init__(self, num_layers: int, dim: int, *, rngs: nnx.Rngs):\n",
-                "    self.blocks = [Block(dim, rngs=rngs) for _ in range(num_layers)]\n",
-                "  \n",
-                "  def __call__(self, x: jax.Array):\n",
-                "    for block in self.blocks:\n",
-                "      x = block(x)\n",
+                "  def __init__(self, din: int, dout: int, *, rngs: Rngs):\n",
+                "    self.din = din\n",
+                "    self.dout = dout\n",
+                "    key = rngs.make_rng()\n",
+                "    self.w = Param(random.uniform(key, (din, dout)))\n",
+                "    self.b = Param(jnp.zeros((dout,)))\n",
+                "\n",
+                "  def __call__(self, x: jax.Array) -> jax.Array:\n",
+                "    return x @ self.w.value + self.b.value\n",
+                "\n",
+                "\n",
+                "class BatchNorm(Module):\n",
+                "\n",
+                "  def __init__(self, din: int, mu: float = 0.95):\n",
+                "    self.mu = mu\n",
+                "    self.scale = Param(jax.numpy.ones((din,)))\n",
+                "    self.bias = Param(jax.numpy.zeros((din,)))\n",
+                "    self.mean = BatchStat(jax.numpy.zeros((din,)))\n",
+                "    self.var = BatchStat(jax.numpy.ones((din,)))\n",
+                "\n",
+                "  def __call__(self, x, train: bool) -> jax.Array:\n",
+                "    if train:\n",
+                "      axis = tuple(range(x.ndim - 1))\n",
+                "      mean = jax.numpy.mean(x, axis=axis)\n",
+                "      var = jax.numpy.var(x, axis=axis)\n",
+                "      # ema update\n",
+                "      self.mean.value = self.mu * self.mean.value + (1 - self.mu) * mean\n",
+                "      self.var.value = self.mu * self.var.value + (1 - self.mu) * var\n",
+                "    else:\n",
+                "      mean, var = self.mean.value, self.var.value\n",
+                "\n",
+                "    scale, bias = self.scale.value, self.bias.value\n",
+                "    x = (x - mean) / jax.numpy.sqrt(var + 1e-5) * scale + bias\n",
                 "    return x\n",
-                "  \n",
-                "model = MLP(num_layers=5, dim=2, rngs=nnx.Rngs(0))\n",
-                "print(f'{model = }'[:500] + '...')"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "One of the benefits of NNX is that nested Modules as easy to inspect and\n",
-                "static analyzers can help you while doing so."
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 6,
-            "metadata": {},
-            "outputs": [
-                {
-                    "name": "stdout",
-                    "output_type": "stream",
-                    "text": [
-                        "model.blocks[1].linear.kernel.value = Array([[0.992858 , 0.9711272],\n",
-                        "       [1.4061186, 0.4704619]], dtype=float32)\n",
-                        "model.blocks[0].bn.scale.value = Array([1., 1.], dtype=float32)\n"
-                    ]
-                }
-            ],
-            "source": [
-                "print(f'{model.blocks[1].linear.kernel.value = }')\n",
-                "print(f'{model.blocks[0].bn.scale.value = }')"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "#### Model Surgery\n",
-                "NNX Modules are mutable by default, this means their structure can be changed\n",
-                "at any time. Also, NNX's Module system supports reference sharing of Modules and\n",
-                "Variables.\n",
-                "\n",
-                "The previous makes Model Surgery quite easy as any submodule could be replace by\n",
-                "e.g. a pretrained Module, a shared Module, or even just a Module/function that\n",
-                "uses the same signature. More over, Variables can also be modified or shared."
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 8,
-            "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "Array([[0., 0.]], dtype=float32)"
-                        ]
-                    },
-                    "execution_count": 8,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
-            "source": [
-                "# Module replacement\n",
-                "pretrained = Block(dim=2, rngs=nnx.Rngs(42)) # imagine this is pretrained\n",
-                "model.blocks[0] = pretrained\n",
-                "# Module sharing\n",
-                "model.blocks[3] = model.blocks[1]\n",
-                "# Monkey patching\n",
-                "def awesome_layer(x): return x\n",
-                "model.blocks[2] = awesome_layer\n",
                 "\n",
-                "# Variable sharing (weight tying)\n",
-                "model.blocks[-1].linear.kernel = model.blocks[0].linear.kernel\n",
                 "\n",
-                "model(jnp.ones((1, 2)))"
+                "class Dropout(Module):\n",
+                "\n",
+                "  def __init__(self, rate: float):\n",
+                "    self.rate = rate\n",
+                "\n",
+                "  def __call__(self, x: jax.Array, *, train: bool, rngs: Rngs) -> jax.Array:\n",
+                "    if train:\n",
+                "      mask = random.bernoulli(rngs.make_rng(), (1 - self.rate), x.shape)\n",
+                "      x = x * mask / (1 - self.rate)\n",
+                "    return x"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## The Functional API\n",
-                "\n",
-                "The Functional API established a clear boundary between reference/object semantics and \n",
-                "value/pytree semantics. It also allows same amount of fine-grained control over the \n",
-                "state Linen/Haiku users are used to. The Functional API consists of 3 basic methods: \n",
-                "`split`, `merge`, and `update`.\n",
-                "\n",
-                "The `StatefulLinear` Module shown below will serve as an example to learn to use the \n",
-                "Functional API. It contains some `nnx.Param` Variables and a custom `Count` Variable\n",
-                "type which is used to keep track of integer scalar state that increases on every \n",
-                "forward pass."
+                "## Scan Over Layers Example"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 9,
+            "execution_count": 3,
             "metadata": {},
             "outputs": [],
             "source": [
-                "class Count(nnx.Variable): pass\n",
+                "class Block(Module):\n",
                 "\n",
-                "class StatefulLinear(nnx.Module):\n",
-                "  def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n",
-                "    self.w = nnx.Param(jax.random.uniform(rngs(), (din, dout)))\n",
-                "    self.b = nnx.Param(jnp.zeros((dout,)))\n",
-                "    self.count = Count(0)\n",
+                "  def __init__(self, din: int, dout: int, *, rngs: Rngs):\n",
+                "    self.linear = Linear(din, dout, rngs=rngs)\n",
+                "    self.bn = BatchNorm(dout)\n",
+                "    self.dropout = Dropout(0.1)\n",
+                "\n",
+                "  def __call__(self, x: jax.Array, *, train: bool, rngs: Rngs) -> jax.Array:\n",
+                "    x = self.linear(x)\n",
+                "    x = self.bn(x, train=train)\n",
+                "    x = jax.nn.gelu(x)\n",
+                "    x = self.dropout(x, train=train, rngs=rngs)\n",
+                "    return x\n",
                 "\n",
-                "  def __call__(self, x: jax.Array):\n",
-                "    self.count.value += 1\n",
-                "    return x @ self.w.value + self.b.value\n",
-                "  \n",
-                "model = StatefulLinear(din=2, dout=3, rngs=nnx.Rngs(0))"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "### State and GraphDef\n",
                 "\n",
-                "A Module can be decomposed into a `State` and `GraphDef` pytrees using the \n",
-                "`.split()` method. State is a Mapping from strings to Variables or nested \n",
-                "States. GraphDef is contains all the static information needed to reconstruct \n",
-                "a Module graph, its analogous to JAX's `PyTreeDef`, and for convenience it \n",
-                "implements an empty pytree."
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 10,
-            "metadata": {},
-            "outputs": [
-                {
-                    "name": "stdout",
-                    "output_type": "stream",
-                    "text": [
-                        "state = State({\n",
-                        "  'w': Param(\n",
-                        "    raw_value=Array([[0.19007349, 0.31424356, 0.3686391 ],\n",
-                        "           [0.7862853 , 0.03352201, 0.50682676]], dtype=float32)\n",
-                        "  ),\n",
-                        "  'b': Param(\n",
-                        "    raw_value=Array([0., 0., 0.], dtype=float32)\n",
-                        "  ),\n",
-                        "  'count': Count(\n",
-                        "    raw_value=0\n",
-                        "  )\n",
-                        "})\n",
-                        "\n",
-                        "static = GraphDef(\n",
-                        "  type=StatefulLinear,\n",
-                        "  index=0,\n",
-                        "  attributes=('w', 'b', 'count'),\n",
-                        "  subgraphs={},\n",
-                        "  static_fields={},\n",
-                        "  variables={\n",
-                        "    'w': VariableDef(\n",
-                        "      type=Param,\n",
-                        "      index=1,\n",
-                        "      me...\n"
-                    ]
-                }
-            ],
-            "source": [
-                "state, static = model.split()\n",
+                "class ScanMLP(Module):\n",
                 "\n",
-                "print(f'{state = }\\n')\n",
-                "print(f'{static = }'[:200] + '...')"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "### Split, Merge, and Update\n",
+                "  def __init__(self, hidden_size: int, n_layers: int, *, rngs: Rngs):\n",
+                "    self.n_layers = n_layers\n",
                 "\n",
-                "`merge` is the reverse of `split`, it takes the GraphDef + State and reconstructs\n",
-                "the Module. As shown in the example below, by using split and merge in sequence \n",
-                "any Module can be lifted to be used in any JAX transform. `update` can\n",
-                "update a Module strucure from a compatible State, this is often used to propagate the state\n",
-                "updates from a transform back to the source object outside."
+                "    # lift init\n",
+                "    key = random.split(rngs.make_rng(), n_layers - 1)\n",
+                "    graphdef: GraphDef[Block] = None  # type: ignore\n",
+                "\n",
+                "    def init_fn(key):\n",
+                "      nonlocal graphdef\n",
+                "      state, graphdef = Block(\n",
+                "          hidden_size, hidden_size, rngs=Rngs(key)\n",
+                "      ).split()\n",
+                "      return state\n",
+                "\n",
+                "    state = jax.vmap(init_fn)(key)\n",
+                "    self.layers = graphdef.merge(state)\n",
+                "    self.linear = Linear(hidden_size, hidden_size, rngs=rngs)\n",
+                "\n",
+                "  def __call__(self, x: jax.Array, *, train: bool, rngs: Rngs) -> jax.Array:\n",
+                "    # lift call\n",
+                "    key: jax.Array = random.split(rngs.make_rng(), self.n_layers - 1)  # type: ignore\n",
+                "    state, graphdef = self.layers.split()\n",
+                "\n",
+                "    def scan_fn(x, inputs: tuple[jax.Array, State]):\n",
+                "      key, state = inputs\n",
+                "      x, (state, _) = graphdef.apply(state)(x, train=train, rngs=Rngs(key))\n",
+                "      return x, state\n",
+                "\n",
+                "    x, state = jax.lax.scan(scan_fn, x, (key, state))\n",
+                "    self.layers.update(state)\n",
+                "    x = self.linear(x)\n",
+                "    return x"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 11,
+            "execution_count": 4,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
-                        "model.count = Count(\n",
-                        "  raw_value=0\n",
-                        ")\n",
-                        "model.count.value = Array(1, dtype=int32, weak_type=True)\n"
+                        "state = State({\n",
+                        "  'layers/bn/bias': Param(value=(4, 10), sharding=None),\n",
+                        "  'layers/bn/mean': BatchStat(value=(4, 10), sharding=None),\n",
+                        "  'layers/bn/scale': Param(value=(4, 10), sharding=None),\n",
+                        "  'layers/bn/var': BatchStat(value=(4, 10), sharding=None),\n",
+                        "  'layers/linear/b': Param(value=(4, 10), sharding=None),\n",
+                        "  'layers/linear/w': Param(value=(4, 10, 10), sharding=None),\n",
+                        "  'linear/b': Param(value=(10,), sharding=None),\n",
+                        "  'linear/w': Param(value=(10, 10), sharding=None)\n",
+                        "})\n",
+                        "graphdef = GraphDef(type=<class '__main__.ScanMLP'>, index=0, submodules={'layers': GraphDef(type=<class '__main__.Block'>, index=1, submodules={'bn': GraphDef(type=<class '__main__.BatchNorm'>, index=2, submodules={}, static_fields={'mu': 0.95}), 'dropout': GraphDef(type=<class '__main__.Dropout'>, index=3, submodules={}, static_fields={'rate': 0.1}), 'linear': GraphDef(type=<class '__main__.Linear'>, index=4, submodules={}, static_fields={'din': 10, 'dout': 10})}, static_fields={}), 'linear': GraphDef(type=<class '__main__.Linear'>, index=5, submodules={}, static_fields={'din': 10, 'dout': 10})}, static_fields={'n_layers': 5})\n"
                     ]
                 }
             ],
             "source": [
-                "print(f'{model.count = }')\n",
-                "\n",
-                "# 1. Use split to create a pytree representation of the Module\n",
-                "state, static = model.split()\n",
+                "module = ScanMLP(hidden_size=10, n_layers=5, rngs=Rngs(random.key(0)))\n",
+                "x = jax.random.normal(random.key(0), (2, 10))\n",
+                "y = module(x, train=True, rngs=Rngs(random.key(1)))\n",
                 "\n",
-                "@jax.jit\n",
-                "def forward(static: nnx.GraphDef, state: nnx.State, x: jax.Array):\n",
-                "  # 2. Use merge to create a new model inside the JAX transformation\n",
-                "  model = static.merge(state)\n",
-                "  # 3. Call the Module\n",
-                "  y = model(x)\n",
-                "  # 4. Use split to propagate State updates\n",
-                "  state, _ = model.split()\n",
-                "  return y, state\n",
-                "\n",
-                "y, state = forward(static, state, x=jnp.ones((1, 2)))\n",
-                "# 5. Update the state of the original Module\n",
-                "model.update(state)\n",
-                "\n",
-                "print(f'{model.count.value = }')"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "The key insight of this pattern is that using mutable references is \n",
-                "fine within a transform context (including the base eager interpreter)\n",
-                "but its necessary to use the Functional API when crossing boundaries.\n",
-                "\n",
-                "**Why aren't Module's just Pytrees?** The main reason is that its very\n",
-                "easy to lose track of shared references by accident this way, for example\n",
-                "if you pass two Module that have a shared Module through a JAX boundary\n",
-                "you will silently lose that shared reference. The Functional API makes this\n",
-                "behavior explicit and thus its much easier to reason about."
+                "state, graphdef = module.split()\n",
+                "print(\"state =\", jax.tree_util.tree_map(jnp.shape, state))\n",
+                "print(\"graphdef =\", graphdef)"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Fine-grained State Control\n",
-                "\n",
-                "Seasoned Linen and Haiku users might recognize that having all the state in\n",
-                "a single structure is not always the best choice as there are cases in which\n",
-                "you might want to handle different subsets of the state differently. This a\n",
-                "common occurrence when interacting with JAX transform, for example, not all\n",
-                "the model's state can or should be differentiated when interacting which `grad`,\n",
-                "or sometimes there is a need to specify what part of the model's state is a\n",
-                "carry and what part is not when using `scan`.\n",
-                "\n",
-                "To solve this `split` allows you to pass one or more `Filter`s to partition\n",
-                "the Variables into mutually exclusive States. The most common Filter being\n",
-                "Variable types as shown below."
+                "### Filtering State"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 12,
+            "execution_count": 5,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "params = State({\n",
-                        "  'w': Param(\n",
-                        "    raw_value=Array([[0.19007349, 0.31424356, 0.3686391 ],\n",
-                        "           [0.7862853 , 0.03352201, 0.50682676]], dtype=float32)\n",
-                        "  ),\n",
-                        "  'b': Param(\n",
-                        "    raw_value=Array([0., 0., 0.], dtype=float32)\n",
-                        "  )\n",
+                        "  'layers/bn/bias': Param(value=(4, 10), sharding=None),\n",
+                        "  'layers/bn/scale': Param(value=(4, 10), sharding=None),\n",
+                        "  'layers/linear/b': Param(value=(4, 10), sharding=None),\n",
+                        "  'layers/linear/w': Param(value=(4, 10, 10), sharding=None),\n",
+                        "  'linear/b': Param(value=(10,), sharding=None),\n",
+                        "  'linear/w': Param(value=(10, 10), sharding=None)\n",
                         "})\n",
-                        "\n",
-                        "counts = State({\n",
-                        "  'count': Count(\n",
-                        "    raw_value=Array(1, dtype=int32, weak_type=True)\n",
-                        "  )\n",
+                        "batch_stats = State({\n",
+                        "  'layers/bn/mean': BatchStat(value=(4, 10), sharding=None),\n",
+                        "  'layers/bn/var': BatchStat(value=(4, 10), sharding=None)\n",
                         "})\n"
                     ]
                 }
             ],
             "source": [
-                "# use Variable type filters to split into multiple States\n",
-                "params, counts, static = model.split(nnx.Param, Count)\n",
-                "\n",
-                "print(f'{params = }\\n')\n",
-                "print(f'{counts = }')"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "**Note**: filters must be exhaustive, if a Variable is not matched an error will be raised.\n",
+                "# split\n",
+                "params = state.extract(Param)\n",
+                "batch_stats = state.extract(BatchStat)\n",
+                "# merge\n",
+                "state = State({**params, **batch_stats})\n",
                 "\n",
-                "As expected the `merge` and `update` methods naturally consume multiple States:"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 13,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "# merge multiple States\n",
-                "model = static.merge(params, counts)\n",
-                "# update with multiple States\n",
-                "model.update(params, counts)"
+                "print(\"params =\", jax.tree_util.tree_map(jnp.shape, params))\n",
+                "print(\"batch_stats =\", jax.tree_util.tree_map(jnp.shape, batch_stats))"
             ]
         }
     ],
     "metadata": {
-        "jupytext": {
-            "formats": "ipynb,md:myst"
-        },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.9.18"
+            "version": "3.10.13"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 2
 }
```

### Comparing `flax-0.8.2/docs/faq.rst` & `flax-0.8.3/docs/faq.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/flax.png` & `flax-0.8.3/docs/flax.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/flip/0000-template.md` & `flax-0.8.3/docs/flip/0000-template.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/flip/1009-optimizer-api.md` & `flax-0.8.3/docs/flip/1009-optimizer-api.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/flip/1777-default-dtype.md` & `flax-0.8.3/docs/flip/1777-default-dtype.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/flip/2396-rnn.md` & `flax-0.8.3/docs/flip/2396-rnn.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/flip/2434-general-metadata.md` & `flax-0.8.3/docs/flip/2434-general-metadata.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/flip/2974-kw-only-dataclasses.md` & `flax-0.8.3/docs/flip/2974-kw-only-dataclasses.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/flip/README.md` & `flax-0.8.3/docs/flip/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/glossary.rst` & `flax-0.8.3/docs/glossary.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst` & `flax-0.8.3/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst`

 * *Files 2% similar despite different names*

```diff
@@ -173,15 +173,15 @@
 
 ``torch.nn.BatchNorm2d`` uses ``0.1`` as the default value for the ``momentum`` parameter while
 |nn.BatchNorm|_ uses ``0.9``. However, this corresponds to the same computation, because PyTorch multiplies
 the estimated statistic with ``(1 − momentum)`` and the new observed value with ``momentum``,
 while Flax multiplies the estimated statistic with ``momentum`` and the new observed value with ``(1 − momentum)``.
 
 .. |nn.BatchNorm| replace:: ``nn.BatchNorm``
-.. _nn.BatchNorm: https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.BatchNorm.html
+.. _nn.BatchNorm: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.BatchNorm
 
 .. testcode::
 
   t_bn = torch.nn.BatchNorm2d(num_features=3, momentum=0.1)
   t_bn.eval()
 
   scale = t_bn.weight.detach().cpu().numpy()
@@ -215,18 +215,18 @@
 ``torch.nn.AvgPool2d`` and |nn.avg_pool()|_ are compatible when using default parameters.
 However, ``torch.nn.AvgPool2d`` has a parameter ``count_include_pad``. When ``count_include_pad=False``,
 the zero-padding will not be considered for the average calculation. There does not exist a similar
 parameter for |nn.avg_pool()|_. However, we can easily implement a wrapper around the pooling
 operation. ``nn.pool()`` is the core function behind |nn.avg_pool()|_ and |nn.max_pool()|_.
 
 .. |nn.avg_pool()| replace:: ``nn.avg_pool()``
-.. _nn.avg_pool(): https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.avg_pool.html
+.. _nn.avg_pool(): https://flax.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.avg_pool
 
 .. |nn.max_pool()| replace:: ``nn.max_pool()``
-.. _nn.max_pool(): https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.max_pool.html
+.. _nn.max_pool(): https://flax.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.max_pool
 
 
 .. testcode::
 
   def avg_pool(inputs, window_shape, strides=None, padding='VALID'):
     """
     Pools the input by taking the average over a window.
@@ -299,12 +299,12 @@
   t_out = np.transpose(t_out.detach().cpu().numpy(), (0, 2, 3, 1))
   np.testing.assert_almost_equal(j_out, t_out, decimal=6)
 
 
 .. _`pull request`: https://github.com/google/jax/pull/5772
 
 .. |nn.ConvTranspose| replace:: ``nn.ConvTranspose``
-.. _nn.ConvTranspose: https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.ConvTranspose.html
+.. _nn.ConvTranspose: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.ConvTranspose
 
 .. |jax.lax.conv_transpose| replace:: ``jax.lax.conv_transpose``
 .. _jax.lax.conv_transpose: https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.conv_transpose.html
```

### Comparing `flax-0.8.2/docs/guides/converting_and_upgrading/haiku_migration_guide.rst` & `flax-0.8.3/docs/guides/converting_and_upgrading/haiku_migration_guide.rst`

 * *Files 1% similar despite different names*

```diff
@@ -194,15 +194,15 @@
           params,
           key,
           inputs, training=True # <== inputs
         )
         return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
 
     grads = jax.grad(loss_fn)(params)
-    params = jax.tree_map(lambda p, g: p - 0.1 * g, params, grads)
+    params = jax.tree_util.tree_map(lambda p, g: p - 0.1 * g, params, grads)
 
     return params
 
   ---
 
   def train_step(key, params, inputs, labels):
     def loss_fn(params):
@@ -210,15 +210,15 @@
           {'params': params},
           inputs, training=True, # <== inputs
           rngs={'dropout': key}
         )
         return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
 
     grads = jax.grad(loss_fn)(params)
-    params = jax.tree_map(lambda p, g: p - 0.1 * g, params, grads)
+    params = jax.tree_util.tree_map(lambda p, g: p - 0.1 * g, params, grads)
 
     return params
 
 .. testcode::
   :hide:
 
   train_step(random.key(0), params, sample_x, jnp.ones((1,), dtype=jnp.int32))
@@ -351,15 +351,15 @@
         None, # <== rng
         inputs, training=True # <== inputs
       )
       loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
       return loss, new_state
 
     grads, new_state = jax.grad(loss_fn, has_aux=True)(params)
-    params = jax.tree_map(lambda p, g: p - 0.1 * g, params, grads)
+    params = jax.tree_util.tree_map(lambda p, g: p - 0.1 * g, params, grads)
 
     return params, new_state
   ---
 
   def train_step(params, batch_stats, inputs, labels):
     def loss_fn(params):
       logits, updates = model.apply(
@@ -367,15 +367,15 @@
         inputs, training=True, # <== inputs
         mutable='batch_stats',
       )
       loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
       return loss, updates["batch_stats"]
 
     grads, batch_stats = jax.grad(loss_fn, has_aux=True)(params)
-    params = jax.tree_map(lambda p, g: p - 0.1 * g, params, grads)
+    params = jax.tree_util.tree_map(lambda p, g: p - 0.1 * g, params, grads)
 
     return params, batch_stats
 
 .. testcode::
   :hide:
 
   train_step(params, batch_stats, sample_x, jnp.ones((1,), dtype=jnp.int32))
```

### Comparing `flax-0.8.2/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst` & `flax-0.8.3/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/converting_and_upgrading/optax_update_guide.rst` & `flax-0.8.3/docs/guides/converting_and_upgrading/optax_update_guide.rst`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 We have proposed to replace :py:mod:`flax.optim` with `Optax
 <https://optax.readthedocs.io>`_ in 2021 with `FLIP #1009
 <https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md>`_ and
 the Flax optimizers have been removed in v0.6.0 - this guide is targeted
 towards :py:mod:`flax.optim` users to help them update their code to Optax.
 
 See also Optax's quick start documentation:
-https://optax.readthedocs.io/en/latest/optax-101.html
+https://optax.readthedocs.io/en/latest/getting_started.html
 
 .. testsetup::
 
   import flax
   import jax
   import jax.numpy as jnp
   import flax.linen as nn
@@ -28,15 +28,15 @@
   learning_rate, momentum, weight_decay, grad_clip_norm = .1, .9, 1e-3, 1.
   loss = lambda params, batch: jnp.array(0.)
 
 Replacing ``flax.optim`` with ``optax``
 ---------------------------------------
 
 Optax has drop-in replacements for all of Flax's optimizers. Refer to Optax's
-documentation `Common Optimizers <https://optax.readthedocs.io/en/latest/api.html>`_
+documentation `Common Optimizers <https://optax.readthedocs.io/en/latest/api/optimizers.html>`_
 for API details.
 
 The usage is very similar, with the difference that ``optax`` does not keep a
 copy of the ``params``, so they need to be passed around separately. Flax
 provides the utility :py:class:`~flax.training.train_state.TrainState` to store
 optimizer state, parameters, and other associated data in a single dataclass
 (not used in code below).
@@ -82,17 +82,17 @@
 
 The function |optax.sgd()|_ used in the code snippet above is simply a wrapper
 for the sequential application of two gradient transformations. Instead of using
 this alias, it is common to use |optax.chain()|_ to combine multiple of these
 generic building blocks.
 
 .. |optax.sgd()| replace:: ``optax.sgd()``
-.. _optax.sgd(): https://optax.readthedocs.io/en/latest/api.html#optax.sgd
+.. _optax.sgd(): https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.sgd
 .. |optax.chain()| replace:: ``optax.chain()``
-.. _optax.chain(): https://optax.readthedocs.io/en/latest/api.html#chain
+.. _optax.chain(): https://optax.readthedocs.io/en/latest/api/combining_optimizers.html#optax.chain
 
 .. codediff::
   :title_left: Pre-defined alias
   :title_right: Combining transformations
 
   # Note that the aliases follow the convention to use positive
   # values for the learning rate by default.
@@ -117,17 +117,17 @@
 
 Some of Flax's optimizers also include a weight decay. In Optax, some optimizers
 also have a weight decay parameter (such as |optax.adamw()|_), and to others the
 weight decay can be added as another "gradient transformation"
 |optax.add_decayed_weights()|_ that adds an update derived from the parameters.
 
 .. |optax.adamw()| replace:: ``optax.adamw()``
-.. _optax.adamw(): https://optax.readthedocs.io/en/latest/api.html#optax.adamw
+.. _optax.adamw(): https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.adamw
 .. |optax.add_decayed_weights()| replace:: ``optax.add_decayed_weights()``
-.. _optax.add_decayed_weights(): https://optax.readthedocs.io/en/latest/api.html#optax.add_decayed_weights
+.. _optax.add_decayed_weights(): https://optax.readthedocs.io/en/latest/api/transformations.html#optax.add_decayed_weights
 
 .. codediff::
   :title_left: flax.optim
   :title_right: optax
   :sync:
 
   optimizer_def = flax.optim.Adam(
@@ -151,15 +151,15 @@
 
 Training can be stabilized by clipping gradients to a global norm (`Pascanu et
 al, 2012 <https://arxiv.org/abs/1211.5063>`_). In Flax this is often done by
 processing the gradients before passing them to the optimizer. With Optax this
 becomes just another gradient transformation |optax.clip_by_global_norm()|_.
 
 .. |optax.clip_by_global_norm()| replace:: ``optax.clip_by_global_norm()``
-.. _optax.clip_by_global_norm(): https://optax.readthedocs.io/en/latest/api.html#optax.clip_by_global_norm
+.. _optax.clip_by_global_norm(): https://optax.readthedocs.io/en/latest/api/transformations.html#optax.clip_by_global_norm
 
 .. codediff::
   :title_left: flax.optim
   :title_right: optax
   :sync:
 
   def train_step(optimizer, batch):
@@ -187,23 +187,23 @@
 |optax.scale_by_schedule()|_. Optax also allows specifying a functions to
 inject arbitrary scalar values for other gradient updates via
 |optax.inject_hyperparams()|_.
 
 Read more about learning rate schedules in the :doc:`lr_schedule` guide.
 
 Read more about schedules defined in Optax under `Optimizer Schedules
-<https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules>`_. the
+<https://optax.readthedocs.io/en/latest/api/optimizer_schedules.html>`_. the
 standard optimizers (like ``optax.adam()``, ``optax.sgd()`` etc.) also accept a
 learning rate schedule as a parameter for ``learning_rate``.
 
 
 .. |optax.scale_by_schedule()| replace:: ``optax.scale_by_schedule()``
-.. _optax.scale_by_schedule(): https://optax.readthedocs.io/en/latest/api.html#optax.scale_by_schedule
+.. _optax.scale_by_schedule(): https://optax.readthedocs.io/en/latest/api/transformations.html#optax.scale_by_schedule
 .. |optax.inject_hyperparams()| replace:: ``optax.inject_hyperparams()``
-.. _optax.inject_hyperparams(): https://optax.readthedocs.io/en/latest/api.html#optax.inject_hyperparams
+.. _optax.inject_hyperparams(): https://optax.readthedocs.io/en/latest/api/optimizer_schedules.html#optax.inject_hyperparams
 
 .. codediff::
   :title_left: flax.optim
   :title_right: optax
   :sync:
 
   def train_step(step, optimizer, batch):
@@ -235,17 +235,17 @@
 gradient transformations will only be called with that partial flattened view of
 the params/gradients. This is not a problem usually, but it makes it hard to
 nest multiple levels of masked gradient transformations (because the inner
 masks will expect the mask to be defined in terms of the partial flattened view
 that is not readily available outside the outer mask).
 
 .. |optax.masked()| replace:: ``optax.masked()``
-.. _optax.masked(): https://optax.readthedocs.io/en/latest/api.html#optax.masked
+.. _optax.masked(): https://optax.readthedocs.io/en/latest/api/optimizer_wrappers.html#optax.masked
 .. |optax.multi_transform()| replace:: ``optax.multi_transform()``
-.. _optax.multi_transform(): https://optax.readthedocs.io/en/latest/api.html#optax.multi_transform
+.. _optax.multi_transform(): https://optax.readthedocs.io/en/latest/api/combining_optimizers.html#optax.multi_transform
 
 .. codediff::
   :title_left: flax.optim
   :title_right: optax
   :sync:
 
   kernels = flax.traverse_util.ModelParamTraversal(lambda p, _: 'kernel' in p)
```

### Comparing `flax-0.8.2/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst` & `flax-0.8.3/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst` & `flax-0.8.3/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst` & `flax-0.8.3/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/data_preprocessing/full_eval.rst` & `flax-0.8.3/docs/guides/data_preprocessing/full_eval.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/data_preprocessing/loading_datasets.ipynb` & `flax-0.8.3/docs/guides/data_preprocessing/loading_datasets.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/data_preprocessing/loading_datasets.md` & `flax-0.8.3/docs/guides/data_preprocessing/loading_datasets.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/flax_fundamentals/arguments.md` & `flax-0.8.3/docs/guides/flax_fundamentals/arguments.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/flax_fundamentals/flax_basics.ipynb` & `flax-0.8.3/docs/guides/flax_fundamentals/flax_basics.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/flax_fundamentals/flax_basics.md` & `flax-0.8.3/docs/guides/flax_fundamentals/flax_basics.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/flax_fundamentals/rng_guide.ipynb` & `flax-0.8.3/docs/guides/flax_fundamentals/rng_guide.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9995259259259259%*

 * *Differences: {"'cells'": '{42: {\'source\': {insert: [(14, "  params = jax.tree_util.tree_map(lambda p, g: p - '*

 * *            '1e-3 * g, variables[\'params\'], grads)\\n")], delete: [14]}}, 73: {\'source\': [\'We '*

 * *            'can use '*

 * *            '[`nn.vmap`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/transformations.html#flax.linen.vmap) '*

 * *            "to create a batched `Dense` layer:']}, 86: {'source': ['We can use "*

 * *            '[`nn.scan`](https://flax.readthedocs.io/en/latest/api_reference/f […]*

```diff
@@ -846,15 +846,15 @@
                 "      {'params': params, 'other_collection': variables['other_collection']},\n",
                 "      x,\n",
                 "      train=True,\n",
                 "      rngs={'dropout': dropout_rng},\n",
                 "    )\n",
                 "    return jnp.mean((y - out) ** 2)\n",
                 "  grads = jax.grad(loss)(variables['params'])\n",
-                "  params = jax.tree_map(lambda p, g: p - 1e-3 * g, variables['params'], grads)\n",
+                "  params = jax.tree_util.tree_map(lambda p, g: p - 1e-3 * g, variables['params'], grads)\n",
                 "  return {\n",
                 "    'params': params,\n",
                 "    'other_collection': variables['other_collection'],\n",
                 "  }, next_rng\n",
                 "\n",
                 "for _ in range(10):\n",
                 "  variables, train_rng = update(variables, train_rng)\n",
@@ -1479,15 +1479,15 @@
                 "### `nn.vmap`"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "We can use [`nn.vmap`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.vmap.html) to create a batched `Dense` layer:"
+                "We can use [`nn.vmap`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/transformations.html#flax.linen.vmap) to create a batched `Dense` layer:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 34,
             "metadata": {
                 "outputId": "f0830f6b-659c-446f-c933-7b2a430f8004"
@@ -1770,15 +1770,15 @@
                 "### `nn.scan`"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "We can use [`nn.scan`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.scan.html) to create a scanned `Module` layer (this is useful for simplifying repetitively stacked submodules):"
+                "We can use [`nn.scan`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/transformations.html#flax.linen.scan) to create a scanned `Module` layer (this is useful for simplifying repetitively stacked submodules):"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 40,
             "metadata": {
                 "outputId": "29d1863b-809f-42ce-894c-1b0810faa41e"
```

### Comparing `flax-0.8.2/docs/guides/flax_fundamentals/rng_guide.md` & `flax-0.8.3/docs/guides/flax_fundamentals/rng_guide.md`

 * *Files 1% similar despite different names*

```diff
@@ -418,15 +418,15 @@
       {'params': params, 'other_collection': variables['other_collection']},
       x,
       train=True,
       rngs={'dropout': dropout_rng},
     )
     return jnp.mean((y - out) ** 2)
   grads = jax.grad(loss)(variables['params'])
-  params = jax.tree_map(lambda p, g: p - 1e-3 * g, variables['params'], grads)
+  params = jax.tree_util.tree_map(lambda p, g: p - 1e-3 * g, variables['params'], grads)
   return {
     'params': params,
     'other_collection': variables['other_collection'],
   }, next_rng
 
 for _ in range(10):
   variables, train_rng = update(variables, train_rng)
@@ -653,15 +653,15 @@
 
 +++
 
 ### `nn.vmap`
 
 +++
 
-We can use [`nn.vmap`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.vmap.html) to create a batched `Dense` layer:
+We can use [`nn.vmap`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/transformations.html#flax.linen.vmap) to create a batched `Dense` layer:
 
 ```{code-cell}
 :outputId: f0830f6b-659c-446f-c933-7b2a430f8004
 
 x = jnp.ones((3, 2))
 
 BatchDense = nn.vmap(
@@ -756,15 +756,15 @@
 BatchModel().init({'params': jax.random.key(0), 'other': jax.random.key(1)}, x)
 ```
 
 ### `nn.scan`
 
 +++
 
-We can use [`nn.scan`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.scan.html) to create a scanned `Module` layer (this is useful for simplifying repetitively stacked submodules):
+We can use [`nn.scan`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/transformations.html#flax.linen.scan) to create a scanned `Module` layer (this is useful for simplifying repetitively stacked submodules):
 
 ```{code-cell}
 :outputId: 29d1863b-809f-42ce-894c-1b0810faa41e
 
 x = jnp.ones((3, 2))
 
 class ResidualMLPBlock(nn.Module):
```

### Comparing `flax-0.8.2/docs/guides/flax_fundamentals/setup_or_nncompact.rst` & `flax-0.8.3/docs/guides/flax_fundamentals/setup_or_nncompact.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/flax_fundamentals/state_params.rst` & `flax-0.8.3/docs/guides/flax_fundamentals/state_params.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/flax_sharp_bits.ipynb` & `flax-0.8.3/docs/guides/flax_sharp_bits.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9997895622895623%*

 * *Differences: {"'cells'": "{2: {'source': {insert: [(7, '2. Add the "*

 * *            '[`flax.linen.Dropout`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.Dropout) '*

 * *            'layer(s) to your model (subclassed from Flax '*

 * *            "[`Module`](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html#module-basics)).\\n')], "*

 * *            'delete: [7]}}}'}*

```diff
@@ -31,15 +31,15 @@
                 "## \ud83d\udd2a `flax.linen.Dropout` layer and randomness\n",
                 "\n",
                 "### TL;DR\n",
                 "\n",
                 "When working on a model with dropout (subclassed from [Flax `Module`](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html#module-basics)), add the `'dropout'` PRNGkey only during the forward pass.\n",
                 "\n",
                 "1. Start with [`jax.random.split()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html#jax-random-split) to explicitly create PRNG keys for `'params'` and `'dropout'`.\n",
-                "2. Add the [`flax.linen.Dropout`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Dropout.html#flax.linen.Dropout) layer(s) to your model (subclassed from Flax [`Module`](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html#module-basics)).\n",
+                "2. Add the [`flax.linen.Dropout`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.Dropout) layer(s) to your model (subclassed from Flax [`Module`](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html#module-basics)).\n",
                 "3. When initializing the model ([`flax.linen.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_apply.html)), there's no need to pass in an extra `'dropout'` PRNG key\u2014just the `'params'` key like in a \"simpler\" model.\n",
                 "4. During the forward pass with [`flax.linen.apply()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_apply.html), pass in `rngs={'dropout': dropout_key}`.\n",
                 "\n",
                 "Check out a full example below.\n",
                 "\n",
                 "### Why this works\n",
                 "\n",
```

### Comparing `flax-0.8.2/docs/guides/flax_sharp_bits.md` & `flax-0.8.3/docs/guides/flax_sharp_bits.md`

 * *Files 1% similar despite different names*

```diff
@@ -23,15 +23,15 @@
 ## 🔪 `flax.linen.Dropout` layer and randomness
 
 ### TL;DR
 
 When working on a model with dropout (subclassed from [Flax `Module`](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html#module-basics)), add the `'dropout'` PRNGkey only during the forward pass.
 
 1. Start with [`jax.random.split()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html#jax-random-split) to explicitly create PRNG keys for `'params'` and `'dropout'`.
-2. Add the [`flax.linen.Dropout`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Dropout.html#flax.linen.Dropout) layer(s) to your model (subclassed from Flax [`Module`](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html#module-basics)).
+2. Add the [`flax.linen.Dropout`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#flax.linen.Dropout) layer(s) to your model (subclassed from Flax [`Module`](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html#module-basics)).
 3. When initializing the model ([`flax.linen.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_apply.html)), there's no need to pass in an extra `'dropout'` PRNG key—just the `'params'` key like in a "simpler" model.
 4. During the forward pass with [`flax.linen.apply()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/init_apply.html), pass in `rngs={'dropout': dropout_key}`.
 
 Check out a full example below.
 
 ### Why this works
```

### Comparing `flax-0.8.2/docs/guides/model_inspection/extracting_intermediates.rst` & `flax-0.8.3/docs/guides/model_inspection/extracting_intermediates.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/model_inspection/model_surgery.ipynb` & `flax-0.8.3/docs/guides/model_inspection/model_surgery.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/model_inspection/model_surgery.md` & `flax-0.8.3/docs/guides/model_inspection/model_surgery.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/parallel_training/ensembling.rst` & `flax-0.8.3/docs/guides/parallel_training/ensembling.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/parallel_training/flax_on_pjit.ipynb` & `flax-0.8.3/docs/guides/parallel_training/flax_on_pjit.ipynb`

 * *Files 4% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9648719877458146%*

 * *Differences: {"'cells'": "{5: {'execution_count': 3, 'outputs': [OrderedDict([('name', 'stderr'), "*

 * *            '(\'output_type\', \'stream\'), (\'text\', ["WARNING:absl:Tensorflow library not '*

 * *            'found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. '*

 * *            '\'gs://...\') cannot be accessed.\\n"])])], \'source\': {insert: [(12, \'import optax '*

 * *            "# Optax for common losses and optimizers.')], delete: [12]}}, 7: {'source': {insert: "*

 * *            "[(7, '\\n')], delet […]*

```diff
@@ -61,31 +61,39 @@
             "source": [
                 "import os\n",
                 "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 42,
+            "execution_count": 3,
             "metadata": {},
-            "outputs": [],
+            "outputs": [
+                {
+                    "name": "stderr",
+                    "output_type": "stream",
+                    "text": [
+                        "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n"
+                    ]
+                }
+            ],
             "source": [
                 "import functools\n",
                 "from typing import Optional, Callable\n",
                 "\n",
                 "import numpy as np\n",
                 "import jax\n",
                 "from jax import lax, random, numpy as jnp\n",
                 "\n",
                 "import flax\n",
                 "from flax import struct, traverse_util, linen as nn\n",
                 "from flax.core import freeze, unfreeze\n",
                 "from flax.training import train_state, checkpoints\n",
                 "\n",
-                "import optax # Optax for common losses and optimizers. "
+                "import optax # Optax for common losses and optimizers."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 4,
             "metadata": {},
             "outputs": [
@@ -109,15 +117,15 @@
                 "The code below shows how to import and set up the JAX-level device API, following JAX's [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) guide:\n",
                 "\n",
                 "1. Start a 2x4 device `mesh` (8 devices) using JAX's `mesh_utils.create_device_mesh`. This layout is the same as the one of a [TPU v3-8](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#single_tpu_board).\n",
                 "\n",
                 "2. Annotate each axis with a name using the `axis_names` parameter in `jax.sharding.Mesh`. A typical way to annotate axis names is `axis_name=('data', 'model')`, where:\n",
                 "  * `'data'`: the mesh dimension used for data-parallel sharding of the batch dimension of inputs and activations.\n",
                 "  * `'model'`: the mesh dimension used for sharding parameters of the model across devices.\n",
-                "  \n",
+                "\n",
                 "3. Make a simple utility function `mesh_sharding` for generating a sharding object from the mesh and any layout."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 5,
             "metadata": {},
@@ -135,16 +143,15 @@
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "[[CpuDevice(id=0) CpuDevice(id=1) CpuDevice(id=2) CpuDevice(id=3)]\n",
                         " [CpuDevice(id=4) CpuDevice(id=5) CpuDevice(id=6) CpuDevice(id=7)]]\n",
-                        "Mesh(device_ids=array([[0, 1, 2, 3],\n",
-                        "       [4, 5, 6, 7]]), axis_names=('data', 'model'))\n"
+                        "Mesh('data': 2, 'model': 4)\n"
                     ]
                 }
             ],
             "source": [
                 "# Create a mesh and annotate each axis with a name.\n",
                 "device_mesh = mesh_utils.create_device_mesh((2, 4))\n",
                 "print(device_mesh)\n",
@@ -163,61 +170,61 @@
             "source": [
                 "## Define a layer\n",
                 "\n",
                 "Before defining a simple model, create an example layer called `DotReluDot` (by subclassing `flax.linen.Module`). The layer creates two parameters `W1` and `W2` for dot product multiplication, and uses the `jax.nn.relu` (ReLU) activation function in-between.\n",
                 "\n",
                 "To shard the parameters efficiently, apply the following APIs to annotate the parameters and intermediate variables:\n",
                 "\n",
-                "1. Use [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html#flax.linen.with_partitioning) to decorate the initializer function when creating sub-layers or raw parameters.\n",
+                "1. Use [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) to decorate the initializer function when creating sub-layers or raw parameters.\n",
                 "\n",
                 "2. Apply [`jax.lax.with_sharding_constraint`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.with_sharding_constraint.html) (formerly, `pjit.with_sharding_constraint`) to annotate intermediate variables like `y` and `z` to force a particular sharding pattern when the ideal constraint is known.\n",
                 "\n",
                 "  * This step is optional, but can sometimes help auto-SPMD to partition efficiently. In the example below, the call is not required, because XLA will figure out the same sharding layout for `y` and `z` regardless."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 43,
+            "execution_count": 7,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class DotReluDot(nn.Module):\n",
                 "  depth: int\n",
                 "  dense_init: Callable = nn.initializers.xavier_normal()\n",
                 "  @nn.compact\n",
                 "  def __call__(self, x):\n",
-                "    \n",
-                "    y = nn.Dense(self.depth, \n",
+                "\n",
+                "    y = nn.Dense(self.depth,\n",
                 "                 kernel_init=nn.with_partitioning(self.dense_init, (None, 'model')),\n",
                 "                 use_bias=False,  # or overwrite with `bias_init`\n",
                 "                 )(x)\n",
                 "\n",
                 "    y = jax.nn.relu(y)\n",
                 "    # Force a local sharding annotation.\n",
                 "    y = with_sharding_constraint(y, mesh_sharding(PartitionSpec('data', 'model')))\n",
                 "\n",
                 "    W2 = self.param(\n",
-                "        'W2', \n",
+                "        'W2',\n",
                 "        nn.with_partitioning(self.dense_init, ('model', None)),\n",
                 "        (self.depth, x.shape[-1]))\n",
-                "    \n",
+                "\n",
                 "    z = jnp.dot(y, W2)\n",
                 "    # Force a local sharding annotation.\n",
                 "    z = with_sharding_constraint(z, mesh_sharding(PartitionSpec('data', None)))\n",
                 "\n",
                 "    # Return a tuple to conform with the API `flax.linen.scan` as shown in the cell below.\n",
                 "    return z, None"
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Note that device axis names like `'data'`, `'model'` or `None` are passed into both [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html) and [`jax.lax.with_sharding_constraint`](https://github.com/google/jax/blob/main/jax/_src/pjit.py#L1516) API calls. This refers to how each dimension of this data should be sharded \u2014 either across one of the device mesh dimensions, or not sharded at all.\n",
+                "Note that device axis names like `'data'`, `'model'` or `None` are passed into both [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) and [`jax.lax.with_sharding_constraint`](https://github.com/google/jax/blob/main/jax/_src/pjit.py#L1516) API calls. This refers to how each dimension of this data should be sharded \u2014 either across one of the device mesh dimensions, or not sharded at all.\n",
                 "\n",
                 "For example:\n",
                 "\n",
                 "* When you define `W1` with shape `(x.shape[-1], self.depth)` and annotate as `(None, 'model')`:\n",
                 "\n",
                 "  * The first dimension (of length `x.shape[-1]`) will be replicated across all devices.\n",
                 "  * The second dimension (of length `self.depth`) will be sharded over the `'model'` axis of the device mesh. This means `W1` will be sharded 4-way on devices `(0, 4)`, `(1, 5)`, `(2, 6)` and `(3, 7)`, on this dimension.\n",
@@ -233,38 +240,38 @@
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Define a model with `flax.linen.scan` lifted transformation\n",
                 "\n",
                 "Having created `DotReluDot`, you can now define the `MLP` model (by subclassing [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module)) as multiple layers of `DotReluDot`.\n",
                 "\n",
-                "To replicate identical layers, you can either use [`flax.linen.scan`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.scan.html), or a for-loop:\n",
+                "To replicate identical layers, you can either use [`flax.linen.scan`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/transformations.html#flax.linen.scan), or a for-loop:\n",
                 "\n",
                 "* `flax.linen.scan` can provide faster compilation times.\n",
                 "* The for-loop can be faster on runtime.\n",
                 "\n",
-                "The code below shows how to apply both methods, and default with the for-loop, so that all the parameters are two-dimensional and you can visualize their sharding. \n",
+                "The code below shows how to apply both methods, and default with the for-loop, so that all the parameters are two-dimensional and you can visualize their sharding.\n",
                 "\n",
                 "The `flax.linen.scan` code is just to show that this API works with [Flax lifted transforms](https://flax.readthedocs.io/en/latest/developer_notes/lift.html#supported-transformations)."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 44,
+            "execution_count": 8,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class MLP(nn.Module):\n",
                 "  num_layers: int\n",
                 "  depth: int\n",
                 "  use_scan: bool\n",
                 "  @nn.compact\n",
                 "  def __call__(self, x):\n",
                 "    if self.use_scan:\n",
-                "      x, _ = nn.scan(DotReluDot, length=self.num_layers, \n",
+                "      x, _ = nn.scan(DotReluDot, length=self.num_layers,\n",
                 "                     variable_axes={\"params\": 0},\n",
                 "                     split_rngs={\"params\": True},\n",
                 "                     metadata_params={nn.PARTITION_NAME: None}\n",
                 "                     )(self.depth)(x)\n",
                 "    else:\n",
                 "      for i in range(self.num_layers):\n",
                 "        x, _ = DotReluDot(self.depth)(x)\n",
@@ -277,15 +284,15 @@
             "metadata": {},
             "source": [
                 "Now, create a `model` instance, and a sample input `x`."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 45,
+            "execution_count": 9,
             "metadata": {},
             "outputs": [],
             "source": [
                 "# MLP hyperparameters.\n",
                 "BATCH, LAYERS, DEPTH, USE_SCAN = 8, 4, 1024, False\n",
                 "# Create fake inputs.\n",
                 "x = jnp.ones((BATCH, DEPTH))\n",
@@ -310,15 +317,15 @@
                 "### The input's sharding\n",
                 "\n",
                 "For data parallelism, you can shard the batched _input_ `x` across the `data` axis by denoting the batch axis as `'data'`. Then, use [`jax.device_put`](https://jax.readthedocs.io/en/latest/_autosummary/jax.device_put.html) to place it onto the correct `device`s."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 46,
+            "execution_count": 10,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/html": [
                             "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                             "\u2502                                                                              \u2502\n",
@@ -366,125 +373,50 @@
                 "\n",
                 "You need to compile `model.init()` (that is, [`flax.linen.Module.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.init)), and its output as a pytree of parameters. Additionally, you may sometimes need wrap it with a [`flax.training.train_state`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState) to track other variables, such as optimizer states, and that would make the output an even more complex pytree.\n",
                 "\n",
                 "To achieve this, luckily, you don't have to hardcode the output's sharding by hand. Instead, you can:\n",
                 "\n",
                 "1. Evaluate `model.init` (in this case, a wrapper of it) abstractly using [`jax.eval_shape`](https://jax.readthedocs.io/en/latest/_autosummary/jax.eval_shape.html).\n",
                 "\n",
-                "1. Use [`flax.linen.get_sharding`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.get_sharding.html) to automatically generate the `jax.sharding.NamedSharding`.\n",
-                "   * This step utilizes the [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html) annotations in the earlier definition to generate the correct sharding for the parameters."
+                "1. Use [`flax.linen.get_sharding`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.get_sharding) to automatically generate the `jax.sharding.NamedSharding`.\n",
+                "   * This step utilizes the [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) annotations in the earlier definition to generate the correct sharding for the parameters."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 47,
+            "execution_count": 11,
             "metadata": {},
             "outputs": [],
             "source": [
                 "def init_fn(k, x, model, optimizer):\n",
                 "  variables = model.init(k, x) # Initialize the model.\n",
                 "  state = train_state.TrainState.create( # Create a `TrainState`.\n",
                 "    apply_fn=model.apply,\n",
                 "    params=variables['params'],\n",
                 "    tx=optimizer)\n",
                 "  return state"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 48,
+            "execution_count": 12,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "TrainState(step=NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec()), apply_fn=<bound method Module.apply of MLP(\n",
+                            "TrainState(step=NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec()), apply_fn=<bound method Module.apply of MLP(\n",
                             "    # attributes\n",
                             "    num_layers = 4\n",
                             "    depth = 1024\n",
                             "    use_scan = False\n",
-                            ")>, params=FrozenDict({\n",
-                            "    DotReluDot_0: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "    DotReluDot_1: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "    DotReluDot_2: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "    DotReluDot_3: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "}), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x140ac9e50>, update=<function chain.<locals>.update_fn at 0x140adb3a0>), opt_state=(ScaleByAdamState(count=NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec()), mu=FrozenDict({\n",
-                            "    DotReluDot_0: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "    DotReluDot_1: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "    DotReluDot_2: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "    DotReluDot_3: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "}), nu=FrozenDict({\n",
-                            "    DotReluDot_0: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "    DotReluDot_1: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "    DotReluDot_2: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "    DotReluDot_3: {\n",
-                            "        Dense_0: {\n",
-                            "            kernel: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec(None, 'model')),\n",
-                            "        },\n",
-                            "        W2: NamedSharding(mesh={'data': 2, 'model': 4}, spec=PartitionSpec('model', None)),\n",
-                            "    },\n",
-                            "})), EmptyState()))"
+                            ")>, params={'DotReluDot_0': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}, 'DotReluDot_1': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}, 'DotReluDot_2': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}, 'DotReluDot_3': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x33e134280>, update=<function chain.<locals>.update_fn at 0x33e134430>), opt_state=(ScaleByAdamState(count=NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec()), mu={'DotReluDot_0': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}, 'DotReluDot_1': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}, 'DotReluDot_2': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}, 'DotReluDot_3': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}}, nu={'DotReluDot_0': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}, 'DotReluDot_1': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}, 'DotReluDot_2': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}, 'DotReluDot_3': {'Dense_0': {'kernel': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))}, 'W2': NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec('model', None))}}), EmptyState()))"
                         ]
                     },
-                    "execution_count": 48,
+                    "execution_count": 12,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "# Create an abstract closure to wrap the function before feeding it in\n",
                 "# because `jax.eval_shape` only takes pytrees as arguments.\n",
@@ -506,15 +438,15 @@
                 "Now you can apply [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) to your `init_fn`, but with two extra arguments: `in_shardings` and `out_shardings`.\n",
                 "\n",
                 "Run it to get the `initialized_state`, in which parameters are sharded exactly as instructed:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 49,
+            "execution_count": 16,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/html": [
                             "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
                             "\u2502       \u2502       \u2502       \u2502       \u2502\n",
@@ -574,15 +506,15 @@
                     },
                     "metadata": {},
                     "output_type": "display_data"
                 }
             ],
             "source": [
                 "jit_init_fn = jax.jit(init_fn, static_argnums=(2, 3),\n",
-                "                      in_shardings=(mesh_sharding(None), x_sharding),  # PRNG key and x\n",
+                "                      in_shardings=(mesh_sharding(()), x_sharding),  # PRNG key and x\n",
                 "                      out_shardings=state_sharding)\n",
                 "\n",
                 "initialized_state = jit_init_fn(k, x, model, optimizer)\n",
                 "\n",
                 "# for weight, partitioned in initialized_state.params['DotReluDot_0'].items():\n",
                 "#     print(f'Sharding of {weight}: {partitioned.names}')\n",
                 "jax.debug.visualize_array_sharding(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value)\n",
@@ -591,22 +523,22 @@
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Inspect the Module output\n",
                 "\n",
-                "Note that in the output of `initialized_state`, the `params` `W1` and `W2` are of type [`flax.linen.Partitioned`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Partitioned.html). This is a wrapper around the actual `jax.Array` that allows Flax to record the axis names associated with it. \n",
+                "Note that in the output of `initialized_state`, the `params` `W1` and `W2` are of type [`flax.linen.Partitioned`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.Partitioned). This is a wrapper around the actual `jax.Array` that allows Flax to record the axis names associated with it.\n",
                 "\n",
-                "You can access the raw `jax.Array` by adding `.value` when outside `jit`, or by `.unbox()` when inside."
+                "You can access the raw `jax.Array`s by calling `flax.linen.meta.unbox()` upon the dictionary, or call `.value` upon individual variable. You can also use `flax.linen.meta.replace_boxed()` to change the underlying `jax.Array` without modifying the sharding annotations."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 14,
+            "execution_count": 18,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "<class 'flax.core.meta.Partitioned'>\n",
@@ -620,122 +552,130 @@
                 "print(type(initialized_state.params['DotReluDot_0']['Dense_0']['kernel']))\n",
                 "print(type(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value))\n",
                 "print(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].names)\n",
                 "print(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value.shape)"
             ]
         },
         {
+            "cell_type": "code",
+            "execution_count": 19,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "# Say for some unknown reason you want to make the whole param tree all-zero\n",
+                "unboxed_params = nn.meta.unbox(initialized_state.params)\n",
+                "all_zero = jax.tree.map(jnp.zeros_like, unboxed_params)\n",
+                "all_zero_params = nn.meta.replace_boxed(initialized_state.params, all_zero)\n",
+                "assert jnp.sum(nn.meta.unbox(all_zero_params['DotReluDot_0']['Dense_0']['kernel'])) == 0"
+            ]
+        },
+        {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "You can also check the underlying [`jax.sharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html) of each parameter, which is now more internal than `NamedSharding`. Note that numbers like `initialized_state.step` are replicated across all devices."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 15,
+            "execution_count": 20,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "GSPMDSharding({devices=[1,4,2]0,4,1,5,2,6,3,7 last_tile_dim_replicate})"
+                            "NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec(None, 'model'))"
                         ]
                     },
-                    "execution_count": 15,
+                    "execution_count": 20,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value.sharding"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 16,
+            "execution_count": 21,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "0\n"
                     ]
                 },
                 {
                     "data": {
                         "text/plain": [
-                            "GSPMDSharding({replicated})"
+                            "NamedSharding(mesh=Mesh('data': 2, 'model': 4), spec=PartitionSpec())"
                         ]
                     },
-                    "execution_count": 16,
+                    "execution_count": 21,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "print(initialized_state.step)\n",
                 "initialized_state.step.sharding"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "You can use [`jax.tree_map`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_map.html) to perform mass computation on a dict of boxed params, in the same way as on a dict of JAX arrays."
+                "You can use [`jax.tree_util.tree_map`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_map.html) to perform mass computation on a dict of boxed params, in the same way as on a dict of JAX arrays."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 17,
+            "execution_count": 22,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
-                        "FrozenDict({\n",
-                        "    Dense_0: {\n",
-                        "        kernel: Partitioned(value=(1024, 1024), names=('model', None), mesh=None),\n",
-                        "    },\n",
-                        "    W1: Partitioned(value=(1024, 1024), names=(None, 'model'), mesh=None),\n",
-                        "})\n",
+                        "{'Dense_0': {'kernel': Partitioned(value=(1024, 1024), names=(None, 'model'), mesh=None)}, 'W2': Partitioned(value=(1024, 1024), names=('model', None), mesh=None)}\n",
                         "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
                         "(1024, 1024)\n"
                     ]
                 }
             ],
             "source": [
-                "diff = jax.tree_map(\n",
-                "    lambda a, b: a - b, \n",
+                "diff = jax.tree_util.tree_map(\n",
+                "    lambda a, b: a - b,\n",
                 "    initialized_state.params['DotReluDot_0'], initialized_state.params['DotReluDot_0'])\n",
-                "print(jax.tree_map(jnp.shape, diff))\n",
+                "print(jax.tree_util.tree_map(jnp.shape, diff))\n",
                 "diff_array = diff['Dense_0']['kernel'].value\n",
                 "print(type(diff_array))\n",
                 "print(diff_array.shape)"
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## Compile the train step and inference \n",
+                "## Compile the train step and inference\n",
                 "\n",
                 "Create a `jit`ted training step as follows:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 18,
+            "execution_count": 23,
             "metadata": {},
             "outputs": [],
             "source": [
-                "@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding), \n",
+                "@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding),\n",
                 "                   out_shardings=state_sharding)\n",
                 "def train_step(state, x):\n",
                 "  # A fake loss function.\n",
                 "  def loss_unrolled(params):\n",
                 "    y = model.apply({'params': params}, x)\n",
                 "    return y.sum()\n",
                 "  grad_fn = jax.grad(loss_unrolled)\n",
@@ -745,15 +685,15 @@
                 "\n",
                 "with mesh:\n",
                 "  new_state = train_step(initialized_state, x)"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 19,
+            "execution_count": 24,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "Sharding of Weight 1:\n"
@@ -842,15 +782,15 @@
             "metadata": {},
             "source": [
                 "Then, create a compiled inference step. Note that the output is also sharded along `(data, None)`."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 20,
+            "execution_count": 25,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
@@ -889,15 +829,15 @@
                         ]
                     },
                     "metadata": {},
                     "output_type": "display_data"
                 }
             ],
             "source": [
-                "@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding), \n",
+                "@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding),\n",
                 "                   out_shardings=x_sharding)\n",
                 "def apply_fn(state, x):\n",
                 "  return state.apply_fn({'params': state.params}, x)\n",
                 "\n",
                 "with mesh:\n",
                 "  y = apply_fn(new_state, x)\n",
                 "print(type(y))\n",
@@ -913,74 +853,74 @@
                 "## Profiling\n",
                 "\n",
                 "If you are running on a TPU pod or a pod slice, you can use a custom `block_all` utility function, as defined below, to measure the performance:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 21,
+            "execution_count": 26,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
-                        "132 ms \u00b1 6.21 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n"
+                        "20.9 ms \u00b1 319 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n"
                     ]
                 }
             ],
             "source": [
                 "%%timeit\n",
                 "\n",
                 "def block_all(xs):\n",
-                "  jax.tree_map(lambda x: x.block_until_ready(), xs)\n",
+                "  jax.tree_util.tree_map(lambda x: x.block_until_ready(), xs)\n",
                 "  return xs\n",
                 "\n",
                 "with mesh:\n",
                 "  new_state = block_all(train_step(initialized_state, x))"
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Logical axis annotation\n",
                 "\n",
-                "JAX's automatic SPMD encourages users to explore different sharding layouts to find the optimal one. To this end, in Flax you actually can annotate the dimensions of any data with more descriptive axis names (not just device mesh axis names like `'data'` and `'model'`). \n",
+                "JAX's automatic SPMD encourages users to explore different sharding layouts to find the optimal one. To this end, in Flax you actually can annotate the dimensions of any data with more descriptive axis names (not just device mesh axis names like `'data'` and `'model'`).\n",
                 "\n",
                 "The `LogicalDotReluDot` and `LogicalMLP` Module definition below are similar to the Modules you created earlier, except for the following:\n",
                 "\n",
                 "1. All axes are annotated with more concrete, meaningful names, such as `'embed'`, `'hidden'`, `'batch'` and `'layer'`. These names are referred to as _logical axis names_ in Flax. They make the dimensional changes inside model definitions more readable.\n",
                 "\n",
-                "2. [`flax.linen.with_logical_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_logical_partitioning.html) replaces `flax.linen.with_partitioning`; and [`flax.linen.with_logical_constraint`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_logical_constraint.html#flax-linen-with-logical-constraint) replaces `jax.lax.with_sharding_constraint`, to recognize the logical axis names."
+                "2. [`flax.linen.with_logical_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_logical_partitioning) replaces `flax.linen.with_partitioning`; and [`flax.linen.with_logical_constraint`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_logical_constraint) replaces `jax.lax.with_sharding_constraint`, to recognize the logical axis names."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 50,
+            "execution_count": 27,
             "metadata": {},
             "outputs": [],
             "source": [
                 "class LogicalDotReluDot(nn.Module):\n",
                 "  depth: int\n",
                 "  dense_init: Callable = nn.initializers.xavier_normal()\n",
                 "  @nn.compact\n",
-                "  def __call__(self, x):    \n",
-                "    y = nn.Dense(self.depth, \n",
+                "  def __call__(self, x):\n",
+                "    y = nn.Dense(self.depth,\n",
                 "                 kernel_init=nn.with_logical_partitioning(self.dense_init, ('embed', 'hidden')),\n",
                 "                 use_bias=False,  # or overwrite with `bias_init`\n",
                 "                 )(x)\n",
                 "\n",
                 "    y = jax.nn.relu(y)\n",
                 "    # Force a local sharding annotation.\n",
                 "    y = with_sharding_constraint(y, mesh_sharding(PartitionSpec('data', 'model')))\n",
                 "\n",
                 "    W2 = self.param(\n",
-                "        'W2', \n",
+                "        'W2',\n",
                 "        nn.with_logical_partitioning(self.dense_init, ('hidden', 'embed')),\n",
                 "        (self.depth, x.shape[-1]))\n",
                 "\n",
                 "    z = jnp.dot(y, W2)\n",
                 "    # Force a local sharding annotation.\n",
                 "    z = nn.with_logical_constraint(z, ('batch', 'embed'))\n",
                 "    return z, None\n",
@@ -988,15 +928,15 @@
                 "class LogicalMLP(nn.Module):\n",
                 "  num_layers: int\n",
                 "  depth: int\n",
                 "  use_scan: bool\n",
                 "  @nn.compact\n",
                 "  def __call__(self, x):\n",
                 "    if self.use_scan:\n",
-                "      x, _ = nn.scan(LogicalDotReluDot, length=self.num_layers, \n",
+                "      x, _ = nn.scan(LogicalDotReluDot, length=self.num_layers,\n",
                 "                    variable_axes={\"params\": 0},\n",
                 "                    split_rngs={\"params\": True},\n",
                 "                    metadata_params={nn.PARTITION_NAME: 'layer'}\n",
                 "                    )(self.depth)(x)\n",
                 "    else:\n",
                 "      for i in range(self.num_layers):\n",
                 "        x, _ = LogicalDotReluDot(self.depth)(x)\n",
@@ -1006,22 +946,22 @@
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Now, initiate a model and try to figure out what sharding its `state` should have.\n",
                 "\n",
-                "To allow the device mesh to take your model correctly, you need to decide which of these logical axis names are mapped to the device axis `'data'` or `'model'`. This rule is a list of (`logical_axis_name`, `device_axis_name`) tuples, and [`flax.linen.logical_to_mesh_sharding`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.logical_to_mesh_sharding.html#flax-linen-logical-to-mesh-sharding) will convert them to the kind of sharding that the device mesh can understand.\n",
+                "To allow the device mesh to take your model correctly, you need to decide which of these logical axis names are mapped to the device axis `'data'` or `'model'`. This rule is a list of (`logical_axis_name`, `device_axis_name`) tuples, and [`flax.linen.logical_to_mesh_sharding`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.logical_to_mesh_sharding) will convert them to the kind of sharding that the device mesh can understand.\n",
                 "\n",
                 "This allows you to change the rules and try out new partition layouts without modifying the model definition."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 51,
+            "execution_count": 28,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "annotations are logical, not mesh-specific:  PartitionSpec('embed', 'hidden')\n",
@@ -1035,66 +975,66 @@
                 "         ('hidden', 'model'))\n",
                 "\n",
                 "logical_model = LogicalMLP(LAYERS, DEPTH, USE_SCAN)\n",
                 "\n",
                 "logical_abstract_variables = jax.eval_shape(\n",
                 "    functools.partial(init_fn, model=logical_model, optimizer=optimizer), k, x)\n",
                 "logical_state_spec = nn.get_partition_spec(logical_abstract_variables)\n",
-                "print('annotations are logical, not mesh-specific: ', \n",
+                "print('annotations are logical, not mesh-specific: ',\n",
                 "      logical_state_spec.params['LogicalDotReluDot_0']['Dense_0']['kernel'])\n",
                 "\n",
                 "logical_state_sharding = nn.logical_to_mesh_sharding(logical_state_spec, mesh, rules)\n",
-                "print('sharding annotations are mesh-specific: ', \n",
+                "print('sharding annotations are mesh-specific: ',\n",
                 "      logical_state_sharding.params['LogicalDotReluDot_0']['Dense_0']['kernel'].spec)"
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "You can verify that the `logical_state_spec` here has the same content as `state_spec` in the previous (\"non-logical\") example. This allows you to `jax.jit` your Module's [`flax.linen.Module.init`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.init) and [`flax.linen.Module.apply`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.apply) the same way in the above above."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 52,
+            "execution_count": 29,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "True"
                         ]
                     },
-                    "execution_count": 52,
+                    "execution_count": 29,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "state_sharding.params['DotReluDot_0'] == logical_state_sharding.params['LogicalDotReluDot_0']"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 53,
+            "execution_count": 31,
             "metadata": {},
             "outputs": [],
             "source": [
                 "logical_jit_init_fn = jax.jit(init_fn, static_argnums=(2, 3),\n",
-                "                      in_shardings=(mesh_sharding(None), x_sharding),  # PRNG key and x\n",
+                "                      in_shardings=(mesh_sharding(()), x_sharding),  # PRNG key and x\n",
                 "                      out_shardings=logical_state_sharding)\n",
                 "\n",
                 "logical_initialized_state = logical_jit_init_fn(k, x, logical_model, optimizer)"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 54,
+            "execution_count": 32,
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "Sharding of Weight 1:\n"
@@ -1198,23 +1138,33 @@
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "## Save the data\n",
                 "\n",
                 "To save the cross-device array, you can use [`flax.training.checkpoints`](https://flax.readthedocs.io/en/latest/_modules/flax/training/checkpoints.html), as shown in the [Save and load checkpoints guide - Multi-host/multi-process checkpointing](https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#multi-host-multi-process-checkpointing). This is especially required if you are running on a multi-host environment (for example, a TPU pod).\n",
                 "\n",
+                "In practice, you might want to save the raw `jax.Array` pytree as checkpoint, instead of the wrapped `Partitioned` values, to reduce complexity. You can restore it as-is and put it back into an annotated pytree with `flax.linen.meta.replace_boxed()`.\n",
+                "\n",
                 "Keep in mind that to restore the arrays to the desired partition, you need to provide a sample `target` pytree that has the same structure and has the desired [`jax.sharding.Sharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Sharding) in place for each JAX array. The sharding you use to restore the array doesn't necessarily need to be the same as the ones you used to store the array."
             ]
         }
     ],
     "metadata": {
         "jupytext": {
             "formats": "ipynb,md:myst"
         },
         "language_info": {
+            "codemirror_mode": {
+                "name": "ipython",
+                "version": 3
+            },
+            "file_extension": ".py",
+            "mimetype": "text/x-python",
             "name": "python",
-            "version": "3.9.6"
+            "nbconvert_exporter": "python",
+            "pygments_lexer": "ipython3",
+            "version": "3.10.13"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 0
 }
```

### Comparing `flax-0.8.2/docs/guides/parallel_training/flax_on_pjit.md` & `flax-0.8.3/docs/guides/parallel_training/flax_on_pjit.md`

 * *Files 5% similar despite different names*

```diff
@@ -30,63 +30,63 @@
 
 ## Setup
 
 Import some necessary dependencies.
 
 **Note:** This guide uses the `--xla_force_host_platform_device_count=8` flag to emulate multiple devices in a CPU environment in a Google Colab/Jupyter Notebook. You don't need this if you are already using a multi-device TPU environment.
 
-```{code-cell}
+```{code-cell} ipython3
 :tags: [skip-execution]
 
 # Once Flax v0.6.10 is released, there is no need to do this.
 # ! pip3 install -qq "git+https://github.com/google/flax.git@main#egg=flax"
 ```
 
-```{code-cell}
+```{code-cell} ipython3
 import os
 os.environ["XLA_FLAGS"] = '--xla_force_host_platform_device_count=8'
 ```
 
-```{code-cell}
+```{code-cell} ipython3
 import functools
 from typing import Optional, Callable
 
 import numpy as np
 import jax
 from jax import lax, random, numpy as jnp
 
 import flax
 from flax import struct, traverse_util, linen as nn
 from flax.core import freeze, unfreeze
 from flax.training import train_state, checkpoints
 
-import optax # Optax for common losses and optimizers. 
+import optax # Optax for common losses and optimizers.
 ```
 
-```{code-cell}
+```{code-cell} ipython3
 print(f'We have 8 fake JAX devices now: {jax.devices()}')
 ```
 
 The code below shows how to import and set up the JAX-level device API, following JAX's [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) guide:
 
 1. Start a 2x4 device `mesh` (8 devices) using JAX's `mesh_utils.create_device_mesh`. This layout is the same as the one of a [TPU v3-8](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#single_tpu_board).
 
 2. Annotate each axis with a name using the `axis_names` parameter in `jax.sharding.Mesh`. A typical way to annotate axis names is `axis_name=('data', 'model')`, where:
   * `'data'`: the mesh dimension used for data-parallel sharding of the batch dimension of inputs and activations.
   * `'model'`: the mesh dimension used for sharding parameters of the model across devices.
-  
+
 3. Make a simple utility function `mesh_sharding` for generating a sharding object from the mesh and any layout.
 
-```{code-cell}
+```{code-cell} ipython3
 from jax.sharding import Mesh, PartitionSpec, NamedSharding
 from jax.lax import with_sharding_constraint
 from jax.experimental import mesh_utils
 ```
 
-```{code-cell}
+```{code-cell} ipython3
 # Create a mesh and annotate each axis with a name.
 device_mesh = mesh_utils.create_device_mesh((2, 4))
 print(device_mesh)
 
 mesh = Mesh(devices=device_mesh, axis_names=('data', 'model'))
 print(mesh)
 
@@ -96,50 +96,50 @@
 
 ## Define a layer
 
 Before defining a simple model, create an example layer called `DotReluDot` (by subclassing `flax.linen.Module`). The layer creates two parameters `W1` and `W2` for dot product multiplication, and uses the `jax.nn.relu` (ReLU) activation function in-between.
 
 To shard the parameters efficiently, apply the following APIs to annotate the parameters and intermediate variables:
 
-1. Use [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html#flax.linen.with_partitioning) to decorate the initializer function when creating sub-layers or raw parameters.
+1. Use [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) to decorate the initializer function when creating sub-layers or raw parameters.
 
 2. Apply [`jax.lax.with_sharding_constraint`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.with_sharding_constraint.html) (formerly, `pjit.with_sharding_constraint`) to annotate intermediate variables like `y` and `z` to force a particular sharding pattern when the ideal constraint is known.
 
   * This step is optional, but can sometimes help auto-SPMD to partition efficiently. In the example below, the call is not required, because XLA will figure out the same sharding layout for `y` and `z` regardless.
 
-```{code-cell}
+```{code-cell} ipython3
 class DotReluDot(nn.Module):
   depth: int
   dense_init: Callable = nn.initializers.xavier_normal()
   @nn.compact
   def __call__(self, x):
-    
-    y = nn.Dense(self.depth, 
+
+    y = nn.Dense(self.depth,
                  kernel_init=nn.with_partitioning(self.dense_init, (None, 'model')),
                  use_bias=False,  # or overwrite with `bias_init`
                  )(x)
 
     y = jax.nn.relu(y)
     # Force a local sharding annotation.
     y = with_sharding_constraint(y, mesh_sharding(PartitionSpec('data', 'model')))
 
     W2 = self.param(
-        'W2', 
+        'W2',
         nn.with_partitioning(self.dense_init, ('model', None)),
         (self.depth, x.shape[-1]))
-    
+
     z = jnp.dot(y, W2)
     # Force a local sharding annotation.
     z = with_sharding_constraint(z, mesh_sharding(PartitionSpec('data', None)))
 
     # Return a tuple to conform with the API `flax.linen.scan` as shown in the cell below.
     return z, None
 ```
 
-Note that device axis names like `'data'`, `'model'` or `None` are passed into both [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html) and [`jax.lax.with_sharding_constraint`](https://github.com/google/jax/blob/main/jax/_src/pjit.py#L1516) API calls. This refers to how each dimension of this data should be sharded — either across one of the device mesh dimensions, or not sharded at all.
+Note that device axis names like `'data'`, `'model'` or `None` are passed into both [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) and [`jax.lax.with_sharding_constraint`](https://github.com/google/jax/blob/main/jax/_src/pjit.py#L1516) API calls. This refers to how each dimension of this data should be sharded — either across one of the device mesh dimensions, or not sharded at all.
 
 For example:
 
 * When you define `W1` with shape `(x.shape[-1], self.depth)` and annotate as `(None, 'model')`:
 
   * The first dimension (of length `x.shape[-1]`) will be replicated across all devices.
   * The second dimension (of length `self.depth`) will be sharded over the `'model'` axis of the device mesh. This means `W1` will be sharded 4-way on devices `(0, 4)`, `(1, 5)`, `(2, 6)` and `(3, 7)`, on this dimension.
@@ -151,45 +151,45 @@
 
 +++
 
 ## Define a model with `flax.linen.scan` lifted transformation
 
 Having created `DotReluDot`, you can now define the `MLP` model (by subclassing [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module)) as multiple layers of `DotReluDot`.
 
-To replicate identical layers, you can either use [`flax.linen.scan`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.scan.html), or a for-loop:
+To replicate identical layers, you can either use [`flax.linen.scan`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/transformations.html#flax.linen.scan), or a for-loop:
 
 * `flax.linen.scan` can provide faster compilation times.
 * The for-loop can be faster on runtime.
 
-The code below shows how to apply both methods, and default with the for-loop, so that all the parameters are two-dimensional and you can visualize their sharding. 
+The code below shows how to apply both methods, and default with the for-loop, so that all the parameters are two-dimensional and you can visualize their sharding.
 
 The `flax.linen.scan` code is just to show that this API works with [Flax lifted transforms](https://flax.readthedocs.io/en/latest/developer_notes/lift.html#supported-transformations).
 
-```{code-cell}
+```{code-cell} ipython3
 class MLP(nn.Module):
   num_layers: int
   depth: int
   use_scan: bool
   @nn.compact
   def __call__(self, x):
     if self.use_scan:
-      x, _ = nn.scan(DotReluDot, length=self.num_layers, 
+      x, _ = nn.scan(DotReluDot, length=self.num_layers,
                      variable_axes={"params": 0},
                      split_rngs={"params": True},
                      metadata_params={nn.PARTITION_NAME: None}
                      )(self.depth)(x)
     else:
       for i in range(self.num_layers):
         x, _ = DotReluDot(self.depth)(x)
     return x
 ```
 
 Now, create a `model` instance, and a sample input `x`.
 
-```{code-cell}
+```{code-cell} ipython3
 # MLP hyperparameters.
 BATCH, LAYERS, DEPTH, USE_SCAN = 8, 4, 1024, False
 # Create fake inputs.
 x = jnp.ones((BATCH, DEPTH))
 # Initialize a PRNG key.
 k = random.key(0)
 
@@ -203,42 +203,42 @@
 
 Next, you need to tell `jax.jit` how to shard our data across devices.
 
 ### The input's sharding
 
 For data parallelism, you can shard the batched _input_ `x` across the `data` axis by denoting the batch axis as `'data'`. Then, use [`jax.device_put`](https://jax.readthedocs.io/en/latest/_autosummary/jax.device_put.html) to place it onto the correct `device`s.
 
-```{code-cell}
+```{code-cell} ipython3
 x_sharding = mesh_sharding(PartitionSpec('data', None)) # dimensions: (batch, length)
 x = jax.device_put(x, x_sharding)
 jax.debug.visualize_array_sharding(x)
 ```
 
 ### The output's sharding
 
 You need to compile `model.init()` (that is, [`flax.linen.Module.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.init)), and its output as a pytree of parameters. Additionally, you may sometimes need wrap it with a [`flax.training.train_state`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState) to track other variables, such as optimizer states, and that would make the output an even more complex pytree.
 
 To achieve this, luckily, you don't have to hardcode the output's sharding by hand. Instead, you can:
 
 1. Evaluate `model.init` (in this case, a wrapper of it) abstractly using [`jax.eval_shape`](https://jax.readthedocs.io/en/latest/_autosummary/jax.eval_shape.html).
 
-1. Use [`flax.linen.get_sharding`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.get_sharding.html) to automatically generate the `jax.sharding.NamedSharding`.
-   * This step utilizes the [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_partitioning.html) annotations in the earlier definition to generate the correct sharding for the parameters.
+1. Use [`flax.linen.get_sharding`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.get_sharding) to automatically generate the `jax.sharding.NamedSharding`.
+   * This step utilizes the [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) annotations in the earlier definition to generate the correct sharding for the parameters.
 
-```{code-cell}
+```{code-cell} ipython3
 def init_fn(k, x, model, optimizer):
   variables = model.init(k, x) # Initialize the model.
   state = train_state.TrainState.create( # Create a `TrainState`.
     apply_fn=model.apply,
     params=variables['params'],
     tx=optimizer)
   return state
 ```
 
-```{code-cell}
+```{code-cell} ipython3
 # Create an abstract closure to wrap the function before feeding it in
 # because `jax.eval_shape` only takes pytrees as arguments.
 abstract_variables = jax.eval_shape(
     functools.partial(init_fn, model=model, optimizer=optimizer), k, x)
 
 # This `state_sharding` has the same pytree structure as `state`, the output
 # of the `init_fn`.
@@ -248,69 +248,77 @@
 
 ## Compile the code
 
 Now you can apply [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) to your `init_fn`, but with two extra arguments: `in_shardings` and `out_shardings`.
 
 Run it to get the `initialized_state`, in which parameters are sharded exactly as instructed:
 
-```{code-cell}
+```{code-cell} ipython3
 jit_init_fn = jax.jit(init_fn, static_argnums=(2, 3),
-                      in_shardings=(mesh_sharding(None), x_sharding),  # PRNG key and x
+                      in_shardings=(mesh_sharding(()), x_sharding),  # PRNG key and x
                       out_shardings=state_sharding)
 
 initialized_state = jit_init_fn(k, x, model, optimizer)
 
 # for weight, partitioned in initialized_state.params['DotReluDot_0'].items():
 #     print(f'Sharding of {weight}: {partitioned.names}')
 jax.debug.visualize_array_sharding(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value)
 jax.debug.visualize_array_sharding(initialized_state.params['DotReluDot_0']['W2'].value)
 ```
 
 ## Inspect the Module output
 
-Note that in the output of `initialized_state`, the `params` `W1` and `W2` are of type [`flax.linen.Partitioned`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Partitioned.html). This is a wrapper around the actual `jax.Array` that allows Flax to record the axis names associated with it. 
+Note that in the output of `initialized_state`, the `params` `W1` and `W2` are of type [`flax.linen.Partitioned`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.Partitioned). This is a wrapper around the actual `jax.Array` that allows Flax to record the axis names associated with it.
 
-You can access the raw `jax.Array` by adding `.value` when outside `jit`, or by `.unbox()` when inside.
+You can access the raw `jax.Array`s by calling `flax.linen.meta.unbox()` upon the dictionary, or call `.value` upon individual variable. You can also use `flax.linen.meta.replace_boxed()` to change the underlying `jax.Array` without modifying the sharding annotations.
 
-```{code-cell}
+```{code-cell} ipython3
 print(type(initialized_state.params['DotReluDot_0']['Dense_0']['kernel']))
 print(type(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value))
 print(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].names)
 print(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value.shape)
 ```
 
+```{code-cell} ipython3
+# Say for some unknown reason you want to make the whole param tree all-zero
+unboxed_params = nn.meta.unbox(initialized_state.params)
+all_zero = jax.tree.map(jnp.zeros_like, unboxed_params)
+all_zero_params = nn.meta.replace_boxed(initialized_state.params, all_zero)
+assert jnp.sum(nn.meta.unbox(all_zero_params['DotReluDot_0']['Dense_0']['kernel'])) == 0
+```
+
 You can also check the underlying [`jax.sharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html) of each parameter, which is now more internal than `NamedSharding`. Note that numbers like `initialized_state.step` are replicated across all devices.
 
-```{code-cell}
+```{code-cell} ipython3
 initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value.sharding
 ```
 
-```{code-cell}
+```{code-cell} ipython3
 print(initialized_state.step)
 initialized_state.step.sharding
 ```
 
-You can use [`jax.tree_map`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_map.html) to perform mass computation on a dict of boxed params, in the same way as on a dict of JAX arrays.
+You can use [`jax.tree_util.tree_map`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_map.html) to perform mass computation on a dict of boxed params, in the same way as on a dict of JAX arrays.
 
-```{code-cell}
-diff = jax.tree_map(
-    lambda a, b: a - b, 
+```{code-cell} ipython3
+diff = jax.tree_util.tree_map(
+    lambda a, b: a - b,
     initialized_state.params['DotReluDot_0'], initialized_state.params['DotReluDot_0'])
-print(jax.tree_map(jnp.shape, diff))
+print(jax.tree_util.tree_map(jnp.shape, diff))
 diff_array = diff['Dense_0']['kernel'].value
 print(type(diff_array))
 print(diff_array.shape)
 ```
 
-## Compile the train step and inference 
+## Compile the train step and inference
 
 Create a `jit`ted training step as follows:
 
-```{code-cell}
-@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding), 
+```{code-cell} ipython3
+@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding),
                    out_shardings=state_sharding)
 def train_step(state, x):
   # A fake loss function.
   def loss_unrolled(params):
     y = model.apply({'params': params}, x)
     return y.sum()
   grad_fn = jax.grad(loss_unrolled)
@@ -318,25 +326,25 @@
   state = state.apply_gradients(grads=grads)
   return state
 
 with mesh:
   new_state = train_step(initialized_state, x)
 ```
 
-```{code-cell}
+```{code-cell} ipython3
 print(f'Sharding of Weight 1:')
 jax.debug.visualize_array_sharding(initialized_state.params['DotReluDot_0']['Dense_0']['kernel'].value)
 print(f'Sharding of Weight 2:')
 jax.debug.visualize_array_sharding(initialized_state.params['DotReluDot_0']['W2'].value)
 ```
 
 Then, create a compiled inference step. Note that the output is also sharded along `(data, None)`.
 
-```{code-cell}
-@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding), 
+```{code-cell} ipython3
+@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding),
                    out_shardings=x_sharding)
 def apply_fn(state, x):
   return state.apply_fn({'params': state.params}, x)
 
 with mesh:
   y = apply_fn(new_state, x)
 print(type(y))
@@ -345,52 +353,52 @@
 jax.debug.visualize_array_sharding(y)
 ```
 
 ## Profiling
 
 If you are running on a TPU pod or a pod slice, you can use a custom `block_all` utility function, as defined below, to measure the performance:
 
-```{code-cell}
+```{code-cell} ipython3
 %%timeit
 
 def block_all(xs):
-  jax.tree_map(lambda x: x.block_until_ready(), xs)
+  jax.tree_util.tree_map(lambda x: x.block_until_ready(), xs)
   return xs
 
 with mesh:
   new_state = block_all(train_step(initialized_state, x))
 ```
 
 ## Logical axis annotation
 
-JAX's automatic SPMD encourages users to explore different sharding layouts to find the optimal one. To this end, in Flax you actually can annotate the dimensions of any data with more descriptive axis names (not just device mesh axis names like `'data'` and `'model'`). 
+JAX's automatic SPMD encourages users to explore different sharding layouts to find the optimal one. To this end, in Flax you actually can annotate the dimensions of any data with more descriptive axis names (not just device mesh axis names like `'data'` and `'model'`).
 
 The `LogicalDotReluDot` and `LogicalMLP` Module definition below are similar to the Modules you created earlier, except for the following:
 
 1. All axes are annotated with more concrete, meaningful names, such as `'embed'`, `'hidden'`, `'batch'` and `'layer'`. These names are referred to as _logical axis names_ in Flax. They make the dimensional changes inside model definitions more readable.
 
-2. [`flax.linen.with_logical_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_logical_partitioning.html) replaces `flax.linen.with_partitioning`; and [`flax.linen.with_logical_constraint`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.with_logical_constraint.html#flax-linen-with-logical-constraint) replaces `jax.lax.with_sharding_constraint`, to recognize the logical axis names.
+2. [`flax.linen.with_logical_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_logical_partitioning) replaces `flax.linen.with_partitioning`; and [`flax.linen.with_logical_constraint`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_logical_constraint) replaces `jax.lax.with_sharding_constraint`, to recognize the logical axis names.
 
-```{code-cell}
+```{code-cell} ipython3
 class LogicalDotReluDot(nn.Module):
   depth: int
   dense_init: Callable = nn.initializers.xavier_normal()
   @nn.compact
-  def __call__(self, x):    
-    y = nn.Dense(self.depth, 
+  def __call__(self, x):
+    y = nn.Dense(self.depth,
                  kernel_init=nn.with_logical_partitioning(self.dense_init, ('embed', 'hidden')),
                  use_bias=False,  # or overwrite with `bias_init`
                  )(x)
 
     y = jax.nn.relu(y)
     # Force a local sharding annotation.
     y = with_sharding_constraint(y, mesh_sharding(PartitionSpec('data', 'model')))
 
     W2 = self.param(
-        'W2', 
+        'W2',
         nn.with_logical_partitioning(self.dense_init, ('hidden', 'embed')),
         (self.depth, x.shape[-1]))
 
     z = jnp.dot(y, W2)
     # Force a local sharding annotation.
     z = nn.with_logical_constraint(z, ('batch', 'embed'))
     return z, None
@@ -398,64 +406,64 @@
 class LogicalMLP(nn.Module):
   num_layers: int
   depth: int
   use_scan: bool
   @nn.compact
   def __call__(self, x):
     if self.use_scan:
-      x, _ = nn.scan(LogicalDotReluDot, length=self.num_layers, 
+      x, _ = nn.scan(LogicalDotReluDot, length=self.num_layers,
                     variable_axes={"params": 0},
                     split_rngs={"params": True},
                     metadata_params={nn.PARTITION_NAME: 'layer'}
                     )(self.depth)(x)
     else:
       for i in range(self.num_layers):
         x, _ = LogicalDotReluDot(self.depth)(x)
     return x
 ```
 
 Now, initiate a model and try to figure out what sharding its `state` should have.
 
-To allow the device mesh to take your model correctly, you need to decide which of these logical axis names are mapped to the device axis `'data'` or `'model'`. This rule is a list of (`logical_axis_name`, `device_axis_name`) tuples, and [`flax.linen.logical_to_mesh_sharding`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.logical_to_mesh_sharding.html#flax-linen-logical-to-mesh-sharding) will convert them to the kind of sharding that the device mesh can understand.
+To allow the device mesh to take your model correctly, you need to decide which of these logical axis names are mapped to the device axis `'data'` or `'model'`. This rule is a list of (`logical_axis_name`, `device_axis_name`) tuples, and [`flax.linen.logical_to_mesh_sharding`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.logical_to_mesh_sharding) will convert them to the kind of sharding that the device mesh can understand.
 
 This allows you to change the rules and try out new partition layouts without modifying the model definition.
 
-```{code-cell}
+```{code-cell} ipython3
 # Unspecified rule means unsharded by default, so no need to specify `('embed', None)` and `('layer', None)`.
 rules = (('batch', 'data'),
          ('hidden', 'model'))
 
 logical_model = LogicalMLP(LAYERS, DEPTH, USE_SCAN)
 
 logical_abstract_variables = jax.eval_shape(
     functools.partial(init_fn, model=logical_model, optimizer=optimizer), k, x)
 logical_state_spec = nn.get_partition_spec(logical_abstract_variables)
-print('annotations are logical, not mesh-specific: ', 
+print('annotations are logical, not mesh-specific: ',
       logical_state_spec.params['LogicalDotReluDot_0']['Dense_0']['kernel'])
 
 logical_state_sharding = nn.logical_to_mesh_sharding(logical_state_spec, mesh, rules)
-print('sharding annotations are mesh-specific: ', 
+print('sharding annotations are mesh-specific: ',
       logical_state_sharding.params['LogicalDotReluDot_0']['Dense_0']['kernel'].spec)
 ```
 
 You can verify that the `logical_state_spec` here has the same content as `state_spec` in the previous ("non-logical") example. This allows you to `jax.jit` your Module's [`flax.linen.Module.init`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.init) and [`flax.linen.Module.apply`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.apply) the same way in the above above.
 
-```{code-cell}
+```{code-cell} ipython3
 state_sharding.params['DotReluDot_0'] == logical_state_sharding.params['LogicalDotReluDot_0']
 ```
 
-```{code-cell}
+```{code-cell} ipython3
 logical_jit_init_fn = jax.jit(init_fn, static_argnums=(2, 3),
-                      in_shardings=(mesh_sharding(None), x_sharding),  # PRNG key and x
+                      in_shardings=(mesh_sharding(()), x_sharding),  # PRNG key and x
                       out_shardings=logical_state_sharding)
 
 logical_initialized_state = logical_jit_init_fn(k, x, logical_model, optimizer)
 ```
 
-```{code-cell}
+```{code-cell} ipython3
 print(f'Sharding of Weight 1:')
 jax.debug.visualize_array_sharding(logical_initialized_state.params['LogicalDotReluDot_0']['Dense_0']['kernel'].value)
 print(f'Sharding of Weight 2:')
 jax.debug.visualize_array_sharding(logical_initialized_state.params['LogicalDotReluDot_0']['W2'].value)
 ```
 
 ## When to use device axis / logical axis
@@ -470,8 +478,10 @@
 
 +++
 
 ## Save the data
 
 To save the cross-device array, you can use [`flax.training.checkpoints`](https://flax.readthedocs.io/en/latest/_modules/flax/training/checkpoints.html), as shown in the [Save and load checkpoints guide - Multi-host/multi-process checkpointing](https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#multi-host-multi-process-checkpointing). This is especially required if you are running on a multi-host environment (for example, a TPU pod).
 
+In practice, you might want to save the raw `jax.Array` pytree as checkpoint, instead of the wrapped `Partitioned` values, to reduce complexity. You can restore it as-is and put it back into an annotated pytree with `flax.linen.meta.replace_boxed()`.
+
 Keep in mind that to restore the arrays to the desired partition, you need to provide a sample `target` pytree that has the same structure and has the desired [`jax.sharding.Sharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Sharding) in place for each JAX array. The sharding you use to restore the array doesn't necessarily need to be the same as the ones you used to store the array.
```

### Comparing `flax-0.8.2/docs/guides/training_techniques/batch_norm.rst` & `flax-0.8.3/docs/guides/training_techniques/batch_norm.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/training_techniques/dropout.rst` & `flax-0.8.3/docs/guides/training_techniques/dropout.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/training_techniques/lr_schedule.rst` & `flax-0.8.3/docs/guides/training_techniques/lr_schedule.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/training_techniques/transfer_learning.ipynb` & `flax-0.8.3/docs/guides/training_techniques/transfer_learning.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/training_techniques/transfer_learning.md` & `flax-0.8.3/docs/guides/training_techniques/transfer_learning.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/guides/training_techniques/use_checkpointing.ipynb` & `flax-0.8.3/docs/guides/training_techniques/use_checkpointing.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9996611439899996%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(13, '**_Ongoing migration to Orbax:_**\\n'), (24, '\\n')], "*

 * *            "delete: [24, 13]}}, 7: {'source': {insert: [(14, 'state = "*

 * *            'state.apply_gradients(grads=jax.tree_util.tree_map(jnp.ones_like, '*

 * *            "state.params))\\n')], delete: [14]}}, 11: {'source': {insert: [(0, 'Next, to use "*

 * *            'versioning and automatic bookkeeping features, you need to wrap '*

 * *            '`orbax.checkpoint.CheckpointManager` over '*

 * *            "`orbax.checkp […]*

```diff
@@ -15,26 +15,26 @@
                 "*  Support for various array types and storage formats\n",
                 "*  Asynchronous saving to reduce training wait time\n",
                 "*  Versioning and automatic bookkeeping of past checkpoints\n",
                 "*  Flexible [`transformations`](https://github.com/google/orbax/blob/main/docs/checkpoint.md#transformations) to tweak and load old checkpoints\n",
                 "*  [`jax.sharding`](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)-based API to save and load in multi-host scenarios\n",
                 "\n",
                 "---\n",
-                "**_Ongoing migration to Orbax:_** \n",
+                "**_Ongoing migration to Orbax:_**\n",
                 "\n",
                 "After July 30 2023, Flax's legacy `flax.training.checkpoints` API will be deprecated in favor of [Orbax](https://github.com/google/orbax).\n",
                 "\n",
                 "*  **If you are a new Flax user**: Use the new `orbax.checkpoint` API, as demonstrated in this guide.\n",
                 "\n",
                 "*  **If you have legacy `flax.training.checkpoints` code in your project**: Consider the following options:\n",
                 "\n",
                 "   * **Migrating your code to Orbax (Recommended)**: Migrate your API calls to `orbax.checkpoint` API by following this [migration guide](https://flax.readthedocs.io/en/latest/guides/converting_and_upgrading/orbax_upgrade_guide.html).\n",
                 "\n",
                 "   * **Automatically use the Orbax backend**: Add `flax.config.update('flax_use_orbax_checkpointing', True)` to your project, which will let your `flax.training.checkpoints` calls automatically use the Orbax backend to save your checkpoints.\n",
-                "     \n",
+                "\n",
                 "     * **Scheduled flip**: This will become the default mode after **May 2023** (tentative date).\n",
                 "\n",
                 "     * Visit [Orbax-as-backend troubleshooting section](https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting) if you meet any issue in the automatic migration.\n",
                 "---\n",
                 "\n",
                 "For backward-compatibility, this guide shows the Orbax-equivalent calls in the Flax legacy `flax.training.checkpoints` API.\n",
                 "\n",
@@ -182,15 +182,15 @@
                 "# Define your class with `@flax.struct.dataclass` decorator to make it compatible.\n",
                 "tx = optax.sgd(learning_rate=0.001)      # An Optax SGD optimizer.\n",
                 "state = train_state.TrainState.create(\n",
                 "    apply_fn=model.apply,\n",
                 "    params=variables['params'],\n",
                 "    tx=tx)\n",
                 "# Perform a simple gradient update similar to the one during a normal training workflow.\n",
-                "state = state.apply_gradients(grads=jax.tree_map(jnp.ones_like, state.params))\n",
+                "state = state.apply_gradients(grads=jax.tree_util.tree_map(jnp.ones_like, state.params))\n",
                 "\n",
                 "# Some arbitrary nested pytree with a dictionary and a NumPy array.\n",
                 "config = {'dimensions': np.array([5, 3])}\n",
                 "\n",
                 "# Bundle everything together.\n",
                 "ckpt = {'model': state, 'config': config, 'data': [x1]}\n",
                 "ckpt"
@@ -229,15 +229,15 @@
             ]
         },
         {
             "cell_type": "markdown",
             "id": "07d4de1a",
             "metadata": {},
             "source": [
-                "Next, to use versioning and automatic bookkeeping features, you need to wrap `orbax.checkpoint.CheckpointManager` over `orbax.checkpoint.PyTreeCheckpointer`. \n",
+                "Next, to use versioning and automatic bookkeeping features, you need to wrap `orbax.checkpoint.CheckpointManager` over `orbax.checkpoint.PyTreeCheckpointer`.\n",
                 "\n",
                 "In addition, provide `orbax.checkpoint.CheckpointManagerOptions` that customizes your needs, such as how often and on what criteria you prefer old checkpoints be deleted. See [documentation](https://github.com/google/orbax/blob/main/docs/checkpoint.md#checkpointmanager) for a full list of options offered.\n",
                 "\n",
                 "`orbax.checkpoint.CheckpointManager` should be placed at the top-level outside your training steps to manage your saves."
             ]
         },
         {
@@ -403,15 +403,15 @@
             "id": "c7fe3bc8",
             "metadata": {},
             "source": [
                 "### With the legacy API\n",
                 "\n",
                 "Note that with the migration to Orbax in progress, `flax.training.checkpointing.restore_checkpoint` can automatically identify whether a checkpoint is saved in the legacy Flax format or with an Orbax backend, and restore the pytree correctly. Therefore, adding `flax.config.update('flax_use_orbax_checkpointing', True)` won't hurt your ability to restore old checkpoints.\n",
                 "\n",
-                "Here's how to restore checkpoints using the legacy API: "
+                "Here's how to restore checkpoints using the legacy API:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 10,
             "id": "150b20a0",
             "metadata": {
@@ -498,15 +498,15 @@
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "empty_state = train_state.TrainState.create(\n",
                 "    apply_fn=model.apply,\n",
-                "    params=jax.tree_map(np.zeros_like, variables['params']),  # values of the tree leaf doesn't matter\n",
+                "    params=jax.tree_util.tree_map(np.zeros_like, variables['params']),  # values of the tree leaf doesn't matter\n",
                 "    tx=tx,\n",
                 ")\n",
                 "empty_config = {'dimensions': np.array([0, 0])}\n",
                 "target = {'model': empty_state, 'config': empty_config, 'data': [jnp.zeros_like(x1)]}\n",
                 "state_restored = orbax_checkpointer.restore('/tmp/flax_ckpt/orbax/single_save', item=target)\n",
                 "state_restored"
             ]
@@ -683,15 +683,15 @@
             "attachments": {},
             "cell_type": "markdown",
             "id": "d5fa9652",
             "metadata": {},
             "source": [
                 "### Scenario 1: When a reference object is partial\n",
                 "\n",
-                "If your reference object is a subtree of your checkpoint, the restoration will ignore the additional field(s) and restore a checkpoint with the same structure as the reference. \n",
+                "If your reference object is a subtree of your checkpoint, the restoration will ignore the additional field(s) and restore a checkpoint with the same structure as the reference.\n",
                 "\n",
                 "Like in the example below, the `batch_stats` field in `CustomTrainState` was ignored, and the checkpoint was restored as a `TrainState`.\n",
                 "\n",
                 "This can also be useful for reading only part of your checkpoint."
             ]
         },
         {
```

### Comparing `flax-0.8.2/docs/guides/training_techniques/use_checkpointing.md` & `flax-0.8.3/docs/guides/training_techniques/use_checkpointing.md`

 * *Files 1% similar despite different names*

```diff
@@ -19,26 +19,26 @@
 *  Support for various array types and storage formats
 *  Asynchronous saving to reduce training wait time
 *  Versioning and automatic bookkeeping of past checkpoints
 *  Flexible [`transformations`](https://github.com/google/orbax/blob/main/docs/checkpoint.md#transformations) to tweak and load old checkpoints
 *  [`jax.sharding`](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)-based API to save and load in multi-host scenarios
 
 ---
-**_Ongoing migration to Orbax:_** 
+**_Ongoing migration to Orbax:_**
 
 After July 30 2023, Flax's legacy `flax.training.checkpoints` API will be deprecated in favor of [Orbax](https://github.com/google/orbax).
 
 *  **If you are a new Flax user**: Use the new `orbax.checkpoint` API, as demonstrated in this guide.
 
 *  **If you have legacy `flax.training.checkpoints` code in your project**: Consider the following options:
 
    * **Migrating your code to Orbax (Recommended)**: Migrate your API calls to `orbax.checkpoint` API by following this [migration guide](https://flax.readthedocs.io/en/latest/guides/converting_and_upgrading/orbax_upgrade_guide.html).
 
    * **Automatically use the Orbax backend**: Add `flax.config.update('flax_use_orbax_checkpointing', True)` to your project, which will let your `flax.training.checkpoints` calls automatically use the Orbax backend to save your checkpoints.
-     
+
      * **Scheduled flip**: This will become the default mode after **May 2023** (tentative date).
 
      * Visit [Orbax-as-backend troubleshooting section](https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting) if you meet any issue in the automatic migration.
 ---
 
 For backward-compatibility, this guide shows the Orbax-equivalent calls in the Flax legacy `flax.training.checkpoints` API.
 
@@ -99,15 +99,15 @@
 # Define your class with `@flax.struct.dataclass` decorator to make it compatible.
 tx = optax.sgd(learning_rate=0.001)      # An Optax SGD optimizer.
 state = train_state.TrainState.create(
     apply_fn=model.apply,
     params=variables['params'],
     tx=tx)
 # Perform a simple gradient update similar to the one during a normal training workflow.
-state = state.apply_gradients(grads=jax.tree_map(jnp.ones_like, state.params))
+state = state.apply_gradients(grads=jax.tree_util.tree_map(jnp.ones_like, state.params))
 
 # Some arbitrary nested pytree with a dictionary and a NumPy array.
 config = {'dimensions': np.array([5, 3])}
 
 # Bundle everything together.
 ckpt = {'model': state, 'config': config, 'data': [x1]}
 ckpt
@@ -124,15 +124,15 @@
 from flax.training import orbax_utils
 
 orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()
 save_args = orbax_utils.save_args_from_target(ckpt)
 orbax_checkpointer.save('/tmp/flax_ckpt/orbax/single_save', ckpt, save_args=save_args)
 ```
 
-Next, to use versioning and automatic bookkeeping features, you need to wrap `orbax.checkpoint.CheckpointManager` over `orbax.checkpoint.PyTreeCheckpointer`. 
+Next, to use versioning and automatic bookkeeping features, you need to wrap `orbax.checkpoint.CheckpointManager` over `orbax.checkpoint.PyTreeCheckpointer`.
 
 In addition, provide `orbax.checkpoint.CheckpointManagerOptions` that customizes your needs, such as how often and on what criteria you prefer old checkpoints be deleted. See [documentation](https://github.com/google/orbax/blob/main/docs/checkpoint.md#checkpointmanager) for a full list of options offered.
 
 `orbax.checkpoint.CheckpointManager` should be placed at the top-level outside your training steps to manage your saves.
 
 ```python outputId="b7132933-566d-440d-c34e-c5468d87cbdc"
 options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=2, create=True)
@@ -180,15 +180,15 @@
 checkpoint_manager.restore(step)
 ```
 
 ### With the legacy API
 
 Note that with the migration to Orbax in progress, `flax.training.checkpointing.restore_checkpoint` can automatically identify whether a checkpoint is saved in the legacy Flax format or with an Orbax backend, and restore the pytree correctly. Therefore, adding `flax.config.update('flax_use_orbax_checkpointing', True)` won't hurt your ability to restore old checkpoints.
 
-Here's how to restore checkpoints using the legacy API: 
+Here's how to restore checkpoints using the legacy API:
 
 ```python outputId="85ffceca-f38d-46b8-e567-d9d38b7885f9"
 raw_restored = checkpoints.restore_checkpoint(ckpt_dir='/tmp/flax_ckpt/flax-checkpointing', target=None)
 raw_restored
 ```
 
 ## Restore with custom dataclasses
@@ -202,15 +202,15 @@
 This section demonstrates how to set up any custom Flax dataclass explicitly, and have the same structure as a saved checkpoint.
 
 Note: Data that was a JAX NumPy array (`jnp.array`) format will be restored as a NumPy array (`numpy.array`). This would not affect your work because JAX will [automatically convert](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html) NumPy arrays to JAX arrays once the computation starts.
 
 ```python outputId="110c6b6e-fe42-4179-e5d8-6b92d355e11b"
 empty_state = train_state.TrainState.create(
     apply_fn=model.apply,
-    params=jax.tree_map(np.zeros_like, variables['params']),  # values of the tree leaf doesn't matter
+    params=jax.tree_util.tree_map(np.zeros_like, variables['params']),  # values of the tree leaf doesn't matter
     tx=tx,
 )
 empty_config = {'dimensions': np.array([0, 0])}
 target = {'model': empty_state, 'config': empty_config, 'data': [jnp.zeros_like(x1)]}
 state_restored = orbax_checkpointer.restore('/tmp/flax_ckpt/orbax/single_save', item=target)
 state_restored
 ```
@@ -263,15 +263,15 @@
 It is recommended to keep your checkpoints up-to-date with your pytree dataclass definitions. However, you might be forced to restore the checkpoints with incompatible reference objects at runtime. When this happens, the checkpoint restoration will try to respect the structure of the reference when given.
 
 Below are examples of a few common scenarios.
 
 
 ### Scenario 1: When a reference object is partial
 
-If your reference object is a subtree of your checkpoint, the restoration will ignore the additional field(s) and restore a checkpoint with the same structure as the reference. 
+If your reference object is a subtree of your checkpoint, the restoration will ignore the additional field(s) and restore a checkpoint with the same structure as the reference.
 
 Like in the example below, the `batch_stats` field in `CustomTrainState` was ignored, and the checkpoint was restored as a `TrainState`.
 
 This can also be useful for reading only part of your checkpoint.
 
 ```python
 restored = checkpoint_manager.restore(5, items=target)
```

### Comparing `flax-0.8.2/docs/index.rst` & `flax-0.8.3/docs/index.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/linen_intro.ipynb` & `flax-0.8.3/docs/linen_intro.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/linen_intro.md` & `flax-0.8.3/docs/linen_intro.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/philosophy.md` & `flax-0.8.3/docs/philosophy.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/quick_start.ipynb` & `flax-0.8.3/docs/quick_start.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/docs/quick_start.md` & `flax-0.8.3/docs/quick_start.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/README.md` & `flax-0.8.3/examples/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/__init__.py` & `flax-0.8.3/examples/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/cloud/README.md` & `flax-0.8.3/examples/cloud/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/cloud/launch_gce.py` & `flax-0.8.3/examples/cloud/launch_gce.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/cloud/startup_script.sh` & `flax-0.8.3/examples/cloud/startup_script.sh`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/README.md` & `flax-0.8.3/examples/imagenet/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/configs/default.py` & `flax-0.8.3/examples/imagenet/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/configs/fake_data_benchmark.py` & `flax-0.8.3/examples/imagenet/configs/fake_data_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/configs/tpu.py` & `flax-0.8.3/examples/imagenet/configs/tpu.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/configs/v100_x8.py` & `flax-0.8.3/examples/imagenet/configs/v100_x8.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/configs/v100_x8_mixed_precision.py` & `flax-0.8.3/examples/imagenet/configs/v100_x8_mixed_precision.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/imagenet.ipynb` & `flax-0.8.3/examples/imagenet/imagenet.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/imagenet_benchmark.py` & `flax-0.8.3/examples/imagenet/imagenet_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/imagenet_fake_data_benchmark.py` & `flax-0.8.3/examples/imagenet/imagenet_fake_data_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/input_pipeline.py` & `flax-0.8.3/examples/imagenet/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/main.py` & `flax-0.8.3/examples/imagenet/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/models.py` & `flax-0.8.3/examples/imagenet/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/models_test.py` & `flax-0.8.3/examples/imagenet/models_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/train.py` & `flax-0.8.3/examples/imagenet/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/imagenet/train_test.py` & `flax-0.8.3/examples/imagenet/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/linen_design_test/attention_simple.py` & `flax-0.8.3/examples/linen_design_test/attention_simple.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/linen_design_test/autoencoder.py` & `flax-0.8.3/examples/linen_design_test/autoencoder.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/linen_design_test/dense.py` & `flax-0.8.3/examples/linen_design_test/dense.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/linen_design_test/linear_regression.py` & `flax-0.8.3/examples/linen_design_test/linear_regression.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/linen_design_test/mlp_explicit.py` & `flax-0.8.3/examples/linen_design_test/mlp_explicit.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/linen_design_test/mlp_inline.py` & `flax-0.8.3/examples/linen_design_test/mlp_inline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/linen_design_test/mlp_lazy.py` & `flax-0.8.3/examples/linen_design_test/mlp_lazy.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/README.md` & `flax-0.8.3/examples/lm1b/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/configs/default.py` & `flax-0.8.3/examples/lm1b/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/input_pipeline.py` & `flax-0.8.3/examples/lm1b/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/input_pipeline_test.py` & `flax-0.8.3/examples/lm1b/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/main.py` & `flax-0.8.3/examples/lm1b/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/models.py` & `flax-0.8.3/examples/lm1b/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/temperature_sampler.py` & `flax-0.8.3/examples/lm1b/temperature_sampler.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/temperature_sampler_test.py` & `flax-0.8.3/examples/lm1b/temperature_sampler_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/tokenizer.py` & `flax-0.8.3/examples/lm1b/tokenizer.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/train.py` & `flax-0.8.3/examples/lm1b/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/train_test.py` & `flax-0.8.3/examples/lm1b/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/lm1b/utils.py` & `flax-0.8.3/examples/lm1b/utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/mnist/README.md` & `flax-0.8.3/examples/mnist/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/mnist/configs/default.py` & `flax-0.8.3/examples/mnist/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/mnist/main.py` & `flax-0.8.3/examples/mnist/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/mnist/mnist.ipynb` & `flax-0.8.3/examples/mnist/mnist.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/mnist/mnist_benchmark.py` & `flax-0.8.3/examples/mnist/mnist_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/mnist/train.py` & `flax-0.8.3/examples/mnist/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/mnist/train_test.py` & `flax-0.8.3/examples/mnist/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/nlp_seq/README.md` & `flax-0.8.3/examples/nlp_seq/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/nlp_seq/input_pipeline.py` & `flax-0.8.3/examples/nlp_seq/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/nlp_seq/input_pipeline_test.py` & `flax-0.8.3/examples/nlp_seq/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/nlp_seq/models.py` & `flax-0.8.3/examples/nlp_seq/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/nlp_seq/train.py` & `flax-0.8.3/examples/nlp_seq/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/README.md` & `flax-0.8.3/examples/ogbg_molpcba/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/configs/default.py` & `flax-0.8.3/examples/ogbg_molpcba/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/configs/default_graph_net.py` & `flax-0.8.3/examples/ogbg_molpcba/configs/default_graph_net.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/configs/hparam_sweep.py` & `flax-0.8.3/examples/ogbg_molpcba/configs/hparam_sweep.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/configs/test.py` & `flax-0.8.3/examples/ogbg_molpcba/configs/test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/input_pipeline.py` & `flax-0.8.3/examples/ogbg_molpcba/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/input_pipeline_test.py` & `flax-0.8.3/examples/ogbg_molpcba/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/main.py` & `flax-0.8.3/examples/ogbg_molpcba/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/models.py` & `flax-0.8.3/examples/ogbg_molpcba/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/models_test.py` & `flax-0.8.3/examples/ogbg_molpcba/models_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/ogbg_molpcba.ipynb` & `flax-0.8.3/examples/ogbg_molpcba/ogbg_molpcba.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py` & `flax-0.8.3/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/train.py` & `flax-0.8.3/examples/ogbg_molpcba/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ogbg_molpcba/train_test.py` & `flax-0.8.3/examples/ogbg_molpcba/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ppo/README.md` & `flax-0.8.3/examples/ppo/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ppo/agent.py` & `flax-0.8.3/examples/ppo/agent.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ppo/configs/default.py` & `flax-0.8.3/examples/ppo/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ppo/env_utils.py` & `flax-0.8.3/examples/ppo/env_utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ppo/models.py` & `flax-0.8.3/examples/ppo/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ppo/ppo_lib.py` & `flax-0.8.3/examples/ppo/ppo_lib.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ppo/ppo_lib_test.py` & `flax-0.8.3/examples/ppo/ppo_lib_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ppo/ppo_main.py` & `flax-0.8.3/examples/ppo/ppo_main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ppo/seed_rl_atari_preprocessing.py` & `flax-0.8.3/examples/ppo/seed_rl_atari_preprocessing.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/ppo/test_episodes.py` & `flax-0.8.3/examples/ppo/test_episodes.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/seq2seq/README.md` & `flax-0.8.3/examples/seq2seq/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/seq2seq/input_pipeline.py` & `flax-0.8.3/examples/seq2seq/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/seq2seq/models.py` & `flax-0.8.3/examples/seq2seq/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/seq2seq/seq2seq.ipynb` & `flax-0.8.3/examples/seq2seq/seq2seq.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/seq2seq/train.py` & `flax-0.8.3/examples/seq2seq/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/seq2seq/train_test.py` & `flax-0.8.3/examples/seq2seq/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/README.md` & `flax-0.8.3/examples/sst2/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/build_vocabulary.py` & `flax-0.8.3/examples/sst2/build_vocabulary.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/configs/default.py` & `flax-0.8.3/examples/sst2/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/input_pipeline.py` & `flax-0.8.3/examples/sst2/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/input_pipeline_test.py` & `flax-0.8.3/examples/sst2/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/main.py` & `flax-0.8.3/examples/sst2/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/models.py` & `flax-0.8.3/examples/sst2/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/models_test.py` & `flax-0.8.3/examples/sst2/models_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/sst2.ipynb` & `flax-0.8.3/examples/sst2/sst2.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/train.py` & `flax-0.8.3/examples/sst2/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/train_test.py` & `flax-0.8.3/examples/sst2/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/vocab.txt` & `flax-0.8.3/examples/sst2/vocab.txt`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/sst2/vocabulary.py` & `flax-0.8.3/examples/sst2/vocabulary.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/vae/README.md` & `flax-0.8.3/examples/vae/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/vae/configs/default.py` & `flax-0.8.3/examples/vae/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/vae/input_pipeline.py` & `flax-0.8.3/examples/vae/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/vae/main.py` & `flax-0.8.3/examples/vae/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/vae/models.py` & `flax-0.8.3/examples/vae/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/vae/reconstruction.png` & `flax-0.8.3/examples/vae/reconstruction.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/vae/sample.png` & `flax-0.8.3/examples/vae/sample.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/vae/train.py` & `flax-0.8.3/examples/vae/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/vae/utils.py` & `flax-0.8.3/examples/vae/utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/wmt/README.md` & `flax-0.8.3/examples/wmt/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/wmt/bleu.py` & `flax-0.8.3/examples/wmt/bleu.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/wmt/configs/default.py` & `flax-0.8.3/examples/wmt/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/wmt/decode.py` & `flax-0.8.3/examples/wmt/decode.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/wmt/input_pipeline.py` & `flax-0.8.3/examples/wmt/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/wmt/input_pipeline_test.py` & `flax-0.8.3/examples/wmt/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/wmt/main.py` & `flax-0.8.3/examples/wmt/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/wmt/models.py` & `flax-0.8.3/examples/wmt/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/wmt/tokenizer.py` & `flax-0.8.3/examples/wmt/tokenizer.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/wmt/train.py` & `flax-0.8.3/examples/wmt/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/examples/wmt/train_test.py` & `flax-0.8.3/examples/wmt/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/__init__.py` & `flax-0.8.3/flax/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/configurations.py` & `flax-0.8.3/flax/configurations.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/__init__.py` & `flax-0.8.3/flax/core/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/axes_scan.py` & `flax-0.8.3/flax/core/axes_scan.py`

 * *Files 10% similar despite different names*

```diff
@@ -36,14 +36,15 @@
 def scan(
   fn: Callable[..., Any],
   in_axes: Any,
   out_axes: Any,
   length: Optional[int] = None,
   reverse: bool = False,
   unroll: int = 1,
+  _split_transpose: bool = False
 ):
   """A wrapper around `jax.lax.scan` with in_axes/out_axes api.
 
   Example::
     def body_fn(b, c, x):
       return b + 2, c + 1, 2 * x
 
@@ -70,14 +71,16 @@
       Use `broadcast` if a return value should not be concatenated and
       is independent of the loop body.
     length: number of iterations. Only needs to be specified if there
       is no scan axis from which it can be derived.
     reverse: scan in reverse order from end to start.
     unroll: how many scan iterations to unroll within a single
       iteration of a loop (default: 1).
+    _split_transpose: An experimental feature to split the transpose of scan
+       into a scan and a map, backed by an experimental Jax lax.scan() feature.
   Returns:
      the function that performs the scan of the form:
      (broadcast_in, carry_in, *args) -> (broadcast_out, carry_out, scan_out).
   """
 
   def transpose_to_front(ax, xs):
     if ax is broadcast:
@@ -154,17 +157,23 @@
           'broadcasted variable has a data dependency on the scan body.'
         )
       out_flat.append(const)
     broadcast_in, constants_out = jax.tree_util.tree_unflatten(
       out_tree(), out_flat
     )
 
-    c, ys = lax.scan(
-      body_fn, init, xs, length=length, reverse=reverse, unroll=unroll
-    )
+    if jax.version.__version_info__ > (0, 4, 25):
+      c, ys = lax.scan(
+        body_fn, init, xs, length=length, reverse=reverse, unroll=unroll,
+        _split_transpose=_split_transpose
+      )
+    else:
+      c, ys = lax.scan(
+        body_fn, init, xs, length=length, reverse=reverse, unroll=unroll
+      )
     ys = jax.tree_util.tree_map(transpose_from_front, out_axes, ys)
     ys = jax.tree_util.tree_map(
       lambda ax, const, y: (const if ax is broadcast else y),
       out_axes,
       constants_out,
       ys,
     )
```

### Comparing `flax-0.8.2/flax/core/flax_functional_engine.ipynb` & `flax-0.8.3/flax/core/flax_functional_engine.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/frozen_dict.py` & `flax-0.8.3/flax/core/frozen_dict.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/lift.py` & `flax-0.8.3/flax/core/lift.py`

 * *Files 0% similar despite different names*

```diff
@@ -868,14 +868,15 @@
   variable_carry: CollectionFilter = False,
   split_rngs: Mapping[PRNGSequenceFilter, bool] = {},
   in_axes=0,
   out_axes=0,
   length: Optional[int] = None,
   reverse: bool = False,
   unroll: int = 1,
+  _split_transpose: bool = False,
   data_transform: Optional[Callable[..., Any]] = None,
   metadata_params: Dict[Any, Any] = {},
 ) -> Callable[..., Any]:
   """A lifted version of ``jax.lax.scan``.
 
   See ``jax.lax.scan`` for the unlifted scan in Jax.
 
@@ -931,14 +932,16 @@
     out_axes: Specifies the axis to scan over for the return value. Should be a
       prefix tree of the return value.
     length: Specifies the number of loop iterations. This only needs
       to be specified if it cannot be derived from the scan arguments.
     reverse: If true, scan from end to start in reverse order.
     unroll: how many scan iterations to unroll within a single
       iteration of a loop (default: 1).
+    _split_transpose: An experimental feature to split the transpose of a scan
+       into a scan and a map, backed by an experimental Jax lax.scan() feature.
     data_transform: optional function to transform raw variable and rng groups,
       intended for inline SPMD annotations.
     metadata_params: arguments dict passed to AxisMetadata instances in the
       variable tree.
 
   Returns:
     The scan function with the signature
@@ -989,14 +992,15 @@
     @functools.partial(
         axes_scan.scan,
         in_axes=(variable_in_axes, rng_axes, in_axes),
         out_axes=(out_axes, variable_out_axes),
         length=length,
         reverse=reverse,
         unroll=unroll,
+        _split_transpose=_split_transpose
     )
     def scanned(broadcast_vars, carry, scan_variable_groups, rng_groups, args):
       carry_vars, c = carry
       variable_groups = (broadcast_vars, carry_vars) + scan_variable_groups
       if data_transform is not None:
         variable_groups, rng_groups = data_transform(
             variable_groups, rng_groups
```

### Comparing `flax-0.8.2/flax/core/meta.py` & `flax-0.8.3/flax/core/meta.py`

 * *Files 1% similar despite different names*

```diff
@@ -24,15 +24,15 @@
 import abc
 import functools
 from typing import Any, Callable, Dict, Generic, Optional, TypeVar
 
 from flax import errors, struct
 from flax.typing import LogicalNames
 import jax
-from jax.experimental import maps
+from jax.interpreters import pxla
 
 A = TypeVar('A')
 B = TypeVar('B')
 TAxisMetadata = TypeVar('TAxisMetadata', bound='AxisMetadata[Any]')
 
 
 class AxisMetadata(Generic[A], metaclass=abc.ABCMeta):
@@ -174,17 +174,17 @@
   )
 
 
 PARTITION_NAME = 'partition_name'
 
 
 def _global_mesh_defined() -> bool:
-  """Checks if global xmap/pjit mesh resource environment is defined."""
-  maps_env = maps.thread_resources.env
-  return maps_env.physical_mesh.devices.shape != ()  # pylint: disable=g-explicit-bool-comparison
+  """Checks if global mesh resource environment is defined."""
+  env = pxla.thread_resources.env
+  return env.physical_mesh.devices.shape != ()  # pylint: disable=g-explicit-bool-comparison
 
 
 class Partitioned(struct.PyTreeNode, AxisMetadata[A]):
   """Wrapper for partitioning metadata.
 
   ``Partitioned`` is used to extend variables with partitioning information
   required for ``jax.experimental.pjit``.
```

### Comparing `flax-0.8.2/flax/core/nn/__init__.py` & `flax-0.8.3/flax/core/nn/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/nn/attention.py` & `flax-0.8.3/flax/core/nn/attention.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/nn/linear.py` & `flax-0.8.3/flax/core/nn/linear.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/nn/normalization.py` & `flax-0.8.3/flax/core/nn/normalization.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/nn/stochastic.py` & `flax-0.8.3/flax/core/nn/stochastic.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/partial_eval.py` & `flax-0.8.3/flax/core/partial_eval.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/scope.py` & `flax-0.8.3/flax/core/scope.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/tracers.py` & `flax-0.8.3/flax/core/tracers.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/core/variables.py` & `flax-0.8.3/flax/core/variables.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/cursor.py` & `flax-0.8.3/flax/cursor.py`

 * *Files 1% similar despite different names*

```diff
@@ -225,16 +225,17 @@
     parent._changes[key] = Cursor(value, self._parent_key)
     return parent._root.build()
 
   def build(self) -> A:
     """Create and return a copy of the original object with accumulated changes.
     This method is to be called after making changes to the Cursor object.
 
-    NOTE: The new object is built bottom-up, the changes will be first applied
-    to the leaf nodes, and then its parent, all the way up to the root.
+    .. note::
+      The new object is built bottom-up, the changes will be first applied
+      to the leaf nodes, and then its parent, all the way up to the root.
 
     Example::
 
       >>> from flax.cursor import cursor
       >>> from flax.training import train_state
       >>> import optax
 
@@ -296,27 +297,26 @@
     The ``update_fn`` has a function signature of ``(str, Any) -> Any``:
 
     - The input arguments are the current key path (in the form of a string delimited
       by ``'/'``) and value at that current key path
     - The output is the new value (either modified by the ``update_fn`` or same as the
       input value if the condition wasn't fulfilled)
 
-    NOTES:
-
-    - If the ``update_fn`` returns a modified value, this method will not recurse any further
-      down that branch to record changes. For example, if we intend to replace an attribute that points
-      to a dictionary with an int, we don't need to look for further changes inside the dictionary,
-      since the dictionary will be replaced anyways.
-    - The ``is`` operator is used to determine whether the return value is modified (by comparing it
-      to the input value). Therefore if the ``update_fn`` modifies a mutable container (e.g. lists,
-      dicts, etc.) and returns the same container, ``.apply_update`` will treat the returned value as
-      unmodified as it contains the same ``id``. To avoid this, return a copy of the modified value.
-    - ``.apply_update`` WILL NOT call the ``update_fn`` to the value at the top-most level of
-      the pytree (i.e. the root node). The ``update_fn`` will first be called on the root node's
-      children, and then the pytree traversal will continue recursively from there.
+    .. note::
+      - If the ``update_fn`` returns a modified value, this method will not recurse any further
+        down that branch to record changes. For example, if we intend to replace an attribute that points
+        to a dictionary with an int, we don't need to look for further changes inside the dictionary,
+        since the dictionary will be replaced anyways.
+      - The ``is`` operator is used to determine whether the return value is modified (by comparing it
+        to the input value). Therefore if the ``update_fn`` modifies a mutable container (e.g. lists,
+        dicts, etc.) and returns the same container, ``.apply_update`` will treat the returned value as
+        unmodified as it contains the same ``id``. To avoid this, return a copy of the modified value.
+      - ``.apply_update`` WILL NOT call the ``update_fn`` to the value at the top-most level of
+        the pytree (i.e. the root node). The ``update_fn`` will first be called on the root node's
+        children, and then the pytree traversal will continue recursively from there.
 
     Example::
 
       >>> import flax.linen as nn
       >>> from flax.cursor import cursor
       >>> import jax, jax.numpy as jnp
 
@@ -392,21 +392,20 @@
     - The output is a boolean, denoting whether to return the child Cursor object at this path
 
     Raises a :meth:`CursorFindError <flax.errors.CursorFindError>` if no object or more
     than one object is found that fulfills the condition of the ``cond_fn``. We raise an
     error because the user should always expect this method to return the only object whose
     corresponding key path and value fulfill the condition of the ``cond_fn``.
 
-    NOTES:
-
-    - If the ``cond_fn`` evaluates to True at a particular key path, this method will not recurse
-      any further down that branch; i.e. this method will find and return the "earliest" child node
-      that fulfills the condition in ``cond_fn`` in a particular key path
-    - ``.find`` WILL NOT search the the value at the top-most level of the pytree (i.e. the root
-      node). The ``cond_fn`` will be evaluated recursively, starting at the root node's children.
+    .. note::
+      - If the ``cond_fn`` evaluates to True at a particular key path, this method will not recurse
+        any further down that branch; i.e. this method will find and return the "earliest" child node
+        that fulfills the condition in ``cond_fn`` in a particular key path
+      - ``.find`` WILL NOT search the the value at the top-most level of the pytree (i.e. the root
+        node). The ``cond_fn`` will be evaluated recursively, starting at the root node's children.
 
     Example::
 
       >>> import flax.linen as nn
       >>> from flax.cursor import cursor
       >>> import jax, jax.numpy as jnp
 
@@ -481,21 +480,20 @@
     """Traverse the Cursor object and return a generator of child Cursor objects that fulfill the
     conditions in the ``cond_fn``. The ``cond_fn`` has a function signature of ``(str, Any) -> bool``:
 
     - The input arguments are the current key path (in the form of a string delimited
       by ``'/'``) and value at that current key path
     - The output is a boolean, denoting whether to return the child Cursor object at this path
 
-    NOTES:
-
-    - If the ``cond_fn`` evaluates to True at a particular key path, this method will not recurse
-      any further down that branch; i.e. this method will find and return the "earliest" child nodes
-      that fulfill the condition in ``cond_fn`` in a particular key path
-    - ``.find_all`` WILL NOT search the the value at the top-most level of the pytree (i.e. the root
-      node). The ``cond_fn`` will be evaluated recursively, starting at the root node's children.
+    .. note::
+      - If the ``cond_fn`` evaluates to True at a particular key path, this method will not recurse
+        any further down that branch; i.e. this method will find and return the "earliest" child nodes
+        that fulfill the condition in ``cond_fn`` in a particular key path
+      - ``.find_all`` WILL NOT search the the value at the top-most level of the pytree (i.e. the root
+        node). The ``cond_fn`` will be evaluated recursively, starting at the root node's children.
 
     Example::
 
       >>> import flax.linen as nn
       >>> from flax.cursor import cursor
       >>> import jax, jax.numpy as jnp
```

### Comparing `flax-0.8.2/flax/errors.py` & `flax-0.8.3/flax/errors.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/__init__.py` & `flax-0.8.3/flax/experimental/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/.gitignore` & `flax-0.8.3/flax/experimental/nnx/.gitignore`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/__init__.py` & `flax-0.8.3/flax/experimental/nnx/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -15,26 +15,35 @@
 from flax.linen.pooling import avg_pool as avg_pool
 from flax.linen.pooling import max_pool as max_pool
 from flax.linen.pooling import min_pool as min_pool
 from flax.linen.pooling import pool as pool
 from flax.typing import Initializer as Initializer
 
 from .nnx import compatibility as compatibility
-from .nnx import graph_utils as graph_utils
-from .nnx.errors import TraceContextError as TraceContextError
+from .nnx import graph as graph
+from .nnx import errors as errors
+from .nnx import errors as helpers
 from .nnx.filterlib import All as All
 from .nnx.filterlib import Not as Not
-from .nnx.graph_utils import GraphDef as GraphDef
+from .nnx.graph import GraphDef as GraphDef
+from .nnx.graph import GraphNode as GraphNode
 from .nnx.helpers import Dict as Dict
-from .nnx.helpers import Sequence as Sequence
+from .nnx.helpers import List as List
+from .nnx.helpers import Sequential as Sequential
 from .nnx.helpers import TrainState as TrainState
-from .nnx.module import GraphDef as GraphDef
 from .nnx.module import M as M
 from .nnx.module import Module as Module
-from .nnx.module import merge as merge
+from .nnx.graph import merge as merge
+from .nnx.graph import UpdateContext as UpdateContext
+from .nnx.graph import split as split
+from .nnx.graph import update as update
+from .nnx.graph import clone as clone
+from .nnx.graph import pop as pop
+from .nnx.graph import state as state
+from .nnx.graph import graphdef as graphdef
 from .nnx.nn import initializers as initializers
 from .nnx.nn.activations import celu as celu
 from .nnx.nn.activations import elu as elu
 from .nnx.nn.activations import gelu as gelu
 from .nnx.nn.activations import glu as glu
 from .nnx.nn.activations import hard_sigmoid as hard_sigmoid
 from .nnx.nn.activations import hard_silu as hard_silu
@@ -61,42 +70,52 @@
 from .nnx.nn.attention import dot_product_attention as dot_product_attention
 from .nnx.nn.attention import make_attention_mask as make_attention_mask
 from .nnx.nn.attention import make_causal_mask as make_causal_mask
 from .nnx.nn.linear import Conv as Conv
 from .nnx.nn.linear import Embed as Embed
 from .nnx.nn.linear import Linear as Linear
 from .nnx.nn.linear import LinearGeneral as LinearGeneral
+from .nnx.nn.linear import Einsum as Einsum
 from .nnx.nn.normalization import BatchNorm as BatchNorm
 from .nnx.nn.normalization import LayerNorm as LayerNorm
 from .nnx.nn.normalization import RMSNorm as RMSNorm
 from .nnx.nn.stochastic import Dropout as Dropout
-from .nnx.pytreelib import Pytree as Pytree
-from .nnx.pytreelib import TreeNode as TreeNode
 from .nnx.rnglib import Rngs as Rngs
 from .nnx.rnglib import RngStream as RngStream
+from .nnx.rnglib import RngState as RngState
+from .nnx.rnglib import RngKey as RngKey
+from .nnx.rnglib import RngCount as RngCount
+from .nnx.rnglib import fork as fork
 from .nnx.spmd import PARTITION_NAME as PARTITION_NAME
 from .nnx.spmd import get_partition_spec as get_partition_spec
 from .nnx.spmd import get_named_sharding as get_named_sharding
 from .nnx.spmd import with_partitioning as with_partitioning
 from .nnx.spmd import with_sharding_constraint as with_sharding_constraint
 from .nnx.state import State as State
-from .nnx.transforms import JIT as JIT
+from .nnx.training import metrics as metrics
+from .nnx.training import optimizer as optimizer
+from .nnx.training.metrics import Metric as Metric
+from .nnx.training.metrics import MultiMetric as MultiMetric
+from .nnx.training.optimizer import Optimizer as Optimizer
+from .nnx.transforms import Jit as Jit
 from .nnx.transforms import Remat as Remat
 from .nnx.transforms import Scan as Scan
 from .nnx.transforms import Vmap as Vmap
 from .nnx.transforms import grad as grad
 from .nnx.transforms import jit as jit
 from .nnx.transforms import remat as remat
 from .nnx.transforms import scan as scan
 from .nnx.transforms import value_and_grad as value_and_grad
 from .nnx.transforms import vmap as vmap
+from .nnx.transforms import eval_shape as eval_shape
 from .nnx.variables import EMPTY as EMPTY
 from .nnx.variables import A as A
 from .nnx.variables import BatchStat as BatchStat
 from .nnx.variables import Cache as Cache
 from .nnx.variables import Empty as Empty
 from .nnx.variables import Intermediate as Intermediate
 from .nnx.variables import Param as Param
-from .nnx.variables import Rng as Rng
 from .nnx.variables import Variable as Variable
+from .nnx.variables import VariableState as VariableState
 from .nnx.variables import VariableMetadata as VariableMetadata
 from .nnx.variables import with_metadata as with_metadata
+from .nnx.visualization import display as display
```

### Comparing `flax-0.8.2/flax/experimental/nnx/docs/demo.ipynb` & `flax-0.8.3/flax/experimental/nnx/docs/demo.ipynb`

 * *Files 8% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9816993109403824%*

 * *Differences: {"'cells'": "{3: {'execution_count': 7, 'outputs': {0: {'text': {insert: [(3, '            "*

 * *            "in_features=4,\\n'), (4, '            out_features=4,\\n'), (5, '            "*

 * *            'use_bias=True,\\n\'), (6, \'            dtype=None,\\n\'), (7, "            '*

 * *            'param_dtype=<class \'jax.numpy.float32\'>,\\n"), (8, \'            '*

 * *            "precision=None,\\n'), (9, '            kernel_init=<function "*

 * *            "variance_scaling.<locals>.init at 0x28ae86dc0>,\\n'), (10, '      […]*

```diff
@@ -26,107 +26,107 @@
             "metadata": {},
             "source": [
                 "### [1] NNX is Pythonic"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 2,
+            "execution_count": 7,
             "id": "d99b73af",
             "metadata": {
                 "outputId": "d8ef66d5-6866-4d5c-94c2-d22512bfe718"
             },
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "model = MLP(\n",
                         "  blocks=[Block(\n",
                         "      linear=Linear(\n",
-                        "        in_features=4,\n",
-                        "        out_features=4,\n",
-                        "        use_bias=True,\n",
-                        "        dtype=None,\n",
-                        "        param_dtype=<class 'jax.numpy.float32'>,\n",
-                        "        precision=None,\n",
-                        "        kernel_init=<function variance_scaling.<locals>.init at 0x7f8aa7e24670>,\n",
-                        "        bias_init=<function zeros at 0x7f8b4a8d55a0>,\n",
-                        "        dot_general=<function dot_general at 0x7f8b4aed8f70>\n",
-                        "      ),\n",
+                        "            in_features=4,\n",
+                        "            out_features=4,\n",
+                        "            use_bias=True,\n",
+                        "            dtype=None,\n",
+                        "            param_dtype=<class 'jax.numpy.float32'>,\n",
+                        "            precision=None,\n",
+                        "            kernel_init=<function variance_scaling.<locals>.init at 0x28ae86dc0>,\n",
+                        "            bias_init=<function zeros at 0x122d39f70>,\n",
+                        "            dot_general=<function dot_general at 0x1218459d0>\n",
+                        "          ),\n",
                         "      bn=BatchNorm(\n",
-                        "        num_features=4,\n",
-                        "        use_running_average=None,\n",
-                        "   \n",
+                        "            num_features=4,\n",
+                        "  \n",
                         "...\n"
                     ]
                 }
             ],
             "source": [
                 "\n",
                 "class Block(nnx.Module):\n",
                 "  def __init__(self, din, dout, *, rngs):\n",
-                "    self.linear = nnx.Linear(din, dout, rngs=rngs,\n",
-                "                    kernel_init=nnx.with_partitioning(nnx.initializers.lecun_normal() , ('data', 'mp')))\n",
+                "    self.linear = nnx.Linear(din, dout, rngs=rngs)\n",
                 "    self.bn = nnx.BatchNorm(dout, rngs=rngs)\n",
                 "\n",
-                "  def __call__(self, x, *, train: bool):\n",
-                "    x = self.linear(x)\n",
-                "    x = self.bn(x, use_running_average=not train)\n",
-                "    x = nnx.relu(x)\n",
-                "    return x\n",
+                "  def __call__(self, x):\n",
+                "    return nnx.relu(self.bn(self.linear(x)))\n",
                 "\n",
                 "\n",
                 "class MLP(nnx.Module):\n",
                 "  def __init__(self, nlayers, dim, *, rngs): # explicit RNG threading\n",
                 "    self.blocks = [\n",
                 "      Block(dim, dim, rngs=rngs) for _ in range(nlayers)\n",
                 "    ]\n",
                 "    self.count = Count(0)  # stateful variables are defined as attributes\n",
                 "\n",
-                "  def __call__(self, x, *, train: bool):\n",
-                "    self.count += 1  # in-place stateful updates\n",
+                "  def __call__(self, x):\n",
+                "    self.count.value += 1  # in-place stateful updates\n",
                 "    for block in self.blocks:\n",
-                "      x = block(x, train=train)\n",
+                "      x = block(x)\n",
                 "    return x\n",
                 "\n",
                 "class Count(nnx.Variable):   # custom Variable types define the \"collections\"\n",
                 "  pass\n",
                 "\n",
                 "model = MLP(5, 4, rngs=nnx.Rngs(0))  # no special `init` method\n",
-                "y = model(jnp.ones((2, 4)), train=False)  # call methods directly\n",
+                "model.set_attributes(deterministic=False, use_running_average=False)  # set flags\n",
+                "y = model(jnp.ones((2, 4)))  # call methods directly\n",
                 "\n",
                 "print(f'{model = }'[:500] + '\\n...')"
             ]
         },
         {
             "cell_type": "markdown",
             "id": "523aa27c",
             "metadata": {},
             "source": [
                 "Because NNX Modules contain their own state, they are very easily to inspect:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 3,
+            "execution_count": 9,
             "id": "6f278ec4",
             "metadata": {
                 "outputId": "10a46b0f-2993-4677-c26d-36a4ddf33449"
             },
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
-                        "model.count = 1\n",
-                        "model.blocks[0].linear.kernel = Array([[ 0.4541134 , -0.5264871 , -0.36505395, -0.57566494],\n",
-                        "       [ 0.3880299 ,  0.56555384,  0.48706698,  0.22677685],\n",
-                        "       [-0.9015692 ,  0.24465257, -0.58447087,  0.18421973],\n",
-                        "       [-0.06992681, -0.64693826,  0.20232539,  1.1200054 ]],      dtype=float32)\n"
+                        "model.count = Count(\n",
+                        "  raw_value=1\n",
+                        ")\n",
+                        "model.blocks[0].linear.kernel = Param(\n",
+                        "  raw_value=Array([[-0.80345297, -0.34071913, -0.9408296 ,  0.01005968],\n",
+                        "         [ 0.26146442,  1.1247735 ,  0.54563737, -0.374164  ],\n",
+                        "         [ 1.0281805 , -0.6798804 , -0.1488401 ,  0.05694951],\n",
+                        "         [-0.44308168, -0.60587114,  0.434087  , -0.40541083]],      dtype=float32)\n",
+                        ")\n"
                     ]
                 }
             ],
             "source": [
                 "print(f'{model.count = }')\n",
                 "print(f'{model.blocks[0].linear.kernel = }')\n",
                 "# print(f'{model.blocks.sdf.kernel = }') # typesafe inspection"
@@ -138,15 +138,15 @@
             "metadata": {},
             "source": [
                 "### [2] Model Surgery is Intuitive"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 4,
+            "execution_count": 10,
             "id": "96f61108",
             "metadata": {
                 "outputId": "e6f86be8-3537-4c48-f471-316ee0fb6c45"
             },
             "outputs": [
                 {
                     "name": "stdout",
@@ -156,58 +156,59 @@
                     ]
                 }
             ],
             "source": [
                 "# Module sharing\n",
                 "model.blocks[1] = model.blocks[3]\n",
                 "# Weight tying\n",
-                "model.blocks[0].linear.variables.kernel = model.blocks[-1].linear.variables.kernel\n",
+                "model.blocks[0].linear.kernel = model.blocks[-1].linear.kernel\n",
                 "# Monkey patching\n",
-                "def my_optimized_layer(x, *, train: bool): return x\n",
+                "def my_optimized_layer(x): return x\n",
                 "model.blocks[2] = my_optimized_layer\n",
                 "\n",
-                "y = model(jnp.ones((2, 4)), train=False)  # still works\n",
+                "y = model(jnp.ones((2, 4)))  # still works\n",
                 "print(f'{y.shape = }')"
             ]
         },
         {
             "cell_type": "markdown",
             "id": "aca5a6cd",
             "metadata": {},
             "source": [
                 "### [3] Interacting with JAX is easy"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 5,
+            "execution_count": 11,
             "id": "c166dcc7",
             "metadata": {
                 "outputId": "9a3f378b-739e-4f45-9968-574651200ede"
             },
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "state = State({\n",
                         "  'blocks': {\n",
                         "    '0': {\n",
                         "      'linear': {\n",
-                        "        'kernel': Array([[-0.33674937,  1.0543901 , -0.524824  ,  0.16665861],\n",
-                        "               [ 0.6607222 ,  0.07498633, -0.165967  , -0.36928803],\n",
-                        "               [-0.7086948 , -0.5809104 ,  0.2939486 , -0.6660238 ],\n",
-                        "               [-0.13412867,  0.09832543,  0.77024055, -0.2405255 ]],      dtype=float32),\n",
-                        "        'bias': Array([0., 0., 0., 0.], dtype=float32)\n",
-                        "      },\n",
-                        "      'bn': {\n",
-                        "        'mean': Array([0., 0., 0., 0.], dtype=float32),\n",
+                        "        'kernel': Param(\n",
+                        "          raw_value=Array([[-0.33095378,  0.67149884,  0.33700302,  0.30972847],\n",
+                        "                 [ 0.8662822 , -0.11225506, -1.0820619 , -0.9906892 ],\n",
+                        "                 [ 0.88298297, -0.2143851 ,  0.48143268,  0.6474548 ],\n",
+                        "                 [-0.7710582 ,  0.3372276 ,  0.15487202,  0.6219269 ]],      dtype=float32)\n",
+                        "        ),\n",
+                        "        'bias': Param(\n",
+                        "          raw_value=Array([0., 0., 0., 0.], dtype=float32)\n",
+                        "        \n",
                         "...\n",
                         "\n",
-                        "static = GraphDef(\n",
+                        "graphdef = GraphDef(\n",
                         "  type=MLP,\n",
                         "  index=0,\n",
                         "  attributes=('blocks', 'count'),\n",
                         "  subgraphs={\n",
                         "    'blocks': GraphDef(\n",
                         "      type=list,\n",
                         "      index=1,\n",
@@ -218,57 +219,57 @@
                         "          index=2,\n",
                         "          attributes=('line\n",
                         "...\n"
                     ]
                 }
             ],
             "source": [
-                "state, static = model.split()\n",
+                "graphdef, state = model.split()\n",
                 "\n",
                 "# state is a dictionary-like JAX pytree\n",
                 "print(f'{state = }'[:500] + '\\n...')\n",
                 "\n",
-                "# static is also a JAX pytree, but containing no data, just metadata\n",
-                "print(f'\\n{static = }'[:300] + '\\n...')"
+                "# graphdef is also a JAX pytree, but just metadata\n",
+                "print(f'\\n{graphdefefefefefef = }'[:300] + '\\n...')"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 6,
+            "execution_count": 12,
             "id": "9f03e3af",
             "metadata": {
                 "outputId": "0007d357-152a-449e-bcb9-b1b5a91d2d8d"
             },
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "y.shape = (2, 4)\n",
-                        "model.count = Array(3, dtype=int32, weak_type=True)\n"
+                        "model.count.value = Array(3, dtype=int32, weak_type=True)\n"
                     ]
                 }
             ],
             "source": [
-                "state, static = model.split()\n",
+                "graphdef, state = model.split()\n",
                 "\n",
                 "@jax.jit\n",
-                "def forward(static: nnx.GraphDef, state: nnx.State, x: jax.Array):\n",
-                "  model = static.merge(state)\n",
-                "  y = model(x, train=True)\n",
+                "def forward(graphdef: nnx.GraphDef, state: nnx.State, x: jax.Array):\n",
+                "  model = graphdef.merge(state)\n",
+                "  y = model(x)\n",
                 "  state, _ = model.split()\n",
                 "  return y, state\n",
                 "\n",
                 "x = jnp.ones((2, 4))\n",
-                "y, state = forward(static,state, x)\n",
+                "y, state = forward(graphdef,state, x)\n",
                 "\n",
                 "model.update(state)\n",
                 "\n",
                 "print(f'{y.shape = }')\n",
-                "print(f'{model.count = }')"
+                "print(f'{model.count.value = }')"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 7,
             "id": "9e23dbb4",
             "metadata": {},
@@ -279,77 +280,82 @@
                     "text": [
                         "y.shape = (2, 4)\n",
                         "model.count = Array(4, dtype=int32, weak_type=True)\n"
                     ]
                 }
             ],
             "source": [
-                "params, batch_stats, counts, static = model.split(nnx.Param, nnx.BatchStat, Count)\n",
+                "params, batch_stats, counts, graphdef = model.split(nnx.Param, nnx.BatchStat, Count)\n",
                 "\n",
                 "@jax.jit\n",
-                "def forward(static: nnx.GraphDef, params, batch_stats, counts, x: jax.Array):\n",
-                "  model = static.merge(params, batch_stats, counts)\n",
+                "def forward(graphdef: nnx.GraphDef, params, batch_stats, counts, x: jax.Array):\n",
+                "  model = graphdef.merge(params, batch_stats, counts)\n",
                 "  y = model(x, train=True)\n",
                 "  params, batch_stats, counts, _ = model.split(nnx.Param, nnx.BatchStat, Count)\n",
                 "  return y, params, batch_stats, counts\n",
                 "\n",
                 "x = jnp.ones((2, 4))\n",
-                "y, params, batch_stats, counts = forward(static, params, batch_stats, counts, x)\n",
+                "y, params, batch_stats, counts = forward(graphdef, params, batch_stats, counts, x)\n",
                 "\n",
                 "model.update(params, batch_stats, counts)\n",
                 "\n",
                 "print(f'{y.shape = }')\n",
                 "print(f'{model.count = }')"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 8,
+            "execution_count": 14,
             "id": "2461bfe8",
             "metadata": {},
             "outputs": [
                 {
                     "name": "stdout",
                     "output_type": "stream",
                     "text": [
                         "y.shape = (2, 4)\n",
-                        "parent.model.count = Array(5, dtype=int32, weak_type=True)\n"
+                        "parent.model.count.value = Array(4, dtype=int32, weak_type=True)\n"
                     ]
                 }
             ],
             "source": [
                 "class Parent(nnx.Module):\n",
-                "\n",
                 "    def __init__(self, model: MLP):\n",
                 "        self.model = model\n",
                 "\n",
-                "    def __call__(self, x, *, train: bool):\n",
-                "\n",
-                "        params, batch_stats, counts, static = self.model.split(nnx.Param, nnx.BatchStat, Count)\n",
+                "    def __call__(self, x):\n",
+                "        params, batch_stats, counts, graphdef = self.model.split(nnx.Param, nnx.BatchStat, Count)\n",
                 "\n",
                 "        @jax.jit\n",
-                "        def forward(static: nnx.GraphDef, params, batch_stats, counts, x: jax.Array):\n",
-                "            model = static.merge(params, batch_stats, counts)\n",
-                "            y = model(x, train=True)\n",
+                "        def forward(graphdef: nnx.GraphDef, params, batch_stats, counts, x: jax.Array):\n",
+                "            model = graphdef.merge(params, batch_stats, counts)\n",
+                "            y = model(x)\n",
                 "            params, batch_stats, counts, _ = model.split(nnx.Param, nnx.BatchStat, Count)\n",
                 "            return y, params, batch_stats, counts\n",
                 "\n",
-                "        y, params, batch_stats, counts = forward(static, params, batch_stats, counts, x)\n",
+                "        y, params, batch_stats, counts = forward(graphdef, params, batch_stats, counts, x)\n",
                 "\n",
                 "        self.model.update(params, batch_stats, counts)\n",
-                "\n",
                 "        return y\n",
                 "\n",
                 "parent = Parent(model)\n",
                 "\n",
-                "y = parent(jnp.ones((2, 4)), train=False)\n",
+                "y = parent(jnp.ones((2, 4)))\n",
                 "\n",
                 "print(f'{y.shape = }')\n",
-                "print(f'{parent.model.count = }')"
+                "print(f'{parent.model.count.value = }')"
             ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": null,
+            "id": "2e340bcb",
+            "metadata": {},
+            "outputs": [],
+            "source": []
         }
     ],
     "metadata": {
         "jupytext": {
             "formats": "ipynb,md:myst"
         },
         "language_info": {
@@ -358,13 +364,13 @@
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.10.12"
+            "version": "3.9.18"
         }
     },
     "nbformat": 4,
     "nbformat_minor": 5
 }
```

### Comparing `flax-0.8.2/flax/experimental/nnx/docs/demo.md` & `flax-0.8.3/flax/experimental/nnx/docs/demo.md`

 * *Files 11% similar despite different names*

```diff
@@ -20,43 +20,40 @@
 
 ```{code-cell} ipython3
 :outputId: d8ef66d5-6866-4d5c-94c2-d22512bfe718
 
 
 class Block(nnx.Module):
   def __init__(self, din, dout, *, rngs):
-    self.linear = nnx.Linear(din, dout, rngs=rngs,
-                    kernel_init=nnx.with_partitioning(nnx.initializers.lecun_normal() , ('data', 'mp')))
+    self.linear = nnx.Linear(din, dout, rngs=rngs)
     self.bn = nnx.BatchNorm(dout, rngs=rngs)
 
-  def __call__(self, x, *, train: bool):
-    x = self.linear(x)
-    x = self.bn(x, use_running_average=not train)
-    x = nnx.relu(x)
-    return x
+  def __call__(self, x):
+    return nnx.relu(self.bn(self.linear(x)))
 
 
 class MLP(nnx.Module):
   def __init__(self, nlayers, dim, *, rngs): # explicit RNG threading
     self.blocks = [
       Block(dim, dim, rngs=rngs) for _ in range(nlayers)
     ]
     self.count = Count(0)  # stateful variables are defined as attributes
 
-  def __call__(self, x, *, train: bool):
-    self.count += 1  # in-place stateful updates
+  def __call__(self, x):
+    self.count.value += 1  # in-place stateful updates
     for block in self.blocks:
-      x = block(x, train=train)
+      x = block(x)
     return x
 
 class Count(nnx.Variable):   # custom Variable types define the "collections"
   pass
 
 model = MLP(5, 4, rngs=nnx.Rngs(0))  # no special `init` method
-y = model(jnp.ones((2, 4)), train=False)  # call methods directly
+model.set_attributes(deterministic=False, use_running_average=False)  # set flags
+y = model(jnp.ones((2, 4)))  # call methods directly
 
 print(f'{model = }'[:500] + '\n...')
 ```
 
 Because NNX Modules contain their own state, they are very easily to inspect:
 
 ```{code-cell} ipython3
@@ -71,100 +68,101 @@
 
 ```{code-cell} ipython3
 :outputId: e6f86be8-3537-4c48-f471-316ee0fb6c45
 
 # Module sharing
 model.blocks[1] = model.blocks[3]
 # Weight tying
-model.blocks[0].linear.variables.kernel = model.blocks[-1].linear.variables.kernel
+model.blocks[0].linear.kernel = model.blocks[-1].linear.kernel
 # Monkey patching
-def my_optimized_layer(x, *, train: bool): return x
+def my_optimized_layer(x): return x
 model.blocks[2] = my_optimized_layer
 
-y = model(jnp.ones((2, 4)), train=False)  # still works
+y = model(jnp.ones((2, 4)))  # still works
 print(f'{y.shape = }')
 ```
 
 ### [3] Interacting with JAX is easy
 
 ```{code-cell} ipython3
 :outputId: 9a3f378b-739e-4f45-9968-574651200ede
 
-state, static = model.split()
+graphdef, state = model.split()
 
 # state is a dictionary-like JAX pytree
 print(f'{state = }'[:500] + '\n...')
 
-# static is also a JAX pytree, but containing no data, just metadata
-print(f'\n{static = }'[:300] + '\n...')
+# graphdef is also a JAX pytree, but just metadata
+print(f'\n{graphdefefefefefef = }'[:300] + '\n...')
 ```
 
 ```{code-cell} ipython3
 :outputId: 0007d357-152a-449e-bcb9-b1b5a91d2d8d
 
-state, static = model.split()
+graphdef, state = model.split()
 
 @jax.jit
-def forward(static: nnx.GraphDef, state: nnx.State, x: jax.Array):
-  model = static.merge(state)
-  y = model(x, train=True)
+def forward(graphdef: nnx.GraphDef, state: nnx.State, x: jax.Array):
+  model = graphdef.merge(state)
+  y = model(x)
   state, _ = model.split()
   return y, state
 
 x = jnp.ones((2, 4))
-y, state = forward(static,state, x)
+y, state = forward(graphdef,state, x)
 
 model.update(state)
 
 print(f'{y.shape = }')
-print(f'{model.count = }')
+print(f'{model.count.value = }')
 ```
 
 ```{code-cell} ipython3
-params, batch_stats, counts, static = model.split(nnx.Param, nnx.BatchStat, Count)
+params, batch_stats, counts, graphdef = model.split(nnx.Param, nnx.BatchStat, Count)
 
 @jax.jit
-def forward(static: nnx.GraphDef, params, batch_stats, counts, x: jax.Array):
-  model = static.merge(params, batch_stats, counts)
+def forward(graphdef: nnx.GraphDef, params, batch_stats, counts, x: jax.Array):
+  model = graphdef.merge(params, batch_stats, counts)
   y = model(x, train=True)
   params, batch_stats, counts, _ = model.split(nnx.Param, nnx.BatchStat, Count)
   return y, params, batch_stats, counts
 
 x = jnp.ones((2, 4))
-y, params, batch_stats, counts = forward(static, params, batch_stats, counts, x)
+y, params, batch_stats, counts = forward(graphdef, params, batch_stats, counts, x)
 
 model.update(params, batch_stats, counts)
 
 print(f'{y.shape = }')
 print(f'{model.count = }')
 ```
 
 ```{code-cell} ipython3
 class Parent(nnx.Module):
-
     def __init__(self, model: MLP):
         self.model = model
 
-    def __call__(self, x, *, train: bool):
-
-        params, batch_stats, counts, static = self.model.split(nnx.Param, nnx.BatchStat, Count)
+    def __call__(self, x):
+        params, batch_stats, counts, graphdef = self.model.split(nnx.Param, nnx.BatchStat, Count)
 
         @jax.jit
-        def forward(static: nnx.GraphDef, params, batch_stats, counts, x: jax.Array):
-            model = static.merge(params, batch_stats, counts)
-            y = model(x, train=True)
+        def forward(graphdef: nnx.GraphDef, params, batch_stats, counts, x: jax.Array):
+            model = graphdef.merge(params, batch_stats, counts)
+            y = model(x)
             params, batch_stats, counts, _ = model.split(nnx.Param, nnx.BatchStat, Count)
             return y, params, batch_stats, counts
 
-        y, params, batch_stats, counts = forward(static, params, batch_stats, counts, x)
+        y, params, batch_stats, counts = forward(graphdef, params, batch_stats, counts, x)
 
         self.model.update(params, batch_stats, counts)
-
         return y
 
 parent = Parent(model)
 
-y = parent(jnp.ones((2, 4)), train=False)
+y = parent(jnp.ones((2, 4)))
 
 print(f'{y.shape = }')
-print(f'{parent.model.count = }')
+print(f'{parent.model.count.value = }')
+```
+
+```{code-cell} ipython3
+
 ```
```

### Comparing `flax-0.8.2/flax/experimental/nnx/docs/images/stateful-transforms.png` & `flax-0.8.3/flax/experimental/nnx/docs/images/stateful-transforms.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/docs/quick_start.ipynb` & `flax-0.8.3/flax/experimental/nnx/docs/quick_start.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9991021266209843%*

 * *Differences: {"'cells'": "{12: {'source': ['jax.tree_util.tree_map(jnp.shape, model.extract(nnx.Param))']}, 14: "*

 * *            "{'source': {insert: [(13, '  params = jax.tree_util.tree_map(lambda w, g: w - 0.001 * "*

 * *            "g, params, grads)\\n')], delete: [13]}}, 15: {'source': {insert: [(6, 'In this next "*

 * *            'example we will use the `.split` method to split the model into a `params: State` and '*

 * *            '`graphdef: GraphDef` objects. We pass the `"params"` filter to check that the '*

 * *            "Modu […]*

```diff
@@ -263,15 +263,15 @@
                     },
                     "execution_count": 8,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "jax.tree_map(jnp.shape, model.extract(nnx.Param))"
+                "jax.tree_util.tree_map(jnp.shape, model.extract(nnx.Param))"
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
@@ -314,15 +314,15 @@
                 "\n",
                 "  def loss_fn(model: CNN):\n",
                 "    logits = model(x)\n",
                 "    return optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n",
                 "\n",
                 "  loss, grads = nnx.value_and_grad(loss_fn, wrt=\"params\")(model)\n",
                 "  params = model.extract(\"params\")\n",
-                "  params = jax.tree_map(lambda w, g: w - 0.001 * g, params, grads)\n",
+                "  params = jax.tree_util.tree_map(lambda w, g: w - 0.001 * g, params, grads)\n",
                 "\n",
                 "  model.update(params)\n",
                 "  print(f\"Step {step}: loss={loss:.4f}\")"
             ]
         },
         {
             "attachments": {},
@@ -331,34 +331,34 @@
             "source": [
                 "The loss is going down \ud83c\udf89.\n",
                 "\n",
                 "### Training with the Functional API\n",
                 "\n",
                 "Now that we have a working model, lets see how to train it with `jax.jit` using NNX's Functional API. The `Module.split` method allows you to convert a Module into pytrees with functional semantics, this allows you to integrate with JAX's functional APIs like `jax.jit` and `jax.grad`.\n",
                 "\n",
-                "In this next example we will use the `.split` method to split the model into a `params: State` and `static: GraphDef` objects. We pass the `\"params\"` filter to check that the Module's state only contain `Variables` with the `params` collection. Having `params` and `static` its pretty easy to implement a jitted `train_step` much like you would in Flax or Haiku. `GraphDef` exposes an `apply` method which accepts some `State` and creates a function that runs the Module's `__call__` method. This function then returns the output of the Module along with the updated state."
+                "In this next example we will use the `.split` method to split the model into a `params: State` and `graphdef: GraphDef` objects. We pass the `\"params\"` filter to check that the Module's state only contain `Variables` with the `params` collection. Having `params` and `graphdef` its pretty easy to implement a jitted `train_step` much like you would in Flax or Haiku. `GraphDef` exposes an `apply` method which accepts some `State` and creates a function that runs the Module's `__call__` method. This function then returns the output of the Module along with the updated state."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 8,
             "metadata": {},
             "outputs": [],
             "source": [
-                "params, static = model.split(\"params\")\n",
+                "graphdef, params = model.split(\"params\")\n",
                 "\n",
                 "\n",
                 "@jax.jit\n",
                 "def train_step(params: nnx.State, x, y):\n",
                 "  def loss_fn(params):\n",
-                "    logits, _updates = static.apply(params)(x)\n",
+                "    logits, _updates = graphdef.apply(params)(x)\n",
                 "    return optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n",
                 "\n",
                 "  loss, grads = jax.value_and_grad(loss_fn)(params)\n",
-                "  params = jax.tree_map(lambda w, g: w - 0.001 * g, params, grads)\n",
+                "  params = jax.tree_util.tree_map(lambda w, g: w - 0.001 * g, params, grads)\n",
                 "\n",
                 "  return loss, params"
             ]
         },
         {
             "attachments": {},
             "cell_type": "markdown",
@@ -416,15 +416,15 @@
         {
             "cell_type": "code",
             "execution_count": 9,
             "metadata": {},
             "outputs": [],
             "source": [
                 "state = nnx.TrainState(\n",
-                "    apply_fn=static.apply,\n",
+                "    graphdef,\n",
                 "    params=params,\n",
                 "    tx=optax.adam(0.001),\n",
                 ")\n",
                 "\n",
                 "\n",
                 "@jax.jit\n",
                 "def train_step(state: nnx.TrainState, x, y):\n",
```

### Comparing `flax-0.8.2/flax/experimental/nnx/docs/why.ipynb` & `flax-0.8.3/flax/experimental/nnx/docs/why.ipynb`

 * *Files 6% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9991824921956296%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(10, ' - [Variable "*

 * *            'Metadata](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) '*

 * *            "for SPMD annotations, optimizer metadata, and other uses.\\n')], delete: [10]}}, 12: "*

 * *            "{'source': {insert: [(6, 'The `Module.split` method allows you to convert into a "*

 * *            '`State` dict-like object that contains the dynamic state of the Module, and a '*

 * *            "`GraphDef` object that  […]*

```diff
@@ -10,15 +10,15 @@
                 "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/flax/experimental/nnx/docs/why.ipynb)\n",
                 "\n",
                 "Four years ago we developed the Flax \"Linen\" API to support modeling research on JAX, with a focus on scaling scaling and performance.  We've learned a lot from our users over these years.\n",
                 "\n",
                 "We introduced some ideas that have proven to be good:\n",
                 " - Organizing variables into [collections](https://flax.readthedocs.io/en/latest/glossary.html#term-Variable-collections) or types to support JAX transforms and segregation of different data types in training loops.\n",
                 " - Automatic and efficient [PRNG management](https://flax.readthedocs.io/en/latest/glossary.html#term-RNG-sequences) (with support for splitting/broadcast control across map transforms)\n",
-                " - [Variable Metadata](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.with_partitioning.html#flax.linen.with_partitioning) for SPMD annotations, optimizer metadata, and other uses.\n",
+                " - [Variable Metadata](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) for SPMD annotations, optimizer metadata, and other uses.\n",
                 "\n",
                 "However, one choice we made was to use functional \"define by call\" semantics for NN programming via the lazy initialization of parameters.  This made for concise (`compact`) implementation code, allowed for a single specification when transforming a layer, and aligned our API with  Haiku.  Lazy initialization meant that the semantics of modules and variables in Flax were non-pythonic and often surprising.  It also led to implementation complexity and obscured the core ideas of transformations on neural nets.\n",
                 "\n",
                 "NNX is an attempt to keep the features that made Linen useful while introducing some new principles:\n",
                 "\n",
                 "- Regular Python semantics for Modules, including (within JIT boundaries) support for mutability and shared references.\n",
                 "- A simple API to interact directly with the JAX, this includes the ability to easily implement custom lifted Modules and other purely functional tricks.\n",
@@ -251,15 +251,15 @@
             "source": [
                 "### Interacting with JAX is easy\n",
                 "\n",
                 "While NNX Modules inherently follow reference semantics, they can be easily converted into a pure functional representation that can be used with JAX transformations and other value-based, functional code.\n",
                 "\n",
                 "NNX has two very simple APIs to interact with JAX: `split` and `merge`.\n",
                 "\n",
-                "The `Module.split` method allows you to convert into a `State` dict-like object that contains the dynamic state of the Module, and a `GraphDef` object that contains the static structure of the Module."
+                "The `Module.split` method allows you to convert into a `State` dict-like object that contains the dynamic state of the Module, and a `GraphDef` object that contains the graphdef structure of the Module."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 96,
             "metadata": {
                 "outputId": "9a3f378b-739e-4f45-9968-574651200ede"
@@ -274,15 +274,15 @@
                         "  'linear/bias': Array([0., 0., 0., 0.], dtype=float32),\n",
                         "  'linear/kernel': Array([[ 0.4541089 , -0.5264876 , -0.36505195, -0.57566494],\n",
                         "         [ 0.38802508,  0.5655534 ,  0.4870657 ,  0.2267774 ],\n",
                         "         [-0.9015767 ,  0.24465278, -0.5844707 ,  0.18421966],\n",
                         "         [-0.06992685, -0.64693886,  0.20232596,  1.1200062 ]],      dtype=float32)\n",
                         "})\n",
                         "\n",
-                        "static = GraphDef(\n",
+                        "graphdef = GraphDef(\n",
                         "  type=CounterLinear,\n",
                         "  index=0,\n",
                         "  static_fields=(),\n",
                         "  variables=(('count', Count(\n",
                         "      value=Empty\n",
                         "    )),),\n",
                         "  submodules=(\n",
@@ -301,21 +301,21 @@
                         ")\n"
                     ]
                 }
             ],
             "source": [
                 "model = CounterLinear(4, 4, rngs=nnx.Rngs(0))\n",
                 "\n",
-                "state, static = model.split()\n",
+                "graphdef, state = model.split()\n",
                 "\n",
                 "# state is a dictionary-like JAX pytree\n",
                 "print(f'{state = }')\n",
                 "\n",
-                "# static is also a JAX pytree, but containing no data, just metadata\n",
-                "print(f'\\n{static = }')"
+                "# graphdef is also a JAX pytree, but containing no data, just metadata\n",
+                "print(f'\\n{graphdef = }')"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "The `GraphDef.merge` method allows you to take a `GraphDef` and one or more `State` objects and merge them back into a `Module` object.\n",
@@ -337,22 +337,22 @@
                         "y.shape = (2, 4)\n",
                         "state[\"count\"] = Array(1, dtype=int32)\n"
                     ]
                 }
             ],
             "source": [
                 "@jax.jit\n",
-                "def forward(static: nnx.GraphDef, state: nnx.State, x: jax.Array):\n",
-                "  model = static.merge(state)\n",
+                "def forward(graphdef: nnx.GraphDef, state: nnx.State, x: jax.Array):\n",
+                "  model = graphdef.merge(state)\n",
                 "  y = model(x)\n",
                 "  state, _ = model.split()\n",
                 "  return y, state\n",
                 "\n",
                 "x = jnp.ones((2, 4))\n",
-                "y, state = forward(static,state, x)\n",
+                "y, state = forward(graphdef,state, x)\n",
                 "\n",
                 "print(f'{y.shape = }')\n",
                 "print(f'{state[\"count\"] = }')"
             ]
         },
         {
             "cell_type": "markdown",
@@ -397,53 +397,53 @@
                 "    keys = rngs.fork(num_models) # split all keys into `num_models`\n",
                 "\n",
                 "    # define pure init fn and vmap\n",
                 "    def vmap_init(keys):\n",
                 "      return CounterLinear(din, dout, rngs=nnx.Rngs(keys)).split(\n",
                 "        nnx.Param, Count\n",
                 "      )\n",
-                "    params, counts, static = jax.vmap(\n",
+                "    params, counts, graphdef = jax.vmap(\n",
                 "      vmap_init, in_axes=(0,), out_axes=(0, None, None)\n",
                 "    )(keys)\n",
                 "\n",
                 "    # update wrapped submodule reference\n",
-                "    self.models = static.merge(params, counts)\n",
+                "    self.models = graphdef.merge(params, counts)\n",
                 "\n",
                 "  def __call__(self, x):\n",
                 "    # get module values, define pure fn,\n",
                 "    # notice that we split the data into two collections by their types.\n",
-                "    params, counts, static = self.models.split(nnx.Param, Count)\n",
+                "    params, counts, graphdef = self.models.split(nnx.Param, Count)\n",
                 "\n",
                 "    # define pure init fn and vmap\n",
-                "    def vmap_apply(x, params, counts, static):\n",
-                "      model = static.merge(params, counts)\n",
+                "    def vmap_apply(x, params, counts, graphdef):\n",
+                "      model = graphdef.merge(params, counts)\n",
                 "      y = model(x)\n",
-                "      params, counts, static = model.split(nnx.Param, Count)\n",
-                "      return y, params, counts, static\n",
+                "      params, counts, graphdef = model.split(nnx.Param, Count)\n",
+                "      return y, params, counts, graphdef\n",
                 "\n",
-                "    y, params, counts, static = jax.vmap(\n",
+                "    y, params, counts, graphdef = jax.vmap(\n",
                 "        vmap_apply,\n",
                 "        in_axes=(None, 0, None, None),\n",
                 "        out_axes=(0, 0, None, None)\n",
-                "    )(x, params, counts, static)\n",
+                "    )(x, params, counts, graphdef)\n",
                 "\n",
                 "    # update wrapped module\n",
                 "    # uses `update` to integrate the new state\n",
-                "    self.models.update(params, counts, static)\n",
+                "    self.models.update(params, counts, graphdef)\n",
                 "    return y\n",
                 "\n",
                 "x = jnp.ones((4,))\n",
                 "ensemble = LinearEnsemble(4, 4, num_models=8, rngs=nnx.Rngs(0))\n",
                 "\n",
                 "# forward pass\n",
                 "y = ensemble(x)\n",
                 "\n",
                 "print(f'{y.shape = }')\n",
                 "print(f'{ensemble.models.count = }')\n",
-                "print(f'state = {jax.tree_map(jnp.shape, ensemble.get_state())}')"
+                "print(f'state = {jax.tree_util.tree_map(jnp.shape, ensemble.get_state())}')"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "#### Convenience lifted transforms"
@@ -676,15 +676,15 @@
                 "  def __call__(self, x):\n",
                 "    return x @ self.kernel + self.bias\n",
                 "\n",
                 "\n",
                 "model = AnnotatedLinear(4, 8, rngs=nnx.Rngs(0))\n",
                 "y = model(jnp.ones((2, 4)))\n",
                 "\n",
-                "state, static = model.split()\n",
+                "graphdef, state = model.split()\n",
                 "\n",
                 "print(f\"{state.variables['kernel'].meta=}\\n{state.variables['kernel'].other_meta=}\")\n",
                 "print(f\"{state.variables['bias'].meta=}\\n{state.variables['bias'].other_meta=}\")"
             ]
         },
         {
             "cell_type": "markdown",
@@ -747,16 +747,16 @@
                 "    return self.linear(x)\n",
                 "\n",
                 "model = Example(in_filters=3,\n",
                 "                out_filters=4,\n",
                 "                input_shape=(2, 6, 6, 3),\n",
                 "                rngs=nnx.Rngs(0))\n",
                 "\n",
-                "state, static = model.split()\n",
-                "jax.tree_map(jnp.shape, state)"
+                "graphdef, state = model.split()\n",
+                "jax.tree_util.tree_map(jnp.shape, state)"
             ]
         }
     ],
     "metadata": {
         "jupytext": {
             "formats": "ipynb,md:myst"
         },
```

### Comparing `flax-0.8.2/flax/experimental/nnx/docs/why.md` & `flax-0.8.3/flax/experimental/nnx/docs/why.md`

 * *Files 3% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/flax/experimental/nnx/docs/why.ipynb)
 
 Four years ago we developed the Flax "Linen" API to support modeling research on JAX, with a focus on scaling scaling and performance.  We've learned a lot from our users over these years.
 
 We introduced some ideas that have proven to be good:
  - Organizing variables into [collections](https://flax.readthedocs.io/en/latest/glossary.html#term-Variable-collections) or types to support JAX transforms and segregation of different data types in training loops.
  - Automatic and efficient [PRNG management](https://flax.readthedocs.io/en/latest/glossary.html#term-RNG-sequences) (with support for splitting/broadcast control across map transforms)
- - [Variable Metadata](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.with_partitioning.html#flax.linen.with_partitioning) for SPMD annotations, optimizer metadata, and other uses.
+ - [Variable Metadata](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) for SPMD annotations, optimizer metadata, and other uses.
 
 However, one choice we made was to use functional "define by call" semantics for NN programming via the lazy initialization of parameters.  This made for concise (`compact`) implementation code, allowed for a single specification when transforming a layer, and aligned our API with  Haiku.  Lazy initialization meant that the semantics of modules and variables in Flax were non-pythonic and often surprising.  It also led to implementation complexity and obscured the core ideas of transformations on neural nets.
 
 NNX is an attempt to keep the features that made Linen useful while introducing some new principles:
 
 - Regular Python semantics for Modules, including (within JIT boundaries) support for mutability and shared references.
 - A simple API to interact directly with the JAX, this includes the ability to easily implement custom lifted Modules and other purely functional tricks.
@@ -141,46 +141,46 @@
 
 ### Interacting with JAX is easy
 
 While NNX Modules inherently follow reference semantics, they can be easily converted into a pure functional representation that can be used with JAX transformations and other value-based, functional code.
 
 NNX has two very simple APIs to interact with JAX: `split` and `merge`.
 
-The `Module.split` method allows you to convert into a `State` dict-like object that contains the dynamic state of the Module, and a `GraphDef` object that contains the static structure of the Module.
+The `Module.split` method allows you to convert into a `State` dict-like object that contains the dynamic state of the Module, and a `GraphDef` object that contains the graphdef structure of the Module.
 
 ```{code-cell}
 :outputId: 9a3f378b-739e-4f45-9968-574651200ede
 
 model = CounterLinear(4, 4, rngs=nnx.Rngs(0))
 
-state, static = model.split()
+graphdef, state = model.split()
 
 # state is a dictionary-like JAX pytree
 print(f'{state = }')
 
-# static is also a JAX pytree, but containing no data, just metadata
-print(f'\n{static = }')
+# graphdef is also a JAX pytree, but containing no data, just metadata
+print(f'\n{graphdef = }')
 ```
 
 The `GraphDef.merge` method allows you to take a `GraphDef` and one or more `State` objects and merge them back into a `Module` object.
 
 Using `split` and `merge` in conjunction allows you to carry your Module in and out of any JAX transformation. Here is a simple jitted `forward` function as an example:
 
 ```{code-cell}
 :outputId: 0007d357-152a-449e-bcb9-b1b5a91d2d8d
 
 @jax.jit
-def forward(static: nnx.GraphDef, state: nnx.State, x: jax.Array):
-  model = static.merge(state)
+def forward(graphdef: nnx.GraphDef, state: nnx.State, x: jax.Array):
+  model = graphdef.merge(state)
   y = model(x)
   state, _ = model.split()
   return y, state
 
 x = jnp.ones((2, 4))
-y, state = forward(static,state, x)
+y, state = forward(graphdef,state, x)
 
 print(f'{y.shape = }')
 print(f'{state["count"] = }')
 ```
 
 #### Custom lifting and transformation
 
@@ -201,53 +201,53 @@
     keys = rngs.fork(num_models) # split all keys into `num_models`
 
     # define pure init fn and vmap
     def vmap_init(keys):
       return CounterLinear(din, dout, rngs=nnx.Rngs(keys)).split(
         nnx.Param, Count
       )
-    params, counts, static = jax.vmap(
+    params, counts, graphdef = jax.vmap(
       vmap_init, in_axes=(0,), out_axes=(0, None, None)
     )(keys)
 
     # update wrapped submodule reference
-    self.models = static.merge(params, counts)
+    self.models = graphdef.merge(params, counts)
 
   def __call__(self, x):
     # get module values, define pure fn,
     # notice that we split the data into two collections by their types.
-    params, counts, static = self.models.split(nnx.Param, Count)
+    params, counts, graphdef = self.models.split(nnx.Param, Count)
 
     # define pure init fn and vmap
-    def vmap_apply(x, params, counts, static):
-      model = static.merge(params, counts)
+    def vmap_apply(x, params, counts, graphdef):
+      model = graphdef.merge(params, counts)
       y = model(x)
-      params, counts, static = model.split(nnx.Param, Count)
-      return y, params, counts, static
+      params, counts, graphdef = model.split(nnx.Param, Count)
+      return y, params, counts, graphdef
 
-    y, params, counts, static = jax.vmap(
+    y, params, counts, graphdef = jax.vmap(
         vmap_apply,
         in_axes=(None, 0, None, None),
         out_axes=(0, 0, None, None)
-    )(x, params, counts, static)
+    )(x, params, counts, graphdef)
 
     # update wrapped module
     # uses `update` to integrate the new state
-    self.models.update(params, counts, static)
+    self.models.update(params, counts, graphdef)
     return y
 
 x = jnp.ones((4,))
 ensemble = LinearEnsemble(4, 4, num_models=8, rngs=nnx.Rngs(0))
 
 # forward pass
 y = ensemble(x)
 
 print(f'{y.shape = }')
 print(f'{ensemble.models.count = }')
-print(f'state = {jax.tree_map(jnp.shape, ensemble.get_state())}')
+print(f'state = {jax.tree_util.tree_map(jnp.shape, ensemble.get_state())}')
 ```
 
 #### Convenience lifted transforms
 
 +++
 
 Like linen, for convenience we still provide simple lifted transforms for standard JAX transforms, usable as class transforms and decorators.  We've endeavored to simplify the API for scan and vmap compared to the flax specifications.
@@ -355,15 +355,15 @@
   def __call__(self, x):
     return x @ self.kernel + self.bias
 
 
 model = AnnotatedLinear(4, 8, rngs=nnx.Rngs(0))
 y = model(jnp.ones((2, 4)))
 
-state, static = model.split()
+graphdef, state = model.split()
 
 print(f"{state.variables['kernel'].meta=}\n{state.variables['kernel'].other_meta=}")
 print(f"{state.variables['bias'].meta=}\n{state.variables['bias'].other_meta=}")
 ```
 
 ## Shape Inference
 
@@ -400,10 +400,10 @@
     return self.linear(x)
 
 model = Example(in_filters=3,
                 out_filters=4,
                 input_shape=(2, 6, 6, 3),
                 rngs=nnx.Rngs(0))
 
-state, static = model.split()
-jax.tree_map(jnp.shape, state)
+graphdef, state = model.split()
+jax.tree_util.tree_map(jnp.shape, state)
 ```
```

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/README.md` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/configs/default.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/input_pipeline.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/input_pipeline_test.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/main.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/models.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/models_test.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/models_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -14,20 +14,14 @@
 
 from __future__ import annotations
 
 import sys
 from pathlib import Path
 from typing import Any
 
-# add project_root to import lm1b Linen model
-project_root = str(Path(__file__).parents[6])
-sys.path.append(project_root)
-from examples.lm1b.models import TransformerLM as TransformerLinen
-
-sys.path.pop()
 
 import dataclasses
 
 import jax
 import jax.numpy as jnp
 from absl.testing import absltest
 from jax import random
@@ -39,14 +33,21 @@
   TransformerConfig,
   TransformerLM,
 )
 from flax.experimental.nnx.examples.lm1b.utils import HasCache
 
 jax.config.update('jax_disable_most_optimizations', True)
 
+# add project_root to import lm1b Linen model
+project_root = str(Path(__file__).absolute().parents[5])
+sys.path.append(project_root)
+from examples.lm1b.models import TransformerLM as TransformerLinen
+
+sys.path.pop()
+
 
 @dataclasses.dataclass(unsafe_hash=True)
 class CompatTransformerConfig(TransformerConfig):
   decode: bool | None = None
   deterministic: bool | None = None
 
 
@@ -84,21 +85,22 @@
     rules = dataclasses.asdict(config.axis_rules)
     flat_params_nnx = params_nnx.flat_state()
     flat_params_linen = traverse_util.flatten_dict(params_linen, sep='/')
 
     def apply_rules(names: tuple[str, ...]):
       return tuple(rules[name] for name in names)
 
-    def copy_var(nnx_name, linen_name):
+    def copy_var(nnx_name: str, linen_name: str):
+      nnx_path = tuple(nnx_name.split('/'))
       assert (
-        flat_params_nnx[nnx_name].raw_value.shape
+        flat_params_nnx[nnx_path].value.shape
         == flat_params_linen[linen_name].value.shape
       )
-      flat_params_nnx[nnx_name].raw_value = flat_params_linen[linen_name].value
-      assert flat_params_nnx[nnx_name].sharding == apply_rules(
+      flat_params_nnx[nnx_path].value = flat_params_linen[linen_name].value
+      assert flat_params_nnx[nnx_path].sharding == apply_rules(
         flat_params_linen[linen_name].names
       )
 
     copy_var('decoder/output_embed/embedding', 'decoder/Embed_0/embedding')
     copy_var(
       'decoder/encoderdecoder_norm/bias', 'decoder/encoderdecoder_norm/bias'
     )
@@ -164,20 +166,21 @@
     config: TransformerConfig,
     cache_nnx: nnx.State,
     cache_linen: dict[str, Any],
   ):
     flat_cache_nnx = cache_nnx.flat_state()
     flat_cache_linen = traverse_util.flatten_dict(cache_linen, sep='/')
 
-    def copy_var(nnx_name, linen_name):
+    def copy_var(nnx_name: str, linen_name: str):
+      nnx_path = tuple(nnx_name.split('/'))
       assert (
-        flat_cache_nnx[nnx_name].raw_value.shape
+        flat_cache_nnx[nnx_path].value.shape
         == flat_cache_linen[linen_name].shape
       )
-      flat_cache_nnx[nnx_name].raw_value = flat_cache_linen[linen_name]
+      flat_cache_nnx[nnx_path].value = flat_cache_linen[linen_name]
 
     for idx in range(config.num_layers):
       copy_var(
         f'decoder/encoderdecoderblock_{idx}/attention/cache_index',
         f'decoder/encoderdecoderblock_{idx}/MultiHeadDotProductAttention_0/cache_index',
       )
       copy_var(
@@ -202,24 +205,24 @@
         kv=None,
         vocab=None,
       ),
       deterministic=True,
       decode=False,
     )
 
-    model_nnx = TransformerLM.create_abstract(config, rngs=nnx.Rngs(0))
-    params_nnx, _ = model_nnx.split(nnx.Param)
+    model_nnx = nnx.eval_shape(lambda: TransformerLM(config, rngs=nnx.Rngs(0)))
+    _, params_nnx = nnx.split(model_nnx, nnx.Param)
 
     model_linen = TransformerLinen(config)
 
     sample_inputs = random.randint(random.PRNGKey(0), (1, 3), 0, 20)
     params_linen = model_linen.init(random.key(0), sample_inputs)['params']
 
     self.transfer_params(config, params_nnx, params_linen)
-    model_nnx.update(params_nnx)
+    nnx.update(model_nnx, params_nnx)
 
     model_nnx.set_attributes(deterministic=True, decode=False)
     output_nnx = model_nnx(sample_inputs)
 
     output_linen: jax.Array = model_linen.apply(
       {'params': params_linen}, sample_inputs
     )
@@ -236,21 +239,21 @@
         kv=None,
         vocab=None,
       ),
       deterministic=True,
       decode=True,
     )
 
-    model_nnx = TransformerLM.create_abstract(config, rngs=nnx.Rngs(0))
-    for _path, m in model_nnx.modules():
+    model_nnx = nnx.eval_shape(lambda: TransformerLM(config, rngs=nnx.Rngs(0)))
+    for _path, m in model_nnx.iter_modules():
       if isinstance(m, HasCache):
         input_shape = (batch_size, config.max_len, config.emb_dim)
         m.init_cache(input_shape, dtype=config.dtype)
 
-    params_nnx, cache_nnx, _ = model_nnx.split(nnx.Param, nnx.Cache)
+    _, params_nnx, cache_nnx = nnx.split(model_nnx, nnx.Param, nnx.Cache)
 
     model_linen = TransformerLinen(config)
 
     flax_init_inputs = random.randint(
       random.PRNGKey(0), (batch_size, config.max_len), 0, config.vocab_size
     )
     ar_decode_inputs = random.randint(
@@ -258,15 +261,15 @@
     )
     variables = model_linen.init(random.key(0), flax_init_inputs)
     params_linen = variables['params']
     cache_linen = variables['cache']
 
     self.transfer_params(config, params_nnx, params_linen)
     self.transfer_cache(config, cache_nnx, cache_linen)
-    model_nnx.update(params_nnx, cache_nnx)
+    nnx.update(model_nnx, params_nnx, cache_nnx)
     model_nnx.set_attributes(deterministic=True, decode=True)
 
     outputs_nnx = []
     outputs_linen = []
 
     for inputs in ar_decode_inputs:
       output_nnx = model_nnx(inputs)
```

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/temperature_sampler.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/temperature_sampler.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/temperature_sampler_test.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/temperature_sampler_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/tokenizer.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/tokenizer.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/train.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/train.py`

 * *Files 6% similar despite different names*

```diff
@@ -189,15 +189,15 @@
 
   weights = jnp.where(inputs > 0, 1, 0).astype(jnp.float32)
 
   dropout_rng = jax.random.fold_in(dropout_rng, state.step)
 
   def loss_fn(params):
     """loss function used for training."""
-    module = state.graphdef.merge(params)
+    module = nnx.merge(state.graphdef, params)
     module.set_attributes(deterministic=False, decode=False)
     logits = module(
       inputs,
       inputs_positions=inputs_positions,
       inputs_segmentation=inputs_segmentation,
       rngs=nnx.Rngs(dropout=dropout_rng),
     )
@@ -218,56 +218,56 @@
 
   return new_state, metrics
 
 
 def eval_step(
   params: nnx.State,
   batch,
-  static: nnx.GraphDef[models.TransformerLM],
+  graphdef: nnx.GraphDef[models.TransformerLM],
   label_smoothing=0.0,
 ):
   """Calculate evaluation metrics on a batch."""
   inputs = batch['inputs']
   weights = jnp.where(inputs > 0, 1.0, 0.0)
-  module = static.merge(params)
+  module = nnx.merge(graphdef, params)
   module.set_attributes(deterministic=True, decode=False)
   logits = module(inputs)
 
   return compute_metrics(logits, inputs, weights, label_smoothing)
 
 
 def predict_step(
   inputs,
   params: nnx.State,
   rngkey: jax.Array,
-  static: nnx.GraphDef[models.TransformerLM],
+  graphdef: nnx.GraphDef[models.TransformerLM],
   eos_id: int,
   max_decode_len: int,
   config: models.TransformerConfig,
   temperature: float,
   top_k: int,
 ):
   """Predict language model on a batch."""
-  module = static.merge(params)
+  module = nnx.merge(graphdef, params)
 
   # TODO(cgarciae): check how pytorch does this.
-  for _path, m in module.modules():
+  for _path, m in module.iter_modules():
     if isinstance(m, HasCache):
       input_shape = (inputs.shape[0], max_decode_len, config.emb_dim)
       m.init_cache(input_shape, dtype=config.dtype)
 
-  cache = module.extract(nnx.Cache)
+  graphdef, params, cache = nnx.split(module, nnx.Param, nnx.Cache)
 
   def tokens_ids_to_logits(flat_ids, cache: nnx.State):
     """Token slice to logits from decoder model."""
     # --> [batch * beam, 1, vocab]
-    module = static.merge(params, cache)
+    module = nnx.merge(graphdef, params, cache)
     module.set_attributes(deterministic=True, decode=True)
     logits = module(flat_ids)
-    cache = module.extract(nnx.Cache)
+    cache = nnx.state(module, nnx.Cache)
     # Remove singleton sequence-length dimension:
     # [batch, 1, vocab] --> [batch, vocab]
     logits = logits.squeeze(axis=1)
     return logits, cache
 
   # Using the above-defined single-step decoder function, run a
   # beam search over possible sequences given input encoding.
@@ -343,15 +343,15 @@
   )
   return eval_summary
 
 
 def generate_prediction(
   *,
   jit_pred_step,
-  static: nnx.GraphDef[models.TransformerLM],
+  graphdef: nnx.GraphDef[models.TransformerLM],
   params: nnx.State,
   tokenized_prompts,
   eos_id,
   inference_rng,
   decode_tokens,
   config: default.Config,
   model_config: models.TransformerConfig,
@@ -375,15 +375,15 @@
     inference_rng, sub_rng = random.split(inference_rng)
     inference_rngs = random.split(sub_rng, n_devices)
 
     predicted = jit_pred_step(
       pred_batch,
       params,
       inference_rngs,
-      static,
+      graphdef,
       eos_id,
       config.max_predict_length,
       model_config,
       config.sampling_temperature,
       config.sampling_top_k,
     )
     predicted = tohost(predicted)
@@ -578,15 +578,15 @@
   with metric_writers.ensure_flushes(writer):
     for step in range(start_step, config.num_train_steps):
       is_last_step = step == config.num_train_steps - 1
 
       # Shard data to devices and do a training step.
       with jax.profiler.StepTraceAnnotation('train', step_num=step):
         batch = next(train_iter)
-        batch = jax.tree_map(lambda x: jnp.asarray(x), batch)
+        batch = jax.tree_util.tree_map(lambda x: jnp.asarray(x), batch)
         state, metrics = jit_train_step(
           state, batch, learning_rate_fn, 0.0, dropout_rngs
         )
         train_metrics.append(metrics)
 
       # Quick indication that training is happening.
       logging.log_first_n(logging.INFO, 'Finished training step %d.', 5, step)
@@ -626,15 +626,15 @@
           writer.write_scalars(
             step, {'eval_' + k: v for k, v in eval_results.items()}
           )
 
         with report_progress.timed('generate_text'):
           exemplars = generate_prediction(
             jit_pred_step=jit_pred_step,
-            static=state.graphdef,
+            graphdef=state.graphdef,
             params=state.params,
             tokenized_prompts=tokenized_prompts,
             eos_id=eos_id,
             inference_rng=inference_rng,
             decode_tokens=decode_tokens,
             config=config,
             model_config=model_config,
```

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/train_test.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/lm1b/utils.py` & `flax-0.8.3/flax/experimental/nnx/examples/lm1b/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -153,17 +153,17 @@
     state_mesh_annotations: the mesh annotations for the train state
   """
 
   # Initialization
 
   with mesh:
     model = constructor(config, rng)
-    params, static = model.split(nnx.Param)
+    graphdef, params = nnx.split(model, nnx.Param)
     state = TrainState.create(
-      apply_fn=static.apply, params=params, tx=tx, graphdef=static
+      apply_fn=graphdef.apply, params=params, tx=tx, graphdef=graphdef
     )
-    state = jax.tree_map(_to_array, state)
+    state = jax.tree_util.tree_map(_to_array, state)
     state_spec = nnx.get_partition_spec(state)
     state = jax.lax.with_sharding_constraint(state, state_spec)
 
   state_sharding = nnx.get_named_sharding(state, mesh)
   return state, state_sharding
```

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/toy_examples/01_functional_api.py` & `flax-0.8.3/flax/experimental/nnx/examples/toy_examples/01_functional_api.py`

 * *Files 8% similar despite different names*

```diff
@@ -53,40 +53,42 @@
     self.count.value += 1
     x = self.linear1(x)
     x = jax.nn.relu(x)
     x = self.linear2(x)
     return x
 
 
-params, counts, modeldef = MLP(
-  din=1, dhidden=32, dout=1, rngs=nnx.Rngs(0)
-).split(nnx.Param, Count)
+graphdef, params, counts = nnx.split(
+  MLP(din=1, dhidden=32, dout=1, rngs=nnx.Rngs(0)), nnx.Param, Count
+)
 
 
 @jax.jit
 def train_step(params, counts, batch):
   x, y = batch
 
   def loss_fn(params):
-    y_pred, (updates, _) = modeldef.apply(params, counts)(x)
-    counts_ = updates.extract(Count)
+    model = nnx.merge(graphdef, params, counts)
+    y_pred = model(x)
+    new_counts = nnx.state(model, Count)
     loss = jnp.mean((y - y_pred) ** 2)
-    return loss, counts_
+    return loss, new_counts
 
   grad, counts = jax.grad(loss_fn, has_aux=True)(params)
   #                          |-------- sgd ---------|
-  params = jax.tree_map(lambda w, g: w - 0.1 * g, params, grad)
+  params = jax.tree_util.tree_map(lambda w, g: w - 0.1 * g, params, grad)
 
   return params, counts
 
 
 @jax.jit
 def test_step(params: nnx.State, counts: nnx.State, batch):
   x, y = batch
-  y_pred, _ = modeldef.apply(params, counts)(x)
+  model = nnx.merge(graphdef, params, counts)
+  y_pred = model(x)
   loss = jnp.mean((y - y_pred) ** 2)
   return {'loss': loss}
 
 
 total_steps = 10_000
 for step, batch in enumerate(dataset(32)):
   params, counts = train_step(params, counts, batch)
@@ -94,15 +96,15 @@
   if step % 1000 == 0:
     logs = test_step(params, counts, (X, Y))
     print(f"step: {step}, loss: {logs['loss']}")
 
   if step >= total_steps - 1:
     break
 
-model = modeldef.merge(params, counts)
+model = nnx.merge(graphdef, params, counts)
 print('times called:', model.count.value)
 
 y_pred = model(X)
 
 plt.scatter(X, Y, color='blue')
 plt.plot(X, y_pred, color='black')
 plt.show()
```

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/toy_examples/02_lifted_transforms.py` & `flax-0.8.3/flax/experimental/nnx/examples/toy_examples/02_lifted_transforms.py`

 * *Files 7% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # limitations under the License.
 
 # %%
 import jax
 import jax.numpy as jnp
 import matplotlib.pyplot as plt
 import numpy as np
+import optax
 
 from flax.experimental import nnx
 
 X = np.linspace(0, 1, 100)[:, None]
 Y = 0.8 * X**2 + 0.1 + np.random.normal(0, 0.1, size=X.shape)
 
 
@@ -54,45 +55,42 @@
     x = self.linear1(x)
     x = jax.nn.relu(x)
     x = self.linear2(x)
     return x
 
 
 model = MLP(din=1, dhidden=32, dout=1, rngs=nnx.Rngs(0))
-
+tx = optax.sgd(1e-3)
+optimizer = nnx.Optimizer(model, tx)
 
 @nnx.jit
-def train_step(model: MLP, batch):
+def train_step(model: MLP, optimizer: nnx.Optimizer, batch):
   x, y = batch
 
   def loss_fn(model: MLP):
     y_pred = model(x)
     return jnp.mean((y - y_pred) ** 2)
 
-  #                                   |--default--|
-  grad: nnx.State = nnx.grad(loss_fn, wrt=nnx.Param)(model)
-  # sdg update
-  model.update(
-    jax.tree_map(lambda w, g: w - 0.1 * g, model.extract(nnx.Param), grad)
-  )
-
-  # no return!!!
+  #                                    |--default--|
+  grads: nnx.State = nnx.grad(loss_fn, wrt=nnx.Param)(model)
+  # sgd update
+  optimizer.update(grads)
 
 
 @nnx.jit
 def test_step(model: MLP, batch):
   x, y = batch
   y_pred = model(x)
   loss = jnp.mean((y - y_pred) ** 2)
   return {'loss': loss}
 
 
 total_steps = 10_000
 for step, batch in enumerate(dataset(32)):
-  train_step(model, batch)
+  train_step(model, optimizer, batch)
 
   if step % 1000 == 0:
     logs = test_step(model, (X, Y))
     print(f"step: {step}, loss: {logs['loss']}")
 
   if step >= total_steps - 1:
     break
```

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/toy_examples/05_vae.py` & `flax-0.8.3/flax/experimental/nnx/examples/toy_examples/05_vae.py`

 * *Files 16% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 # %%
 import typing as tp
-from functools import partial
 
 import jax
 import jax.numpy as jnp
 import matplotlib.pyplot as plt
 import numpy as np
 import optax
 from datasets import load_dataset
@@ -50,29 +49,30 @@
 
 # %%
 class Encoder(nnx.Module):
   def __init__(self, din: int, dmid: int, dout: int, *, rngs: nnx.Rngs):
     self.linear1 = nnx.Linear(din, dmid, rngs=rngs)
     self.linear_mean = nnx.Linear(dmid, dout, rngs=rngs)
     self.linear_std = nnx.Linear(dmid, dout, rngs=rngs)
+    self.rngs = rngs
 
-  def __call__(self, x: jax.Array, *, rngs: nnx.Rngs) -> jax.Array:
+  def __call__(self, x: jax.Array) -> jax.Array:
     x = x.reshape((x.shape[0], -1))  # flatten
     x = self.linear1(x)
     x = jax.nn.relu(x)
 
     mean = self.linear_mean(x)
     std = jnp.exp(self.linear_std(x))
 
     self.kl_loss = Loss(
       jnp.mean(
         0.5 * jnp.mean(-jnp.log(std**2) - 1.0 + std**2 + mean**2, axis=-1)
       )
     )
-    key = rngs.noise()
+    key = self.rngs.noise()
     z = mean + std * jax.random.normal(key, mean.shape)
     return z
 
 
 class Decoder(nnx.Module):
   def __init__(self, din: int, dmid: int, dout: int, *, rngs: nnx.Rngs):
     self.linear1 = nnx.Linear(din, dmid, rngs=rngs)
@@ -97,100 +97,88 @@
   ):
     self.output_shape = output_shape
     self.encoder = Encoder(din, hidden_size, latent_size, rngs=rngs)
     self.decoder = Decoder(
       latent_size, hidden_size, int(np.prod(output_shape)), rngs=rngs
     )
 
-  def __call__(self, x: jax.Array, *, rngs: nnx.Rngs) -> jax.Array:
-    z = self.encoder(x, rngs=rngs)
+  def __call__(self, x: jax.Array) -> jax.Array:
+    z = self.encoder(x)
     logits = self.decoder(z)
     logits = jnp.reshape(logits, (-1, *self.output_shape))
     return logits
 
   def generate(self, z):
     logits = self.decoder(z)
     logits = jnp.reshape(logits, (-1, *self.output_shape))
     return nnx.sigmoid(logits)
 
 
-params, static = VAE(
+model = VAE(
   din=int(np.prod(image_shape)),
   hidden_size=256,
   latent_size=latent_size,
   output_shape=image_shape,
-  rngs=nnx.Rngs(0),
-).split(nnx.Param)
-
-state = nnx.TrainState(
-  static,
-  params=params,
-  tx=optax.adam(1e-3),
+  rngs=nnx.Rngs(0, noise=1),
 )
 
+optimizer = nnx.Optimizer(model, optax.adam(1e-3))
 
-# %%
-@jax.jit
-def train_step(state: nnx.TrainState[VAE], x: jax.Array, key: jax.Array):
-  def loss_fn(params: nnx.State):
-    rngs = nnx.Rngs(noise=jax.random.fold_in(key, state.step))
-    logits, (updates, _) = state.apply(params)(x, rngs=rngs)
 
-    losses = updates.extract(Loss)
+# %%
+@nnx.jit
+def train_step(model: VAE, optimizer: nnx.Optimizer, x: jax.Array):
+  def loss_fn(model: VAE):
+    logits = model(x)
+    losses = nnx.pop(model, Loss)
     kl_loss = sum(jax.tree_util.tree_leaves(losses), 0.0)
     reconstruction_loss = jnp.mean(
       optax.sigmoid_binary_cross_entropy(logits, x)
     )
-    # jax.debug.print("kl_loss={kl_loss}", kl_loss=kl_loss)
-
     loss = reconstruction_loss + 0.1 * kl_loss
     return loss
 
-  loss, grads = jax.value_and_grad(loss_fn)(state.params)
-  state = state.apply_gradients(grads=grads)
+  loss, grads = nnx.value_and_grad(loss_fn)(model)
+  optimizer.update(grads)
 
-  return state, loss
+  return loss
 
 
-@partial(jax.jit, donate_argnums=(0,))
-def forward(
-  state: nnx.TrainState[VAE], x: jax.Array, key: jax.Array
-) -> jax.Array:
-  rngs = nnx.Rngs(noise=key)
-  y_pred = state.apply('params')(x, rngs=rngs)[0]
+@nnx.jit
+def forward(model: VAE, x: jax.Array) -> jax.Array:
+  y_pred = model(x)
   return jax.nn.sigmoid(y_pred)
 
 
-@jax.jit
-def sample(state: nnx.TrainState[VAE], z: jax.Array) -> jax.Array:
-  return state.apply('params').generate(z)[0]
+@nnx.jit
+def sample(model: VAE, z: jax.Array) -> jax.Array:
+  return model.generate(z)
 
 
 # %%
-key = jax.random.key(0)
 
 for epoch in range(epochs):
   losses = []
   for step in range(steps_per_epoch):
     idxs = np.random.randint(0, len(X_train), size=(batch_size,))
     x_batch = X_train[idxs]
 
-    state, loss = train_step(state, x_batch, key)
+    loss = train_step(model, optimizer, x_batch)
     losses.append(np.asarray(loss))
 
   print(f'Epoch {epoch} loss: {np.mean(losses)}')
 
 # exit()
 # %%
 # get random samples
 idxs = np.random.randint(0, len(X_test), size=(5,))
 x_sample = X_test[idxs]
 
 # get predictions
-y_pred = forward(state, x_sample, key)
+y_pred = forward(model, x_sample)
 
 # plot reconstruction
 figure = plt.figure(figsize=(3 * 5, 3 * 2))
 plt.title('Reconstruction Samples')
 for i in range(5):
   plt.subplot(2, 5, i + 1)
   plt.imshow(x_sample[i], cmap='gray')
@@ -199,15 +187,15 @@
   # # tbwriter.add_figure("VAE Example", figure, epochs)
 
 plt.show()
 
 # %%
 # plot generative samples
 z_samples = np.random.normal(scale=1.5, size=(12, latent_size))
-samples = sample(state, z_samples)
+samples = sample(model, z_samples)
 
 figure = plt.figure(figsize=(3 * 5, 3 * 2))
 plt.title('Generative Samples')
 for i in range(5):
   plt.subplot(2, 5, 2 * i + 1)
   plt.imshow(samples[i], cmap='gray')
   plt.subplot(2, 5, 2 * i + 2)
```

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/toy_examples/06_scan_over_layers.py` & `flax-0.8.3/flax/experimental/nnx/tests/test_helpers.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,80 +8,62 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Tuple
-
 import jax
 import jax.numpy as jnp
+import optax
 
 from flax.experimental import nnx
 
+class TrainState(nnx.TrainState):
+  batch_stats: nnx.State
 
-class Block(nnx.Module):
-  def __init__(self, dim: int, *, rngs: nnx.Rngs):
-    self.linear = nnx.Linear(dim, dim, rngs=rngs)
-    self.dropout = nnx.Dropout(0.5)
-
-  def __call__(self, x: jax.Array, *, rngs: nnx.Rngs) -> jax.Array:
-    x = self.linear(x)
-    x = self.dropout(x, rngs=rngs)
-    x = jax.nn.gelu(x)
-    return x
-
-
-class ScanMLP(nnx.Module):
-  """
-  An MLP that uses `vmap` during `__init__` to create a Block instance
-  with an additional `layer` axis, and `scan` during `__call__` to apply
-  the sequence of layers iteratively over the input / output `x`.
-  """
-
-  def __init__(self, dim: int, *, n_layers: int, rngs: nnx.Rngs):
-    self.n_layers = n_layers
-    # fork Rngs, split keys into `n_layers`
-    keys = rngs.fork(n_layers)
-
-    def create_block(keys):
-      # create Block instance and return its split
-      return Block(dim, rngs=nnx.Rngs(keys)).split()
-
-    # call vmap over create_block, passing the split `params` key
-    # and immediately merge to get a Block instance
-    self.layers = nnx.merge(jax.vmap(create_block)(keys))
-
-  def __call__(self, x: jax.Array, *, rngs: nnx.Rngs) -> jax.Array:
-    # fork Rngs, split keys into `n_layers`
-    keys = rngs.fork(self.n_layers)
-    # split Module to get params
-    params, static = self.layers.split(nnx.Param)
-
-    def scan_fn(
-      x: jax.Array, inputs: Tuple[nnx.State, dict[str, nnx.RngStream]]
-    ) -> Tuple[jax.Array, nnx.State]:
-      params, keys = inputs
-      # merge back Module and Rngs
-      module = static.merge(params)
-      # forward pass
-      x = module(x, rngs=nnx.Rngs(keys))
-      # split state and return
-      params, _ = module.split(nnx.Param)
-      return x, params
-
-    # call scan passing x as the carry, and params + keys as the input
-    x, params = jax.lax.scan(scan_fn, x, (params, keys))
-    # update layers state and return
-    self.layers.update(params)
-    return x
-
-
-model = ScanMLP(10, n_layers=5, rngs=nnx.Rngs(0))
-
-x = jnp.ones((3, 10))
-model.set_attributes(deterministic=False)
-y = model(x, rngs=nnx.Rngs(dropout=1))
 
-print(jax.tree_map(jnp.shape, model.get_state()))
-print(y.shape)
+class TestHelpers:
+  def test_train_state(self):
+    m = nnx.Dict(a=nnx.Param(1), b=nnx.BatchStat(2))
+
+    graphdef, params, batch_stats = nnx.split(m, nnx.Param, nnx.BatchStat)
+
+    state = TrainState.create(
+      graphdef,
+      params=params,
+      tx=optax.sgd(1.0),
+      batch_stats=batch_stats,
+    )
+
+    leaves = jax.tree_util.tree_leaves(state)
+
+  def test_train_state_methods(self):
+    class Foo(nnx.Module):
+      def __init__(self, *, rngs: nnx.Rngs):
+        self.linear = nnx.Linear(2, 4, rngs=rngs)
+        self.batch_norm = nnx.BatchNorm(4, rngs=rngs)
+
+      def __call__(self, x: jax.Array, train: bool) -> jax.Array:
+        x = self.linear(x)
+        x = self.batch_norm(x, use_running_average=not train)
+        return x
+
+    module = Foo(rngs=nnx.Rngs(0))
+    graphdef, params, batch_stats = nnx.split(module, nnx.Param, nnx.BatchStat)
+
+    state = TrainState.create(
+      graphdef,
+      params=params,
+      tx=optax.sgd(1.0),
+      batch_stats=batch_stats,
+    )
+
+    x = jax.numpy.ones((1, 2))
+    y, _updates = state.apply('params', 'batch_stats')(x, train=True)
+
+    assert y.shape == (1, 4)
+
+    # fake gradient
+    grads = jax.tree_util.tree_map(jnp.ones_like, state.params)
+    # test apply_gradients
+    state = state.apply_gradients(grads)
```

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/toy_examples/08_save_load_checkpoints.py` & `flax-0.8.3/flax/experimental/nnx/examples/toy_examples/08_save_load_checkpoints.py`

 * *Files 9% similar despite different names*

```diff
@@ -35,28 +35,29 @@
 
 def create_model(seed: int):
   return MLP(10, 20, 30, rngs=nnx.Rngs(seed))
 
 
 def create_and_save(seed: int, path: str):
   model = create_model(seed)
-  state = model.get_state()
+  state = nnx.state(model)
   # Save the parameters
   checkpointer = orbax.PyTreeCheckpointer()
   checkpointer.save(f'{path}/state', state)
 
 
 def load_model(path: str) -> MLP:
   # create that model with abstract shapes
-  state, static = jax.eval_shape(lambda: create_model(0).split())
+  model = nnx.eval_shape(lambda: create_model(0))
+  state = nnx.state(model)
   # Load the parameters
   checkpointer = orbax.PyTreeCheckpointer()
   state = checkpointer.restore(f'{path}/state', item=state)
-  # Merge the parameters into the model
-  model = static.merge(state)
+  # update the model with the loaded state
+  nnx.update(model, state)
   return model
 
 
 with TemporaryDirectory() as tmpdir:
   # create a checkpoint
   create_and_save(42, tmpdir)
   # load model from checkpoint
```

### Comparing `flax-0.8.2/flax/experimental/nnx/examples/toy_examples/09_parameter_surgery.py` & `flax-0.8.3/flax/experimental/nnx/examples/toy_examples/09_parameter_surgery.py`

 * *Files 12% similar despite different names*

```diff
@@ -42,15 +42,20 @@
 model = Classifier(rngs=nnx.Rngs(42))
 model.backbone = load_pretrained()
 
 
 # create a filter to select all the parameters that are not part of the
 # backbone, i.e. the classifier parameters
 is_trainable = lambda path, node: (
-  path.startswith('backbone') and isinstance(node, nnx.Param)
+  'backbone' in path and isinstance(node, nnx.Param)
 )
 
 # split the parameters into trainable and non-trainable parameters
-trainable_params, non_trainable, static = model.split(is_trainable, ...)
+graphdef, trainable_params, non_trainable = nnx.split(model, is_trainable, ...)
 
-print('trainable_params =', jax.tree_map(jax.numpy.shape, trainable_params))
-print('non_trainable = ', jax.tree_map(jax.numpy.shape, non_trainable))
+print(
+  'trainable_params =',
+  jax.tree_util.tree_map(jax.numpy.shape, trainable_params),
+)
+print(
+  'non_trainable = ', jax.tree_util.tree_map(jax.numpy.shape, non_trainable)
+)
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/__init__.py` & `flax-0.8.3/flax/experimental/nnx/nnx/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/compatibility.py` & `flax-0.8.3/flax/experimental/nnx/nnx/compatibility.py`

 * *Files 3% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import dataclasses
 import typing as tp
 from typing import Any
 
+from flax.experimental import nnx
 from flax import linen
 from flax.experimental.nnx.nnx import variables as variableslib
 from flax.experimental.nnx.nnx.module import GraphDef, Module
 from flax.experimental.nnx.nnx.rnglib import Rngs
 from flax.experimental.nnx.nnx.state import State
 
 M = tp.TypeVar('M', bound=Module)
@@ -34,15 +35,15 @@
   kwargs: dict[str, tp.Any]
 
   def init(self, *, rngs: tp.Optional[Rngs] = None) -> State:
     kwargs = {}
     if rngs is not None:
       kwargs['rngs'] = rngs
     module = self.module_type(*self.args, **self.kwargs, **kwargs)
-    state, graphdef = module.split()
+    graphdef, state = nnx.split(module)
     self.graphdef = graphdef
     return state
 
   def apply(self, *states: tp.Any):
     assert self.graphdef is not None
     return self.graphdef.apply(*states)
 
@@ -61,15 +62,17 @@
     *args: tp.Any,
     rngs: tp.Optional[Rngs] = None,
     **kwargs: tp.Any,
   ):
     self.module = module
 
     _rngs = (
-      {name: stream.key for name, stream in rngs._rngs.items()} if rngs else {}
+      {name: stream.key.raw_value for name, stream in rngs.items()}
+      if rngs
+      else {}
     )
     # rename default to params
     if 'params' not in _rngs and 'default' in _rngs:
       _rngs['params'] = _rngs['default']
       del _rngs['default']
 
     variables = module.init(_rngs, *args, **kwargs)
@@ -79,15 +82,15 @@
       for collection, value in variables.items()
     }
 
   def __call__(
     self, *args: Any, rngs: tp.Optional[Rngs] = None, **kwargs: Any
   ) -> Any:
     _rngs = (
-      {name: stream.key for name, stream in rngs._rngs.items()} if rngs else {}
+      {name: stream.key.value for name, stream in rngs.items()} if rngs else {}
     )
 
     variables = {
       collection: value.value for collection, value in self.states.items()
     }
     out = self.module.apply(variables, *args, rngs=_rngs, **kwargs)
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/errors.py` & `flax-0.8.3/flax/experimental/nnx/nnx/errors.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/filterlib.py` & `flax-0.8.3/flax/experimental/nnx/nnx/filterlib.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,30 +10,40 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import builtins
 import dataclasses
-from flax.typing import Path
+from flax.typing import PathParts
 import typing as tp
 
 if tp.TYPE_CHECKING:
   ellipsis = builtins.ellipsis
 else:
   ellipsis = tp.Any
 
-Predicate = tp.Callable[[Path, tp.Any], bool]
+Predicate = tp.Callable[[PathParts, tp.Any], bool]
+
 FilterLiteral = tp.Union[type, str, Predicate, bool, ellipsis, None]
-Filter = tp.Union[FilterLiteral, tuple[FilterLiteral, ...], list[FilterLiteral]]
+Filter = tp.Union[FilterLiteral, tuple['Filter', ...], list['Filter']]
+
+
+@tp.runtime_checkable
+class _HasTag(tp.Protocol):
+  tag: str
+
+@tp.runtime_checkable
+class _HasType(tp.Protocol):
+  type: type
 
 
 def to_predicate(filter: Filter) -> Predicate:
   if isinstance(filter, str):
-    return AtPath(filter)
+    return WithTag(filter)
   elif isinstance(filter, type):
     return OfType(filter)
   elif isinstance(filter, bool):
     return Everything() if filter else Nothing()
   elif filter is Ellipsis:
     return Everything()
   elif filter is None:
@@ -43,58 +53,62 @@
   elif isinstance(filter, (list, tuple)):
     return Any(*filter)
   else:
     raise TypeError(f'Invalid collection filter: {filter:!r}. ')
 
 
 @dataclasses.dataclass
-class AtPath:
-  path: str
+class WithTag:
+  tag: str
 
-  def __call__(self, path: Path, x: tp.Any):
-    return self.path == path
+  def __call__(self, path: PathParts, x: tp.Any):
+    return isinstance(x, _HasTag) and x.tag == self.tag
 
 
 @dataclasses.dataclass
 class OfType:
   type: type
 
-  def __call__(self, path: Path, x: tp.Any):
-    return isinstance(x, self.type)
+  def __call__(self, path: PathParts, x: tp.Any):
+    return (
+      isinstance(x, self.type)
+      or isinstance(x, _HasType)
+      and issubclass(x.type, self.type)
+    )
 
 
 class Any:
   def __init__(self, *filters: Filter):
     self.predicates = tuple(
       to_predicate(collection_filter) for collection_filter in filters
     )
 
-  def __call__(self, path: Path, x: tp.Any):
+  def __call__(self, path: PathParts, x: tp.Any):
     return any(predicate(path, x) for predicate in self.predicates)
 
 
 class All:
   def __init__(self, *filters: Filter):
     self.predicates = tuple(
       to_predicate(collection_filter) for collection_filter in filters
     )
 
-  def __call__(self, path: Path, x: tp.Any):
+  def __call__(self, path: PathParts, x: tp.Any):
     return all(predicate(path, x) for predicate in self.predicates)
 
 
 class Not:
-  def __init__(self, collection_filter: Filter):
+  def __init__(self, collection_filter: Filter, /):
     self.predicate = to_predicate(collection_filter)
 
-  def __call__(self, path: Path, x: tp.Any):
+  def __call__(self, path: PathParts, x: tp.Any):
     return not self.predicate(path, x)
 
 
 class Everything:
-  def __call__(self, path: Path, x: tp.Any):
+  def __call__(self, path: PathParts, x: tp.Any):
     return True
 
 
 class Nothing:
-  def __call__(self, path: Path, x: tp.Any):
+  def __call__(self, path: PathParts, x: tp.Any):
     return False
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/helpers.py` & `flax-0.8.3/flax/experimental/nnx/nnx/helpers.py`

 * *Files 9% similar despite different names*

```diff
@@ -28,35 +28,36 @@
 from __future__ import annotations
 
 import inspect
 import typing as tp
 
 import jax
 import jax.numpy as jnp
-import numpy as np
 import optax
 
-from flax.experimental.nnx.nnx import pytreelib
+from flax.experimental.nnx.nnx.graph import Key
 from flax.experimental.nnx.nnx.module import GraphDef, Module
 from flax.experimental.nnx.nnx.proxy_caller import ApplyCaller
 from flax.experimental.nnx.nnx.rnglib import Rngs
 from flax.experimental.nnx.nnx.state import State
+from flax.training.train_state import struct
 
 A = tp.TypeVar('A')
 M = tp.TypeVar('M', bound=Module)
+TS = tp.TypeVar('TS', bound='TrainState')
 
 
 class Dict(Module, tp.Mapping[str, A]):
   @tp.overload
-  def __init__(self, __iterable: tp.Iterable[tp.Tuple[str, A]]):
+  def __init__(self, iterable: tp.Iterable[tp.Tuple[str, A]], /):
     ...
 
   @tp.overload
   def __init__(
-    self, __mapping: tp.Optional[tp.Mapping[str, A]] = None, **kwargs: A
+    self, mapping: tp.Optional[tp.Mapping[str, A]] = None, /, **kwargs: A
   ):
     ...
 
   def __init__(self, *args, **kwargs):
     for name, value in dict(*args, **kwargs).items():
       setattr(self, name, value)
 
@@ -69,44 +70,67 @@
   def __getattr__(self, key) -> A:
     return super().__getattribute__(key)
 
   def __setattr__(self, key, value):
     super().__setattr__(key, value)
 
   def __iter__(self) -> tp.Iterator[str]:
-    return (k for k in vars(self) if k != '_module__state')
+    return (k for k in vars(self) if k != '_graph_node__state')
 
   def __len__(self) -> int:
     return len(vars(self))
 
 
-class Sequence(Module, tp.Generic[A]):
-  def __init__(self, layers: tp.Iterable[A]):
+class List(Module, tp.Generic[A]):
+  def __init__(self, elems: tp.Iterable[A], /):
     i = 0
-    for i, value in enumerate(layers):
+    for i, value in enumerate(elems):
       setattr(self, str(i), value)
     self._length = i + 1
 
   def __getitem__(self, key: int) -> A:
-    if key >= len(self):
+    if key >= len(self) or key < -len(self):
       raise IndexError(f'index {key} out of range for {self}')
+    if key < 0:
+      key = self._length + key
     return getattr(self, str(key))
 
   def __setitem__(self, key: int, value: A):
     if key >= len(self):
       raise IndexError(f'index {key} out of range for {self}')
     setattr(self, str(key), value)
 
   def __iter__(self) -> tp.Iterator[A]:
     for i in range(len(self)):
       yield getattr(self, str(i))
 
   def __len__(self) -> int:
     return self._length
 
+  def _graph_node_flatten(self):
+    nodes: list[tuple[Key, tp.Any]] = sorted(
+      (int(key), value)
+      for key, value in vars(self).items()
+      if key not in ('_graph_node__state', '_length')
+    )
+    nodes.append(('_length', self._length))
+    return nodes, type(self)
+
+  def _graph_node_set_key(self, key: Key, value: tp.Any):
+    if isinstance(key, int):
+      key = str(key)
+    return super()._graph_node_set_key(key, value)
+
+  def _graph_node_pop_key(self, key: Key):
+    if isinstance(key, int):
+      key = str(key)
+    return super()._graph_node_pop_key(key)
+
+
+class Sequential(List):
   def __call__(self, *args, rngs: tp.Optional[Rngs] = None, **kwargs) -> tp.Any:
     output: tp.Any = None
 
     for i, f in enumerate(self):
       if not callable(f):
         raise TypeError(f'Sequence[{i}] is not callable: {f}')
       if i > 0:
@@ -130,52 +154,65 @@
 class ModuleDefApply(tp.Protocol, tp.Generic[M]):
   def __call__(
     self, state: State, *states: State
   ) -> ApplyCaller[tuple[State, GraphDef[M]]]:
     ...
 
 
-class TrainState(pytreelib.Pytree, tp.Generic[M]):
-  def __init__(
-    self,
+class TrainState(tp.Generic[M], struct.PyTreeNode):
+  graphdef: GraphDef[M]
+  params: State
+  tx: optax.GradientTransformation = struct.field(pytree_node=False)
+  opt_state: optax.OptState
+  step: jax.Array
+
+  @classmethod
+  def create(
+    cls,
     graphdef: GraphDef[M],
     *,
     params: State,
     tx: optax.GradientTransformation,
     step: int = 0,
     **kwargs,
   ):
-    self.graphdef = graphdef
-    self.params: State = pytreelib.TreeNode(params)
-    self.tx = tx
-    self.opt_state = pytreelib.TreeNode(tx.init(self.params))
-    self.step = pytreelib.TreeNode(jnp.asarray(step))
-    for name, value in kwargs.items():
-      if isinstance(value, (jax.Array, np.ndarray, State)):
-        value = pytreelib.TreeNode(value)
-      setattr(self, name, value)
+    return cls(
+      graphdef=graphdef,
+      params=params,
+      tx=tx,
+      opt_state=tx.init(params),
+      step=jnp.asarray(step),
+      **kwargs,
+    )
 
   if tp.TYPE_CHECKING:
 
     def __getattr__(self, key: str) -> tp.Any:
       ...
 
   def apply(
     self, state: tp.Union[State, str], *states: tp.Union[State, str]
-  ) -> ApplyCaller[tuple[State, GraphDef[M]]]:
+  ) -> ApplyCaller[tuple[GraphDef[M], State]]:
     states = (state, *states)
 
-    _states = (
-      getattr(self, state) if isinstance(state, str) else state
-      for state in states
-    )
+    _states: list[State] = []
+
+    for _state in states:
+      if isinstance(_state, str):
+        _state_key = _state
+        _state = getattr(self, _state_key)
+        if not isinstance(_state, State):
+          raise TypeError(
+            f'Expected {self.__class__.__name__}.{_state_key} to be a State, got {type(_state)}'
+          )
+      _states.append(_state)
 
     return self.graphdef.apply(*_states)
 
-  def apply_gradients(self, grads: State, **kwargs) -> 'TrainState[M]':
+  def apply_gradients(self: TS, grads: State, **kwargs) -> TS:
     updates, opt_state = self.tx.update(grads, self.opt_state, self.params)
     params = optax.apply_updates(self.params, updates)  # type: ignore
     step = self.step + 1
     return self.replace(
       params=params,
       opt_state=opt_state,
       step=step,
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/ids.py` & `flax-0.8.3/flax/experimental/nnx/nnx/ids.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/nn/__init__.py` & `flax-0.8.3/flax/experimental/nnx/nnx/nn/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/nn/activations.py` & `flax-0.8.3/flax/experimental/nnx/nnx/nn/activations.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/nn/attention.py` & `flax-0.8.3/flax/experimental/nnx/nnx/nn/attention.py`

 * *Files 1% similar despite different names*

```diff
@@ -149,23 +149,24 @@
 ):
   """Computes dot-product attention given query, key, and value.
 
   This is the core function for applying attention based on
   https://arxiv.org/abs/1706.03762. It calculates the attention weights given
   query and key and combines the values using the attention weights.
 
-  Note: query, key, value needn't have any batch dimensions.
+  .. note::
+    ``query``, ``key``, ``value`` needn't have any batch dimensions.
 
   Args:
-    query: queries for calculating attention with shape of `[batch..., q_length,
-      num_heads, qk_depth_per_head]`.
-    key: keys for calculating attention with shape of `[batch..., kv_length,
-      num_heads, qk_depth_per_head]`.
-    value: values to be used in attention with shape of `[batch..., kv_length,
-      num_heads, v_depth_per_head]`.
+    query: queries for calculating attention with shape of ``[batch..., q_length,
+      num_heads, qk_depth_per_head]``.
+    key: keys for calculating attention with shape of ``[batch..., kv_length,
+      num_heads, qk_depth_per_head]``.
+    value: values to be used in attention with shape of ``[batch..., kv_length,
+      num_heads, v_depth_per_head]``.
     bias: bias for the attention weights. This should be broadcastable to the
       shape `[batch..., num_heads, q_length, kv_length]`. This can be used for
       incorporating causal masks, padding masks, proximity bias, etc.
     mask: mask for the attention weights. This should be broadcastable to the
       shape `[batch..., num_heads, q_length, kv_length]`. This can be used for
       incorporating causal masks. Attention weights are masked out if their
       corresponding mask value is `False`.
@@ -593,29 +594,29 @@
     ``decode=True``, this method must be called first before performing
     forward inference.
 
     Example usage::
 
       >>> from flax.experimental import nnx
       >>> import jax.numpy as jnp
-
+      ...
       >>> rngs = nnx.Rngs(42)
-
+      ...
       >>> x = jnp.ones((1, 3))
       >>> model_nnx = nnx.MultiHeadAttention(
       ...   num_heads=2,
       ...   in_features=3,
       ...   qkv_features=6,
       ...   out_features=6,
       ...   decode=True,
       ...   rngs=rngs,
-      >>> )
-
-      >>> # out_nnx = model_nnx(x) <-- throws an error because cache isn't initialized
-
+      ... )
+      ...
+      >>> # out_nnx = model_nnx(x)  <-- throws an error because cache isn't initialized
+      ...
       >>> model_nnx.init_cache(x.shape)
       >>> out_nnx = model_nnx(x)
     """
     cache_shape = (*input_shape[:-1], self.num_heads, self.head_dim)
     self.cached_key = nnx.Cache(jnp.zeros(cache_shape, dtype))
     self.cached_value = nnx.Cache(jnp.zeros(cache_shape, dtype))
     self.cache_index = nnx.Cache(jnp.array(0, dtype=jnp.int32))
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/nn/dtypes.py` & `flax-0.8.3/flax/experimental/nnx/nnx/nn/dtypes.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/nn/initializers.py` & `flax-0.8.3/flax/experimental/nnx/nnx/nn/initializers.py`

 * *Files 3% similar despite different names*

```diff
@@ -23,14 +23,15 @@
 from jax.nn.initializers import kaiming_normal as kaiming_normal
 from jax.nn.initializers import kaiming_uniform as kaiming_uniform
 from jax.nn.initializers import lecun_normal as lecun_normal
 from jax.nn.initializers import lecun_uniform as lecun_uniform
 from jax.nn.initializers import normal as normal
 from jax.nn.initializers import ones as ones
 from jax.nn.initializers import orthogonal as orthogonal
+from jax.nn.initializers import truncated_normal as truncated_normal
 from jax.nn.initializers import uniform as uniform
 from jax.nn.initializers import variance_scaling as variance_scaling
 from jax.nn.initializers import xavier_normal as xavier_normal
 from jax.nn.initializers import xavier_uniform as xavier_uniform
 from jax.nn.initializers import zeros as zeros
 from flax.typing import Initializer
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/nn/linear.py` & `flax-0.8.3/flax/experimental/nnx/nnx/nn/linear.py`

 * *Files 14% similar despite different names*

```diff
@@ -24,28 +24,30 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
 import typing as tp
-from types import MappingProxyType
 
 import jax
 import jax.numpy as jnp
 import numpy as np
 from jax import lax
+import opt_einsum
 
+from flax.core.frozen_dict import FrozenDict
 from flax.experimental import nnx
 from flax.experimental.nnx.nnx import rnglib, variables
-from flax.experimental.nnx.nnx.module import Module
+from flax.experimental.nnx.nnx.module import Module, first_from
 from flax.experimental.nnx.nnx.nn import dtypes, initializers
 from flax.typing import (
   Array,
   Dtype,
+  Shape,
   Initializer,
   PrecisionLike,
   DotGeneralT,
   ConvGeneralDilatedT,
   PaddingLike,
   LaxPadding,
 )
@@ -103,32 +105,40 @@
 
 
 class LinearGeneral(Module):
   """A linear transformation with flexible axes.
 
   Example usage::
 
-    >>> import flax.linen as nn
+    >>> from flax.experimental import nnx
     >>> import jax, jax.numpy as jnp
-
-    >>> # equivalent to `nn.Linear(features=4)`
-    >>> layer = nn.LinearGeneral(features=4)
+    ...
+    >>> # equivalent to `nnx.Linear(2, 4)`
+    >>> layer = nnx.LinearGeneral(2, 4, rngs=nnx.Rngs(0))
+    >>> layer.kernel.value.shape
+    (2, 4)
     >>> # output features (4, 5)
-    >>> layer = nn.LinearGeneral(features=(4, 5))
-    >>> params = layer.init(jax.random.key(0), jnp.ones((1, 3)))
-    >>> jax.tree_map(jnp.shape, params)
-    {'params': {'bias': (4, 5), 'kernel': (3, 4, 5)}}
+    >>> layer = nnx.LinearGeneral(2, (4, 5), rngs=nnx.Rngs(0))
+    >>> layer.kernel.value.shape
+    (2, 4, 5)
+    >>> layer.bias.value.shape
+    (4, 5)
     >>> # apply transformation on the the second and last axes
-    >>> layer = nn.LinearGeneral(features=(4, 5), axis=(1, -1))
-    >>> params = layer.init(jax.random.key(0), jnp.ones((1, 3, 6, 7)))
-    >>> jax.tree_map(jnp.shape, params)
-    {'params': {'bias': (4, 5), 'kernel': (3, 7, 4, 5)}}
+    >>> layer = nnx.LinearGeneral((2, 3), (4, 5), axis=(1, -1), rngs=nnx.Rngs(0))
+    >>> layer.kernel.value.shape
+    (2, 3, 4, 5)
+    >>> layer.bias.value.shape
+    (4, 5)
+    >>> y = layer(jnp.ones((16, 2, 3)))
+    >>> y.shape
+    (16, 4, 5)
 
   Attributes:
-    features: int or tuple with number of output features.
+    in_features: int or tuple with number of output features.
+    out_features: int or tuple with number of output features.
     axis: int or tuple with axes to apply the transformation on. For instance,
       (-2, -1) will apply the transformation to the last two axes.
     batch_dims: tuple with batch axes.
     use_bias: whether to add a bias to the output (default: True).
     dtype: the dtype of the computation (default: infer from input and params).
     param_dtype: the dtype passed to parameter initializers (default: float32).
     kernel_init: initializer function for the weight matrix.
@@ -139,30 +149,30 @@
 
   def __init__(
     self,
     in_features: Size | tp.Sequence[Size],
     out_features: Size | tp.Sequence[Size],
     *,
     axis: Axis | tp.Sequence[Axis] = -1,
-    batch_axis: tp.Mapping[Axis, Size] = MappingProxyType({}),
+    batch_axis: tp.Mapping[Axis, Size] = FrozenDict({}),
     use_bias: bool = True,
     dtype: Dtype | None = None,
     param_dtype: Dtype = jnp.float32,
     kernel_init: Initializer = default_kernel_init,
     bias_init: Initializer = initializers.zeros_init(),
     precision: PrecisionLike = None,
     # Deprecated. Will be removed.
     dot_general: DotGeneralT | None = None,
     dot_general_cls: tp.Any = None,
     rngs: rnglib.Rngs,
   ):
     self.in_features = _canonicalize_tuple(in_features)
     self.out_features = _canonicalize_tuple(out_features)
     self.axis = _canonicalize_tuple(axis)
-    self.batch_axis = MappingProxyType(batch_axis)
+    self.batch_axis = FrozenDict(batch_axis)
     self.use_bias = use_bias
     self.dtype = dtype
     self.param_dtype = param_dtype
     self.kernel_init = kernel_init
     self.bias_init = bias_init
     self.precision = precision
     self.dot_general = dot_general
@@ -189,15 +199,15 @@
 
     def kernel_init_wrap(rng, shape, dtype):
       flat_shape = (
         np.prod(shape[:n_batch_axis])
         * np.prod(shape[n_batch_axis : n_in_features + n_batch_axis]),
         np.prod(shape[-n_out_features:]),
       )
-      flat_shape = jax.tree_map(int, flat_shape)
+      flat_shape = jax.tree_util.tree_map(int, flat_shape)
       kernel = self.kernel_init(rng, flat_shape, dtype)
       if isinstance(kernel, variables.VariableMetadata):
         kernel.raw_value = jnp.reshape(kernel.raw_value, shape)
       else:
         kernel = jnp.reshape(kernel, shape)
 
       return kernel
@@ -352,14 +362,168 @@
       precision=self.precision,
     )
     if bias is not None:
       y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))
     return y
 
 
+class Einsum(Module):
+  """An einsum transformation with learnable kernel and bias.
+
+  Example usage::
+
+    >>> from flax.experimental import nnx
+    >>> import jax.numpy as jnp
+    ...
+    >>> layer = nnx.Einsum('nta,hab->nthb', (8, 2, 4), (8, 4), rngs=nnx.Rngs(0))
+    >>> layer.kernel.value.shape
+    (8, 2, 4)
+    >>> layer.bias.value.shape
+    (8, 4)
+    >>> y = layer(jnp.ones((16, 11, 2)))
+    >>> y.shape
+    (16, 11, 8, 4)
+
+  Attributes:
+    einsum_str: a string to denote the einsum equation. The equation must
+      have exactly two operands, the lhs being the input passed in, and
+      the rhs being the learnable kernel. Exactly one of ``einsum_str``
+      in the constructor argument and call argument must be not None,
+      while the other must be None.
+    kernel_shape: the shape of the kernel.
+    bias_shape: the shape of the bias. If this is None, a bias won't be used.
+    dtype: the dtype of the computation (default: infer from input and params).
+    param_dtype: the dtype passed to parameter initializers (default: float32).
+    precision: numerical precision of the computation see ``jax.lax.Precision``
+      for details.
+    kernel_init: initializer function for the weight matrix.
+    bias_init: initializer function for the bias.
+    rngs: rng key.
+  """
+
+  def __init__(
+    self,
+    einsum_str: str,
+    kernel_shape: Shape,
+    bias_shape: tp.Optional[Shape] = None,
+    *,
+    dtype: tp.Optional[Dtype] = None,
+    param_dtype: Dtype = jnp.float32,
+    precision: PrecisionLike = None,
+    kernel_init: Initializer = default_kernel_init,
+    bias_init: Initializer = initializers.zeros_init(),
+    rngs: rnglib.Rngs,
+  ):
+    einsum_str = einsum_str.replace(' ', '')
+    self._einsum_str_check(einsum_str)
+
+    kernel_key = rngs.params()
+    self.kernel = nnx.Param(kernel_init(kernel_key, kernel_shape, param_dtype))
+
+    if bias_shape is not None:
+      bias_key = rngs.params()
+      self.bias = nnx.Param(bias_init(bias_key, bias_shape, param_dtype))
+    else:
+      self.bias = None
+
+    self.einsum_str = einsum_str
+    self.kernel_shape = kernel_shape
+    self.bias_shape = bias_shape
+    self.dtype = dtype
+    self.param_dtype = param_dtype
+    self.precision = precision
+    self.kernel_init = kernel_init
+    self.bias_init = bias_init
+
+  def __call__(
+    self, inputs: Array, einsum_str: tp.Optional[str] = None
+  ) -> Array:
+    """Applies a linear transformation to the inputs along the last dimension.
+
+    Args:
+      inputs: The nd-array to be transformed.
+      einsum_str: a string to denote the einsum equation. The equation must
+        have exactly two operands, the lhs being the input passed in, and
+        the rhs being the learnable kernel. Exactly one of ``einsum_str``
+        in the constructor argument and call argument must be not None,
+        while the other must be None.
+
+    Returns:
+      The transformed input.
+    """
+    einsum_str = first_from(
+      einsum_str,
+      self.einsum_str,
+      error_msg="""No `einsum_str` argument was provided to Einsum
+        as either a __call__ argument, or class attribute.""",
+    )
+    einsum_str = einsum_str.replace(' ', '')
+    self._einsum_str_check(einsum_str)
+
+    inputs, kernel, bias = dtypes.promote_dtype(
+      inputs,
+      self.kernel.value,
+      self.bias.value if self.bias is not None else self.bias,
+      dtype=self.dtype,
+    )
+
+    y = jnp.einsum(einsum_str, inputs, kernel, precision=self.precision)
+
+    if bias is not None:
+      broadcasted_bias_shape = self._infer_broadcasted_bias_shape(
+        einsum_str, inputs, kernel
+      )
+      y += jnp.reshape(bias, broadcasted_bias_shape)
+    return y
+
+  def _infer_broadcasted_bias_shape(
+    self, einsum_str: str, lhs: Array, rhs: Array
+  ):
+    """Infer the broadcasted bias shape given the ``einsum_str``, ``lhs``
+    and ``rhs`` arrays. This is needed reshaping the bias and it to the
+    output during forward inference.
+
+    This function first replaces all ellipses with actual letter characters,
+    then computes the broadcasted bias shape by checking to see which axes in
+    the rhs array remain in the resulting array after einsumming. These axes
+    are the embedding/feature dimensions, and all other axes in rhs are
+    reduction axes.
+    """
+    # More details on the parsing function: https://github.com/dgasmith/opt_einsum/blob/c826bb7df16f470a69f7bf90598fc27586209d11/opt_einsum/parser.py#L246
+    # returns the einsum string representation of the operands and result, with
+    # ellipsis replaced by actual letter characters
+    operands_str, result_str, _ = opt_einsum.parser.parse_einsum_input(
+      (einsum_str, lhs, rhs)
+    )
+
+    # rhs_dict is a dict{character:index} mapping that maps every character in
+    # the rhs einsum string representation to its corresponding index position in the string
+    rhs_dict = {c: i for i, c in enumerate(operands_str.split(',')[1])}
+    assert len(rhs_dict) == len(self.kernel_shape)
+
+    broadcasted_bias_shape = [1] * len(result_str)
+    for i, c in enumerate(result_str):
+      if c in rhs_dict:
+        broadcasted_bias_shape[i] = self.kernel_shape[rhs_dict[c]]
+
+    return broadcasted_bias_shape
+
+  def _einsum_str_check(self, einsum_str):
+    if '->' not in einsum_str:
+      raise ValueError(
+        '`einsum_str` equation must be explicit and include "->".'
+      )
+    if einsum_str.count(',') != 1:
+      raise ValueError(
+        '`einsum_str` equation must have exactly two operands and '
+        'therefore, exactly one comma character, instead of '
+        f'{einsum_str.count(",")}'
+      )
+
+
 class Conv(Module):
   """Convolution Module wrapping `lax.conv_general_dilated[_local]`.
 
   Attributes:
     features: number of convolution filters.
     kernel_size: shape of the convolutional kernel. For 1D convolution,
       the kernel size can be passed as an integer, which will be interpreted
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/nn/normalization.py` & `flax-0.8.3/flax/experimental/nnx/nnx/nn/normalization.py`

 * *Files 0% similar despite different names*

```diff
@@ -212,15 +212,15 @@
       calculation for the variance.
   """
 
   def __init__(
     self,
     num_features: int,
     *,
-    use_running_average: tp.Optional[bool] = None,
+    use_running_average: bool = False,
     axis: int = -1,
     momentum: float = 0.99,
     epsilon: float = 1e-5,
     dtype: tp.Optional[Dtype] = None,
     param_dtype: Dtype = jnp.float32,
     use_bias: bool = True,
     use_scale: bool = True,
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/nn/stochastic.py` & `flax-0.8.3/flax/experimental/nnx/nnx/nn/stochastic.py`

 * *Files 15% similar despite different names*

```diff
@@ -8,22 +8,38 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Optional, Sequence
+# Copyright 2024 The Flax Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from __future__ import annotations
+
+import dataclasses
+from typing import Sequence
 
+import jax
 import jax.numpy as jnp
 from jax import lax, random
 
 from flax.experimental.nnx.nnx import rnglib
 from flax.experimental.nnx.nnx.module import Module, first_from
-import dataclasses
 
 
 @dataclasses.dataclass
 class Dropout(Module):
   """Create a dropout layer.
 
   Attributes:
@@ -33,24 +49,25 @@
       masked, whereas if true, no mask is applied and the inputs are returned
       as is.
     rng_collection: the rng collection name to use when requesting an rng key.
   """
 
   rate: float
   broadcast_dims: Sequence[int] = ()
-  deterministic: Optional[bool] = None
+  deterministic: bool = False
   rng_collection: str = 'dropout'
+  rngs: rnglib.Rngs | None = None
 
   def __call__(
     self,
     inputs,
     *,
-    deterministic: Optional[bool] = None,
-    rngs: Optional[rnglib.Rngs] = None,
-  ):
+    deterministic: bool | None = None,
+    rngs: rnglib.Rngs | None = None,
+  ) -> jax.Array:
     """Applies a random dropout mask to the input.
 
     Args:
       inputs: the inputs that should be randomly masked.
       deterministic: if false the inputs are scaled by `1 / (1 - rate)` and
         masked, whereas if true, no mask is applied and the inputs are returned
         as is.
@@ -58,28 +75,30 @@
     Returns:
       The masked inputs reweighted to preserve mean.
     """
     deterministic = first_from(
       deterministic,
       self.deterministic,
       error_msg="""No `deterministic` argument was provided to Dropout
-          as either a __call__ argument, class attribute, or nnx.flag.""",
+          as either a __call__ argument or class attribute""",
     )
 
     if (self.rate == 0.0) or deterministic:
       return inputs
 
     # Prevent gradient NaNs in 1.0 edge-case.
     if self.rate == 1.0:
       return jnp.zeros_like(inputs)
 
-    if rngs is None:
-      raise ValueError(
-        "Dropout needs to generate a random mask but no 'rngs' were provided."
-      )
+    rngs = first_from(
+      rngs,
+      self.rngs,
+      error_msg="""`deterministic` is False, but no `rngs` argument was provided to Dropout
+          as either a __call__ argument or class attribute.""",
+    )
 
     keep_prob = 1.0 - self.rate
     rng = rngs[self.rng_collection]()
     broadcast_shape = list(inputs.shape)
     for dim in self.broadcast_dims:
       broadcast_shape[dim] = 1
     mask = random.bernoulli(rng, p=keep_prob, shape=broadcast_shape)
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/proxy_caller.py` & `flax-0.8.3/flax/experimental/nnx/nnx/proxy_caller.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/reprlib.py` & `flax-0.8.3/flax/experimental/nnx/nnx/reprlib.py`

 * *Files 17% similar despite different names*

```diff
@@ -14,14 +14,17 @@
 
 import contextlib
 import dataclasses
 import threading
 import typing as tp
 from abc import ABC, abstractmethod
 
+A = tp.TypeVar('A')
+B = tp.TypeVar('B')
+
 
 @dataclasses.dataclass
 class ReprContext(threading.local):
   indent_stack: tp.List[str] = dataclasses.field(default_factory=lambda: [''])
 
 
 REPR_CONTEXT = ReprContext()
@@ -81,28 +84,42 @@
 
   def _repr_elem(elem: tp.Any) -> str:
     if not isinstance(elem, Attr):
       raise TypeError(f'Item must be Elem, got {type(elem).__name__}')
 
     value = elem.value if isinstance(elem.value, str) else repr(elem.value)
 
-    if '\n' in value and not isinstance(elem.value, Representable):
-      value = value.replace('\n', '\n' + get_indent())
+    value = value.replace('\n', '\n' + config.elem_indent)
 
-    return (
-      f'{get_indent()}{elem.start}{elem.key}{config.value_sep}{value}{elem.end}'
-    )
+    return f'{config.elem_indent}{elem.start}{elem.key}{config.value_sep}{value}{elem.end}'
 
   with add_indent(config.elem_indent):
     elems = list(map(_repr_elem, iterator))
   elems = ',\n'.join(elems)
 
   if elems:
-    elems = '\n' + elems + '\n' + get_indent()
+    elems = '\n' + elems + '\n'
   else:
     elems = config.empty_repr
 
   type_repr = (
     config.type if isinstance(config.type, str) else config.type.__name__
   )
 
   return f'{type_repr}{config.start}{elems}{config.end}'
+
+class MappingReprMixin(tp.Mapping[A, B]):
+  def __nnx_repr__(self):
+    yield Object(type='', value_sep=': ', start='{', end='}')
+
+    for key, value in self.items():
+      yield Attr(repr(key), value)
+
+@dataclasses.dataclass(repr=False)
+class PrettyMapping(Representable):
+  mapping: tp.Mapping
+
+  def __nnx_repr__(self):
+    yield Object(type='', value_sep=': ', start='{', end='}')
+
+    for key, value in self.mapping.items():
+      yield Attr(repr(key), value)
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/rnglib.py` & `flax-0.8.3/flax/experimental/nnx/nnx/rnglib.py`

 * *Files 16% similar despite different names*

```diff
@@ -28,141 +28,160 @@
 from __future__ import annotations
 
 import dataclasses
 import functools
 import typing as tp
 
 import jax
+import jax.numpy as jnp
 
-from flax.experimental.nnx.nnx import errors, filterlib, tracers
+from flax.experimental.nnx.nnx import graph
+from flax.experimental.nnx.nnx.state import State
+from flax.experimental.nnx.nnx.variables import Variable
+from flax.experimental.nnx.nnx import filterlib
+from flax.experimental.nnx.nnx.graph import GraphNode
 
 Counts = list[int]
 AxesValue = tp.Union[int, None]
-Pattern = tp.Union[AxesValue, tuple[AxesValue, ...]]
+SplitPattern = tp.Union[AxesValue, tuple[AxesValue, ...]]
 
 
 class Missing:
   pass
 
 
 MISSING = Missing()
 
 
-@dataclasses.dataclass
-class RngStream:
-  key: jax.Array  # dynamic
-  count: int  # static
+class RngState(Variable[jax.Array]):
+  pass
+
+
+class RngCount(RngState):
+  pass
+
+
+class RngKey(RngState):
+  tag: str
+
+class RngKeyBackup(RngState):
+  pass
+
+
+NotKey = filterlib.All(RngState, filterlib.Not(RngKey))
+
+
+@dataclasses.dataclass(repr=False)
+class RngStream(GraphNode):
+  def __init__(
+    self,
+    tag: str,
+    key: jax.Array,
+    count: jax.Array,
+  ):
+    self.key = RngKey(key, tag=tag)
+    self.count = RngCount(count)
+    self.key_backups: list[RngKeyBackup] = []
 
   def __post_init__(self):
     if not isinstance(self.key, jax.Array):
       raise TypeError(f'key must be a jax.Array, got {type(self.key)}')
 
-  def make_rng(self) -> jax.Array:
-    count = self.count
-    self.count += 1
-    return jax.random.fold_in(self.key, count)
+  def __call__(self) -> jax.Array:
+    self.check_valid_context(
+      'Cannot call RngStream from a different trace level'
+    )
+    key = jax.random.fold_in(self.key.value, self.count.value)
+    self.count.value += 1
+    return key
 
-  def fork(self, pattern: Pattern) -> jax.Array:
+  def fork(self, pattern: SplitPattern) -> jax.Array:
     if pattern is None:
       # broadcast key
-      key = self.make_rng()
+      key = self()
     else:
       if isinstance(pattern, int):
         num_splits = pattern
       else:
         num_splits = tuple(x if x is not None else 1 for x in pattern)
-      key = jax.random.split(self.key, num_splits)
-      self.count += 1
+      key = jax.random.split(self.key.value, num_splits)
+      self.count.value += 1
     return key
 
 
 RngValue = tp.Union[int, jax.Array]
 RngDict = tp.Union[
   tp.Mapping[str, int],
   tp.Mapping[str, jax.Array],
   tp.Mapping[str, RngValue],
 ]
 
 
-class Rngs(tp.Mapping[str, tp.Callable[[], jax.Array]]):
-  __slots__ = ('_trace_state', '_rngs', '_counts')
-
+class Rngs(GraphNode, tp.Mapping[str, tp.Callable[[], jax.Array]]):
   def __init__(
     self,
     default: RngValue | RngDict | None = None,
     /,
     **rngs: RngValue,
   ):
     if default is not None:
       if isinstance(default, tp.Mapping):
         rngs = {**default, **rngs}
       else:
         rngs['default'] = default
 
-    self._rngs = {
-      name: RngStream(
+    for name, value in rngs.items():
+      stream = RngStream(
+        tag=name,
         key=jax.random.key(value) if isinstance(value, int) else value,
-        count=0,
+        count=jnp.array(0, dtype=jnp.uint32),
       )
-      for name, value in rngs.items()
-    }
-    self._trace_state = tracers.TraceState()
-
-  def _make_rng(self, name: str, error_type: Exception) -> jax.Array:
-    if not self.is_valid():
-      raise errors.TraceContextError(
-        'Cannot use Rngs from a different trace level'
-      )
-    if name not in self._rngs:
-      if 'default' not in self._rngs:
+      setattr(self, name, stream)
+
+  def _get_stream(self, name: str, error_type: Exception) -> RngStream:
+    rngs_vars = vars(self)
+    if name not in rngs_vars:
+      if 'default' not in rngs_vars:
         raise error_type(f"No RNG named {name!r} or 'default' found in Rngs.")
-      stream = self._rngs['default']
+      stream = rngs_vars['default']
     else:
-      stream = self._rngs[name]
+      stream = rngs_vars[name]
 
-    return stream.make_rng()
+    return stream
 
-  def __getitem__(self, name: str) -> tp.Callable[[], jax.Array]:
-    return lambda: self._make_rng(name, KeyError)
+  def __getitem__(self, name: str):
+    return self._get_stream(name, KeyError)
 
-  def __getattr__(self, name: str) -> tp.Callable[[], jax.Array]:
-    return lambda: self._make_rng(name, AttributeError)
+  def __getattr__(self, name: str):
+    return self._get_stream(name, AttributeError)
 
   def __call__(self):
     return self.default()
 
   def __iter__(self) -> tp.Iterator[str]:
-    return iter(self._rngs)
+    for name in vars(self):
+      if name != '_graph_node__state':
+        yield name
 
   def __len__(self) -> int:
-    return len(self._rngs)
+    return len(vars(self)) - 1
 
   def __contains__(self, name: tp.Any) -> bool:
-    return name in self._rngs
+    return name in vars(self)
 
-  def replace(self, **kwargs: tp.Union[int, jax.Array, RngStream]) -> 'Rngs':
-    rngs: dict[str, tp.Any] = self._rngs.copy()
-    rngs.update(kwargs)
-    return Rngs(**rngs)
-
-  def is_valid(self) -> bool:
-    return self._trace_state.is_valid()
 
   def fork(
     self,
-    _default: Pattern | dict[filterlib.Filter, Pattern] | Missing = MISSING,
+    _default: SplitPattern
+    | dict[filterlib.Filter, SplitPattern]
+    | Missing = MISSING,
     /,
-    **patterns: Pattern,
+    **patterns: SplitPattern,
   ) -> ForkedKeys:
-    if not self.is_valid():
-      raise errors.TraceContextError(
-        'Cannot use Rngs from a different trace level'
-      )
-
-    filter_patterns: list[tuple[filterlib.Filter, Pattern]]
+    filter_patterns: list[tuple[filterlib.Filter, SplitPattern]]
     if isinstance(_default, dict):
       # merge default and patterns
       filter_patterns = [
         *_default.items(),
         *patterns.items(),
         (..., None),  # broadcast all remaining
       ]
@@ -177,17 +196,20 @@
       (filterlib.to_predicate(filter_), pattern)
       for filter_, pattern in filter_patterns
     ]
 
     splits: dict[str, jax.Array] = {}
     broadcasts: dict[str, jax.Array] = {}
 
-    for name, stream in self._rngs.items():
+    for name, stream in self.items():
       for predicate, pattern in predicate_pattern:
-        if predicate(name, stream):
+        stream_path = (name,)
+        # here we check if the stream's RngKey tag matches the predicate
+        # the stream_path is no longer needed, but we keep it for consistency
+        if predicate(stream_path, stream.key):
           fork = stream.fork(pattern)
           if pattern is None:
             broadcasts[name] = fork
           else:
             splits[name] = fork
           break
       else:
@@ -255,7 +277,50 @@
 
 jax.tree_util.register_pytree_with_keys(
   ForkedKeys,
   functools.partial(_split_rng_flatten, with_keys=True),
   _split_rng_unflatten,
   flatten_func=functools.partial(_split_rng_flatten, with_keys=False),
 )
+
+def fork(
+  state: State,
+  split_filter: filterlib.Filter,
+  split_pattern: SplitPattern,
+) -> tuple[State, State]:
+  if split_pattern is None:
+    raise RuntimeError('Split pattern cannot be None, this is a bug.')
+  if isinstance(split_pattern, int):
+    num_splits = split_pattern
+  else:
+    num_splits = tuple(x if x is not None else 1 for x in split_pattern)
+
+  not_keys, split_state, broadcast_state = state.split(
+    NotKey, split_filter, ...
+  )
+  broadcast_state = State.merge(not_keys, broadcast_state)
+
+  def split_key(key: tp.Any) -> jax.Array:
+    if not isinstance(key, jax.Array):
+      raise TypeError(f'key must be a jax.Array, got {type(key)}')
+
+    return jax.random.split(key, num_splits)
+
+  split_state = jax.tree.map(split_key, split_state)
+
+  return split_state, broadcast_state
+
+def backup_keys(node: tp.Any, /):
+  streams: list[RngStream] = []
+  for _, stream in graph.iter_nodes(node):
+    if isinstance(stream, RngStream):
+      stream.key_backups.append(RngKeyBackup(stream.key.value))
+      streams.append(stream)
+  return streams
+
+
+def restore_keys(streams: list[RngStream], /):
+  for stream in streams:
+    if not stream.key_backups:
+      raise RuntimeError('No key backups found.')
+    backup = stream.key_backups.pop()
+    stream.key.value = backup.value
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/spmd.py` & `flax-0.8.3/flax/experimental/nnx/nnx/spmd.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,19 +12,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import functools
 import typing as tp
 
 import jax
-from jax.experimental import maps
+from jax.interpreters import pxla
 from jax.sharding import Mesh, PartitionSpec
 
 from flax.experimental.nnx.nnx import variables
-from flax.experimental.nnx.nnx.pytreelib import TreeNode
 from flax.experimental.nnx.nnx.state import State
 from flax.typing import (
   Array,
   ArrayPytree,  # pylint: disable=invalid-name
   PartitionSpecPytree,  # pylint: disable=invalid-name
   Sharding,
 )
@@ -41,46 +40,48 @@
 
 def add_axis(
   state: State, index: int, params: tp.Mapping[tp.Any, tp.Any]
 ) -> State:
   axis_name = _get_partition_name(params)
 
   def _add_axis(x: tp.Any):
-    if isinstance(x, variables.Variable):
+    if isinstance(x, variables.VariableState):
       if isinstance(x, HasSharding) and x.sharding is not None:
         sharding = list(x.sharding)
         while len(sharding) < index:
           sharding.append(None)
         sharding.insert(index, axis_name)
         x.sharding = tuple(sharding)
 
       x.add_axis(axis_name, index)
     return x
 
-  return jax.tree_map(
-    _add_axis, state, is_leaf=lambda x: isinstance(x, variables.Variable)
+  return jax.tree_util.tree_map(
+    _add_axis, state, is_leaf=lambda x: isinstance(x, variables.VariableState)
   )
 
 
 def remove_axis(
   state: State, index: int, params: tp.Mapping[tp.Any, tp.Any]
 ) -> State:
   axis_name = _get_partition_name(params)
 
   def _remove_axis(x: tp.Any):
-    if isinstance(x, variables.Variable):
+    if isinstance(x, variables.VariableState):
       if isinstance(x, HasSharding) and x.sharding is not None:
         sharding = list(x.sharding)
         assert sharding.pop(index) == axis_name
         x.sharding = tuple(sharding)
       x.remove_axis(axis_name, index)
     return x
 
-  return jax.tree_map(
-    _remove_axis, state, is_leaf=lambda x: isinstance(x, variables.Variable)
+  return jax.tree_util.tree_map(
+    _remove_axis,
+    state,
+    is_leaf=lambda x: isinstance(x, variables.VariableState),
   )
 
 
 def _get_partition_name(params: tp.Mapping[tp.Any, tp.Any]) -> str:
   if PARTITION_NAME not in params:
     raise ValueError(
       'Trying to transform a Partitioned variable but "partition_name" '
@@ -95,44 +96,43 @@
   def _maybe_replicate(x):
     if hasattr(x, 'shape'):
       return PartitionSpec()
     else:
       return None
 
   def f(x):
-    if isinstance(x, variables.Variable):
+    if isinstance(x, (variables.VariableState, variables.Variable)):
       if isinstance(x, HasSharding) and x.sharding:
-        return x.replace(raw_value=PartitionSpec(*x.sharding))
+        return x.replace(PartitionSpec(*x.sharding))
       else:
-        return x.replace(raw_value=_maybe_replicate(x.raw_value))
+        return x.replace(_maybe_replicate(x.value))
 
     return _maybe_replicate(x)
 
-  return jax.tree_map(
-    f,
-    tree,
-    is_leaf=lambda x: isinstance(x, variables.Variable)
-    and not isinstance(x, TreeNode),
+  return jax.tree_util.tree_map(
+    f, tree, is_leaf=lambda x: isinstance(x, variables.VariableState)
   )
 
 
 def get_named_sharding(tree: A, mesh: jax.sharding.Mesh) -> A:
   spec = get_partition_spec(tree)
-  sharding = jax.tree_map(lambda p: jax.sharding.NamedSharding(mesh, p), spec)
+  sharding = jax.tree_util.tree_map(
+    lambda p: jax.sharding.NamedSharding(mesh, p), spec
+  )
   return sharding
 
 
 # Dynamic Axis Mapping Rngs
 # ------------------------------------------------------------------------------
 
 
 def _global_mesh_defined() -> bool:
-  """Checks if global xmap/pjit mesh resource environment is defined."""
-  maps_env = maps.thread_resources.env
-  return maps_env.physical_mesh.devices.shape != ()  # pylint: disable=g-explicit-bool-comparison
+  """Checks if global mesh resource environment is defined."""
+  env = pxla.thread_resources.env
+  return env.physical_mesh.devices.shape != ()  # pylint: disable=g-explicit-bool-comparison
 
 
 def _with_sharding_constraint(
   x: Array,
   axis_resources: tp.Optional[jax.sharding.PartitionSpec],
   mesh: tp.Optional[jax.sharding.Mesh] = None,
 ):
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/state.py` & `flax-0.8.3/flax/experimental/nnx/nnx/state.py`

 * *Files 9% similar despite different names*

```diff
@@ -24,27 +24,33 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
 import typing as tp
+import typing_extensions as tpe
 
 import jax
 import jax.tree_util as jtu
+import numpy as np
 
 from flax import traverse_util
 from flax.experimental.nnx.nnx import filterlib, reprlib
-from flax.experimental.nnx.nnx.variables import Variable
-from flax.typing import Path
+from flax.experimental.nnx.nnx.variables import VariableState
+from flax.typing import Key, PathParts
 
 A = tp.TypeVar('A')
 
-Key = str
-FlatState = dict[Path, Variable[Variable]]
+StateLeaf = tp.Union[VariableState[tp.Any], np.ndarray, jax.Array]
+FlatState = dict[PathParts, StateLeaf]
+
+
+def is_state_leaf(x: tp.Any) -> tpe.TypeGuard[StateLeaf]:
+  return isinstance(x, (VariableState, np.ndarray, jax.Array))
 
 
 class NestedStateRepr(reprlib.Representable):
   def __init__(self, state: State):
     self.state = state
 
   def __nnx_repr__(self):
@@ -56,51 +62,43 @@
       yield r
 
 
 class State(tp.MutableMapping[Key, tp.Any], reprlib.Representable):
   def __init__(
     self,
     mapping: tp.Union[
-      tp.Mapping[Key, tp.Any],
-      tp.Iterator[tp.Tuple[Key, tp.Any]],
+      tp.Mapping[Key, tp.Mapping | StateLeaf],
+      tp.Iterator[tuple[Key, tp.Mapping | StateLeaf]],
     ],
     /,
   ):
     if tp.TYPE_CHECKING:
       self._mapping = dict(mapping)
     else:
       super().__setattr__('_mapping', dict(mapping))
 
   @property
   def raw_mapping(self) -> dict[Key, dict[str, tp.Any] | tp.Any]:
     return self._mapping
 
-  def __getitem__(self, key: Key | int) -> Variable | State:
-    if isinstance(key, int):
-      key = str(key)
+  def __contains__(self, key: Key) -> bool:
+    return key in self._mapping
+
+  def __getitem__(self, key: Key) -> State | StateLeaf:
     value = self._mapping[key]
-    if isinstance(value, Variable):
-      return value
-    return State(value)
+    if isinstance(value, tp.Mapping):
+      return State(value)
+    return value
 
-  def __getattr__(self, key: Key) -> Variable | State:
+  def __getattr__(self, key: Key) -> State | StateLeaf:
     if '_mapping' not in vars(self) or key not in self._mapping:
-      raise AttributeError(f'No attribute {key} in State')
-
+      raise AttributeError(f"No attribute '{key}' in State")
     return self[key]
 
-  def __setitem__(self, key: Key | int, value: Variable | State) -> None:
-    if isinstance(key, int):
-      key = str(key)
-
-    if not isinstance(value, (Variable, State)):
-      raise ValueError(
-        f'Trying to set key {key} to a value'
-        f' that is not a Variable or State, got: {value}.'
-      )
+  def __setitem__(self, key: Key, value: State | StateLeaf) -> None:
     if isinstance(value, State):
       self._mapping[key] = value._mapping
     else:
       self._mapping[key] = value
 
   __setattr__ = __setitem__
 
@@ -117,39 +115,41 @@
     yield reprlib.Object(type(self), value_sep=': ', start='({', end='})')
 
     for k, v in self.items():
       if isinstance(v, State):
         v = NestedStateRepr(v)
       yield reprlib.Attr(repr(k), v)
 
-  def flat_state(self) -> dict[Key, Variable[Variable]]:
-    return traverse_util.flatten_dict(self._mapping, sep='/')  # type: ignore
+  def flat_state(self) -> FlatState:
+    return traverse_util.flatten_dict(self._mapping)  # type: ignore
 
   @classmethod
-  def from_flat_path(cls, flat_state: FlatState, /) -> State:
-    nested_state = traverse_util.unflatten_dict(flat_state, sep='/')
+  def from_flat_path(
+    cls, flat_state: tp.Mapping[PathParts, StateLeaf], /
+  ) -> State:
+    nested_state = traverse_util.unflatten_dict(flat_state)
     return cls(nested_state)
 
   @tp.overload
   def split(self, first: filterlib.Filter, /) -> 'State':
     ...
 
   @tp.overload
   def split(
     self,
     first: filterlib.Filter,
     second: filterlib.Filter,
     /,
     *filters: filterlib.Filter,
-  ) -> tp.Tuple['State', ...]:
+  ) -> tuple['State', ...]:
     ...
 
   def split(
     self, first: filterlib.Filter, /, *filters: filterlib.Filter
-  ) -> tp.Union['State', tp.Tuple['State', ...]]:
+  ) -> tp.Union['State', tuple['State', ...]]:
     filters = (first, *filters)
     *states, rest = _split_state(self, *filters)
 
     if rest:
       raise ValueError(
         'Non-exhaustive filters, got a non-empty remainder: '
         f'{list(rest.keys())}.\nUse `...` to match all remaining elements.'
@@ -158,37 +158,37 @@
     if len(states) == 1:
       states = states[0]
     else:
       states = tuple(states)
     return states
 
   @tp.overload
-  def extract(
+  def filter(
     self,
     first: filterlib.Filter,
     /,
   ) -> 'State':
     ...
 
   @tp.overload
-  def extract(
+  def filter(
     self,
     first: filterlib.Filter,
     second: filterlib.Filter,
     /,
     *filters: filterlib.Filter,
-  ) -> tp.Tuple['State', ...]:
+  ) -> tuple['State', ...]:
     ...
 
-  def extract(
+  def filter(
     self,
     first: filterlib.Filter,
     /,
     *filters: filterlib.Filter,
-  ) -> tp.Union['State', tp.Tuple['State', ...]]:
+  ) -> tp.Union['State', tuple['State', ...]]:
     *states, _rest = _split_state(self, first, *filters)
 
     assert len(states) == len(filters) + 1
 
     if len(states) == 1:
       states = states[0]
     else:
@@ -221,57 +221,53 @@
 
     self_flat = self.flat_state()
     other_flat = other.flat_state()
     diff = {k: v for k, v in self_flat.items() if k not in other_flat}
 
     return State.from_flat_path(diff)
 
-
 def _state_flatten_with_keys(x: State):
-  items = sorted(x._mapping.items(), key=lambda item: item[0])
+  items = sorted(x._mapping.items())
   children = tuple((jtu.DictKey(key), value) for key, value in items)
-  return children, tuple(x._mapping.keys())
+  return children, tuple(key for key, _ in items)
 
 
 def _state_unflatten(
-  static: tp.Tuple[Path, ...] | None,
-  leaves: tp.Tuple[Variable, ...] | tuple[dict[str, Variable]],
+  static: tuple[Key, ...],
+  leaves: tuple[StateLeaf, ...] | tuple[dict[Key, StateLeaf]],
 ):
-  return State(zip(static, leaves)) if static else State(leaves[0])
-
-
-def _state_flatten(x: State):
-  return (x._mapping,), None
+  return State(zip(static, leaves))
 
 
 jax.tree_util.register_pytree_with_keys(
   State,
   _state_flatten_with_keys,
   _state_unflatten,
-  flatten_func=_state_flatten,
 )
 
 
 def _split_state(
   state: State,
   *filters: filterlib.Filter,
-) -> tp.Tuple[State, ...]:
+) -> tuple[State, ...]:
   for i, filter_ in enumerate(filters):
-    if filter_ is ... and i != len(filters) - 1:
-      raise ValueError(
-        'Ellipsis `...` can only be used as the last filter, '
-        f'got it at index {i}.'
-      )
+    if filter_ in (..., True) and i != len(filters) - 1:
+      remaining_filters = filters[i + 1 :]
+      if not all(f in (..., True) for f in remaining_filters):
+        raise ValueError(
+          '`...` or `True` can only be used as the last filters, '
+          f'got {filter_} it at index {i}.'
+        )
   predicates = tuple(map(filterlib.to_predicate, filters))
 
   flat_state = state.flat_state()
 
   # we have n + 1 states, where n is the number of predicates
   # the last state is for values that don't match any predicate
-  flat_states: tp.Tuple[FlatState, ...] = tuple(
+  flat_states: tuple[FlatState, ...] = tuple(
     {} for _ in range(len(predicates) + 1)
   )
 
   for path, value in flat_state.items():
     for i, predicate in enumerate(predicates):
       if predicate(path, value):
         flat_states[i][path] = value
```

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/tracers.py` & `flax-0.8.3/flax/experimental/nnx/nnx/tracers.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/nnx/transforms.py` & `flax-0.8.3/flax/experimental/nnx/nnx/transforms.py`

 * *Files 20% similar despite different names*

```diff
@@ -27,23 +27,26 @@
 # limitations under the License.
 from __future__ import annotations
 
 import dataclasses
 import functools
 import typing as tp
 from abc import abstractmethod
-from types import MappingProxyType
-from typing import Any
+
+from flax.core.frozen_dict import FrozenDict
 
 import jax
+import jax.core
 import jax.numpy as jnp
 import jax.stages
 
+from jax._src.tree_util import broadcast_prefix
 from flax.experimental.nnx.nnx import (
   filterlib,
+  graph,
   rnglib,
   spmd,
   variables,
 )
 from flax.experimental.nnx.nnx.module import GraphDef, Module, ModuleMeta
 from flax.experimental.nnx.nnx.proxy_caller import (
   CallableProxy,
@@ -55,27 +58,28 @@
 A = tp.TypeVar('A')
 C = tp.TypeVar('C')
 B = tp.TypeVar('B')
 F = tp.TypeVar('F', bound=tp.Callable[..., tp.Any])
 G = tp.TypeVar('G', bound=tp.Callable[..., tp.Any])
 M = tp.TypeVar('M', bound=Module)
 N = tp.TypeVar('N', bound=Module)
-
+StrInt = tp.TypeVar('StrInt', str, int)
 AxisName = tp.Hashable
 Leaves = tp.List[Leaf]
+Index = int
 
-
-def _check_args(args: tuple[tp.Any, ...]):
-  """Check if Rngs is passed as a positional argument and raise an error."""
-  for arg in args:
-    if isinstance(arg, rnglib.Rngs):
-      raise ValueError(
-        "Rngs must be passed as a keyword argument named 'rngs', not a"
-        ' positional argument'
-      )
+def _normalize_sequence(
+  x: StrInt | tp.Iterable[StrInt] | None, /
+) -> tuple[StrInt, ...]:
+  if x is None:
+    return ()
+  elif isinstance(x, (str, int)):
+    return (x,)  # type: ignore
+  else:
+    return tuple(x)
 
 
 class LiftedModule(Module, tp.Generic[M]):
   @abstractmethod
   def _call(self, accessor: DelayedAccessor, *args, **kwargs) -> tp.Any:
     ...
 
@@ -88,15 +92,14 @@
     return self.call(*args, **kwargs)  # type: ignore
 
   @property
   def call(self) -> tp.Any:
     module = self
 
     def check_and_call(accessor: DelayedAccessor, *args, **kwargs):
-      _check_args(args)
       return self._call(accessor, *args, **kwargs)
 
     proxy = CallableProxy(check_and_call)
 
     while isinstance(module._submodule, LiftedModule):
       module = module._submodule
       proxy = proxy.call
@@ -106,303 +109,500 @@
 
 # -------------------------------
 # jit
 # -------------------------------
 
 UNSPECIFIED = object()
 
+@dataclasses.dataclass(frozen=True)
+class JitStaticInputs:
+  graphdef: GraphDef[tuple[tp.Any, ...]]
+  ctx: graph.UpdateContext
+
+
+jax.tree_util.register_static(JitStaticInputs)
+
+
+@dataclasses.dataclass(frozen=True)
+class JitStaticOutputs:
+  graphdef: GraphDef[tuple[tp.Any, ...]]
+  index_mapping: dict[Index, Index]
+
+
+jax.tree_util.register_static(JitStaticOutputs)
+
+def _default_constrain_object_state(state: State) -> State:
+  state_spec = spmd.get_partition_spec(state)
+  state = jax.lax.with_sharding_constraint(state, state_spec)
+  return state
+
 
 @dataclasses.dataclass
 class JITOptions:
   in_shardings: tp.Any
   out_shardings: tp.Any
-  static_argnums: tp.Union[int, tp.Sequence[int], None]
-  static_argnames: tp.Union[str, tp.Iterable[str], None]
-  donate_argnums: tp.Union[int, tp.Sequence[int]]
+  static_argnums: tuple[int, ...]
+  static_argnames: tuple[str, ...]
+  donate_argnums: tuple[int, ...]
+  donate_argnames: tuple[str, ...]
   keep_unused: bool
   device: tp.Optional[jax.Device]
   backend: tp.Optional[str]
   inline: bool
   abstracted_axes: tp.Optional[tp.Any]
+  # nnx specific
+  donate_state: bool
+  constrain_state: tp.Callable[[State], State] | None
+
+  @classmethod
+  def from_jit_kwargs(
+    cls,
+    in_shardings: tp.Any,
+    out_shardings: tp.Any,
+    static_argnums: int | tp.Sequence[int] | None,
+    static_argnames: str | tp.Iterable[str] | None,
+    donate_argnums: int | tp.Sequence[int] | None,
+    donate_argnames: str | tp.Iterable[str] | None,
+    keep_unused: bool,
+    device: tp.Optional[jax.Device],
+    backend: tp.Optional[str],
+    inline: bool,
+    abstracted_axes: tp.Optional[tp.Any],
+    donate_state: bool,
+    constrain_state: bool | tp.Callable[[State], State],
+  ):
+    _static_argnums = _normalize_sequence(static_argnums)
+    _static_argnames = _normalize_sequence(static_argnames)
+    _donate_argnums = _normalize_sequence(donate_argnums)
+    _donate_argnames = _normalize_sequence(donate_argnames)
+
+    if donate_state:
+      _donate_argnames = (*_donate_argnames, '_nnx_jit_state')
+
+    if callable(constrain_state):
+      _constrain_object_state = constrain_state
+    elif constrain_state:
+      _constrain_object_state = _default_constrain_object_state
+    else:
+      _constrain_object_state = None
+
+    return cls(
+      in_shardings=in_shardings,
+      out_shardings=out_shardings,
+      static_argnums=_static_argnums,
+      static_argnames=_static_argnames,
+      donate_argnums=_donate_argnums,
+      donate_argnames=_donate_argnames,
+      keep_unused=keep_unused,
+      device=device,
+      backend=backend,
+      inline=inline,
+      abstracted_axes=abstracted_axes,
+      donate_state=donate_state,
+      constrain_state=_constrain_object_state,
+    )
 
-  def get_kwargs(self) -> dict[str, tp.Any]:
+  def get_jit_kwargs(self) -> dict[str, tp.Any]:
     kwargs = vars(self).copy()
+    del kwargs['donate_state']
+    del kwargs['constrain_state']
     if kwargs['in_shardings'] is UNSPECIFIED:
       kwargs.pop('in_shardings')
     if kwargs['out_shardings'] is UNSPECIFIED:
       kwargs.pop('out_shardings')
     return kwargs
 
 
 class JITMeta(ModuleMeta):
   def __call__(
     self,
     module_constructor: tp.Callable[..., M],
     *,
     in_shardings: tp.Any = UNSPECIFIED,
     out_shardings: tp.Any = UNSPECIFIED,
-    static_argnums: tp.Union[int, tp.Sequence[int], None] = None,
-    static_argnames: tp.Union[str, tp.Iterable[str], None] = None,
-    donate_argnums: tp.Union[int, tp.Sequence[int]] = (),
+    static_argnums: int | tp.Sequence[int] | None = None,
+    static_argnames: str | tp.Iterable[str] | None = None,
+    donate_argnums: int | tp.Sequence[int] | None = None,
+    donate_argnames: str | tp.Iterable[str] | None = None,
     keep_unused: bool = False,
     device: tp.Optional[jax.Device] = None,
     backend: tp.Optional[str] = None,
     inline: bool = False,
     abstracted_axes: tp.Optional[tp.Any] = None,
-  ) -> tp.Callable[..., 'JIT[M]']:
+    # nnx specific
+    donate_state: bool = False,
+    constrain_state: bool | tp.Callable[[State], State] = False,
+  ) -> tp.Callable[..., 'Jit[M]']:
     super_call = super().__call__
 
-    def _create_jit(*args, **kwargs) -> JIT[M]:
-      _check_args(args)
+    def _create_jit(*args, **kwargs) -> Jit[M]:
       return super_call(
         module_constructor=module_constructor,
         in_shardings=in_shardings,
         out_shardings=out_shardings,
         static_argnums=static_argnums,
         static_argnames=static_argnames,
         donate_argnums=donate_argnums,
+        donate_argnames=donate_argnames,
         keep_unused=keep_unused,
         device=device,
         backend=backend,
         inline=inline,
         abstracted_axes=abstracted_axes,
+        # nnx specific
+        donate_state=donate_state,
+        constrain_state=constrain_state,
         # submodule args
         module_init_args=args,
         module_init_kwargs=kwargs,
       )
 
     return _create_jit
 
 
-class JittedFn(tp.Protocol, tp.Generic[M]):
+class JittedFn(tp.Protocol):
   def __call__(
-    self, state_and_def: tuple[State | tuple[State, ...], GraphDef[M]]
-  ) -> tuple[tuple[State | tuple[State, ...], GraphDef[M]], tp.Any]:
+    self,
+    *args: tp.Any,
+    _nnx_jit_static: JitStaticInputs,
+    _nnx_jit_state: State,
+    **kwargs: tp.Any,
+  ) -> tuple[
+    tp.Any, State, GraphDef[tuple[tuple[tp.Any, ...], tuple[tp.Any, ...]]]
+  ]:
     ...
 
 
-def get_jitted_fn(_module_type: type[M], f, options: JITOptions) -> JittedFn[M]:
-  jit_kwargs = options.get_kwargs()
+def get_jitted_fn(f, options: JITOptions) -> JittedFn:
+  jit_kwargs = options.get_jit_kwargs()
 
   @functools.partial(jax.jit, **jit_kwargs)
   def jitted_fn(
-    state_and_def: tuple[State | tuple[State, ...], GraphDef[M]],
-    *args,
-    **kwargs,
-  ):
-    _check_args(args)
-    states, graphdef = state_and_def
+    *args: tp.Any,
+    _nnx_jit_static: JitStaticInputs,
+    _nnx_jit_state: State,
+    **kwargs: tp.Any,
+  ) -> tuple[tp.Any, State, GraphDef[tuple[tp.Any, ...]]]:
+    ctx = _nnx_jit_static.ctx
+    graphdef = _nnx_jit_static.graphdef
+    state: State = _nnx_jit_state
 
-    if isinstance(states, State):
-      states = (states,)
+    if options.constrain_state is not None:
+      state = options.constrain_state(state)
 
-    if 'rngs' in kwargs:
-      kwargs['rngs'] = rnglib.Rngs(kwargs['rngs'])
-    module = graphdef.merge(*states)
-    out = f(module, *args, **kwargs)
+    input_graph_nodes = ctx.merge(graphdef, state)
 
-    updates = module.split()
-    out = (updates, out)
+    (args, kwargs) = graph.insert_graph_nodes((args, kwargs), input_graph_nodes)
 
-    return out
+    out = f(*args, **kwargs)
 
-  return jitted_fn
+    out, output_graph_nodes = graph.extract_graph_nodes(out)
 
+    graphdef, state = ctx.split((input_graph_nodes, output_graph_nodes))
 
-def jit_init(
-  jitted_fn: JittedFn[M],
-  module: M,
-  args: tuple[tp.Any, ...],
-  kwargs: dict[str, tp.Any],
-) -> None:
-  if not isinstance(module, Module):
-    raise TypeError(f'Expected Module, got {type(module).__name__}')
-
-  module = tp.cast(M, module)
-
-  if 'rngs' in kwargs and isinstance(rngs := kwargs['rngs'], rnglib.Rngs):
-    kwargs['rngs'] = rngs.fork()
-
-  state_and_def = module.split()
-  out = jitted_fn(state_and_def, *args, **kwargs)
-  updates, _ = out
-  module.update(updates)
+    if options.constrain_state is not None:
+      state = options.constrain_state(state)
+
+    return out, state, graphdef
+
+  return jitted_fn
 
 
 def jit_apply(
-  jitted_fn: JittedFn[M],
-  module: M,
+  options: JITOptions,
+  jitted_fn: JittedFn,
   args: tuple[tp.Any, ...],
   kwargs: dict[str, tp.Any],
 ) -> tp.Any:
-  if not isinstance(module, Module):
-    raise TypeError(f'Expected Module, got {type(module).__name__}')
-
-  module = tp.cast(M, module)
-
-  if 'rngs' in kwargs and isinstance(rngs := kwargs['rngs'], rnglib.Rngs):
-    kwargs['rngs'] = rngs.fork()
+  ctx = graph.UpdateContext()
+  (args, kwargs), input_graph_nodes = graph.extract_graph_nodes((args, kwargs))
+  graphdef, state = ctx.split(input_graph_nodes)
 
-  state_and_def = module.split()
-  updates, out = jitted_fn(state_and_def, *args, **kwargs)
-  module.update(updates)
+  out, output_state, output_graphdef = jitted_fn(
+    *args,
+    _nnx_jit_static=JitStaticInputs(graphdef, ctx),
+    _nnx_jit_state=state,
+    **kwargs,
+  )
+  input_graph_nodes, output_graph_nodes = ctx.update(
+    output_graphdef, output_state
+  )
+  out = graph.insert_graph_nodes(out, output_graph_nodes)
   return out
 
 
-class JIT(LiftedModule[M], metaclass=JITMeta):
+class Jit(LiftedModule[M], metaclass=JITMeta):
   def __init__(
     self,
     module_constructor: tp.Callable[..., M],
     *,
     in_shardings: tp.Any = UNSPECIFIED,
     out_shardings: tp.Any = UNSPECIFIED,
-    static_argnums: tp.Union[int, tp.Sequence[int], None] = None,
-    static_argnames: tp.Union[str, tp.Iterable[str], None] = None,
-    donate_argnums: tp.Union[int, tp.Sequence[int]] = (),
+    static_argnums: int | tp.Sequence[int] | None = None,
+    static_argnames: str | tp.Iterable[str] | None = None,
+    donate_argnums: int | tp.Sequence[int] | None = None,
+    donate_argnames: str | tp.Iterable[str] | None = None,
     keep_unused: bool = False,
     device: tp.Optional[jax.Device] = None,
     backend: tp.Optional[str] = None,
     inline: bool = False,
     abstracted_axes: tp.Optional[tp.Any] = None,
+    # nnx specific
+    donate_state: bool = False,
+    constrain_state: bool | tp.Callable[[State], State] = False,
     # submodule args
     module_init_args: tuple[tp.Any, ...],
     module_init_kwargs: dict[str, tp.Any],
   ):
-    self.options = JITOptions(
+    self.options = JITOptions.from_jit_kwargs(
       in_shardings=in_shardings,
       out_shardings=out_shardings,
       static_argnums=static_argnums,
       static_argnames=static_argnames,
       donate_argnums=donate_argnums,
+      donate_argnames=donate_argnames,
       keep_unused=keep_unused,
       device=device,
       backend=backend,
       inline=inline,
       abstracted_axes=abstracted_axes,
+      donate_state=donate_state,
+      constrain_state=constrain_state,
     )
     self.accessor: tp.Optional[DelayedAccessor] = None
 
     def jit_call_module(module, *args, **kwargs):
       assert self.accessor is not None
-      f = self.accessor(module)
-      return f(*args, **kwargs)
+      method = self.accessor(module)
+      return method(*args, **kwargs)
 
-    self.jitted_fn: JittedFn[M] = get_jitted_fn(
-      M, jit_call_module, self.options
-    )
+    self.jitted_fn: JittedFn[M] = get_jitted_fn(jit_call_module, self.options)
     self.module_constructor = module_constructor
     self.jit_module = self.module_constructor(
       *module_init_args, **module_init_kwargs
     )
 
   @property
   def _submodule(self) -> M:
     return self.jit_module
 
-  def _call(self, accessor: DelayedAccessor, *args, **kwargs) -> Any:
+  def _call(self, accessor: DelayedAccessor, *args, **kwargs) -> tp.Any:
     self.accessor = accessor
     try:
-      out = jit_apply(self.jitted_fn, self.jit_module, args, kwargs)
+      out = jit_apply(
+        self.options, self.jitted_fn, (self.jit_module, *args), kwargs
+      )
     finally:
       self.accessor = None
     return out
 
 
 def jit(
-  f: F,
+  fun: F,
   *,
   in_shardings: tp.Any = UNSPECIFIED,
   out_shardings: tp.Any = UNSPECIFIED,
-  static_argnums: tp.Union[int, tp.Sequence[int], None] = None,
-  static_argnames: tp.Union[str, tp.Iterable[str], None] = None,
-  donate_argnums: tp.Union[int, tp.Sequence[int]] = (),
+  static_argnums: int | tp.Sequence[int] | None = None,
+  static_argnames: str | tp.Iterable[str] | None = None,
+  donate_argnums: int | tp.Sequence[int] | None = None,
+  donate_argnames: str | tp.Iterable[str] | None = None,
   keep_unused: bool = False,
   device: tp.Optional[jax.Device] = None,
   backend: tp.Optional[str] = None,
   inline: bool = False,
   abstracted_axes: tp.Optional[tp.Any] = None,
-  is_init: tp.Optional[bool] = None,
+  # nnx specific
+  donate_state: bool = False,
+  constrain_state: bool | tp.Callable[[State], State] = False,
 ) -> F:
-  if is_init is None:
-    is_init = f.__name__ == '__init__'
-
-  if static_argnames is None:
-    static_argnames = []
-  elif isinstance(static_argnames, str):
-    static_argnames = [static_argnames]
-  else:
-    static_argnames = list(static_argnames)
-
-  options = JITOptions(
+  """
+  Lifted version of ``jax.jit`` that can handle Modules / graph nodes as
+  arguments.
+
+  Args:
+    fun: Function to be jitted. ``fun`` should be a pure function, as
+      side-effects may only be executed once.
+
+      The arguments and return value of ``fun`` should be arrays,
+      scalars, or (nested) standard Python containers (tuple/list/dict) thereof.
+      Positional arguments indicated by ``static_argnums`` can be anything at
+      all, provided they are hashable and have an equality operation defined.
+      Static arguments are included as part of a compilation cache key, which is
+      why hash and equality operators must be defined.
+
+      JAX keeps a weak reference to ``fun`` for use as a compilation cache key,
+      so the object ``fun`` must be weakly-referenceable. Most :class:`Callable`
+      objects will already satisfy this requirement.
+    in_shardings: Pytree of structure matching that of arguments to ``fun``,
+      with all actual arguments replaced by resource assignment specifications.
+      It is also valid to specify a pytree prefix (e.g. one value in place of a
+      whole subtree), in which case the leaves get broadcast to all values in
+      that subtree.
+
+      The ``in_shardings`` argument is optional. JAX will infer the shardings
+      from the input :py:class:`jax.Array`'s and defaults to replicating the input
+      if the sharding cannot be inferred.
+
+      The valid resource assignment specifications are:
+        - :py:class:`XLACompatibleSharding`, which will decide how the value
+            will be partitioned. With this, using a mesh context manager is not
+            required.
+        - :py:obj:`None`, will give JAX the freedom to choose whatever sharding
+          it wants.
+          For in_shardings, JAX will mark is as replicated but this behavior
+          can change in the future.
+          For out_shardings, we will rely on the XLA GSPMD partitioner to
+          determine the output shardings.
+
+      The size of every dimension has to be a multiple of the total number of
+      resources assigned to it. This is similar to pjit's in_shardings.
+    out_shardings: Like ``in_shardings``, but specifies resource
+      assignment for function outputs. This is similar to pjit's
+      out_shardings.
+
+      The ``out_shardings`` argument is optional. If not specified, :py:func:`jax.jit`
+      will use GSPMD's sharding propagation to figure out what the sharding of the
+      output(s) should be.
+    static_argnums: An optional int or collection of ints that specify which
+      positional arguments to treat as static (compile-time constant).
+      Operations that only depend on static arguments will be constant-folded in
+      Python (during tracing), and so the corresponding argument values can be
+      any Python object.
+
+      Static arguments should be hashable, meaning both ``__hash__`` and
+      ``__eq__`` are implemented, and immutable. Calling the jitted function
+      with different values for these constants will trigger recompilation.
+      Arguments that are not arrays or containers thereof must be marked as
+      static.
+
+      If neither ``static_argnums`` nor ``static_argnames`` is provided, no
+      arguments are treated as static. If ``static_argnums`` is not provided but
+      ``static_argnames`` is, or vice versa, JAX uses
+      :code:`inspect.signature(fun)` to find any positional arguments that
+      correspond to ``static_argnames``
+      (or vice versa). If both ``static_argnums`` and ``static_argnames`` are
+      provided, ``inspect.signature`` is not used, and only actual
+      parameters listed in either ``static_argnums`` or ``static_argnames`` will
+      be treated as static.
+    static_argnames: An optional string or collection of strings specifying
+      which named arguments to treat as static (compile-time constant). See the
+      comment on ``static_argnums`` for details. If not
+      provided but ``static_argnums`` is set, the default is based on calling
+      ``inspect.signature(fun)`` to find corresponding named arguments.
+    donate_argnums: Specify which positional argument buffers are "donated" to
+      the computation. It is safe to donate argument buffers if you no longer
+      need them once the computation has finished. In some cases XLA can make
+      use of donated buffers to reduce the amount of memory needed to perform a
+      computation, for example recycling one of your input buffers to store a
+      result. You should not reuse buffers that you donate to a computation, JAX
+      will raise an error if you try to. By default, no argument buffers are
+      donated.
+
+      If neither ``donate_argnums`` nor ``donate_argnames`` is provided, no
+      arguments are donated. If ``donate_argnums`` is not provided but
+      ``donate_argnames`` is, or vice versa, JAX uses
+      :code:`inspect.signature(fun)` to find any positional arguments that
+      correspond to ``donate_argnames``
+      (or vice versa). If both ``donate_argnums`` and ``donate_argnames`` are
+      provided, ``inspect.signature`` is not used, and only actual
+      parameters listed in either ``donate_argnums`` or ``donate_argnames`` will
+      be donated.
+
+      For more details on buffer donation see the
+      `FAQ <https://jax.readthedocs.io/en/latest/faq.html#buffer-donation>`_.
+    donate_argnames: An optional string or collection of strings specifying
+      which named arguments are donated to the computation. See the
+      comment on ``donate_argnums`` for details. If not
+      provided but ``donate_argnums`` is set, the default is based on calling
+      ``inspect.signature(fun)`` to find corresponding named arguments.
+    keep_unused: If `False` (the default), arguments that JAX determines to be
+      unused by `fun` *may* be dropped from resulting compiled XLA executables.
+      Such arguments will not be transferred to the device nor provided to the
+      underlying executable. If `True`, unused arguments will not be pruned.
+    device: This is an experimental feature and the API is likely to change.
+      Optional, the Device the jitted function will run on. (Available devices
+      can be retrieved via :py:func:`jax.devices`.) The default is inherited
+      from XLA's DeviceAssignment logic and is usually to use
+      ``jax.devices()[0]``.
+    backend: This is an experimental feature and the API is likely to change.
+      Optional, a string representing the XLA backend: ``'cpu'``, ``'gpu'``, or
+      ``'tpu'``.
+    inline: Specify whether this function should be inlined into enclosing
+      jaxprs (rather than being represented as an application of the xla_call
+      primitive with its own subjaxpr). Default False.
+    donate_state: Optional, bool. If True, the object state of the
+      graph node's state will be donated to the computation. Default False.
+    constrain_state: Optional, bool or callable. If True, the object
+      state of the graph node's state will be constrained to the partition
+      specified by the graph node's partition spec as computed by
+      :func:`nnx.spmd.get_partition_spec`. If a callable, the object State will
+      passed to the callable which must return the constrained object State. If
+      False, the object state will not be constrained. Default False.
+
+  Returns:
+    A wrapped version of ``fun``, set up for just-in-time compilation.
+  """
+  options = JITOptions.from_jit_kwargs(
     in_shardings=in_shardings,
     out_shardings=out_shardings,
     static_argnums=static_argnums,
     static_argnames=static_argnames,
     donate_argnums=donate_argnums,
+    donate_argnames=donate_argnames,
     keep_unused=keep_unused,
     device=device,
     backend=backend,
     inline=inline,
     abstracted_axes=abstracted_axes,
+    donate_state=donate_state,
+    constrain_state=constrain_state,
   )
-  jitted_fn = get_jitted_fn(Module, f, options)
-
-  if is_init:
-
-    @functools.wraps(f)
-    def jit_init_wrapper(module: Module, *args, **kwargs):
-      _check_args(args)
-      jit_init(jitted_fn, module, args, kwargs)
+  jitted_fn = get_jitted_fn(fun, options)
 
-    wrapper = jit_init_wrapper
-    wrapper.inner = jitted_fn
-  else:
-
-    @functools.wraps(f)
-    def jit_apply_wrapper(module: Module, *args, **kwargs):
-      _check_args(args)
-      return jit_apply(jitted_fn, module, args, kwargs)
+  @functools.wraps(fun)
+  def jit_apply_wrapper(*args, **kwargs):
+    return jit_apply(options, jitted_fn, args, kwargs)
 
-    wrapper = jit_apply_wrapper
-    wrapper.inner = jitted_fn
+  wrapper = jit_apply_wrapper
+  wrapper.inner = jitted_fn
 
   return wrapper  # type: ignore
 
 
 # -------------------------------
 # grad
 # -------------------------------
 
 
 @dataclasses.dataclass
 class GradOptions:
-  wrt: filterlib.Filter
+  argnums: tuple[int, ...]
   has_aux: bool
   holomorphic: bool
   allow_int: bool
   reduce_axes: tp.Sequence[AxisName]
   return_value: bool
+  wrt: filterlib.Filter
 
 
 class GradMeta(ModuleMeta):
   def __call__(
     self,
     module_constructor: tp.Callable[..., M],
-    *,
-    wrt: filterlib.Filter = variables.Param,
     has_aux: bool = False,
     holomorphic: bool = False,
     allow_int: bool = False,
     reduce_axes: tp.Sequence[AxisName] = (),
     return_value: bool = False,
+    *,
+    wrt: filterlib.Filter = variables.Param,
   ) -> tp.Callable[..., 'Grad[M]']:
     super_call = super().__call__
 
     def _create_grad(*args, **kwargs) -> Grad[M]:
-      _check_args(args)
       return super_call(
         module_constructor=module_constructor,
         wrt=wrt,
         has_aux=has_aux,
         holomorphic=holomorphic,
         allow_int=allow_int,
         reduce_axes=reduce_axes,
@@ -415,84 +615,103 @@
     return _create_grad
 
 
 class Grad(LiftedModule[M], metaclass=GradMeta):
   def __init__(
     self,
     module_constructor: tp.Callable[..., M],
-    *,
-    wrt: filterlib.Filter = variables.Param,
+    argnums: int | tp.Sequence[int] = 0,
     has_aux: bool = False,
     holomorphic: bool = False,
     allow_int: bool = False,
     reduce_axes: tp.Sequence[AxisName] = (),
     return_value: bool = False,
+    *,
+    wrt: filterlib.Filter = variables.Param,
     # submodule args
     module_init_args: tuple[tp.Any, ...],
     module_init_kwargs: dict[str, tp.Any],
   ):
+    _argnums = _normalize_sequence(argnums)
     self.options = GradOptions(
-      wrt=wrt,
+      argnums=_argnums,
       has_aux=has_aux,
       holomorphic=holomorphic,
       allow_int=allow_int,
       reduce_axes=reduce_axes,
       return_value=return_value,
+      wrt=wrt,
     )
     self.module_constructor = module_constructor
     self.grad_module = self.module_constructor(
       *module_init_args, **module_init_kwargs
     )
 
   @property
   def _submodule(self) -> M:
     return self.grad_module
 
-  def _call(self, accessor: DelayedAccessor, *args, **kwargs) -> Any:
+  def _call(self, accessor: DelayedAccessor, *args, **kwargs) -> tp.Any:
     def grad_call_apply(module, *args, **kwargs):
-      return accessor(module)(*args, **kwargs)
+      method = accessor(module)
+      return method(*args, **kwargs)
+
+    return grad_apply(self.options, grad_call_apply, (self.grad_module, *args))
 
-    return grad_apply(
-      self.options, grad_call_apply, self.grad_module, *args, **kwargs
-    )
 
+def grad_apply(options: GradOptions, f, args: tuple[tp.Any, ...]):
+  _, input_nodes = graph.extract_graph_nodes(args)
 
-def grad_apply(options: GradOptions, f, module: Module, *args, **kwargs):
-  if not isinstance(module, Module):
-    raise TypeError(f'Expected a Module, got {type(module).__name__}')
+  _args = list(args)
+  diff_graph_nodes: dict[int, tp.Any] = {
+    i: arg
+    for i, arg in enumerate(args)
+    if i in options.argnums and graph.is_node(arg)
+  }
 
-  predicate = filterlib.to_predicate(options.wrt)
+  _, diff_state, _ = graph.split(diff_graph_nodes, options.wrt, ...)
+  for i in diff_graph_nodes:
+    _args[i] = diff_state[i]
 
-  diff, nondiff, graphdef = module.split(predicate, ...)
   transform = jax.value_and_grad if options.return_value else jax.grad
+  out_nodes = None
+
+  argnums = options.argnums[0] if len(options.argnums) == 1 else options.argnums
 
   @functools.partial(
     transform,
-    argnums=0,  # we'll handle this ourselves
+    argnums=argnums,
     has_aux=True,
     holomorphic=options.holomorphic,
     allow_int=options.allow_int,
     reduce_axes=options.reduce_axes,
   )
-  def grad_fn(diff: State):
-    nonlocal graphdef
+  def grad_fn(*args):
+    nonlocal out_nodes
+
+    _args = list(args)
+    for i, graph_node in diff_graph_nodes.items():
+      diff_state: State = _args[i]
+      graph.update(graph_node, diff_state)
+      _args[i] = graph_node
+
+    out = f(*_args)
+    out, out_nodes = graph.extract_graph_nodes(out)
 
-    module = graphdef.merge(diff, nondiff)
-    out = f(module, *args, **kwargs)
+    _, updates, _ = graph.flatten((input_nodes, out_nodes))
 
-    updates, graphdef = module.split()
     if options.has_aux:
       loss, aux = out
       out = (loss, (updates, aux))
     else:
       out = (out, updates)
 
     return out
 
-  out = grad_fn(diff)
+  out = grad_fn(*_args)
 
   updates: State
   if options.return_value:
     if options.has_aux:
       (loss, (updates, aux)), grads = out
       out = (loss, aux), grads
     else:
@@ -501,631 +720,674 @@
   else:
     if options.has_aux:
       grads, (updates, aux) = out
       out = grads, aux
     else:
       out, updates = out
 
-  module.update(updates, graphdef)
+  graph.update((input_nodes, out_nodes), updates)
   return out
 
 
-@tp.overload
 def grad(
   f: tp.Callable[..., tp.Any],
-  wrt: filterlib.Filter = variables.Param,
-  *,
+  argnums: int | tp.Sequence[int] = 0,
+  has_aux: bool = False,
   holomorphic: bool = False,
   allow_int: bool = False,
   reduce_axes: tp.Sequence[AxisName] = (),
-) -> tp.Callable[..., State]:
-  ...
+  *,
+  wrt: filterlib.Filter = variables.Param,
+) -> tp.Callable[..., tp.Any]:
+  """Lifted version of ``jax.grad`` that can handle Modules / graph nodes as
+  arguments.
+
+  The differentiable state of each graph node is defined by the `wrt` filter,
+  which by default is set to `nnx.Param`. Internally the ``State`` of
+  graph nodes is extracted, filtered according to `wrt` filter, and
+  passed to the underlying ``jax.grad`` function. The gradients
+  of graph nodes are of type ``State``.
 
+  Example::
 
-@tp.overload
-def grad(
-  f: tp.Callable[..., tp.Any],
-  wrt: filterlib.Filter = variables.Param,
-  *,
-  has_aux: tp.Literal[True],
-  holomorphic: bool = False,
-  allow_int: bool = False,
-  reduce_axes: tp.Sequence[AxisName] = (),
-) -> tp.Callable[..., tuple[State, tp.Any]]:
-  ...
+    >>> from flax.experimental import nnx
+    ...
+    >>> m = nnx.Linear(2, 3, rngs=nnx.Rngs(0))
+    >>> x = jnp.ones((1, 2))
+    >>> y = jnp.ones((1, 3))
+    ...
+    >>> loss_fn = lambda m, x, y: jnp.mean((m(x) - y) ** 2)
+    >>> grad_fn = nnx.grad(loss_fn, wrt=nnx.Param)
+    ...
+    >>> grads = grad_fn(m, x, y)
+    >>> jax.tree_util.tree_map(jnp.shape, grads)
+    State({
+      'bias': VariableState(
+        type=Param,
+        value=(3,)
+      ),
+      'kernel': VariableState(
+        type=Param,
+        value=(2, 3)
+      )
+    })
 
+  Args:
+    fun: Function to be differentiated. Its arguments at positions specified by
+      ``argnums`` should be arrays, scalars, graph nodes or standard Python
+      containers. Argument arrays in the positions specified by ``argnums`` must
+      be of inexact (i.e., floating-point or complex) type. It should return a
+      scalar (which includes arrays with shape ``()`` but not arrays with shape
+      ``(1,)`` etc.)
+    argnums: Optional, integer or sequence of integers. Specifies which
+      positional argument(s) to differentiate with respect to (default 0).
+    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the
+      first element is considered the output of the mathematical function to be
+      differentiated and the second element is auxiliary data. Default False.
+    holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be
+      holomorphic. If True, inputs and outputs must be complex. Default False.
+    allow_int: Optional, bool. Whether to allow differentiating with
+      respect to integer valued inputs. The gradient of an integer input will
+      have a trivial vector-space dtype (float0). Default False.
+    reduce_axes: Optional, tuple of axis names. If an axis is listed here, and
+      ``fun`` implicitly broadcasts a value over that axis, the backward pass
+      will perform a ``psum`` of the corresponding gradient. Otherwise, the
+      gradient will be per-example over named axes. For example, if ``'batch'``
+      is a named batch axis, ``grad(f, reduce_axes=('batch',))`` will create a
+      function that computes the total gradient while ``grad(f)`` will create
+      one that computes the per-example gradient.
+    wrt: Optional, filterlib.Filter. Filter to extract the differentiable state
+      of each graph node. Default is `nnx.Param`.
+
+  """
 
-def grad(
-  f: tp.Callable[..., tp.Any],
-  wrt: filterlib.Filter = variables.Param,
-  *,
-  has_aux: bool = False,
-  holomorphic: bool = False,
-  allow_int: bool = False,
-  reduce_axes: tp.Sequence[AxisName] = (),
-) -> tp.Callable[..., tp.Union[tuple[State, tp.Any], State]]:
   if f.__name__ == '__init__':
     raise ValueError('Cannot use `grad` with `__init__`')
 
+  _argnums = _normalize_sequence(argnums)
   options = GradOptions(
+    argnums=_argnums,
     wrt=wrt,
     has_aux=has_aux,
     holomorphic=holomorphic,
     allow_int=allow_int,
     reduce_axes=reduce_axes,
     return_value=False,
   )
 
   @functools.wraps(f)
-  def grad_wrapper(module: Module, *args, **kwargs):
-    _check_args(args)
-    return grad_apply(options, f, module, *args, **kwargs)
+  def grad_wrapper(*args):
+    return grad_apply(options, f, args)
 
   return grad_wrapper  # type: ignore
 
 
-@tp.overload
-def value_and_grad(
-  f: tp.Callable[..., tp.Any],
-  wrt: filterlib.Filter = variables.Param,
-  *,
-  holomorphic: bool = False,
-  allow_int: bool = False,
-  reduce_axes: tp.Sequence[AxisName] = (),
-) -> tp.Callable[..., tuple[jax.Array, State]]:
-  ...
-
-
-@tp.overload
-def value_and_grad(
-  f: tp.Callable[..., tp.Any],
-  wrt: filterlib.Filter = variables.Param,
-  *,
-  has_aux: tp.Literal[True],
-  holomorphic: bool = False,
-  allow_int: bool = False,
-  reduce_axes: tp.Sequence[AxisName] = (),
-) -> tp.Callable[..., tuple[tuple[jax.Array, tp.Any], State]]:
-  ...
 
 
 def value_and_grad(
   f: tp.Callable[..., tp.Any],
-  wrt: filterlib.Filter = variables.Param,
-  *,
+  argnums: int | tp.Sequence[int] = 0,
   has_aux: bool = False,
   holomorphic: bool = False,
   allow_int: bool = False,
   reduce_axes: tp.Sequence[AxisName] = (),
-) -> tp.Callable[
-  ...,
-  tp.Union[tuple[tuple[jax.Array, tp.Any], State], tuple[jax.Array, State]],
-]:
+  *,
+  wrt: filterlib.Filter = variables.Param,
+) -> tp.Callable[..., tp.Any]:
   if f.__name__ == '__init__':
     raise ValueError('Cannot use `value_and_grad` with `__init__`')
 
+  _argnums = _normalize_sequence(argnums)
   options = GradOptions(
-    wrt=wrt,
+    argnums=_argnums,
     has_aux=has_aux,
     holomorphic=holomorphic,
     allow_int=allow_int,
     reduce_axes=reduce_axes,
     return_value=True,
+    wrt=wrt,
   )
 
   @functools.wraps(f)
-  def value_and_grad_wrapper(module: Module, *args, **kwargs):
-    _check_args(args)
-    return grad_apply(options, f, module, *args, **kwargs)
+  def value_and_grad_wrapper(*args):
+    return grad_apply(options, f, args)
 
   return value_and_grad_wrapper  # type: ignore
 
 
 # -------------------------------
 # scan
 # -------------------------------
 
-
 @dataclasses.dataclass
 class ScanOptions:
-  variable_axes: tp.Mapping[filterlib.Filter, int]
-  broadcast_rngs: filterlib.Filter
-  in_args_axes: tp.Any
-  in_kwargs_axes: tp.Any
-  out_axes: tp.Any
-  length: tp.Optional[int]
+  length: int | None
   reverse: bool
-  unroll: int
-  scan_metadata: tp.Mapping[str, tp.Any]
+  unroll: int | bool
+  _split_transpose: bool
+  # extended api
+  in_axes: tp.Any
+  in_axes_kwargs: tp.Any
+  out_axes: tp.Any
+  carry_argnum: int
+  # nnx specific
+  state_axes: tp.Mapping[filterlib.Filter, int]
+  split_rngs: filterlib.Filter
+  transform_metadata: tp.Mapping[str, tp.Any]
   scan_output: bool
 
 
 class ScanMeta(ModuleMeta):
   def __call__(
     self,
     module_constructor: tp.Callable[..., M],
     *,
-    variable_axes: tp.Mapping[filterlib.Filter, int] = MappingProxyType({}),
-    broadcast_rngs: filterlib.Filter = None,
-    in_args_axes: tp.Any = 0,
-    in_kwargs_axes: tp.Any = 0,
-    out_axes: tp.Any = 0,
-    length: tp.Optional[int] = None,
+    length: int | None = None,
     reverse: bool = False,
-    unroll: int = 1,
-    scan_metadata: tp.Mapping[str, tp.Any] = MappingProxyType({}),
+    unroll: int | bool = 1,
+    _split_transpose: bool = False,
+    # extended api
+    in_axes: int | None | tp.Sequence[tp.Any] = 0,
+    in_axes_kwargs: tp.Any = 0,
+    out_axes: tp.Any = 0,
+    carry_argnum: int = 1,
+    # nnx specific
+    state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
+    split_rngs: filterlib.Filter = ...,
+    transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
     scan_output: bool = True,
   ) -> tp.Callable[..., 'Scan[M]']:
     super_call = super().__call__
 
     def _create_scan(*args, **kwargs) -> Scan[M]:
-      _check_args(args)
       return super_call(
         module_constructor=module_constructor,
         module_init_args=args,
         module_init_kwargs=kwargs,
-        variable_axes=variable_axes,
-        broadcast_rngs=broadcast_rngs,
-        in_args_axes=in_args_axes,
-        in_kwargs_axes=in_kwargs_axes,
-        out_axes=out_axes,
+        # base api
         length=length,
         reverse=reverse,
         unroll=unroll,
-        scan_metadata=scan_metadata,
+        _split_transpose=_split_transpose,
+        # extended api
+        in_axes=in_axes,
+        in_axes_kwargs=in_axes_kwargs,
+        out_axes=out_axes,
+        carry_argnum=carry_argnum,
+        # nnx specific
+        state_axes=state_axes,
+        split_rngs=split_rngs,
+        transform_metadata=transform_metadata,
         scan_output=scan_output,
       )
 
     return _create_scan
 
 
 class Scan(LiftedModule[M], metaclass=ScanMeta):
   def __init__(
     self,
     module_constructor: tp.Callable[..., M],
     *,
-    variable_axes: tp.Mapping[filterlib.Filter, int] = MappingProxyType({}),
-    broadcast_rngs: filterlib.Filter = None,
-    in_args_axes: tp.Any = 0,
-    in_kwargs_axes: tp.Any = 0,
-    out_axes: tp.Any = 0,
-    length: tp.Optional[int] = None,
+    length: int | None = None,
     reverse: bool = False,
-    unroll: int = 1,
-    scan_metadata: tp.Mapping[str, tp.Any] = MappingProxyType({}),
+    unroll: int | bool = 1,
+    _split_transpose: bool = False,
+    # extended api
+    in_axes: int | None | tp.Sequence[tp.Any] = 0,
+    in_axes_kwargs: tp.Any = 0,
+    out_axes: tp.Any = 0,
+    carry_argnum: int = 1,
+    # nnx specific
+    state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
+    split_rngs: filterlib.Filter = ...,
+    transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
     scan_output: bool = True,
     # submodule args
     module_init_args: tuple[tp.Any, ...],
     module_init_kwargs: dict[str, tp.Any],
   ):
     self.module_constructor = module_constructor
     self.options = ScanOptions(
-      variable_axes=variable_axes,
-      broadcast_rngs=broadcast_rngs,
-      in_args_axes=in_args_axes,
-      in_kwargs_axes=in_kwargs_axes,
-      out_axes=out_axes,
       length=length,
       reverse=reverse,
       unroll=unroll,
-      scan_metadata=scan_metadata,
+      _split_transpose=_split_transpose,
+      # extended api
+      in_axes=in_axes,
+      in_axes_kwargs=in_axes_kwargs,
+      out_axes=out_axes,
+      carry_argnum=carry_argnum,
+      # nnx specific
+      state_axes=state_axes,
+      split_rngs=split_rngs,
+      transform_metadata=transform_metadata,
       scan_output=scan_output,
     )
-    self.scan_module = scan_init(
-      self.options, module_constructor, module_init_args, module_init_kwargs
-    )
+    # use Vmap to handle initialisation
+    vmapped_module = Vmap(
+      module_constructor,
+      in_axes=in_axes,
+      out_axes=None,
+      axis_name=None,
+      axis_size=length,
+      spmd_axis_name=None,
+      state_axes=state_axes,
+      split_rngs=split_rngs,
+      in_axes_kwargs=in_axes_kwargs,
+      transform_metadata=transform_metadata,
+    )(*module_init_args, **module_init_kwargs)
+
+    self.scan_module = vmapped_module.vmap_module
 
   @property
   def _submodule(self) -> M:
     return self.scan_module
 
   def _call(
     self, accessor: DelayedAccessor, *args, **kwargs
   ) -> tuple[tp.Any, tp.Any]:
-    if len(args) < 1:
-      raise TypeError(
-        f'Expected at least 1 positional arguments, got {len(args)}'
-      )
-    _check_args(args)
-    carry_arg, args = args[0], args[1:]
-
     def scan_call_apply(module, *args, **kwargs):
-      return accessor(module)(*args, **kwargs)
+      method = accessor(module)
+      return method(*args, **kwargs)
 
     return scan_apply(
       self.options,
       scan_call_apply,
-      self.scan_module,
-      carry_arg,
-      args,
+      (self._submodule, *args),
       kwargs,
     )
 
 
-class ScanCall(tp.Protocol, tp.Generic[C, B]):
-  def __call__(
-    self,
-    module: Module,
-    carry_arg: C,
-    *args: tp.Any,
-    **kwargs: tp.Any,
-  ) -> tuple[C, B] | C:
-    ...
-
-
-def scan_init(
-  options: ScanOptions,
-  module_constructor: tp.Callable[..., M],
-  module_init_args: tuple[tp.Any, ...],
-  module_init_kwargs: dict[str, tp.Any],
-) -> M:
-  if options.variable_axes and options.length is None:
-    raise ValueError('Cannot use variable_axes without specifying a length')
-
-  _check_args(module_init_args)
-
-  rngs = module_init_kwargs.pop('rngs', None)
-
-  if rngs is not None and not isinstance(rngs, rnglib.Rngs):
-    raise TypeError(f'Expected a Rngs, got {type(rngs).__name__}')
-
-  split_keys = []
-
-  if rngs is not None:
-    if not isinstance(rngs, rnglib.Rngs):
-      raise TypeError(f'Expected a Rngs, got {type(rngs).__name__}')
-
-    forked_rngs = rngs.fork(
-      {filterlib.Not(options.broadcast_rngs): options.length}
-    )
-    split_keys, broadcast_keys = forked_rngs.splits, forked_rngs.broadcasts
-
-    if split_keys and options.length is None:
-      raise ValueError('Cannot split RNGs without specifying a length')
-
-  else:
-    split_keys = None
-    broadcast_keys = None
-
-  graphdef: tp.Optional[GraphDef[M]] = None
-
-  def _init_state(split_keys, broadcast_keys):
-    nonlocal graphdef
-
-    if split_keys is not None:
-      assert broadcast_keys is not None
-      module_init_kwargs['rngs'] = rnglib.Rngs(**split_keys, **broadcast_keys)
-
-    module = module_constructor(*module_init_args, **module_init_kwargs)
-
-    # lift module
-    filters = (*options.variable_axes.keys(), ...)
-
-    *states, graphdef = module.split(*filters)
-
-    return tuple(states)
-
-  if split_keys is not None or options.variable_axes:
-    init_out_axes = (*options.variable_axes.values(), None)
-    _init_state = jax.vmap(
-      _init_state,
-      in_axes=(0, None),
-      out_axes=init_out_axes,
-      axis_size=options.length,
-    )
-
-  *axes_states, carry_state = _init_state(split_keys, broadcast_keys)
-  graphdef = tp.cast(GraphDef[M], graphdef)
-
-  # add additional axis name to Variable.sharding
-  if spmd.PARTITION_NAME in options.scan_metadata:
-    axes_states = [
-      spmd.add_axis(state, index, options.scan_metadata)
-      for state, index in zip(axes_states, options.variable_axes.values())
-    ]
-
-  module = graphdef.merge(*axes_states, carry_state)
-
-  return module
-
-
 def scan_apply(
   options: ScanOptions,
-  f: ScanCall[C, B],
-  module: Module,
-  carry_arg: C,
+  f: tp.Callable[..., tuple[C, B] | C],
   args: tuple[tp.Any, ...],
   kwargs: dict[str, tp.Any],
 ) -> tuple[C, B] | C:
-  rngs = kwargs.pop('rngs', None)
+  # extract nodes
+  (args, kwargs), input_graph_nodes = graph.extract_graph_nodes((args, kwargs))
+  input_rng_streams = rnglib.backup_keys(input_graph_nodes)
 
-  # split module state
-  filters = (*options.variable_axes.keys(), ...)
-  *scan_states, carry_state, graphdef = module.split(*filters)
+  # extract carry arg
+  carry_arg, args = _extract_carry_arg(args, options.carry_argnum)
 
-  # transpose axes state
-  scan_states = tuple(
-    jax.tree_util.tree_map(lambda x: jnp.moveaxis(x, axis, 0), axes_state)
-    for axes_state, axis in zip(scan_states, options.variable_axes.values())
+  ctx = graph.UpdateContext()
+  # split module state
+  filters = (*options.state_axes.keys(), ...)
+  graphdef, rng_state, *scan_states, carry_state = ctx.split(
+    input_graph_nodes, rnglib.RngState, *filters
   )
+
   # transpose axes arg
-  scan_args = jax.tree_util.tree_map(
-    lambda axis, node: jax.tree_util.tree_map(
-      lambda x: jnp.moveaxis(x, axis, 0), node
-    )
-    if axis is not None
-    else None,
-    options.in_args_axes,
-    args,
-    is_leaf=lambda x: x is None,
-  )
-  broadcast_args = jax.tree_map(
-    lambda axis, node: node if axis is None else None,
-    options.in_args_axes,
-    args,
-    is_leaf=lambda x: x is None,
-  )
-  scan_kwargs = jax.tree_util.tree_map(
-    lambda axis, node: jax.tree_util.tree_map(
-      lambda x: jnp.moveaxis(x, axis, 0), node
-    )
-    if axis is not None
-    else None,
-    options.in_kwargs_axes,
-    kwargs,
-    is_leaf=lambda x: x is None,
-  )
-  broadcast_kwargs = jax.tree_util.tree_map(
-    lambda axis, node: None if axis is not None else node,
-    options.in_kwargs_axes,
-    kwargs,
-    is_leaf=lambda x: x is None,
+  flatdef, flat_scan, flat_carry = _transpose_and_split(
+    (args, kwargs, scan_states),
+    (
+      options.in_axes,
+      options.in_axes_kwargs,
+      list(options.state_axes.values()),
+    ),
   )
 
   # infer length
-  lengths: tp.Set[int] = set(
-    x.shape[0]
-    for x in jax.tree_util.tree_leaves((scan_states, scan_args, scan_kwargs))
+  lengths: set[int] = set(
+    x.shape[axis]  # type: ignore
+    for x, axis in zip(flat_scan, flatdef.flat_axes)
+    if axis is not None
   )
 
   if len(lengths) > 1:
     raise ValueError(
-      'Inconsistent lengths between variable_axes states and '
+      'Inconsistent lengths between state_axes states and '
       f'arguments: {lengths}'
     )
   elif len(lengths) == 0:
     if options.length is None:
       raise ValueError(
-        'Cannot infer length from variable_axes states or axes_arg, '
+        'Cannot infer length from state_axes states or axes_arg, '
         'please specify `length`'
       )
     length = options.length
   else:
     length = lengths.pop()
     if options.length is not None and options.length != length:
       raise ValueError(
         f'Specified length {options.length} is not the same as the inferred '
         f'length {length}'
       )
 
   # split rng state
-  if rngs is not None:
-    if not isinstance(rngs, rnglib.Rngs):
-      raise TypeError(f'Expected a Rngs, got {type(rngs).__name__}')
-    forked_rngs = rngs.fork({filterlib.Not(options.broadcast_rngs): length})
-    split_keys, broadcast_keys = forked_rngs.splits, forked_rngs.broadcasts
-  else:
-    split_keys = None
-    broadcast_keys = None
-
-  moduledef_out: tp.Optional[GraphDef[Module]] = None
+  split_keys, carry_keys = rnglib.fork(
+    rng_state,
+    options.split_rngs,
+    length,
+  )
 
   def scan_fn(
-    carry: tuple[State, tp.Any],
+    carry: tuple[
+      State,  # carry_keys
+      State,  # carry_state
+      tp.Any,  # carry_arg
+    ],
     scan: tuple[
-      dict[str, rnglib.RngStream] | None,
-      tuple[State, ...],
-      tuple[tp.Any, ...],
-      dict[str, tp.Any],
+      State,  # split_keys
+      list[jax.Array | None],  # flat_scan
     ],
   ):
-    nonlocal moduledef_out
-    carry_state, carry_arg = carry
-    split_keys, scan_states, scan_args, scan_kwargs = scan
+    carry_keys, carry_state, carry_arg = carry
+    split_keys, flat_scan = scan
 
     # merge args and kwargs
-    args = jax.tree_util.tree_map(
-      lambda axis, scan, broadcast: scan if axis is not None else broadcast,
-      options.in_args_axes,
-      scan_args,
-      broadcast_args,
-      is_leaf=lambda x: x is None,
-    )
-    kwargs = jax.tree_util.tree_map(
-      lambda axis, scan, broadcast: scan if axis is not None else broadcast,
-      options.in_kwargs_axes,
-      scan_kwargs,
-      broadcast_kwargs,
-      is_leaf=lambda x: x is None,
-    )
-
-    # merge rng state
-    if split_keys is not None:
-      assert broadcast_keys is not None
-      kwargs['rngs'] = rnglib.Rngs(**split_keys, **broadcast_keys)
-
+    args, kwargs, scan_states = _unflatten_splits(
+      flatdef, flat_scan, flat_carry
+    )
     # remove metadata axis name from Variable.sharding
-    if spmd.PARTITION_NAME in options.scan_metadata:
+    if spmd.PARTITION_NAME in options.transform_metadata:
       scan_states = [
-        spmd.remove_axis(state, index, options.scan_metadata)
-        for state, index in zip(scan_states, options.variable_axes.values())
+        spmd.remove_axis(state, index, options.transform_metadata)
+        for state, index in zip(scan_states, options.state_axes.values())
       ]
 
+    # insert carry arg
+    args = _insert_carry_arg(args, options.carry_argnum, carry_arg)
+
     # merge module state
-    module = graphdef.merge(*scan_states, carry_state)
+    input_graph_nodes = ctx.merge(
+      graphdef, *scan_states, carry_state, split_keys, carry_keys
+    )
+    (args, kwargs) = graph.insert_graph_nodes((args, kwargs), input_graph_nodes)
 
-    output = f(module, carry_arg, *args, **kwargs)
+    out = f(*args, **kwargs)
 
     if options.scan_output:
-      if not isinstance(output, tuple) or len(output) != 2:
+      if not isinstance(out, tuple) or len(out) != 2:
         raise ValueError(
           'Expected a tuple of length 2 as the output of the scan function, '
-          f'got {output}'
+          f'got {out}'
         )
-      output = tp.cast(tuple[C, B], output)
-      carry_out, scan_out = output
+      out = tp.cast(tuple[C, B], out)
+      carry_arg_out, scan_args_out = out
     else:
-      output = tp.cast(C, output)
-      carry_out = output
-      scan_out = None
+      out = tp.cast(C, out)
+      carry_arg_out = out
+      scan_args_out = None
+
+    (
+      (carry_arg_out, scan_args_out),
+      output_graph_nodes,
+    ) = graph.extract_graph_nodes((carry_arg_out, scan_args_out))
 
     # split module state
-    *scan_states_out, carry_state_out, moduledef_out = module.split(*filters)
-    carry_state_new = carry_state_out - carry_state
+    (
+      graphdef_out,
+      rng_state_out,
+      *scan_states_out,
+      carry_state_out,
+    ) = ctx.split(
+      (input_graph_nodes, output_graph_nodes),
+      rnglib.RngState,
+      *filters,
+    )
 
-    # remove new carry state
-    carry_state_out = carry_state_out - carry_state_new
+    not_keys_out, split_keys_out, carry_keys_out = rng_state_out.split(
+      rnglib.NotKey, options.split_rngs, ...
+    )
+    carry_keys_out = State.merge(not_keys_out, carry_keys_out)
+
+    if 1 in carry_state_out:
+      raise ValueError(
+        f'Cannot add new carry state during scan, got {carry_state_out[1]}'
+      )
+    if 0 in carry_state_out:
+      carry_state_out = carry_state_out[0]
+      assert isinstance(carry_state_out, State)
+    if 1 in carry_keys_out:
+      raise ValueError(
+        f'Cannot add new carry keys during scan, got {carry_keys_out[1]}'
+      )
+    if 0 in carry_keys_out:
+      carry_keys_out = carry_keys_out[0]
+      assert isinstance(carry_keys_out, State)
 
     # add metadata axis name to Variable.sharding
-    if spmd.PARTITION_NAME in options.scan_metadata:
+    if spmd.PARTITION_NAME in options.transform_metadata:
       scan_states_out = [
-        spmd.add_axis(state, index, options.scan_metadata)
-        for state, index in zip(scan_states_out, options.variable_axes.values())
+        spmd.add_axis(state, index, options.transform_metadata)
+        for state, index in zip(scan_states_out, options.state_axes.values())
       ]
 
-    full_carry_out = (carry_state_out, carry_out)
-    full_scan_out = (scan_states_out, carry_state_new, scan_out)
+    carry_out = (carry_keys_out, carry_state_out, carry_arg_out)
+    scan_out = (graphdef_out, scan_args_out, scan_states_out, split_keys_out)
 
-    return full_carry_out, full_scan_out
+    return carry_out, scan_out
 
-  carry = (carry_state, carry_arg)
-  scan = (split_keys, scan_states, scan_args, scan_kwargs)
+  carry = (carry_keys, carry_state, carry_arg)
+  scan = (split_keys, flat_scan)
 
-  full_carry_out, full_scan_out = jax.lax.scan(
+  carry_out, scan_out = jax.lax.scan(
     scan_fn,
     carry,
     scan,
     length=length,
     reverse=options.reverse,
     unroll=options.unroll,
+    _split_transpose=options._split_transpose,
   )
-  carry_state, carry_out = full_carry_out
-  scan_states, carry_state_new, scan_out = full_scan_out
-  assert moduledef_out is not None
-
-  # transpose axes state
-  scan_states = tuple(
-    jax.tree_util.tree_map(lambda x: jnp.moveaxis(x, 0, axis), axes_state)
-    for axes_state, axis in zip(scan_states, options.variable_axes.values())
+  carry_keys_out, carry_state_out, carry_arg_out = carry_out
+  graphdef_out, scan_args_out, scan_states_out, split_keys_out = scan_out
+
+  scan_args_out, scan_states_out = _transpose_tree(
+    (scan_args_out, scan_states_out),
+    (options.out_axes, list(options.state_axes.values())),
+    axis_is_source=False,
   )
-  # transpose axes arg
-  scan_out = jax.tree_util.tree_map(
-    lambda axis, node: jax.tree_util.tree_map(
-      lambda x: jnp.moveaxis(x, 0, axis), node
-    ),
-    options.out_axes,
-    scan_out,
+
+  if carry_state_out:
+    carry_state_out = State({0: carry_state_out._mapping})
+  if carry_keys_out:
+    carry_keys_out = State({0: carry_keys_out._mapping})
+  _, output_graph_nodes = ctx.update(
+    graphdef_out,
+    *scan_states_out,
+    carry_state_out,
+    carry_keys_out,
+    split_keys_out,
+  )
+
+  carry_arg_out, scan_args_out = graph.insert_graph_nodes(
+    (carry_arg_out, scan_args_out), output_graph_nodes
   )
-  # slice new carry state
-  carry_state_new = jax.tree_util.tree_map(lambda x: x[0], carry_state_new)
 
-  module.update(((*scan_states, carry_state, carry_state_new), moduledef_out))
+  rnglib.restore_keys(input_rng_streams)
 
   if options.scan_output:
-    return carry_out, scan_out
+    scan_args_out = tp.cast(B, scan_args_out)
+    return carry_arg_out, scan_args_out
   else:
-    return carry_out
+    return carry_arg_out
+
+
+@dataclasses.dataclass(frozen=True)
+class FlatDef(tp.Generic[A]):
+  type: type[A]
+  treedef: jax.tree_util.PyTreeDef
+  flat_axes: list[int | None]
+
+jax.tree_util.register_static(FlatDef)
+
+def _transpose_tree(tree: A, axes, /, *, axis_is_source: bool) -> A:
+  flatdef, flat_transposes, _ = _transpose_and_split(
+    tree, axes, allow_none=False, axis_is_source=axis_is_source
+  )
+  return flatdef.treedef.unflatten(flat_transposes)
+
+
+def _transpose_and_split(
+  tree: A, axes, /, *, allow_none: bool = True, axis_is_source: bool = True
+) -> tuple[
+  FlatDef[A],
+  list[jax.Array | None],
+  list[tp.Any],
+]:
+  flat_axes: list[int | None] = broadcast_prefix(
+    axes, tree, is_leaf=lambda x: x is None
+  )
+  flat_tree, treedef = jax.tree.flatten(tree)
+
+  flat_broadcasts: list[tp.Any] = []
+  flat_transposes: list[jax.Array | None] = []
+
+  for i, (axis, node) in enumerate(zip(flat_axes, flat_tree)):
+    if axis is None:
+      if not allow_none:
+        raise ValueError('None axis not allowed')
+
+      flat_broadcasts.append(node)
+      flat_transposes.append(None)
+    else:
+      if not isinstance(node, jax.Array):
+        raise TypeError(
+          f'Expected a jax.Array, got {type(node).__name__} for axis {axis}'
+        )
+      # normalize axis
+      if axis < 0:
+        if axis < -len(node.shape):
+          raise ValueError(
+            f'Axis {axis} out of bounds for array with shape {node.shape}'
+          )
+        axis = len(node.shape) + axis
+        flat_axes[i] = axis
+
+      if axis_is_source:
+        node = jnp.moveaxis(node, axis, 0)
+      else:
+        node = jnp.moveaxis(node, 0, axis)
+      flat_broadcasts.append(None)
+      flat_transposes.append(node)
+
+  flatdef = FlatDef(type(tree), treedef, flat_axes)
+
+  return flatdef, flat_transposes, flat_broadcasts
+
+def _unflatten_splits(
+  flatdef: FlatDef[A],
+  flat_transposes: list[jax.Array | None],
+  flat_broadcasts: list[tp.Any] | None = None,
+  /,
+  *,
+  allow_none: bool = True,
+) -> A:
+  flat_axes = flatdef.flat_axes
+  treedef = flatdef.treedef
+  if flat_broadcasts is None:
+    if allow_none:
+      raise ValueError('flat_broadcasts must be provided if allow_none is True')
+    flat_broadcasts = [None] * len(flat_axes)
+
+  flat_tree = []
+  for axis, transpose, broadcast in zip(
+    flat_axes, flat_transposes, flat_broadcasts
+  ):
+    if axis is None:
+      if not allow_none:
+        raise ValueError('None axis not allowed')
+      flat_tree.append(broadcast)
+    else:
+      if transpose is None:
+        raise ValueError('None transpose not allowed')
+      flat_tree.append(transpose)
+
+  tree = treedef.unflatten(flat_tree)
+  return tree
+
+
+def _extract_carry_arg(
+  args: tuple[tp.Any, ...], carry_argnum: int, /
+) -> tuple[tp.Any, tuple[tp.Any, ...]]:
+  # extract carry arg
+  if len(args) < carry_argnum + 1:
+    raise TypeError(
+      f'Expected at least {carry_argnum + 1} positional arguments, '
+      f'got {len(args)}'
+    )
+
+  args_ = list(args)
+  carry_arg = args_[carry_argnum]
+  args_[carry_argnum] = None
+  args = tuple(args_)
+
+  return carry_arg, args
+
+
+def _insert_carry_arg(
+  args: tuple[tp.Any, ...], carry_argnum: int, carry_arg: tp.Any, /
+) -> tuple[tp.Any, ...]:
+  args_ = list(args)
+  args_[carry_argnum] = carry_arg
+  args = tuple(args_)
+
+  return args
 
 
 def scan(
   f: F,
   *,
-  variable_axes: tp.Mapping[filterlib.Filter, int] = MappingProxyType({}),
-  broadcast_rngs: filterlib.Filter = None,
-  in_args_axes: tp.Any = 0,
-  in_kwargs_axes: tp.Any = 0,
-  out_axes: tp.Any = 0,
-  length: tp.Optional[int] = None,
+  length: int | None = None,
   reverse: bool = False,
-  unroll: int = 1,
-  is_init: tp.Optional[bool] = None,
-  scan_metadata: tp.Mapping[str, tp.Any] = {},
+  unroll: int | bool = 1,
+  _split_transpose: bool = False,
+  # extended api
+  in_axes: int | None | tp.Sequence[tp.Any] = 0,
+  in_axes_kwargs: tp.Any = 0,
+  out_axes: tp.Any = 0,
+  carry_argnum: int = 0,
+  # nnx specific
+  state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
+  split_rngs: filterlib.Filter = ...,
+  transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
   scan_output: bool = True,
 ) -> F:
-  if is_init is None:
-    is_init = f.__name__ == '__init__'
-
   options = ScanOptions(
-    variable_axes=variable_axes,
-    broadcast_rngs=broadcast_rngs,
-    in_args_axes=in_args_axes,
-    in_kwargs_axes=in_kwargs_axes,
-    out_axes=out_axes,
     length=length,
     reverse=reverse,
     unroll=unroll,
-    scan_metadata=scan_metadata,
+    _split_transpose=_split_transpose,
+    in_axes=in_axes,
+    in_axes_kwargs=in_axes_kwargs,
+    out_axes=out_axes,
+    carry_argnum=carry_argnum,
+    state_axes=state_axes,
+    split_rngs=split_rngs,
+    transform_metadata=transform_metadata,
     scan_output=scan_output,
   )
 
-  if is_init:
-
-    @functools.wraps(f)
-    def scan_init_wrapper(module: Module, *args, **kwargs):
-      def module_constructor(*args, **kwargs):
-        _check_args(args)
-        f(module, *args, **kwargs)
-        return module
-
-      lifted_module = scan_init(options, module_constructor, args, kwargs)
-      module.update(lifted_module)
-
-    wrapper = scan_init_wrapper
-
-  else:
-
-    @functools.wraps(f)
-    def scan_apply_wrapper(
-      module: Module,
-      *args,
-      **kwargs,
-    ) -> tuple[C, tp.Any]:
-      if len(args) < 2:
-        raise TypeError(
-          f'Expected at least 2 positional arguments, got {len(args)}'
-        )
-      _check_args(args)
-
-      carry_arg, args = args[0], args[1:]
-      return scan_apply(options, f, module, carry_arg, args, kwargs)
+  @functools.wraps(f)
+  def scan_apply_wrapper(*args, **kwargs) -> C | tuple[C, tp.Any]:
+    return scan_apply(options, f, args, kwargs)
 
-    wrapper = scan_apply_wrapper
-
-  return wrapper  # type: ignore
+  return scan_apply_wrapper  # type: ignore
 
 
 # -------------------------------
 # remat
 # -------------------------------
 
 
 class RematMeta(ModuleMeta):
   def __call__(
     self,
     module_constructor: tp.Callable[..., M],
-    # variables: lift.CollectionFilter = True,
-    # rngs: lift.PRNGSequenceFilter = True,
     prevent_cse: bool = True,
-    static_argnums: tp.Union[int, tuple[int, ...]] = (),
-    policy: tp.Optional[tp.Callable[..., bool]] = None,
+    static_argnums: int | tuple[int, ...] = (),
+    policy: tp.Callable[..., bool] | None = None,
   ) -> tp.Callable[..., 'Remat[M]']:
     super_call = super().__call__
 
     def create_remat(*args, **kwargs) -> Remat[M]:
-      _check_args(args)
       return super_call(
         module_constructor=module_constructor,
         module_init_args=args,
         module_init_kwargs=kwargs,
         prevent_cse=prevent_cse,
         static_argnums=static_argnums,
         policy=policy,
@@ -1133,35 +1395,35 @@
 
     return create_remat
 
 
 @dataclasses.dataclass
 class RematOptions:
   prevent_cse: bool
-  static_argnums: tp.Union[int, tuple[int, ...]]
-  policy: tp.Optional[tp.Callable[..., bool]]
+  static_argnums: int | tuple[int, ...]
+  policy: tp.Callable[..., bool] | None
 
   def __post_init__(self):
     if isinstance(self.static_argnums, int):
       self.static_argnums = (self.static_argnums,)
 
-    # add 2 as an offset to account for state and keys
+    # add 1 as an offset to account for state parameter
     self.static_argnums = tuple(
-      x + 2 if x >= 0 else x for x in self.static_argnums
+      x + 1 if x >= 0 else x for x in self.static_argnums
     )
 
 
 class Remat(LiftedModule[M], metaclass=RematMeta):
   def __init__(
     self,
     *,
     module_constructor: tp.Callable[..., M],
     prevent_cse: bool = True,
-    static_argnums: tp.Union[int, tuple[int, ...]] = (),
-    policy: tp.Optional[tp.Callable[..., bool]] = None,
+    static_argnums: int | tuple[int, ...] = (),
+    policy: tp.Callable[..., bool] | None = None,
     # submodule args
     module_init_args: tuple[tp.Any, ...],
     module_init_kwargs: dict[str, tp.Any],
   ):
     self.options = RematOptions(
       prevent_cse=prevent_cse,
       static_argnums=static_argnums,
@@ -1172,482 +1434,426 @@
       *module_init_args, **module_init_kwargs
     )
 
   @property
   def _submodule(self) -> M:
     return self.remat_module
 
-  def _call(
-    self,
-    accessor: DelayedAccessor,
-    *args,
-    rngs: tp.Optional[rnglib.Rngs] = None,
-  ) -> tp.Any:
-    def remat_call_apply(module, *args, **kwargs):
-      return accessor(module)(*args, **kwargs)
+  def _call(self, accessor: DelayedAccessor, *args) -> tp.Any:
+    def remat_apply_call(module, *args):
+      method = accessor(module)
+      return method(*args)
 
     return remat_apply(
       self.options,
-      remat_call_apply,
-      self.remat_module,
-      args,
-      rngs,
+      remat_apply_call,
+      (self.remat_module, *args),
     )
 
 
-class RematCall(tp.Protocol):
-  def __call__(self, *args, rngs: tp.Optional[rnglib.Rngs]) -> tp.Any:
-    ...
-
-
 def remat_apply(
   options: RematOptions,
-  f: RematCall,
-  module: Module,
+  f: tp.Callable[..., tp.Any],
   args: tuple[tp.Any, ...],
-  rngs: tp.Optional[rnglib.Rngs],
 ):
-  _check_args(args)
-
-  state, graphdef = module.split()
-  keys = rngs.fork() if rngs is not None else None
-
-  def _remat_fn(
-    state: State,
-    keys: tp.Optional[dict[str, jax.Array]],
-    *args,
-  ) -> tuple[tuple[State, GraphDef[Module]], tp.Any]:
-    kwargs = {}
-    if keys is not None:
-      kwargs['rngs'] = rnglib.Rngs(keys)
+  ctx = graph.UpdateContext()
+  args, input_nodes = graph.extract_graph_nodes(args)
+  graphdef, state = ctx.split(input_nodes)
+
+  def _remat_fn(state: State, *args):
+    input_nodes = ctx.merge(graphdef, state)
+    args = graph.insert_graph_nodes(args, input_nodes)
+    out = f(*args)
+
+    out, output_nodes = graph.extract_graph_nodes(out)
+    new_graphdef, new_state = ctx.split((input_nodes, output_nodes))
+    return (new_graphdef, new_state), out
 
-    module = graphdef.merge(state)
-    out = f(module, *args, **kwargs)
-
-    state_and_def = module.split()
-
-    return state_and_def, out
-
-  state_and_def: tuple[State, GraphDef[Module]]
-  state_and_def, out = jax.checkpoint(
+  (new_graphdef, new_state), out = jax.checkpoint(
     _remat_fn,
     prevent_cse=options.prevent_cse,
     static_argnums=options.static_argnums,
     policy=options.policy,
-  )(state, keys, *args)
+  )(state, *args)
 
-  module.update(state_and_def)
+  _, output_nodes = ctx.update(new_graphdef, new_state)
+  out = graph.insert_graph_nodes(out, output_nodes)
 
   return out
 
 
 def remat(
   f: F,
   *,
-  # variables: lift.CollectionFilter,
-  # rngs: lift.PRNGSequenceFilter,
   prevent_cse: bool = True,
-  static_argnums: tp.Union[int, tuple[int, ...]] = (),
-  policy: tp.Optional[tp.Callable[..., bool]] = None,
-  is_init: tp.Optional[bool] = None,
+  static_argnums: int | tuple[int, ...] = (),
+  policy: tp.Callable[..., bool] | None = None,
 ) -> F:
-  if is_init is None:
-    is_init = f.__name__ == '__init__'
-
   options = RematOptions(
-    # variables=variables,
-    # rngs=rngs,
     prevent_cse=prevent_cse,
     static_argnums=static_argnums,
     policy=policy,
   )
 
-  if is_init:
-    return f
-  else:
-
-    @functools.wraps(f)
-    def remat_wrapper(
-      module: Module, *args, rngs: tp.Optional[rnglib.Rngs] = None
-    ):
-      return remat_apply(options, f, module, args, rngs)
+  @functools.wraps(f)
+  def remat_wrapper(*args):
+    return remat_apply(options, f, args)
 
-    return remat_wrapper  # type: ignore
+  return remat_wrapper  # type: ignore
 
 
 # -------------------------------
 # vmap
 # -------------------------------
 
-
 @dataclasses.dataclass
 class VmapOptions:
-  variable_axes: tp.Mapping[filterlib.Filter, int]
-  broadcast_rngs: filterlib.Filter
-  in_args_axes: tp.Any
-  in_kwargs_axes: tp.Any
+  in_axes: int | None | tp.Sequence[tp.Any]
   out_axes: tp.Any
+  axis_name: AxisName | None
   axis_size: int | None
-  axis_name: str | None
-  spmd_axis_name: str | None
-  vmap_metadata: tp.Mapping[str, tp.Any]
+  spmd_axis_name: AxisName | tuple[AxisName, ...] | None
+  # nnx specific
+  state_axes: tp.Mapping[filterlib.Filter, int]
+  split_rngs: filterlib.Filter
+  in_axes_kwargs: tp.Any
+  transform_metadata: tp.Mapping[str, tp.Any]
 
 
 class VmapMeta(ModuleMeta):
   def __call__(
     self,
     module_constructor: tp.Callable[..., M],
     *,
-    variable_axes: tp.Mapping[filterlib.Filter, int] = MappingProxyType({}),
-    broadcast_rngs: filterlib.Filter = None,
-    in_args_axes: tp.Any = 0,
-    in_kwargs_axes: tp.Any = 0,
+    in_axes: int | None | tp.Sequence[tp.Any] = 0,
     out_axes: tp.Any = 0,
+    axis_name: AxisName | None = None,
     axis_size: int | None = None,
-    axis_name: str | None = None,
-    spmd_axis_name: str | None = None,
-    vmap_metadata: tp.Mapping[str, tp.Any] = MappingProxyType({}),
+    spmd_axis_name: AxisName | tuple[AxisName, ...] | None = None,
+    # nnx specific
+    in_axes_kwargs: tp.Any = 0,
+    state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
+    split_rngs: filterlib.Filter = ...,
+    transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
   ) -> tp.Callable[..., 'Vmap[M]']:
     super_call = super().__call__
 
-    def _create_scan(*args, **kwargs) -> Scan[M]:
-      _check_args(args)
+    def _create_vmap(*args, **kwargs) -> Scan[M]:
       return super_call(
         module_constructor=module_constructor,
-        module_init_args=args,
-        module_init_kwargs=kwargs,
-        variable_axes=variable_axes,
-        broadcast_rngs=broadcast_rngs,
-        in_args_axes=in_args_axes,
-        in_kwargs_axes=in_kwargs_axes,
+        in_axes=in_axes,
         out_axes=out_axes,
         axis_size=axis_size,
         axis_name=axis_name,
         spmd_axis_name=spmd_axis_name,
-        vmap_metadata=vmap_metadata,
+        # nnx specific
+        in_axes_kwargs=in_axes_kwargs,
+        state_axes=state_axes,
+        split_rngs=split_rngs,
+        transform_metadata=transform_metadata,
+        # submodule args
+        module_init_args=args,
+        module_init_kwargs=kwargs,
       )
 
-    return _create_scan
+    return _create_vmap
 
 
 class Vmap(LiftedModule[M], metaclass=VmapMeta):
   def __init__(
     self,
     module_constructor: tp.Callable[..., M],
     *,
-    variable_axes: tp.Mapping[filterlib.Filter, int] = MappingProxyType({}),
-    broadcast_rngs: filterlib.Filter = None,
-    in_args_axes: tp.Any = 0,
-    in_kwargs_axes: tp.Any = 0,
+    in_axes: int | None | tp.Sequence[tp.Any] = 0,
     out_axes: tp.Any = 0,
+    axis_name: AxisName | None = None,
     axis_size: int | None = None,
-    axis_name: str | None = None,
-    spmd_axis_name: str | None = None,
-    vmap_metadata: tp.Mapping[str, tp.Any] = MappingProxyType({}),
+    spmd_axis_name: AxisName | tuple[AxisName, ...] | None = None,
+    # nnx specific
+    in_axes_kwargs: tp.Any = 0,
+    state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
+    split_rngs: filterlib.Filter = ...,
+    transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
     # submodule args
     module_init_args: tuple[tp.Any, ...],
     module_init_kwargs: dict[str, tp.Any],
   ):
     self.module_constructor = module_constructor
     self.options = VmapOptions(
-      variable_axes=variable_axes,
-      broadcast_rngs=broadcast_rngs,
-      in_args_axes=in_args_axes,
-      in_kwargs_axes=in_kwargs_axes,
+      in_axes=in_axes,
       out_axes=out_axes,
-      axis_size=axis_size,
       axis_name=axis_name,
+      axis_size=axis_size,
       spmd_axis_name=spmd_axis_name,
-      vmap_metadata=vmap_metadata,
+      # nnx specific
+      in_axes_kwargs=in_axes_kwargs,
+      state_axes=state_axes,
+      split_rngs=split_rngs,
+      transform_metadata=transform_metadata,
     )
-    self.vmap_module = vmap_init(
-      self.options, module_constructor, module_init_args, module_init_kwargs
+
+    (
+      (module_init_args, module_init_kwargs),
+      init_nodes,
+    ) = graph.extract_graph_nodes((module_init_args, module_init_kwargs))
+
+    def vmap_init(init_nodes):
+      (args, kwargs) = graph.insert_graph_nodes(
+        (module_init_args, module_init_kwargs), init_nodes
+      )
+      return module_constructor(*args, **kwargs)
+
+    init_options = dataclasses.replace(
+      self.options,
+      in_axes=None,
+      out_axes=None,
     )
+    self.vmap_module = vmap_apply(init_options, vmap_init, (init_nodes,), {})
 
   @property
   def _submodule(self) -> M:
     return self.vmap_module
 
-  def _call(
-    self, accessor: DelayedAccessor, *args, **kwargs
-  ) -> tuple[tp.Any, tp.Any]:
-    _check_args(args)
-
-    def vmap_call_apply(module, *args, **kwargs):
-      return accessor(module)(*args, **kwargs)
+  def _call(self, accessor: DelayedAccessor, *args, **kwargs):
+    def vmap_apply_call(module, *args, **kwargs):
+      method = accessor(module)
+      return method(*args, **kwargs)
 
     return vmap_apply(
       self.options,
-      vmap_call_apply,
-      self.vmap_module,
-      args,
+      vmap_apply_call,
+      (self._submodule, *args),
       kwargs,
     )
 
-
-class VmapCall(tp.Protocol):
-  def __call__(
-    self,
-    module: Module,
-    *args: tp.Any,
-    **kwargs: tp.Any,
-  ) -> tp.Any:
-    ...
-
-
-def vmap_init(
-  options: VmapOptions,
-  module_constructor: tp.Callable[..., M],
-  module_init_args: tuple[tp.Any, ...],
-  module_init_kwargs: dict[str, tp.Any],
-) -> M:
-  if options.variable_axes and options.axis_size is None:
-    raise ValueError('Cannot use variable_axes without specifying a length')
-
-  _check_args(module_init_args)
-
-  rngs = module_init_kwargs.pop('rngs', None)
-
-  if rngs is not None and not isinstance(rngs, rnglib.Rngs):
-    raise TypeError(f'Expected a Rngs, got {type(rngs).__name__}')
-
-  if rngs is not None:
-    if not isinstance(rngs, rnglib.Rngs):
-      raise TypeError(f'Expected a Rngs, got {type(rngs).__name__}')
-    forked_rngs = rngs.fork(
-      {filterlib.Not(options.broadcast_rngs): options.axis_size}
-    )
-    split_keys, broadcast_keys = forked_rngs.splits, forked_rngs.broadcasts
-    if split_keys and options.axis_size is None:
-      raise ValueError('Cannot split RNGs without specifying a length')
-  else:
-    split_keys = None
-    broadcast_keys = None
-
-  graphdef: tp.Optional[GraphDef[M]] = None
-
-  def _init_state(split_keys, broadcast_keys):
-    nonlocal graphdef
-
-    if split_keys is not None:
-      assert broadcast_keys is not None
-      module_init_kwargs['rngs'] = rnglib.Rngs(**split_keys, **broadcast_keys)
-
-    module = module_constructor(*module_init_args, **module_init_kwargs)
-
-    # lift module
-    filters = (*options.variable_axes.keys(), ...)
-
-    *states, graphdef = module.split(*filters)
-
-    return tuple(states)
-
-  if split_keys is not None or options.variable_axes:
-    init_out_axes = (*options.variable_axes.values(), None)
-    _init_state = jax.vmap(
-      _init_state,
-      in_axes=(0, None),
-      out_axes=init_out_axes,
-      axis_size=options.axis_size,
-    )
-
-  *axes_states, carry_state = _init_state(split_keys, broadcast_keys)
-  graphdef = tp.cast(GraphDef[M], graphdef)
-
-  # add additional axis name to Variable.sharding
-  if spmd.PARTITION_NAME in options.vmap_metadata:
-    axes_states = [
-      spmd.add_axis(state, index, options.vmap_metadata)
-      for state, index in zip(axes_states, options.variable_axes.values())
-    ]
-
-  module = graphdef.merge(*axes_states, carry_state)
-  return module
-
-
 def vmap_apply(
   options: VmapOptions,
-  f: VmapCall,
-  module: Module,
+  f: tp.Callable[..., A],
   args: tuple[tp.Any, ...],
   kwargs: dict[str, tp.Any],
-) -> tp.Any:
-  rngs = kwargs.pop('rngs', None)
+) -> A:
+  (args, kwargs), input_graph_nodes = graph.extract_graph_nodes((args, kwargs))
+  input_rng_streams = rnglib.backup_keys(input_graph_nodes)
 
+  ctx = graph.UpdateContext()
   # split module state
-  filters = (*options.variable_axes.keys(), ...)
-  *vectorized_states, broadcast_state, graphdef = module.split(*filters)
+  filters = (*options.state_axes.keys(), ...)
+  graphdef, rng_state, *vectorized_states, broadcast_state = ctx.split(
+    input_graph_nodes, rnglib.RngState, *filters
+  )
 
   # infer length
   axis_sizes: tp.Set[int] = set()
   args_sizes = jax.tree_util.tree_map(
     lambda axis, node: jax.tree_util.tree_map(lambda x: x.shape[axis], node)
     if axis is not None
     else None,
-    options.in_args_axes,
+    options.in_axes,
     args,
     is_leaf=lambda x: x is None,
   )
   kwargs_sizes = jax.tree_util.tree_map(
     lambda axis, node: jax.tree_util.tree_map(lambda x: x.shape[axis], node)
     if axis is not None
     else None,
-    options.in_kwargs_axes,
+    options.in_axes_kwargs,
     kwargs,
     is_leaf=lambda x: x is None,
   )
   axis_sizes.update(jax.tree_util.tree_leaves(args_sizes))
   axis_sizes.update(jax.tree_util.tree_leaves(kwargs_sizes))
 
   if len(axis_sizes) > 1:
     raise ValueError(
-      'Inconsistent lengths between variable_axes states and '
+      'Inconsistent lengths between state_axes states and '
       f'arguments: {axis_sizes}'
     )
   elif len(axis_sizes) == 0:
     if options.axis_size is None:
       raise ValueError(
-        'Cannot infer length from variable_axes states or axes_arg, '
+        'Cannot infer length from state_axes states or axes_arg, '
         'please specify `length`'
       )
     axis_size = options.axis_size
   else:
     axis_size = axis_sizes.pop()
     if options.axis_size is not None and options.axis_size != axis_size:
       raise ValueError(
         f'Specified axis_size {options.axis_size} is not the same as the'
         f' inferred length {axis_size}'
       )
 
-  # split rng state
-  if rngs is not None:
-    if not isinstance(rngs, rnglib.Rngs):
-      raise TypeError(f'Expected a Rngs, got {type(rngs).__name__}')
-
-    forked_rngs = rngs.fork({filterlib.Not(options.broadcast_rngs): axis_size})
-    split_keys, broadcast_keys = forked_rngs.splits, forked_rngs.broadcasts
-  else:
-    split_keys = None
-    broadcast_keys = None
-
-  moduledef_out: tp.Optional[GraphDef[Module]] = None
+  split_keys, broadcast_keys = rnglib.fork(
+    rng_state,
+    options.split_rngs,
+    axis_size,
+  )
 
   keys_axes = 0
-  states_axes = list(options.variable_axes.values())
-  args_axes = options.in_args_axes
-  kwargs_axes = options.in_kwargs_axes
+  states_axes = list(options.state_axes.values())
+  args_axes = options.in_axes
+  kwargs_axes = options.in_axes_kwargs
   out_axes = options.out_axes
+  broadcast_state_axes = None
+  graphdef_out_axes = None
+  keys_axes_out = 0
 
   @functools.partial(
     jax.vmap,
     in_axes=(keys_axes, states_axes, args_axes, kwargs_axes),
-    out_axes=(None, states_axes, out_axes),
+    out_axes=(
+      graphdef_out_axes,
+      broadcast_state_axes,
+      states_axes,
+      keys_axes_out,
+      out_axes,
+    ),
     axis_name=options.axis_name,
     axis_size=axis_size,
     spmd_axis_name=options.spmd_axis_name,
   )
   def vmap_fn(
-    split_keys: dict[str, rnglib.RngStream] | None,
+    split_keys: State,
     vectorized_states: list[State],
     args: tuple[tp.Any, ...],
     kwargs: dict[str, tp.Any],
   ):
-    nonlocal moduledef_out
-
-    # merge rng state
-    if split_keys is not None:
-      assert broadcast_keys is not None
-      kwargs['rngs'] = rnglib.Rngs(**split_keys, **broadcast_keys)
-
     # remove metadata axis name from Variable.sharding
-    if spmd.PARTITION_NAME in options.vmap_metadata:
+    if spmd.PARTITION_NAME in options.transform_metadata:
       vectorized_states = [
-        spmd.remove_axis(state, index, options.vmap_metadata)
-        for state, index in zip(
-          vectorized_states, options.variable_axes.values()
-        )
+        spmd.remove_axis(state, index, options.transform_metadata)
+        for state, index in zip(vectorized_states, options.state_axes.values())
       ]
 
     # merge module state
-    module = graphdef.merge(*vectorized_states, broadcast_state)
+    input_graph_nodes = ctx.merge(
+      graphdef, *vectorized_states, broadcast_state, split_keys, broadcast_keys
+    )
+
+    (args, kwargs) = graph.insert_graph_nodes((args, kwargs), input_graph_nodes)
 
-    output = f(module, *args, **kwargs)
+    out = f(*args, **kwargs)
+
+    out, output_graph_nodes = graph.extract_graph_nodes(out)
 
     # split module state
-    *vectorized_states_out, broadcast_state_out, moduledef_out = module.split(
-      *filters
+    (
+      graphdef_out,
+      rng_state_out,
+      *vectorized_states_out,
+      broadcast_state_out,
+    ) = ctx.split(
+      (input_graph_nodes, output_graph_nodes),
+      rnglib.RngState,
+      *filters,
+    )
+
+    not_keys_out, split_keys_out, broadcast_keys_out = rng_state_out.split(
+      rnglib.NotKey, options.split_rngs, ...
+    )
+
+    broadcast_state_out = State.merge(
+      broadcast_state_out, broadcast_keys_out, not_keys_out
     )
 
     # add metadata axis name to Variable.sharding
-    if spmd.PARTITION_NAME in options.vmap_metadata:
+    if spmd.PARTITION_NAME in options.transform_metadata:
       vectorized_states_out = [
-        spmd.add_axis(state, index, options.vmap_metadata)
+        spmd.add_axis(state, index, options.transform_metadata)
         for state, index in zip(
-          vectorized_states_out, options.variable_axes.values()
+          vectorized_states_out, options.state_axes.values()
         )
       ]
 
-    return broadcast_state_out, vectorized_states_out, output
-
-  broadcast_state, vectorized_states, output = vmap_fn(
-    split_keys, vectorized_states, args, kwargs
+    return (
+      graphdef_out,
+      broadcast_state_out,
+      vectorized_states_out,
+      split_keys_out,
+      out,
+    )
+
+  (
+    graphdef_out,
+    broadcast_state,
+    vectorized_states,
+    split_keys_out,
+    out,
+  ) = vmap_fn(split_keys, vectorized_states, args, kwargs)
+
+  _, output_graph_nodes = ctx.update(
+    graphdef_out,
+    *vectorized_states,
+    broadcast_state,
+    split_keys_out,
   )
-  assert moduledef_out is not None
 
-  module.update(((*vectorized_states, broadcast_state), moduledef_out))
+  out = graph.insert_graph_nodes(out, output_graph_nodes)
 
-  return output
+  rnglib.restore_keys(input_rng_streams)
+
+  return out
 
 
 def vmap(
   f: F,
   *,
-  variable_axes: tp.Mapping[filterlib.Filter, int] = MappingProxyType({}),
-  broadcast_rngs: filterlib.Filter = None,
-  in_args_axes: tp.Any = 0,
-  in_kwargs_axes: tp.Any = 0,
+  in_axes: int | None | tp.Sequence[tp.Any] = 0,
   out_axes: tp.Any = 0,
+  axis_name: AxisName | None = None,
   axis_size: int | None = None,
-  axis_name: str | None = None,
-  spmd_axis_name: str | None = None,
-  vmap_metadata: tp.Mapping[str, tp.Any] = MappingProxyType({}),
-  is_init: tp.Optional[bool] = None,
+  spmd_axis_name: AxisName | tuple[AxisName, ...] | None = None,
+  # nnx specific
+  in_axes_kwargs: tp.Any = 0,
+  state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
+  split_rngs: filterlib.Filter = ...,
+  transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
 ) -> F:
-  if is_init is None:
-    is_init = f.__name__ == '__init__'
-
   options = VmapOptions(
-    variable_axes=variable_axes,
-    broadcast_rngs=broadcast_rngs,
-    in_args_axes=in_args_axes,
-    in_kwargs_axes=in_kwargs_axes,
+    state_axes=state_axes,
+    split_rngs=split_rngs,
+    in_axes=in_axes,
+    in_axes_kwargs=in_axes_kwargs,
     out_axes=out_axes,
     axis_size=axis_size,
     axis_name=axis_name,
     spmd_axis_name=spmd_axis_name,
-    vmap_metadata=vmap_metadata,
+    transform_metadata=transform_metadata,
   )
 
-  if is_init:
+  @functools.wraps(f)
+  def vmap_apply_wrapper(*args, **kwargs) -> tp.Any:
+    return vmap_apply(options, f, args, kwargs)
 
-    @functools.wraps(f)
-    def vmap_init_wrapper(module: Module, *args, **kwargs):
-      def module_constructor(*args, **kwargs):
-        _check_args(args)
-        f(module, *args, **kwargs)
-        return module
+  wrapper = vmap_apply_wrapper
 
-      lifted_module = vmap_init(options, module_constructor, args, kwargs)
-      module.update(lifted_module)
+  return wrapper  # type: ignore
+
+# -------------------------------
+# eval_shape
+# -------------------------------
 
-    wrapper = vmap_init_wrapper
 
-  else:
+def eval_shape(
+  f: tp.Callable[..., A],
+  *args: tp.Any,
+  **kwargs: tp.Any,
+) -> A:
+  (args, kwargs), input_nodes = graph.extract_graph_nodes((args, kwargs))
+  graphdef, state = graph.split(input_nodes)
 
-    @functools.wraps(f)
-    def vmap_apply_wrapper(module: Module, *args, **kwargs) -> tp.Any:
-      _check_args(args)
-      return vmap_apply(options, f, module, args, kwargs)
+  @functools.wraps(f)
+  def _eval_shape_fn(state: State, *args, **kwargs):
+    input_nodes = graph.merge(graphdef, state)
+    args, kwargs = graph.insert_graph_nodes((args, kwargs), input_nodes)
+    out = f(*args, **kwargs)
+    out, output_nodes = graph.extract_graph_nodes(out)
+    graphdef_out, state_out = graph.split(output_nodes)
+    return graphdef_out, state_out, out
 
-    wrapper = vmap_apply_wrapper
+  graphdef_out, state_out, out = jax.eval_shape(
+    _eval_shape_fn, state, *args, **kwargs
+  )
 
-  return wrapper  # type: ignore
+  output_nodes = graph.merge(graphdef_out, state_out)
+  out = graph.insert_graph_nodes(out, output_nodes)
+  return out
```

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/__init__.py` & `flax-0.8.3/flax/experimental/nnx/tests/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/nn/test_attention.py` & `flax-0.8.3/flax/experimental/nnx/tests/nn/test_attention.py`

 * *Files 2% similar despite different names*

```diff
@@ -64,25 +64,25 @@
         deterministic=False,
       ),
       rng,
     )
     module.set_attributes(decode=False)
 
     _ = module(x, True)
-    intermediates = module.pop(nnx.Intermediate)
-    assert intermediates['attention_layers/0/attention_weights'].raw_value[
+    intermediates = nnx.pop(module, nnx.Intermediate)
+    assert intermediates['attention_layers'][0]['attention_weights'].value[
       0
     ].shape == (4, 8, 6, 6)
-    assert 'attention_layers/1/attention_weights' not in intermediates
-    assert intermediates['attention_layers/2/attention_weights'].raw_value[
+    assert 1 not in intermediates['attention_layers']
+    assert intermediates['attention_layers'][2]['attention_weights'].value[
       0
     ].shape == (4, 8, 6, 6)
 
     _ = module(x)
-    intermediates = module.pop(nnx.Intermediate)
+    intermediates = nnx.pop(module, nnx.Intermediate)
     assert not intermediates  # empty
 
   def test_autoregressive_decode_with_x64(self):
     with jax.experimental.enable_x64():
       x = jnp.ones((1, 4, 4))
       module = nnx.MultiHeadAttention(
         in_features=4,
```

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/nn/test_conv.py` & `flax-0.8.3/flax/experimental/nnx/tests/nn/test_conv.py`

 * *Files 3% similar despite different names*

```diff
@@ -58,28 +58,31 @@
     kernel_size = (7, 4)
 
     # Cannot use string padding specification for transpose conv
     if isinstance(input_dilation, Sequence) or input_dilation > 1:
       padding = (4, 2)
 
     x = jax.numpy.ones(INPUT_SHAPE)
-    model_nnx = nnx.Conv.create_abstract(
-      IN_FEATURES,
-      OUT_FEATURES,
-      kernel_size,
-      strides,
-      padding=padding,
-      input_dilation=input_dilation,
-      kernel_dilation=kernel_dilation,
-      feature_group_count=feature_group_count,
-      use_bias=use_bias,
-      dtype=dtype,
-      param_dtype=param_dtype,
-      precision=precision,
-      rngs=rngs,
+    model_nnx = nnx.eval_shape(
+      lambda rngs: nnx.Conv(
+        IN_FEATURES,
+        OUT_FEATURES,
+        kernel_size,
+        strides,
+        padding=padding,
+        input_dilation=input_dilation,
+        kernel_dilation=kernel_dilation,
+        feature_group_count=feature_group_count,
+        use_bias=use_bias,
+        dtype=dtype,
+        param_dtype=param_dtype,
+        precision=precision,
+        rngs=rngs,
+      ),
+      rngs,
     )
     model = linen.Conv(
       OUT_FEATURES,
       kernel_size=kernel_size,
       strides=strides,
       padding=padding,
       input_dilation=input_dilation,
```

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/nn/test_embed.py` & `flax-0.8.3/flax/experimental/nnx/tests/nn/test_embed.py`

 * *Files 8% similar despite different names*

```diff
@@ -40,20 +40,23 @@
   ):
     key = jax.random.key(42)
     rngs = nnx.Rngs(42)
     IN_FEATURES = 32
     NUM_EMBEDDINGS = num_embeddings
 
     x = jax.numpy.arange(NUM_EMBEDDINGS, dtype=input_dtype)
-    model_nnx = nnx.Embed.create_abstract(
-      NUM_EMBEDDINGS,
-      IN_FEATURES,
-      dtype=dtype,
-      param_dtype=param_dtype,
-      rngs=rngs,
+    model_nnx = nnx.eval_shape(
+      lambda rngs: nnx.Embed(
+        NUM_EMBEDDINGS,
+        IN_FEATURES,
+        dtype=dtype,
+        param_dtype=param_dtype,
+        rngs=rngs,
+      ),
+      rngs,
     )
     model = linen.Embed(
       NUM_EMBEDDINGS, IN_FEATURES, dtype=dtype, param_dtype=param_dtype
     )
     variables = model.init(key, x)
     model_nnx.embedding.value = variables['params']['embedding']
```

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/nn/test_linear.py` & `flax-0.8.3/flax/experimental/nnx/tests/nn/test_linear.py`

 * *Files 19% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 import jax.numpy as jnp
 from absl.testing import parameterized
 from jax.lax import Precision
 from numpy.testing import assert_array_equal
 
 from flax import linen
 from flax.experimental import nnx
-from flax.typing import Dtype, PrecisionLike
+from flax.typing import Dtype, PrecisionLike, Shape
 
 
 class TestLinearGeneral:
   def test_basic(self):
     module = nnx.LinearGeneral(2, 3, rngs=nnx.Rngs(0))
     y = module(jnp.ones((1, 2)))
 
@@ -48,44 +48,102 @@
 class TestLinenConsistency(parameterized.TestCase):
   @parameterized.product(
     use_bias=[True, False],
     dtype=[jnp.float32, jnp.float16],
     param_dtype=[jnp.float32, jnp.float16],
     precision=[Precision.DEFAULT, Precision.HIGH, Precision.HIGHEST],
   )
-  def test_nnx_linen_equivalence(
+  def test_nnx_linear_equivalence(
     self,
     use_bias: bool,
     dtype: tp.Optional[Dtype],
     param_dtype: Dtype,
     precision: PrecisionLike,
   ):
     key = jax.random.key(42)
     rngs = nnx.Rngs(42)
     IN_FEATURES = 32
     OUT_FEATURES = 64
 
     x = jax.numpy.ones((1, IN_FEATURES))
-    model_nnx = nnx.Linear.create_abstract(
-      IN_FEATURES,
+    model_nnx = nnx.eval_shape(
+      lambda rngs: nnx.Linear(
+        IN_FEATURES,
+        OUT_FEATURES,
+        use_bias=use_bias,
+        dtype=dtype,
+        param_dtype=param_dtype,
+        precision=precision,
+        rngs=rngs,
+      ),
+      rngs,
+    )
+    model = linen.Dense(
       OUT_FEATURES,
       use_bias=use_bias,
       dtype=dtype,
       param_dtype=param_dtype,
       precision=precision,
+    )
+    variables = model.init(key, x)
+    model_nnx.kernel.value = variables['params']['kernel']
+    if use_bias:
+      model_nnx.bias.value = variables['params']['bias']
+
+    out_nnx = model_nnx(x)
+    out = model.apply(variables, x)
+    assert_array_equal(out, out_nnx)
+
+  @parameterized.product(
+    einsum_str=['defab,bcef->adefc', 'd...ab,bc...->ad...c'],
+    bias_shape=[None, (6, 7, 5)],
+    dtype=[jnp.float32, jnp.float16],
+    param_dtype=[jnp.float32, jnp.float16],
+    precision=[Precision.DEFAULT, Precision.HIGH, Precision.HIGHEST],
+  )
+  def test_nnx_einsum_equivalence(
+    self,
+    einsum_str,
+    bias_shape: tp.Optional[Shape],
+    dtype: tp.Optional[Dtype],
+    param_dtype: Dtype,
+    precision: PrecisionLike,
+  ):
+    key = jax.random.key(42)
+    rngs = nnx.Rngs(42)
+    INPUT_SHAPE = (8, 6, 7, 3, 4)
+    KERNEL_SHAPE = (4, 5, 6, 7)
+
+    x = jax.random.normal(key, INPUT_SHAPE)
+    model_nnx = nnx.Einsum(
+      einsum_str,
+      KERNEL_SHAPE,
+      bias_shape,
+      dtype=dtype,
+      param_dtype=param_dtype,
+      precision=precision,
       rngs=rngs,
     )
-    model = linen.Dense(
-      OUT_FEATURES,
-      use_bias=use_bias,
+    model = linen.Einsum(
+      KERNEL_SHAPE,
+      einsum_str,
+      use_bias=True if bias_shape is not None else False,
       dtype=dtype,
       param_dtype=param_dtype,
       precision=precision,
     )
+
+    variables = model.init(key, x)
+    variables['params']['kernel'] = model_nnx.kernel.value
+    if bias_shape is not None:
+      variables['params']['bias'] = model_nnx.bias.value
+    out_nnx = model_nnx(x)
+    out = model.apply(variables, x)
+    assert_array_equal(out, out_nnx)
+
     variables = model.init(key, x)
     model_nnx.kernel.value = variables['params']['kernel']
-    if use_bias:
+    if bias_shape is not None:
       model_nnx.bias.value = variables['params']['bias']
-
     out_nnx = model_nnx(x)
     out = model.apply(variables, x)
     assert_array_equal(out, out_nnx)
```

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/nn/test_normalization.py` & `flax-0.8.3/flax/experimental/nnx/tests/nn/test_normalization.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/test_compatibility.py` & `flax-0.8.3/flax/experimental/nnx/tests/test_compatibility.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/test_containers.py` & `flax-0.8.3/flax/experimental/nnx/tests/test_containers.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/test_ids.py` & `flax-0.8.3/flax/experimental/nnx/tests/test_ids.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/test_integration.py` & `flax-0.8.3/flax/experimental/nnx/tests/test_integration.py`

 * *Files 4% similar despite different names*

```diff
@@ -50,18 +50,19 @@
     def train_step(model: Model, x, y):
       @nnx.grad
       def loss_fn(model: Model):
         y_pred = model(x)
         return jnp.mean((y - y_pred) ** 2)
 
       grads = loss_fn(model)
-      model.update(
+      nnx.update(
+        model,
         jax.tree_util.tree_map(
-          lambda w, g: w - 0.1 * g, model.extract(nnx.Param), grads
-        )
+          lambda w, g: w - 0.1 * g, nnx.state(model, nnx.Param), grads
+        ),
       )
 
     model = Model(rngs=nnx.Rngs(0))
 
     x = np.random.uniform(size=(4, 2))
     y = np.random.uniform(size=(4, 2))
     model.set_attributes(use_running_average=False)
@@ -93,41 +94,42 @@
       def __call__(self, x):
         x = self.block1(x)
         x = self.block2(x)
         return x
 
     @jax.jit
     def train_step(state: nnx.State, graphdef: nnx.GraphDef[Model], x, y):
-      model = graphdef.merge(state)
+      model = nnx.merge(graphdef, state)
       model.set_attributes(use_running_average=False)
 
       @nnx.grad
       def loss_fn(model: Model):
         y_pred = model(x)
         return jnp.mean((y - y_pred) ** 2)
 
       grads = loss_fn(model)
-      model.update(
+      nnx.update(
+        model,
         jax.tree_util.tree_map(
-          lambda w, g: w - 0.1 * g, model.extract(nnx.Param), grads
-        )
+          lambda w, g: w - 0.1 * g, nnx.state(model, nnx.Param), grads
+        ),
       )
 
-      return model.split()
+      return nnx.split(model)
 
     graphdef: nnx.GraphDef[Model]
-    state, graphdef = Model(rngs=nnx.Rngs(0)).split()
+    graphdef, state = nnx.split(Model(rngs=nnx.Rngs(0)))
 
     x = np.random.uniform(size=(4, 2))
     y = np.random.uniform(size=(4, 2))
 
     for _i in range(3):
-      state, graphdef = train_step(state, graphdef, x, y)
+      graphdef, state = train_step(state, graphdef, x, y)
 
-    model = graphdef.merge(state)
+    model = nnx.merge(graphdef, state)
 
     assert model.block1.linear.bias is not None
     assert model.block2.linear.bias is not None
     assert model.block1.linear.kernel is model.block2.linear.kernel
     assert model.block1.linear.bias is model.block2.linear.bias
     assert model.block1.bn is not model.block2.bn
 
@@ -157,18 +159,19 @@
       def loss_fn(model):
         y_pred = model(x)
         return jax.numpy.mean((y_pred - y) ** 2)
 
       # compute gradient
       grads: nnx.State = nnx.grad(loss_fn, wrt=nnx.Param)(model)
       # SGD update
-      model.update(
+      nnx.update(
+        model,
         jax.tree_util.tree_map(
-          lambda w, g: w - 0.1 * g, model.extract(nnx.Param), grads
-        )
+          lambda w, g: w - 0.1 * g, nnx.state(model, nnx.Param), grads
+        ),
       )
 
     # execute the training step
     train_step(model, x, y)
     assert model.count.value == 2
 
   def test_functional_example(self):
@@ -188,33 +191,33 @@
 
     model = Linear(din=12, dout=2, rngs=nnx.Rngs(0))
     # forward pass
     x = jnp.ones((8, 12))
     y = model(x)
     assert model.count.value == 1
 
-    params, counts, graphdef = model.split(nnx.Param, Count)
+    graphdef, params, counts = nnx.split(model, nnx.Param, Count)
 
     @jax.jit
     def train_step(params, counts, x, y):
       def loss_fn(params):
-        y_pred, (updates, _) = graphdef.apply(params, counts)(x)
+        y_pred, (_, updates) = graphdef.apply(params, counts)(x)
         loss = jax.numpy.mean((y_pred - y) ** 2)
-        return loss, updates.extract(Count)
+        return loss, updates.filter(Count)
 
       # compute gradient
       grads, counts = jax.grad(loss_fn, has_aux=True)(params)
       # SGD update
-      params = jax.tree_map(lambda w, g: w - 0.1 * g, params, grads)
+      params = jax.tree_util.tree_map(lambda w, g: w - 0.1 * g, params, grads)
 
       return params, counts
 
     # execute the training step
     params, counts = train_step(params, counts, x, y)
-    model = graphdef.merge(params, counts)
+    model = nnx.merge(graphdef, params, counts)
     assert model.count.value == 2
 
   def test_intermediates_example(self):
     class Linear(nnx.Module):
       def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):
         key = rngs.params()
         self.w = nnx.Param(jax.random.uniform(key, (din, dout)))
@@ -225,15 +228,15 @@
         self.y = nnx.Intermediate(y)
         return y
 
     model = Linear(12, 2, rngs=nnx.Rngs(0))
 
     y = model(jnp.ones((8, 12)))
 
-    intermediates = model.pop(nnx.Intermediate)
+    intermediates = nnx.pop(model, nnx.Intermediate)
 
     assert 'y' in intermediates
 
   def test_intermediates_example_functional(self):
     class Linear(nnx.Module):
       def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):
         key = rngs.params()
@@ -243,14 +246,14 @@
       def __call__(self, x):
         y = x @ self.w.value + self.b.value[None]
         self.y = nnx.Intermediate(y)
         return y
 
     model = Linear(12, 2, rngs=nnx.Rngs(0))
 
-    state, graphdef = model.split()
+    graphdef, state = nnx.split(model)
 
-    y, (state, _) = graphdef.apply(state)(jnp.ones((8, 12)))
+    y, (_, state) = graphdef.apply(state)(jnp.ones((8, 12)))
 
     intermediates, state = state.split(nnx.Intermediate, ...)
 
     assert 'y' in intermediates
```

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/test_module.py` & `flax-0.8.3/flax/experimental/nnx/tests/test_module.py`

 * *Files 13% similar despite different names*

```diff
@@ -24,58 +24,59 @@
 from flax.experimental import nnx
 
 A = TypeVar('A')
 
 
 class TestModule:
   def test_has_module_state(self):
-    class Foo(nnx.Module): ...
+    class Foo(nnx.Module):
+      ...
 
     foo = Foo()
 
-    assert hasattr(foo, '_module__state')
+    assert hasattr(foo, '_graph_node__state')
 
   def test_trace_level(self):
     m = nnx.Dict(a=nnx.Param(1))
 
     @jax.jit
     def f():
       with pytest.raises(
-        nnx.TraceContextError,
-        match='Cannot mutate Module from different trace level',
+        nnx.errors.TraceContextError,
+        match="Cannot mutate 'Dict' from different trace level",
       ):
         m.a = 2
 
     f()
 
   def test_tree_map(self):
     m = nnx.Dict(a=nnx.Param(1))
 
-    state, static = m.split()
+    graphdef, state = nnx.split(m)
 
-    state = jax.tree_map(lambda x: x + 1, state)
+    state = jax.tree_util.tree_map(lambda x: x + 1, state)
 
   def test_split_2(self):
     m = nnx.Dict(a=nnx.Param(1))
 
-    empty, some, static = m.split(None, ...)
+    graphdef, empty, some = nnx.split(m, None, ...)
 
-    some = jax.tree_map(lambda x: x + 1, some)
+    some = jax.tree_util.tree_map(lambda x: x + 1, some)
 
   def test_split_merge(self):
     m = nnx.Dict(a=nnx.Param(1))
 
     @jax.jit
-    def g(state: nnx.State, graphdef: nnx.GraphDef[nnx.Dict[int]]):
-      m = graphdef.merge(state)
+    def g(graphdef: nnx.GraphDef[nnx.Dict[int]], state: nnx.State):
+      m = nnx.merge(graphdef, state)
       m.a = 2
-      return m.split()
+      return nnx.split(m)
 
-    state, graphdef = g(*m.split())
-    m2 = graphdef.merge(state)
+    graphdef, state = g(*nnx.split(m))
+    m2 = nnx.merge(graphdef, state)
 
     assert m2.a == 2
 
   def test_no_trace_level_error_on_grad(self):
     # No trace level error occurs because jax doesn't update
     # its top trace for grad.
     m = nnx.Dict(a=nnx.Param(1.0))
@@ -104,144 +105,127 @@
 
     assert isinstance(y, jax.Array)
 
   def test_shared_module(self):
     m1 = nnx.Dict(a=nnx.Param(1), b=nnx.Param(2))
     m2 = nnx.Dict(x=m1, y=m1, z=nnx.Param(3))
 
-    m3 = nnx.merge(m2.split())
+    m3 = nnx.merge(*nnx.split(m2))
 
     assert m3['x'] is m3['y']
     assert m3['x']['a'] is m3['y']['a']
     assert m3['x']['b'] is m3['y']['b']
 
   def test_module_graph(self):
     class Foo(nnx.Module):
       def __init__(self):
         self.a = nnx.Param(1)
         self.sub = self
 
     m = Foo()
 
-    state, graphdef = m.split()
+    graphdef, state = nnx.split(m)
     assert len(state) == 1
 
-    m2 = graphdef.merge(state)
+    m2 = nnx.merge(graphdef, state)
     assert m2 is m2.sub
 
   def test_deref_through_jit(self):
     r1 = nnx.Variable(1)
     r2 = nnx.Variable(2)
 
-    m = m0 = nnx.Dict({'a': nnx.Sequence([r1, r2]), 'b': r1})
+    m = m0 = nnx.Dict({'a': nnx.List([r1, r2]), 'b': r1})
 
     @jax.jit
-    def f(state: nnx.State, graphdef: nnx.GraphDef[nnx.Dict[Any]]):
-      m = graphdef.merge(state)
+    def f(graphdef: nnx.GraphDef[nnx.Dict[Any]], state: nnx.State):
+      m = nnx.merge(graphdef, state)
 
       assert m['a'][0] is m['b']
       assert m['a'][1] is not m['b']
 
-      return m.split()
+      return nnx.split(m)
 
-    state, graphdef = f(*m.split())
-    m = graphdef.merge(state)
+    graphdef, state = f(*nnx.split(m))
+    m = nnx.merge(graphdef, state)
 
     assert m['a'][0] is m['b']
     assert m['a'][1] is not m['b']
 
     # compare with original
     assert m['a'][0] is not m0['a'][0]
     assert m['a'][1] is not m0['a'][1]
     assert m['b'] is not m0['b']
 
   def test_cross_barrier(self):
     m = nnx.Dict(a=nnx.Param(1))
 
     @jax.jit
-    def g(state: nnx.State, graphdef: nnx.GraphDef[nnx.Dict[nnx.Param[int]]]):
-      m = graphdef.merge(state)
+    def g(graphdef: nnx.GraphDef[nnx.Dict[nnx.Param[int]]], state: nnx.State):
+      m = nnx.merge(graphdef, state)
       m.a.value += 1
-      return m.split()
+      return nnx.split(m)
 
-    state, graphdef = g(*m.split())
-    m2 = graphdef.merge(state)
+    graphdef, state = g(*nnx.split(m))
+    m2 = nnx.merge(graphdef, state)
     assert m2 is not m
     assert m.a.value == 1
     assert m2.a.value == 2
 
   def test_no_rejit(self):
     n = 0
     m = nnx.Dict(a=nnx.Param(1))
 
     @jax.jit
     def g(state_and_def):
       nonlocal n
       n += 1
-      m = nnx.merge(state_and_def)
+      m = nnx.merge(*state_and_def)
       m.a.value += 1
-      return m.split()
+      return nnx.split(m)
 
-    m2 = nnx.merge(g(m.split()))
+    m2 = nnx.merge(*g(nnx.split(m)))
 
     assert n == 1
     assert m2 is not m
     assert m.a.value == 1
     assert m2.a.value == 2
 
-    g(m.split())
+    g(nnx.split(m))
     assert n == 1
 
-    g(m2.split())
+    g(nnx.split(m2))
     assert n == 1
 
     m2.b = nnx.Param(10)
-    g(m2.split())
+    g(nnx.split(m2))
 
     assert n == 2
 
   def test_deref_number_of_fields(self):
     r1 = nnx.Variable(1)
     r2 = nnx.Variable(2)
     v1 = 3
     m = nnx.Dict(
       {
-        'a': nnx.Sequence([r1, r2, v1]),
+        'a': nnx.List([r1, r2, v1]),
         'b': nnx.Dict({'c': r1, 'd': r2}),
       }
     )
 
-    p, graphdef = m.split()
+    graphdef, p = nnx.split(m)
     assert len(p.flat_state()) == 2
     assert len(jax.tree_util.tree_leaves(p)) == 2
 
-  def test_deref_array_attributes_not_allowed(self):
-    # test arrays are nodes
-    r1 = nnx.Variable(1)
-    r2 = nnx.Variable(2)
-    v1 = jax.numpy.array(3)
-
-    with pytest.raises(
-      ValueError,
-      match=f"Trying to assign a '{type(v1).__name__}' to the Module",
-    ):
-      m = nnx.Dict(
-        {
-          'a': nnx.Sequence([r1, r2, v1]),
-          'b': nnx.Dict({'c': r1, 'd': r2}),
-        }
-      )
-
   def test_clone(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(1), nnx.Param(2), 3]),
+      a=nnx.List([nnx.Param(1), nnx.Param(2), 3]),
       b=nnx.Dict(c=nnx.Param(1), d=nnx.Param(2)),
     )
 
-    m2 = m.clone()
+    m2 = nnx.clone(m)
 
     assert m is not m2
     assert m2.a[0] == m2.b.c
     assert m2.a[1] == m2.b.d
 
     assert m.a[0] == m2.a[0]
     assert m.a[1] == m2.a[1]
@@ -259,18 +243,18 @@
     y1 = m(2)
     y2 = m(10)
 
     assert y1 == 3
     assert y2 == 11
     assert m.y.value == (3, 11)
 
-    intermediates = m.pop(nnx.Intermediate)
+    intermediates = nnx.pop(m, nnx.Intermediate)
 
-    assert isinstance(intermediates.y, nnx.Intermediate)
-    assert intermediates['y'].raw_value == (3, 11)
+    assert issubclass(intermediates.y.type, nnx.Intermediate)
+    assert intermediates['y'].value == (3, 11)
 
     assert not hasattr(m, 'y')
 
   def test_sow_existing_non_variable_field(self):
     class Foo(nnx.Module):
       def __init__(self) -> None:
         self.y = 10
@@ -296,58 +280,35 @@
         return y
 
     m = Foo()
 
     with pytest.raises(ValueError, match='to be of type'):
       m(2)
 
-  def test_update_static_state(self):
-    class Foo(nnx.Module):
-      def add_field(self):
-        self.a = 1
-
-    m1 = Foo()
-    m2 = Foo()
-    m2.add_field()
-
-    m1.update(m2)
-
-    assert m1.a == 1
-
-  def test_update_moduledef(self):
-    class Foo(nnx.Module):
-      def add_field(self):
-        self.a = 1
-
-    m1 = Foo()
-    m2 = Foo()
-    m2.add_field()
-
-    m1.update(m2.get_graphdef())
-
-    assert m1.a == 1
-
   def test_update_static_state_submodules(self):
     class Bar(nnx.Module):
       def __init__(self) -> None:
         self.x = 1
 
       def add_field(self):
         self.y = 2
 
     class Foo(nnx.Module):
       def __init__(self) -> None:
         self.a = Bar()
         self.b = self.a
 
     m1 = Foo()
-    m2 = Foo()
-    m2.a.add_field()
+    with nnx.UpdateContext() as ctx:
+      graphdef, state = ctx.split(m1)
+      m2 = ctx.merge(graphdef, state)
+      m2.a.add_field()
+      new_graphdef, state = ctx.split(m2)
 
-    m1.update(m2)
+      ctx.update(new_graphdef, state)
 
     assert m1.a.x == 1
     assert m1.a.y == 2
     assert m1.b.x == 1
     assert m1.b.y == 2
 
   def test_update_new_submodule(self):
@@ -359,18 +320,21 @@
       def __init__(self) -> None:
         self.a = Bar()
 
       def add_module(self):
         self.b = Bar()
 
     m1 = Foo()
-    m2 = Foo()
+    ctx = nnx.UpdateContext()
+    graphdef, state = ctx.split(m1)
+    m2 = ctx.merge(graphdef, state)
     m2.add_module()
+    new_graphdef, state = ctx.split(m2)
 
-    m1.update(m2)
+    ctx.update(new_graphdef, state)
 
     assert m1.a.x == 1
     assert m1.b.x == 1
 
   def test_update_update_submodule(self):
     class Bar(nnx.Module):
       def __init__(self) -> None:
@@ -378,83 +342,68 @@
 
     class Foo(nnx.Module):
       def __init__(self) -> None:
         self.a = Bar()
         self.b = self.a
 
     m1 = Foo()
-    m2 = Foo()
+    ctx = nnx.UpdateContext()
+    graphdef, state = ctx.split(m1)
+    m2 = ctx.merge(graphdef, state)
     m2.a.x = 2
-
-    m1.update(m2)
+    new_graphdef, state = ctx.split(m2)
+    ctx.update(new_graphdef, state)
 
     assert m1.a.x == 2
     assert m1.b.x == 2
 
-  def test_update_add_shared_error(self):
+  def test_update_add_shared(self):
     class Bar(nnx.Module):
       def __init__(self) -> None:
         self.x = 1
 
     class Foo(nnx.Module):
       def __init__(self) -> None:
         self.a = Bar()
         self.b = self.a
 
       def add_submodule(self):
         self.c = self.a
 
     m1 = Foo()
-    m2 = Foo()
+    ctx = nnx.UpdateContext()
+    graphdef, state = ctx.split(m1)
+    m2 = ctx.merge(graphdef, state)
     m2.add_submodule()
+    new_graphdef, state = ctx.split(m2)
+    ctx.update(new_graphdef, state)
 
-    assert hasattr(m2, 'c')
-
-    with pytest.raises(ValueError, match='Trying to add a new node at path'):
-      m1.update(m2)
-
-  def test_update_add_shared_error_new_first(self):
-    class Bar(nnx.Module):
-      def __init__(self) -> None:
-        self.x = 1
-
-    class Foo(nnx.Module):
-      def __init__(self) -> None:
-        self.b = Bar()
-        self.c = self.b
-
-      def add_submodule(self):
-        self.a = self.b
-
-    m1 = Foo()
-    m2 = Foo()
-    m2.add_submodule()
-
-    assert hasattr(m2, 'a')
-
-    m2 = m2.clone()  # clone to sort the fields
-
-    with pytest.raises(ValueError, match='Trying to add a new node at path'):
-      m1.update(m2)
+    assert hasattr(m1, 'c')
 
   def test_create_abstract(self):
-    linear = nnx.Linear.create_abstract(2, 3, rngs=nnx.Rngs(0))
+    linear = nnx.eval_shape(lambda: nnx.Linear(2, 3, rngs=nnx.Rngs(0)))
 
     assert linear.kernel.value == jax.ShapeDtypeStruct((2, 3), jnp.float32)
     assert linear.bias.value == jax.ShapeDtypeStruct((3,), jnp.float32)
 
   def test_partial_init(self):
     linear = nnx.Linear(2, 3, rngs=nnx.Rngs(0))
-    state = linear.get_state()
+    state = nnx.state(linear)
 
     del state['bias']
 
-    linear2 = nnx.Linear.partial_init(state)(
-      2, 3, bias_init=nnx.initializers.ones_init(), rngs=nnx.Rngs(1)
-    )
+    @nnx.jit
+    def partial_init(state: nnx.State):
+      m = nnx.Linear(
+        2, 3, bias_init=nnx.initializers.ones_init(), rngs=nnx.Rngs(1)
+      )
+      nnx.update(m, state)
+      return m
+
+    linear2 = partial_init(state)
 
     np.testing.assert_allclose(linear.kernel.value, linear2.kernel.value)
     np.testing.assert_allclose(linear.bias.value, 0)
     np.testing.assert_allclose(linear2.bias.value, 1)
 
   def test_deepcopy(self):
     class Foo(nnx.Module):
@@ -518,59 +467,87 @@
     block.set_attributes(
       deterministic=True,
       use_running_average=True,
       unknown=True,
       raise_if_not_found=False,
     )
 
+  def test_init(self):
+    class Linear(nnx.Module):
+      def __init__(self, dout, rngs: nnx.Rngs):
+        self.dout = dout
+        self.rngs = rngs
+
+      def __call__(self, x):
+        if self.is_initializing():
+          din = x.shape[-1]
+          if not hasattr(self, 'w'):
+            key = self.rngs.params()
+            self.w = nnx.Param(jax.random.uniform(key, (din, self.dout)))
+          if not hasattr(self, 'b'):
+            self.b = nnx.Param(jnp.zeros((self.dout,)))
+        return x @ self.w + self.b[None]
+
+    linear = Linear(3, nnx.Rngs(0))
+    x = jnp.ones((5, 2))
+    y = linear.init(x)
+    assert linear.w.value.shape == (2, 3)
+    assert linear.b.value.shape == (3,)
+    assert y.shape == (5, 3)
+    assert not linear.is_initializing()
+
 
 class TestModulePytree:
   def test_tree_map(self):
     class Foo(nnx.Module, experimental_pytree=True):
       def __init__(self):
         self.node = nnx.Param(1)
-        self.static = 1
+        self.graphdef = 1
 
     m = Foo()
 
-    m = jax.tree_map(lambda x: x + 1, m)
+    m = jax.tree_util.tree_map(lambda x: x + 1, m)
 
     assert m.node.value == 2
-    assert m.static == 1
+    assert m.graphdef == 1
 
 
 class TestModuleDataclass:
   def test_basic(self):
     @dataclasses.dataclass
     class Foo(nnx.Module):
       a: int
-      b: nnx.TreeNode[int]
+      b: nnx.Variable[int]
       c: nnx.Param[int]
       d: nnx.Variable[int]
       e: nnx.Variable[int]
       f: int
 
     m = Foo(
-      a=1,  # static
-      b=nnx.TreeNode(2),  # node
+      a=1,  # graphdef
+      b=nnx.Variable(2),  # node
       c=nnx.Param(3),  # param
       d=nnx.Variable(4),  # var
       e=nnx.BatchStat(5),  # var
-      f=6,  # static int
+      f=6,  # graphdef int
     )
 
-    state, graphdef = m.split()
+    graphdef, state = nnx.split(m)
 
     assert len(state) == 4
-    assert state.b == nnx.TreeNode(2)
-    assert state.c == nnx.Param(3)
-    assert state.d == nnx.Variable(4)
-    assert state.e == nnx.BatchStat(5)
+    assert state.b.value == 2
+    assert state.b.type == nnx.Variable
+    assert state.c.value == 3
+    assert state.c.type == nnx.Param
+    assert state.d.value == 4
+    assert state.d.type == nnx.Variable
+    assert state.e.value == 5
+    assert state.e.type == nnx.BatchStat
 
-  def test_context_none_after_init(self):
+  def test_post_init(self):
     @dataclasses.dataclass
     class DFoo(nnx.Module):
       din: int
       dout: int
       rngs: nnx.Rngs
 
       def __post_init__(self):
@@ -578,15 +555,14 @@
 
       def __call__(self, x):
         return self.bar(x)
 
     m = DFoo(1, 1, rngs=nnx.Rngs(0))
 
     assert hasattr(m, 'bar')
-    assert m.rngs is None
 
   def test_setup_is_called(self):
     @dataclasses.dataclass
     class DFoo(nnx.Module):
       din: int
       dout: int
       rngs: nnx.Rngs
@@ -596,15 +572,14 @@
 
       def __call__(self, x):
         return self.bar(x)
 
     m = DFoo(1, 1, rngs=nnx.Rngs(0))
 
     assert hasattr(m, 'bar')
-    assert m.rngs is None
 
 
 class TestModuleDef:
   def test_apply(self):
     class Foo(nnx.Module):
       def __init__(self, c: float, *, rngs: nnx.Rngs):
         self.w = nnx.Param(jax.random.uniform(rngs.params(), ()))
@@ -613,19 +588,18 @@
       def __call__(self, x, *, rngs: nnx.Rngs):
         key = rngs.e()
         return self.w.value * x + jax.random.normal(key, ()) + self.c
 
     rngs = nnx.Rngs(0)
     foo = Foo(c=1.0, rngs=rngs)
 
-    states, graphdef = foo.split()
+    graphdef, states = nnx.split(foo)
 
     assert isinstance(states, nnx.State)
-    assert isinstance(states.w, nnx.Param)
-    # assert isinstance(states["c"], jax.Array)
+    assert issubclass(states.w.type, nnx.Param)
 
     y, _updates = graphdef.apply(states)(x=2.0, rngs=nnx.Rngs(e=1))
 
     assert isinstance(y, jax.Array)
 
   def test_derefed_mod_apply(self):
     class Foo(nnx.Module):
@@ -637,37 +611,69 @@
 
       def __call__(self, x, *, rngs: nnx.Rngs):
         key = rngs.e()
         return self.w.value * x + jax.random.normal(key, ()) + self.c.value
 
     foo = Foo(c=1.0, rngs=nnx.Rngs(0))
 
-    state, graphdef = foo.split()
+    graphdef, state = nnx.split(foo)
 
     assert isinstance(graphdef, nnx.GraphDef)
     assert isinstance(state, nnx.State)
-    assert isinstance(state.w, nnx.Param)
-    assert isinstance(state.c, nnx.Variable)
+    assert issubclass(state.w.type, nnx.Param)
+    assert issubclass(state.c.type, nnx.Variable)
 
-    y, (state, graphdef) = graphdef.apply(state)(x=2.0, rngs=nnx.Rngs(e=1))
+    y, (graphdef, state) = graphdef.apply(state)(x=2.0, rngs=nnx.Rngs(e=1))
 
     assert isinstance(y, jax.Array)
 
   def test_modules_iterator(self):
     class Foo(nnx.Module):
       def __init__(self, *, rngs: nnx.Rngs):
         self.submodules = [
           {'a': nnx.Linear(1, 1, rngs=rngs)},
           {'b': nnx.Conv(1, 1, 1, rngs=rngs)},
         ]
 
     module = Foo(rngs=nnx.Rngs(0))
 
-    modules = list(module.modules())
+    modules = list(module.iter_modules())
 
     assert len(modules) == 3
-    assert modules[0][0] == ''
+    assert modules[0][0] == ()
     assert isinstance(modules[0][1], Foo)
-    assert modules[1][0] == 'submodules/0/a'
+    assert modules[1][0] == ('submodules', 0, 'a')
     assert isinstance(modules[1][1], nnx.Linear)
-    assert modules[2][0] == 'submodules/1/b'
+    assert modules[2][0] == ('submodules', 1, 'b')
     assert isinstance(modules[2][1], nnx.Conv)
+
+  def test_array_in_module(self):
+    class Foo(nnx.Module):
+      def __init__(self):
+        self.a = jnp.array(1.0)
+
+    foo = Foo()
+
+    graphdef, state = nnx.split(foo)
+
+    assert isinstance(state, nnx.State)
+    assert isinstance(state.a, jax.Array)
+
+    foo2 = nnx.merge(graphdef, state)
+
+    assert isinstance(foo2.a, jax.Array)
+
+  def test_state_in_module(self):
+    class Foo(nnx.Module):
+      def __init__(self):
+        self.a = nnx.State({'b': jnp.array(1.0)})
+
+    foo = Foo()
+
+    graphdef, state = nnx.split(foo)
+
+    assert isinstance(state, nnx.State)
+    assert isinstance(state.a, nnx.State)
+
+    foo2 = nnx.merge(graphdef, state)
+
+    assert isinstance(foo2.a, nnx.State)
```

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/test_partitioning.py` & `flax-0.8.3/flax/experimental/nnx/tests/test_partitioning.py`

 * *Files 25% similar despite different names*

```diff
@@ -18,142 +18,146 @@
 
 from flax.experimental import nnx
 
 
 class TestPartitioning:
   def test_partition(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(1), nnx.BatchStat(2)]),
+      a=nnx.List([nnx.Param(1), nnx.BatchStat(2)]),
       b=nnx.Param(2),
       c=100,
     )
 
-    params, rest, graphdef = m.split(nnx.Param, ...)
+    graphdef, params, rest = nnx.split(m, nnx.Param, ...)
 
     assert len(params) == 2
     assert len(rest) == 1
 
     # check params
-    assert params['a']['0'].raw_value == m.a[0].value
-    assert params['b'].raw_value == m.b.value
+    assert params['a'][0].value == m.a[0].value
+    assert params['b'].value == m.b.value
 
     # check rest
-    assert rest['a']['1'].raw_value == m.a[1].value
+    assert rest['a'][1].value == m.a[1].value
 
-    m2 = graphdef.merge(params, rest)
+    m2 = nnx.merge(graphdef, params, rest)
 
     assert m2.a[0].value == m.a[0].value
     assert m2.a[1].value == m.a[1].value
     assert m2.b.value == m.b.value
     assert m2.c == 100
 
   def test_complete_partitioning(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(1), nnx.Param(2), nnx.Variable(3)]),
+      a=nnx.List([nnx.Param(1), nnx.Param(2), nnx.Variable(3)]),
       b=nnx.Dict(c=nnx.Param(1), d=nnx.BatchStat(2)),
     )
 
     # no error
-    m.split(nnx.Param, nnx.BatchStat, nnx.Variable)
+    nnx.split(m, nnx.Param, nnx.BatchStat, nnx.Variable)
 
   def test_complete_partitioning_plus_ellipsis(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(1), nnx.Param(2), nnx.Variable(3)]),
+      a=nnx.List([nnx.Param(1), nnx.Param(2), nnx.Variable(3)]),
       b=nnx.Dict(c=nnx.Param(1), d=nnx.BatchStat(2)),
     )
 
     # no error if additional ... is passed at the end
-    m.split(nnx.Param, nnx.BatchStat, nnx.Variable, ...)
+    nnx.split(m, nnx.Param, nnx.BatchStat, nnx.Variable, ...)
 
   def test_inclomplete_partition_error(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(1), nnx.Param(2), nnx.Variable(3)]),
+      a=nnx.List([nnx.Param(1), nnx.Param(2), nnx.Variable(3)]),
       b=nnx.Dict(c=nnx.Param(1), d=nnx.BatchStat(2)),
     )
 
     with pytest.raises(
       ValueError, match='Non-exhaustive filters, got a non-empty remainder'
     ):
-      m.split(nnx.Param)
+      nnx.split(m, nnx.Param)
 
   def test_ellipsis_not_last_error(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(1), nnx.Param(2), nnx.Variable(3)]),
+      a=nnx.List([nnx.Param(1), nnx.Param(2), nnx.Variable(3)]),
       b=nnx.Dict(c=nnx.Param(1), d=nnx.BatchStat(2)),
     )
 
     with pytest.raises(
-      ValueError, match='Ellipsis `...` can only be used as the last filter,'
+      ValueError, match='`...` or `True` can only be used as the last filters'
     ):
-      m.split(..., nnx.Param)
+      nnx.split(m, ..., nnx.Param)
 
   def test_update_from(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(1), nnx.BatchStat(3)]),
+      a=nnx.List([nnx.Param(1), nnx.BatchStat(3)]),
       b=nnx.Param(2),
       c=100,
     )
 
-    state = m.split()[0]
-    state = jax.tree_map(lambda x: x * 2, state)
+    state = nnx.split(
+      m,
+    )[1]
+    state = jax.tree_util.tree_map(lambda x: x * 2, state)
 
-    m.update(state)
+    nnx.update(m, state)
 
     assert m.a[0].value == 2
     assert m.a[1].value == 6
     assert m.b.value == 4
     assert m.c == 100
 
   def test_update_from_with_array_leaf(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(1), nnx.BatchStat(3)]),
+      a=nnx.List([nnx.Param(1), nnx.BatchStat(3)]),
       b=nnx.Param(2),
       c=nnx.Variable(jax.numpy.array(100)),
     )
 
-    state, graphdef = m.split()
-    state = jax.tree_map(lambda x: x * 2, state)
+    graphdef, state = nnx.split(
+      m,
+    )
+    state = jax.tree_util.tree_map(lambda x: x * 2, state)
 
-    m.update(state)
+    nnx.update(m, state)
 
     assert m.a[0].value == 2
     assert m.a[1].value == 6
     assert m.b.value == 4
     assert m.c.value == 200
 
   def test_grad_example(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(1.0), nnx.BatchStat(-10)]),
+      a=nnx.List([nnx.Param(1.0), nnx.BatchStat(-10)]),
       b=nnx.Param(2.0),
       c=100,
     )
 
-    params = m.extract(nnx.Param)
+    params = nnx.state(m, nnx.Param)
 
     def loss(params):
       return sum(2 * p for p in jax.tree_util.tree_leaves(params))
 
     grads = jax.grad(loss)(params)
-    m.update(grads)
+    nnx.update(m, grads)
 
     assert m.a[0].value == 2.0
     assert m.a[1].value == -10
     assert m.b.value == 2.0
     assert m.c == 100
 
   def test_get_paritition(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(10.0), nnx.Param(20.0)]),
+      a=nnx.List([nnx.Param(10.0), nnx.Param(20.0)]),
       b=nnx.Param(10.0),
       c=7,
       d=5.0,
     )
 
     # test Variables not shared
     assert vars(m.a)['0'] is not vars(m)['b']
 
-    state = m.extract(nnx.Variable)
-    assert state['a']['0'].raw_value == m.a[0].value
-    assert state['a']['1'].raw_value == m.a[1].value
-    assert state['b'].raw_value == m.b.value
+    state = nnx.state(m, nnx.Variable)
+    assert state['a'][0].value == m.a[0].value
+    assert state['a'][1].value == m.a[1].value
+    assert state['b'].value == m.b.value
     assert state.b is not state.a[0]
     assert len(state.flat_state()) == 3
```

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/test_rngs.py` & `flax-0.8.3/flax/experimental/nnx/tests/test_rngs.py`

 * *Files 25% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from functools import partial
 from typing import Any
 
 import jax
 import jax.numpy as jnp
 import pytest
 
 from flax.experimental import nnx
@@ -34,150 +35,189 @@
     rngs = nnx.Rngs(some_name=0)
     with pytest.raises(AttributeError, match='No RNG named'):
       key = rngs.dropout()
 
   def test_rng_stream(self):
     key0 = jax.random.key(0)
     rngs = nnx.Rngs(params=key0)
-    assert rngs._rngs['params'].count == 0
+    assert rngs.params.count.value == 0
 
     key1 = rngs.params()
-    assert rngs._rngs['params'].count == 1
-    assert rngs._rngs['params'].key is key0
+    assert rngs.params.count.value == 1
+    assert rngs.params.key.value is key0
     assert not jnp.allclose(key0, key1)
 
     key2 = rngs.params()
-    assert rngs._rngs['params'].count == 2
-    assert rngs._rngs['params'].key is key0
+    assert rngs.params.count.value == 2
+    assert rngs.params.key.value is key0
     assert not jnp.allclose(key1, key2)
 
-  def test_rng_fork(self):
-    key0 = jax.random.key(0)
-    rngs1 = nnx.Rngs(params=key0)
-    rngs2 = nnx.Rngs(rngs1.fork())
-
-    assert rngs2._rngs['params'].count == 0
-
-    key1 = rngs1.params()
-    key2 = rngs2.params()
-
-    assert not jnp.allclose(key1, key2)
 
   def test_rng_trace_level_constraints(self):
     rngs = nnx.Rngs(0)
 
     @jax.jit
     def f():
       with pytest.raises(
-        nnx.TraceContextError,
-        match='Cannot use Rngs from a different trace level',
+        nnx.errors.TraceContextError,
+        match='Cannot call RngStream from a different trace level',
       ):
         rngs.params()
 
     f()
 
-    @jax.jit
-    def f():
-      with pytest.raises(
-        nnx.TraceContextError,
-        match='Cannot use Rngs from a different trace level',
-      ):
-        rngs.fork()
-
-    f()
-
     rngs1: Any = None
 
     @jax.jit
-    def g():
+    def h():
       nonlocal rngs1
       rngs1 = nnx.Rngs(1)
 
-    g()
+    h()
 
     assert isinstance(rngs1, nnx.Rngs)
     with pytest.raises(
-      nnx.TraceContextError,
-      match='Cannot use Rngs from a different trace level',
+      nnx.errors.TraceContextError,
+      match='Cannot call RngStream from a different trace level',
     ):
       rngs1.params()
 
-  def test_partition_merge(self):
-    rngs = nnx.Rngs(dropout=0)
-
-    keys = rngs.fork()
+  def test_jit_updates(self):
+    class Foo(nnx.Module):
+      def __init__(self, not_rngs):
+        rngs = not_rngs
+        self.linear = nnx.Linear(2, 2, rngs=rngs)
+        self.dropout = nnx.Dropout(0.5, deterministic=False)
+
+      def __call__(self, x, rngs):
+        x = self.linear(x)
+        x = self.dropout(x, rngs=rngs)
+        return x
 
-    assert 'dropout' in keys
-
-    rngs2 = nnx.Rngs(keys)
-
-    key1 = rngs.dropout()
-    key2 = rngs2.dropout()
-    assert not jnp.allclose(key1, key2)
+    rngs = nnx.Rngs(0)
+    m = Foo(rngs)
 
-    rngs3 = nnx.Rngs(keys)
-    key3 = rngs3.dropout()
-    assert jnp.allclose(key2, key3)
+    # +1 for the Linear kernel, +1 for the Linear bias
+    assert rngs['default'].count.value == 2
 
-  def test_fork_broadcast(self):
-    rngs = nnx.Rngs(params=0, dropout=1)
-    jax.random.key
+    @nnx.jit
+    def f(m: Foo, x: jax.Array, not_rngs: nnx.Rngs):
+      rngs = not_rngs
+      x = m(x, rngs)
+      x = m(x, rngs)
+      return x
+
+    x = jnp.ones((2, 2))
+    x = f(m, x, rngs)
+
+    # +1 for the Dropout mask
+    assert rngs['default'].count.value == 4
+
+  def test_lifting_rng_state(self):
+    class Foo(nnx.Module):
+      def __init__(self, rngs):
+        self.rngs = rngs
+        self.dropout = nnx.Dropout(0.5, deterministic=False)
+        self.linear = nnx.Linear(2, 3, rngs=rngs)
+
+      def __call__(self, x):
+        x = self.linear(x)
+        x = self.dropout(x, rngs=self.rngs)
+        return x
+
+    rngs = nnx.Rngs(params=0, dropout=1)
+    m = Foo(rngs)
+    _, params, dropout_keys, param_keys, rng_counts = nnx.split(
+      m, nnx.Param, 'dropout', 'params', nnx.RngCount
+    )
 
-    keys = rngs.fork()  # all broadcast
+    assert m.rngs.params.count.value == 2
+    assert m.rngs['dropout'].count.value == 0
+    assert len(dropout_keys.flat_state()) == 1
+    assert len(param_keys.flat_state()) == 1
+    assert len(rng_counts.flat_state()) == 2
+
+    # split dropout keys
+    split_dropout_keys = jax.tree_util.tree_map(
+      lambda x: jax.random.split(x, 4), dropout_keys
+    )
+    # replicate params
+    params = jax.tree_util.tree_map(
+      lambda x: jnp.stack([x] * 4, axis=0), params
+    )
 
-    assert keys['params'].shape == ()
-    assert keys['dropout'].shape == ()
-    assert jnp.allclose(
-      keys['params'], jax.random.fold_in(jax.random.key(0), 0)
+    @partial(
+      jax.vmap,
+      in_axes=(0, 0, None, None, 0),
+      out_axes=(0, 0, None),
     )
-    assert jnp.allclose(
-      keys['dropout'], jax.random.fold_in(jax.random.key(1), 0)
+    def f(params, dropout_keys, param_keys, rng_counts, x):
+      nnx.update(m, params, dropout_keys, param_keys, rng_counts)
+      y = m(x)
+      _, params, dropout_keys, param_keys, rng_counts = nnx.split(
+        m, nnx.Param, 'dropout', 'params', nnx.RngCount
+      )
+      return y, params, rng_counts
+
+    x = jnp.ones((4, 1, 2))
+    y, params, rng_counts = f(
+      params,
+      split_dropout_keys,
+      param_keys,
+      rng_counts,
+      x,
     )
 
-  def test_fork_split(self):
-    rngs = nnx.Rngs(params=0, dropout=1)
-    keys = rngs.fork(4)  # split all
+    nnx.update(m, params, dropout_keys, param_keys, rng_counts)
 
-    assert keys['params'].shape == (4,)
-    assert keys['dropout'].shape == (4,)
+    assert y.shape == (4, 1, 3)
+    assert m.rngs.params.count.value == 2
+    assert m.rngs['dropout'].count.value == 1
 
-  def test_fork_split_and_broadcast(self):
+  def test_state_fork_split(self):
     rngs = nnx.Rngs(params=0, dropout=1)
-    forked = rngs.fork(params=4, dropout=None)
+    graphdef, state = nnx.split(rngs, nnx.RngState)
+    split, broadcast = nnx.fork(state, ..., 4)
 
-    assert forked['params'].shape == (4,)
-    assert forked['dropout'].shape == ()
+    assert len(jax.tree.leaves(split)) == 2
+    assert len(jax.tree.leaves(broadcast)) == 2
+    assert split.params.key.value.shape == (4,)
+    assert split.dropout.key.value.shape == (4,)
+    assert broadcast.params.count.value == 0
+    assert broadcast.dropout.count.value == 0
 
-  def test_fork_filters(self):
+  def test_state_fork_split_and_broadcast(self):
     rngs = nnx.Rngs(params=0, dropout=1)
-    forked = rngs.fork({'params': 4})
-
-    assert forked['params'].shape == (4,)
-    assert forked['dropout'].shape == ()
+    graphdef, state = nnx.split(rngs, nnx.RngState)
+    split, broadcast = nnx.fork(state, 'params', 4)
 
-  def test_fork_multidimensional_split(self):
-    rngs = nnx.Rngs(params=0, dropout=1)
-    keys = rngs.fork((4, None, 3))  # split all
+    assert len(jax.tree.leaves(split)) == 1
+    assert len(jax.tree.leaves(broadcast)) == 3
+    assert split.params.key.value.shape == (4,)
+    assert broadcast.dropout.key.value.shape == ()
+    assert broadcast.params.count.value == 0
+    assert broadcast.dropout.count.value == 0
 
-    assert keys['params'].shape == (4, 1, 3)
-    assert keys['dropout'].shape == (4, 1, 3)
 
-  def test_fork_multidimensional_split_mixed(self):
+  def test_state_fork_multidimensional_split(self):
     rngs = nnx.Rngs(params=0, dropout=1)
-    keys = rngs.fork(params=(4, None, 3))  # split all
+    graphdef, state = nnx.split(rngs, nnx.RngState)
+    split, broadcast = nnx.fork(state, ..., (4, None, 3))
 
-    assert keys['params'].shape == (4, 1, 3)
-    assert keys['dropout'].shape == ()
+    assert len(jax.tree.leaves(split)) == 2
+    assert len(jax.tree.leaves(broadcast)) == 2
+    assert split.params.key.value.shape == (4, 1, 3)
+    assert split.dropout.key.value.shape == (4, 1, 3)
+    assert broadcast.params.count.value == 0
+    assert broadcast.dropout.count.value == 0
 
-  def test_rng_stream_pytree(self):
+  def test_state_fork_multidimensional_split_mixed(self):
     rngs = nnx.Rngs(params=0, dropout=1)
+    graphdef, state = nnx.split(rngs, nnx.RngState)
+    split, broadcast = nnx.fork(state, 'params', (4, None, 3))
 
-    keys = rngs.fork(dropout=4)
-    keys2 = jax.tree_util.tree_map(lambda x: x, keys)
-
-    assert 'dropout' in keys.splits
-    assert 'params' in keys.broadcasts
-
-    assert keys2 is not keys
-    assert set(keys.keys()) == set(keys2.keys())
-    assert set(keys.splits.keys()) == set(keys2.splits.keys())
-    assert set(keys.broadcasts.keys()) == set(keys2.broadcasts.keys())
+    assert len(jax.tree.leaves(split)) == 1
+    assert len(jax.tree.leaves(broadcast)) == 3
+    assert split.params.key.value.shape == (4, 1, 3)
+    assert broadcast.dropout.key.value.shape == ()
+    assert broadcast.params.count.value == 0
+    assert broadcast.dropout.count.value == 0
```

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/test_spmd.py` & `flax-0.8.3/flax/experimental/nnx/tests/test_spmd.py`

 * *Files 12% similar despite different names*

```diff
@@ -35,45 +35,66 @@
         )
 
       def __call__(self, x):
         return x @ self.w
 
     @jax.jit
     def create_module():
-      return Foo().split()
+      return nnx.split(Foo())
 
     mesh = Mesh(mesh_utils.create_device_mesh((2, 2)), ('model', 'data'))
 
     with mesh:
-      m: Foo = nnx.merge(create_module())
+      m: Foo = nnx.merge(*create_module())
 
     assert m.w.shape == (8, 2)
     assert m.w.sharding.shard_shape(m.w.shape) == (4, 1)
 
+  def test_init_all_devices(self):
+    class Foo(nnx.Module):
+      def __init__(self):
+        self.w = nnx.Param(
+          nnx.with_partitioning(
+            lambda: jnp.ones((8, 2)),
+            sharding=('model', 'data'),
+          )()
+        )
+
+      def __call__(self, x):
+        return x @ self.w
+
+    @jax.jit
+    def create_module():
+      return nnx.split(Foo())
+
+    mesh = Mesh(mesh_utils.create_device_mesh((1, 1)), ('model', 'data'))
+
+    with mesh:
+      m: Foo = nnx.merge(*create_module())
+
+    assert m.w.value.shape == (8, 2)
+    assert m.w.value.sharding.shard_shape(m.w.value.shape) == (8, 2)
+
   def test_get_partition_spec(self):
     class Foo(nnx.Module):
       def __init__(self):
         self.w = nnx.Param(
           nnx.with_partitioning(
             lambda: jnp.ones((8, 2)),
             sharding=('row', 'col'),
           )()
         )
 
       def __call__(self, x):
         return x @ self.w
 
-    params, graphdef = Foo().split()
-    state = nnx.TrainState(
+    graphdef, params = nnx.split(Foo())
+    state = nnx.TrainState.create(
       graphdef,
       params=params,
       tx=optax.adam(1e-3),
     )
     state_spec = nnx.get_partition_spec(state)
 
-    assert state_spec.params['w'].raw_value == PartitionSpec('row', 'col')
-    assert state_spec.opt_state[0].mu['w'].raw_value == PartitionSpec(
-      'row', 'col'
-    )
-    assert state_spec.opt_state[0].nu['w'].raw_value == PartitionSpec(
-      'row', 'col'
-    )
+    assert state_spec.params['w'].value == PartitionSpec('row', 'col')
+    assert state_spec.opt_state[0].mu['w'].value == PartitionSpec('row', 'col')
+    assert state_spec.opt_state[0].nu['w'].value == PartitionSpec('row', 'col')
```

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/test_state.py` & `flax-0.8.3/flax/experimental/nnx/tests/test_state.py`

 * *Files 13% similar despite different names*

```diff
@@ -15,50 +15,50 @@
 from absl.testing.absltest import TestCase
 
 from flax.experimental import nnx
 
 
 class StateTest(TestCase):
   def test_create_state(self):
-    state = nnx.State({'a': nnx.Param(1), 'b': {'c': nnx.Param(2)}})
+    state = nnx.State({'a': nnx.Param.state(1), 'b': {'c': nnx.Param.state(2)}})
 
-    assert state['a'].raw_value == 1
-    assert state['b']['c'].raw_value == 2
+    assert state['a'].value == 1
+    assert state['b']['c'].value == 2
 
   def test_get_attr(self):
-    state = nnx.State({'a': nnx.Param(1), 'b': {'c': nnx.Param(2)}})
+    state = nnx.State({'a': nnx.Param.state(1), 'b': {'c': nnx.Param.state(2)}})
 
-    assert state.a.raw_value == 1
-    assert state.b.c.raw_value == 2
+    assert state.a.value == 1
+    assert state.b.c.value == 2
 
   def test_set_attr(self):
-    state = nnx.State({'a': nnx.Param(1), 'b': {'c': nnx.Param(2)}})
+    state = nnx.State({'a': nnx.Param.state(1), 'b': {'c': nnx.Param.state(2)}})
 
-    state.a.raw_value = 3
-    state.b.c.raw_value = 4
+    state.a.value = 3
+    state.b.c.value = 4
 
-    assert state['a'].raw_value == 3
-    assert state['b']['c'].raw_value == 4
+    assert state['a'].value == 3
+    assert state['b']['c'].value == 4
 
   def test_set_attr_variables(self):
-    state = nnx.State({'a': nnx.Param(1), 'b': {'c': nnx.Param(2)}})
+    state = nnx.State({'a': nnx.Param.state(1), 'b': {'c': nnx.Param.state(2)}})
 
-    state.a.raw_value = 3
-    state.b.c.raw_value = 4
+    state.a.value = 3
+    state.b.c.value = 4
 
-    assert isinstance(state.a, nnx.Param)
-    assert state.a.raw_value == 3
-    assert isinstance(state.b.c, nnx.Param)
-    assert state.b.c.raw_value == 4
+    assert issubclass(state.a.type, nnx.Param)
+    assert state.a.value == 3
+    assert issubclass(state.b.c.type, nnx.Param)
+    assert state.b.c.value == 4
 
   def test_integer_access(self):
     class Foo(nnx.Module):
       def __init__(self, *, rngs: nnx.Rngs):
         self.layers = [nnx.Linear(1, 2, rngs=rngs), nnx.Linear(2, 3, rngs=rngs)]
 
     module = Foo(rngs=nnx.Rngs(0))
-    state = module.get_state()
+    state = nnx.state(module)
 
     assert module.layers[0].kernel.value.shape == (1, 2)
-    assert state.layers[0].kernel.raw_value.shape == (1, 2)
+    assert state.layers[0].kernel.value.shape == (1, 2)
     assert module.layers[1].kernel.value.shape == (2, 3)
-    assert state.layers[1].kernel.raw_value.shape == (2, 3)
+    assert state.layers[1].kernel.value.shape == (2, 3)
```

### Comparing `flax-0.8.2/flax/experimental/nnx/tests/test_transforms.py` & `flax-0.8.3/flax/experimental/nnx/tests/test_transforms.py`

 * *Files 20% similar despite different names*

```diff
@@ -13,15 +13,17 @@
 # limitations under the License.
 
 import typing as tp
 from functools import partial
 
 import jax
 import jax.numpy as jnp
+import numpy as np
 import pytest
+from jax.experimental import mesh_utils
 
 from flax.experimental import nnx
 
 
 class TestJIT:
   def test_jit(self):
     m = nnx.Dict(a=nnx.Param(1))
@@ -104,131 +106,440 @@
 
       @nnx.jit
       def __call__(self, x: jax.Array) -> jax.Array:
         nonlocal n
         n += 1
         return jnp.dot(x, self.w.value)
 
-    m = nnx.JIT(Foo)(2, 3, rngs=nnx.Rngs(0))
+    m = nnx.Jit(Foo)(2, 3, rngs=nnx.Rngs(0))
 
     y = m(jnp.ones((1, 2)))
     assert y.shape == (1, 3)
     assert n == 1
     y = m(jnp.ones((1, 2)))
     assert n == 1
 
+  def test_cached_unflatten(self):
+    n = 0
+
+    class Foo(nnx.Module):
+      def __init__(self, *, rngs: nnx.Rngs):
+        self.a = nnx.Linear(2, 2, rngs=rngs)
+        self.b = nnx.BatchNorm(2, rngs=rngs)
+
+    @nnx.jit
+    def f(m: Foo):
+      nonlocal n
+      n += 1
+      m.a, m.b = m.b, m.a
+
+    m = Foo(rngs=nnx.Rngs(0))
+    a = m.a
+    b = m.b
+    a_kernel = a.kernel.value
+    a_bias = a.bias.value
+    b_scale = b.scale.value
+    b_bias = b.bias.value
+    b_mean = b.mean.value
+    b_var = b.var.value
+
+    f(m)
+
+    assert n == 1
+    assert m.a is b
+    assert m.b is a
+    np.testing.assert_allclose(a_kernel, a.kernel.value)
+    np.testing.assert_allclose(a_bias, a.bias.value)
+    np.testing.assert_allclose(b_scale, b.scale.value)
+    np.testing.assert_allclose(b_bias, b.bias.value)
+    np.testing.assert_allclose(b_mean, b.mean.value)
+    np.testing.assert_allclose(b_var, b.var.value)
+
+    f(m)
+
+    assert n == 2
+    assert m.a is a
+    assert m.b is b
+
+    f(m)
+
+    assert n == 2
+    assert m.a is b
+    assert m.b is a
+
+    f(m)
+
+    assert n == 2
+    assert m.a is a
+    assert m.b is b
+
+  def test_cached_unflatten_same_type(self):
+    n = 0
+
+    class Foo(nnx.Module):
+      def __init__(self, *, rngs: nnx.Rngs):
+        self.a = nnx.Linear(2, 2, rngs=rngs)
+        self.b = nnx.Linear(2, 2, rngs=rngs)
+
+    @nnx.jit
+    def f(m: Foo):
+      nonlocal n
+      n += 1
+      m.a, m.b = m.b, m.a
+
+    m = Foo(rngs=nnx.Rngs(0))
+    a = m.a
+    b = m.b
+
+    f(m)
+
+    assert n == 1
+    assert m.a is b
+    assert m.b is a
+
+    f(m)
+
+    assert n == 1
+    assert m.a is a
+    assert m.b is b
+
+  def test_objects_in_pytree(self):
+    n = 0
+
+    class Foo(nnx.Module):
+      def __init__(self, *, rngs: nnx.Rngs):
+        self.a = nnx.Linear(2, 2, rngs=rngs)
+        self.b = nnx.Linear(2, 2, rngs=rngs)
+
+    class FooDict(tp.TypedDict):
+      foo: Foo
+
+    @nnx.jit
+    def f(tree: tuple[FooDict]):
+      nonlocal n
+      n += 1
+      m = tree[0]['foo']
+      m.a, m.b = m.b, m.a
+
+    m = Foo(rngs=nnx.Rngs(0))
+    a = m.a
+    b = m.b
+
+    f(({'foo': m},))
+
+    assert n == 1
+    assert m.a is b
+    assert m.b is a
+
+    f(({'foo': m},))
+
+    assert n == 1
+    assert m.a is a
+    assert m.b is b
+
+  def test_cached_unflatten_swap_variables(self):
+    class Foo(nnx.Module):
+      def __init__(self):
+        self.a = nnx.Param(1)
+        self.b = nnx.Param(2)
+
+    @nnx.jit
+    def f(m: Foo):
+      m.a, m.b = m.b, m.a
+
+    m = Foo()
+    a = m.a
+    b = m.b
+
+    f(m)
+
+    assert m.a is b
+    assert m.b is a
+
+  def test_cached_unflatten_add_self_reference(self):
+    n = 0
+
+    class Foo(nnx.Module):
+      def __init__(self):
+        self.ref: tp.Optional[Foo] = None
+
+    @nnx.jit
+    def f(m: Foo):
+      nonlocal n
+      n += 1
+      m.ref = m
+
+    m = Foo()
+
+    f(m)
+
+    assert n == 1
+    assert m.ref is m
+
+    f(m)
+
+    assert n == 2
+    assert m.ref is m
+
+    f(m)
+
+    assert n == 2
+    assert m.ref is m
+
+  def test_cached_unflatten_ref_in_output(self):
+    n = 0
+
+    class Foo(nnx.Module):
+      def __init__(self):
+        self.ref: tp.Optional[Foo] = None
+
+    @nnx.jit
+    def f(m: Foo):
+      nonlocal n
+      n += 1
+      m.ref = m
+      return m
+
+    m = Foo()
+
+    m2 = f(m)
+
+    assert n == 1
+    assert m.ref is m
+    assert m2 is m
+
+    m2 = f(m)
+
+    assert n == 2
+    assert m.ref is m
+    assert m2 is m
+
+    m2 = f(m)
+
+    assert n == 2
+    assert m.ref is m
+    assert m2 is m
+
+  def test_apply_shardings(self):
+    n_devices = max(jax.local_device_count() // 2, 1)
+    devices = mesh_utils.create_device_mesh((n_devices, n_devices))
+    mesh = jax.sharding.Mesh(devices, ('a', 'b'))
+
+    rngs = nnx.Rngs(0)
+    m = nnx.Linear(
+      16,
+      32,
+      rngs=rngs,
+      kernel_init=nnx.with_partitioning(
+        nnx.initializers.lecun_normal(), ('a', 'b')
+      ),
+    )
+
+    @partial(nnx.jit, constrain_state=True)
+    def constrain_object(m):
+      pass
+
+    with mesh:
+      constrain_object(m)
+
+    m.kernel.value.sharding
+
+
 
 class TestGrad:
   def test_grad(self):
     p1 = nnx.Param(10.0)
     p2 = nnx.Param(20.0)
 
     m = nnx.Dict(
-      a=nnx.Sequence([p1, p2]),
+      a=nnx.List([p1, p2]),
       b=p1,
       c=7,
       d=5.0,
     )
 
     @nnx.grad
     def f(m: nnx.Dict):
       # sum all params
       return m['a'][0].value + m['a'][1].value + m['b'].value
 
     grads = f(m)
 
     assert m.a[0] is m.b
     assert isinstance(grads, nnx.State)
-    assert grads['a']['0'].raw_value == 2.0
-    assert isinstance(grads.a['0'], nnx.Variable)
-    assert grads['a']['1'].raw_value == 1.0
-    assert isinstance(grads.a['1'], nnx.Variable)
+    assert grads['a'][0].value == 2.0
+    assert issubclass(grads.a[0].type, nnx.Variable)
+    assert grads['a'][1].value == 1.0
+    assert issubclass(grads.a[1].type, nnx.Variable)
     assert len(grads.flat_state()) == 2
 
-    m.update(grads)
+    nnx.update(m, grads)
 
     assert m.a[0] is m.b
     assert m['a'][0].value == 2.0
     assert m['a'][1].value == 1.0
     assert m['b'].value == 2.0
     assert m['c'] == 7
     assert m['d'] == 5.0
 
   def test_grad_with_multiple_ref_types(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(10.0), nnx.BatchStat(20.0)]),
+      a=nnx.List([nnx.Param(10.0), nnx.BatchStat(20.0)]),
       b=nnx.Param(10.0),
       c=7,
       d=5.0,
     )
 
     @nnx.grad
     def f(m: nnx.Dict):
       # sum all params
       return m.a[0].value + m.a[1].value + m.b.value
 
     grads = f(m)
 
     assert isinstance(grads, nnx.State)
-    assert grads['a']['0'].raw_value == 1.0
-    assert isinstance(grads.a['0'], nnx.Param)
+    assert grads['a'][0].value == 1.0
+    assert issubclass(grads.a[0].type, nnx.Param)
     assert len(grads) == 2
 
-    m.update(grads)
+    nnx.update(m, grads)
 
     assert m.a[0].value == 1.0
     assert m.a[1].value == 20.0
     assert m.b.value == 1.0
     assert m.c == 7
     assert m.d == 5.0
 
   def test_grad_with_type_predicate(self):
     m = nnx.Dict(
-      a=nnx.Sequence([nnx.Param(10.0), nnx.BatchStat(20.0)]),
+      a=nnx.List([nnx.Param(10.0), nnx.BatchStat(20.0)]),
       b=nnx.Param(10.0),
       c=7,
       d=5.0,
     )
 
     @partial(nnx.grad, wrt=nnx.BatchStat)
     def f(m: nnx.Dict):
       # sum all params
       return m.a[0].value + m.a[1].value + m.b.value
 
     grads = f(m)
 
     assert isinstance(grads, nnx.State)
-    assert grads['a']['1'].raw_value == 1.0
-    assert isinstance(grads.a['1'], nnx.BatchStat)
+    assert grads['a'][1].value == 1.0
+    assert issubclass(grads.a[1].type, nnx.BatchStat)
     assert len(grads) == 1
 
-    m.update(grads)
+    nnx.update(m, grads)
 
     assert m.a[0].value == 10.0
     assert m.a[1].value == 1.0
     assert m.b.value == 10.0
     assert m.c == 7
     assert m.d == 5.0
 
+  def test_multiple_inputs(self):
+    rngs = nnx.Rngs(0)
+    m = nnx.Linear(2, 3, rngs=rngs)
+    loss_fn = lambda m, x, y: jnp.mean((m(x) - y) ** 2)
+    grad_fn = nnx.grad(loss_fn, wrt=nnx.Param)
+    x = jax.random.uniform(rngs(), (1, 2))
+    y = jnp.ones((1, 3))
+    grads = grad_fn(m, x, y)
+
+    assert 'kernel' in grads
+    assert grads.kernel.value.shape == (2, 3)
+    assert 'bias' in grads
+    assert grads.bias.value.shape == (3,)
+
+  def test_multiple_graph_nodes(self):
+    rngs = nnx.Rngs(0)
+    m1 = nnx.Linear(2, 3, rngs=rngs)
+    m2 = nnx.Linear(3, 3, rngs=rngs)
+    loss_fn = lambda m1, m2, x, y: jnp.mean((m2(m1(x)) - y) ** 2)
+    grad_fn = nnx.grad(loss_fn, argnums=(0, 1), wrt=nnx.Param)
+    x = jax.random.uniform(rngs(), (1, 2))
+    y = jnp.ones((1, 3))
+    grads_m1, grads_m2 = grad_fn(m1, m2, x, y)
+
+    assert 'kernel' in grads_m1
+    assert grads_m1.kernel.value.shape == (2, 3)
+    assert 'bias' in grads_m1
+    assert grads_m1.bias.value.shape == (3,)
+    assert 'kernel' in grads_m2
+    assert grads_m2.kernel.value.shape == (3, 3)
+    assert 'bias' in grads_m2
+    assert grads_m2.bias.value.shape == (3,)
+
+  def test_multiple_graph_nodes_mix_positions(self):
+    rngs = nnx.Rngs(0)
+    m1 = nnx.Linear(2, 3, rngs=rngs)
+    m2 = nnx.Linear(3, 3, rngs=rngs)
+    loss_fn = lambda x, m1, y, m2: jnp.mean((m2(m1(x)) - y) ** 2)
+    grad_fn = nnx.grad(loss_fn, argnums=(1, 3), wrt=nnx.Param)
+    x = jax.random.uniform(rngs(), (1, 2))
+    y = jnp.ones((1, 3))
+    grads_m1, grads_m2 = grad_fn(x, m1, y, m2)
+
+    assert 'kernel' in grads_m1
+    assert grads_m1.kernel.value.shape == (2, 3)
+    assert 'bias' in grads_m1
+    assert grads_m1.bias.value.shape == (3,)
+    assert 'kernel' in grads_m2
+    assert grads_m2.kernel.value.shape == (3, 3)
+    assert 'bias' in grads_m2
+    assert grads_m2.bias.value.shape == (3,)
+
 
 class TestScan:
   def test_basic(self):
     class Block(nnx.Module):
       def __init__(self, *, rngs: nnx.Rngs):
         self.linear = nnx.Linear(3, 3, rngs=rngs)
+        # self.node = nnx.Variable(jnp.ones((2,)))
+
+      def __call__(self, x: jax.Array):
+        x = self.linear(x)
+        x = nnx.gelu(x)
+        return x
+
+    @partial(nnx.scan, state_axes={nnx.Param: 0}, length=5)
+    def create_block(_, rngs: nnx.Rngs):
+      return None, Block(rngs=rngs)
+
+    _, module = create_block(None, nnx.Rngs(0))
+
+    assert module.linear.kernel.value.shape == (5, 3, 3)
+    assert module.linear.bias.value.shape == (5, 3)
+    # assert module.node.value.shape == (2,)
+
+    @partial(nnx.scan, in_axes=None, state_axes={nnx.Param: 0}, length=5)
+    def forward_block(_, block: Block, x: jax.Array):
+      return None, block(x)
+
+    x = jnp.ones((1, 3))
+    out, y = forward_block(None, module, x)
+
+    assert y.shape == (5, 1, 3)
+    assert out is None
+
+  def test_basic_combinator(self):
+    class Block(nnx.Module):
+      def __init__(self, *, rngs: nnx.Rngs):
+        self.linear = nnx.Linear(3, 3, rngs=rngs)
         self.node = nnx.Variable(jnp.ones((2,)))
 
       def __call__(self, x: jax.Array) -> tp.Tuple[jax.Array, None]:
         x = self.linear(x)
         x = nnx.gelu(x)
         return x, None
 
     MLP = nnx.Scan(
       Block,
-      variable_axes={nnx.Param: 0},
+      state_axes={nnx.Param: 0},
       length=5,
     )
 
     module = MLP(rngs=nnx.Rngs(0))
 
     assert module.scan_module.linear.kernel.value.shape == (5, 3, 3)
     assert module.scan_module.linear.bias.value.shape == (5, 3)
@@ -249,15 +560,15 @@
       def __call__(self, x: jax.Array):
         x = self.linear(x)
         x = nnx.gelu(x)
         return x
 
     MLP = nnx.Scan(
       Block,
-      variable_axes={nnx.Param: 0},
+      state_axes={nnx.Param: 0},
       length=5,
       scan_output=False,
     )
 
     module = MLP(rngs=nnx.Rngs(0))
 
     assert module.scan_module.linear.kernel.value.shape == (5, 3, 3)
@@ -278,15 +589,15 @@
       def __call__(self, x: jax.Array):
         x = self.linear(x)
         x = nnx.gelu(x)
         return x, (x, x)
 
     MLP = nnx.Scan(
       Block,
-      variable_axes={nnx.Param: 0},
+      state_axes={nnx.Param: 0},
       length=5,
       out_axes=(1, 2),
     )
 
     module = MLP(rngs=nnx.Rngs(0))
 
     assert module.scan_module.linear.kernel.value.shape == (5, 3, 3)
@@ -313,15 +624,15 @@
         x = x + a
         x = self.linear(x)
         x = nnx.gelu(x)
         return x, None
 
     MLP = nnx.Scan(
       Block,
-      variable_axes={nnx.Param: 0},
+      state_axes={nnx.Param: 0},
       length=5,
     )
 
     module = MLP(rngs=nnx.Rngs(0))
 
     assert module.scan_module.linear.kernel.value.shape == (5, 3, 3)
     assert module.scan_module.linear.bias.value.shape == (5, 3)
@@ -348,17 +659,17 @@
         x = x + a + b
         x = self.linear(x)
         x = nnx.gelu(x)
         return x, None
 
     MLP = nnx.Scan(
       Block,
-      variable_axes={nnx.Param: 0},
+      state_axes={nnx.Param: 0},
       length=5,
-      in_args_axes=(0, None),
+      in_axes=(None, None, 0, None),
     )
 
     module = MLP(rngs=nnx.Rngs(0))
 
     assert module.scan_module.linear.kernel.value.shape == (5, 3, 3)
     assert module.scan_module.linear.bias.value.shape == (5, 3)
     assert module.scan_module.node.value.shape == (2,)
@@ -383,15 +694,15 @@
         x = self.linear(x)
         x = self.bn(x)
         x = self.dropout(x, rngs=rngs)
         x = nnx.gelu(x)
         return x
 
     MLP = nnx.Scan(
-      Block, variable_axes={nnx.Param: 0}, length=5, scan_output=False
+      Block, state_axes={nnx.Param: 0}, length=5, scan_output=False
     )
 
     module = MLP(rngs=nnx.Rngs(0))
     module.set_attributes(deterministic=False, use_running_average=False)
 
     assert module.scan_module.linear.kernel.value.shape == (5, 3, 3)
     assert module.scan_module.linear.bias.value.shape == (5, 3)
@@ -415,18 +726,18 @@
         x = self.bn(x)
         x = self.dropout(x, rngs=rngs)
         x = nnx.gelu(x)
         return x
 
     MLP = nnx.Scan(
       Block,
-      variable_axes={nnx.Param: 0},
+      state_axes={nnx.Param: 0},
       length=5,
       # params is split, dropout is broadcast
-      broadcast_rngs=['dropout'],
+      split_rngs=['dropout'],
       scan_output=False,
     )
 
     module = MLP(rngs=nnx.Rngs(0))
     module.set_attributes(deterministic=False, use_running_average=False)
 
     assert module.scan_module.linear.kernel.value.shape == (5, 3, 3)
@@ -435,30 +746,33 @@
 
     x = jnp.ones((1, 3))
     y = module(x, rngs=nnx.Rngs(1))
 
     assert y.shape == (1, 3)
 
   def test_complex_decorator(self):
-    scan_over_layers = partial(
-      nnx.scan,
-      variable_axes={nnx.Param: 0},
-      length=5,
-    )
-
     class Block(nnx.Module):
-      @scan_over_layers
+      @partial(
+        nnx.vmap,
+        state_axes={nnx.Param: 0},
+        axis_size=5,
+      )
       def __init__(self, *, rngs: nnx.Rngs):
         self.d = 3
         self.linear = nnx.Linear(3, 3, rngs=rngs)
         self.bn = nnx.BatchNorm(3, rngs=rngs)
         self.dropout = nnx.Dropout(0.5)
         self.node = nnx.Variable(jnp.ones((2,)))
 
-      @scan_over_layers
+      @partial(
+        nnx.scan,
+        state_axes={nnx.Param: 0},
+        length=5,
+        carry_argnum=1,
+      )
       def __call__(
         self, x: jax.Array, _, *, rngs: nnx.Rngs
       ) -> tp.Tuple[jax.Array, None]:
         x = self.linear(x)
         x = self.bn(x)
         x = self.dropout(x, rngs=rngs)
         x = nnx.gelu(x)
@@ -495,61 +809,61 @@
           rngs=rngs,
         )
 
       def __call__(self, x: jax.Array, _) -> tp.Tuple[jax.Array, None]:
         x = self.linear(x)
 
         # test sharding layer axes is not present inside scan
-        state = self.linear.get_state()
-        assert state.kernel.raw_value.shape == (3, 3)
+        state = nnx.state(self.linear)
+        assert state.kernel.value.shape == (3, 3)
         assert state.kernel.sharding == ('din', 'dout')
-        assert state.bias.raw_value.shape == (3,)
+        assert state.bias.value.shape == (3,)
         assert state.bias.sharding == ('dout',)
 
         return x, None
 
     MLP = nnx.Scan(
       Block,
-      variable_axes={nnx.Param: 0},
+      state_axes={nnx.Param: 0},
       length=5,
-      scan_metadata={nnx.PARTITION_NAME: 'layers'},
+      transform_metadata={nnx.PARTITION_NAME: 'layers'},
     )
 
     m = MLP(rngs=nnx.Rngs(0))
 
     # test sharding layers axes is set
-    state = m.get_state()
-    assert state.scan_module.linear.kernel.raw_value.shape == (
+    state = nnx.state(m)
+    assert state.scan_module.linear.kernel.value.shape == (
       5,
       3,
       3,
     )
     assert state.scan_module.linear.kernel.sharding == (
       'layers',
       'din',
       'dout',
     )
-    assert state.scan_module.linear.bias.raw_value.shape == (5, 3)
+    assert state.scan_module.linear.bias.value.shape == (5, 3)
     assert state.scan_module.linear.bias.sharding == (
       'layers',
       'dout',
     )
 
     x = jnp.ones((1, 3))
     y, out = m(x, None)
 
     # test sharding axes is preserved
-    state = m.get_state()
-    assert state.scan_module.linear.kernel.raw_value.shape == (5, 3, 3)
+    state = nnx.state(m)
+    assert state.scan_module.linear.kernel.value.shape == (5, 3, 3)
     assert state.scan_module.linear.kernel.sharding == (
       'layers',
       'din',
       'dout',
     )
-    assert state.scan_module.linear.bias.raw_value.shape == (5, 3)
+    assert state.scan_module.linear.bias.value.shape == (5, 3)
     assert state.scan_module.linear.bias.sharding == (
       'layers',
       'dout',
     )
 
   def test_type_error_less_than_one_args(self):
     class Block(nnx.Module):
@@ -557,67 +871,47 @@
         self.linear = nnx.Linear(3, 3, rngs=rngs)
 
       def __call__(self):
         return None, None
 
     MLP = nnx.Scan(
       Block,
-      variable_axes={nnx.Param: 0},
+      state_axes={nnx.Param: 0},
       length=5,
     )
 
     mlp = MLP(rngs=nnx.Rngs(0))
 
     with pytest.raises(
-      TypeError, match='Expected at least 1 positional argument'
+      TypeError, match='Expected at least 2 positional argument'
     ):
       mlp()
 
-  def test_value_error_positional_argument_type_context(self):
-    class Block(nnx.Module):
-      def __init__(self, rngs: nnx.Rngs):
-        self.linear = nnx.Linear(3, 3, rngs=rngs)
-
-      def __call__(self, x: jax.Array) -> tp.Tuple[jax.Array, None]:
-        x = self.linear(x)
-        return x, None
-
-    MLP = nnx.Scan(
-      Block,
-      variable_axes={nnx.Param: 0},
-      length=5,
-    )
-
-    with pytest.raises(
-      ValueError, match='Rngs must be passed as a keyword argument named'
-    ):
-      MLP(nnx.Rngs(0))
-
 
 class TestRemat:
   def test_basic_remat(self):
     RematLinear = nnx.Remat(nnx.Linear)
 
     module = RematLinear(2, 3, rngs=nnx.Rngs(0))
 
     y = module(jnp.ones((1, 2)))
 
     assert y.shape == (1, 3)
 
   def test_remat_decorator(self):
     class RematLinear(nnx.Module):
-      @nnx.remat
-      def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):
+      @partial(nnx.remat, static_argnums=(1, 2))
+      def __init__(self, din: int, dout: int, rngs: nnx.Rngs):
         self.linear = nnx.Linear(din, dout, rngs=rngs)
 
       @nnx.remat
       def __call__(self, x: jax.Array) -> jax.Array:
         return self.linear(x)
 
-    module = RematLinear(2, 3, rngs=nnx.Rngs(0))
+    module = RematLinear(2, 3, nnx.Rngs(0))
 
     y = module(jnp.ones((1, 2)))
 
     assert y.shape == (1, 3)
 
   def test_remat_with_scan(self):
     class LinearBlock(nnx.Module):
@@ -628,15 +922,15 @@
         x = self.linear(x)
         return x, None
 
     RematLinear = nnx.Remat(LinearBlock)
 
     ScanRematLinear = nnx.Scan(
       RematLinear,
-      variable_axes={nnx.Param: 0},
+      state_axes={nnx.Param: 0},
       length=5,
     )
 
     m = ScanRematLinear(rngs=nnx.Rngs(0))
 
     assert m.scan_module.remat_module.linear.kernel.value.shape == (5, 3, 3)
     assert m.scan_module.remat_module.linear.bias.value.shape == (5, 3)
@@ -644,26 +938,30 @@
     y, _ = m(jnp.ones((1, 3)), None)
     assert y.shape == (1, 3)
 
     y, _ = m(jnp.ones((1, 3)), None)
     assert y.shape == (1, 3)
 
   def test_remat_with_scan_decorator(self):
-    scan = partial(
-      nnx.scan,
-      variable_axes={nnx.Param: 0},
-      length=5,
-    )
-
     class ScanLinear(nnx.Module):
-      @scan
+      @partial(
+        nnx.vmap,
+        state_axes={nnx.Param: 0},
+        axis_size=5,
+      )
       def __init__(self, *, rngs: nnx.Rngs):
         self.linear = nnx.Linear(3, 3, rngs=rngs)
 
-      @scan
+      @partial(
+        nnx.scan,
+        in_axes=None,
+        state_axes={nnx.Param: 0},
+        length=5,
+        carry_argnum=1,
+      )
       @nnx.remat
       def __call__(self, x: jax.Array, _) -> tp.Tuple[jax.Array, None]:
         x = self.linear(x)
         return x, None
 
     m = ScanLinear(rngs=nnx.Rngs(0))
 
@@ -673,30 +971,185 @@
     y, _ = m(jnp.ones((1, 3)), None)
     assert y.shape == (1, 3)
 
 
 class TestVmap:
   def test_basic(self):
     class Block(nnx.Module):
+      def __init__(self, rngs: nnx.Rngs):
+        self.linear = nnx.Linear(3, 3, rngs=rngs)
+        self.dropout = nnx.Dropout(0.5, deterministic=False, rngs=rngs)
+
+      def __call__(self, x: jax.Array) -> jax.Array:
+        x = self.linear(x)
+        x = nnx.relu(x)
+        x = self.dropout(x)
+        return x
+
+    def create_block(rngs: nnx.Rngs):
+      return Block(rngs)
+
+    vectorized_create_block = nnx.vmap(
+      create_block, state_axes={nnx.Param: 0}, axis_size=5
+    )
+
+    rngs = nnx.Rngs(0)
+    initial_key = rngs.default.key.value
+    module = vectorized_create_block(rngs)
+
+    assert rngs.default.count.value == 2
+    assert rngs.default.key.value == initial_key
+    assert not jnp.allclose(
+      module.linear.kernel.value[0],
+      module.linear.kernel.value[1],
+    )
+    assert module.linear.kernel.value.shape == (5, 3, 3)
+    assert module.linear.bias.value.shape == (5, 3)
+
+    x = jnp.ones((5, 1, 3))
+
+    def forward_block(module, x):
+      return module(x)
+
+    vectorized_forward_block = nnx.vmap(
+      forward_block, state_axes={nnx.Param: 0}, axis_size=5
+    )
+
+    y = vectorized_forward_block(module, x)
+
+    assert y.shape == (5, 1, 3)
+    assert rngs.default.count.value == 3
+    assert rngs.default.key.value == initial_key
+
+    y2 = vectorized_forward_block(module, x)
+
+    assert not jnp.allclose(y, y2)
+
+  def test_basic_demo(self):
+    class Block(nnx.Module):
+      def __init__(self, rngs: nnx.Rngs):
+        self.linear = nnx.Linear(3, 3, rngs=rngs)
+        self.dropout = nnx.Dropout(0.5, deterministic=False, rngs=rngs)
+
+      def __call__(self, x: jax.Array) -> jax.Array:
+        return self.dropout(nnx.relu(self.linear(x)))
+
+    @partial(nnx.vmap, axis_size=5)
+    def create_block(rngs: nnx.Rngs):
+      return Block(rngs)
+
+    @partial(nnx.vmap, axis_size=5)
+    def forward_block(module: Block, x):
+      return module(x)
+
+    rngs = nnx.Rngs(0)
+    module = create_block(rngs)
+
+    assert rngs.default.count.value == 2
+    assert module.linear.kernel.value.shape == (5, 3, 3)
+    assert module.linear.bias.value.shape == (5, 3)
+    assert not jnp.allclose(
+      module.linear.kernel.value[0],
+      module.linear.kernel.value[1],
+    )
+
+    x = jnp.ones((5, 1, 3))
+
+    y = forward_block(module, x)
+
+    assert y.shape == (5, 1, 3)
+    assert rngs.default.count.value == 3
+
+    y2 = forward_block(module, x)
+
+    # dropout is working!
+    assert not jnp.allclose(y, y2)
+
+  def test_replicate(self):
+    din = 3
+    dout = 10
+
+    class Block(nnx.Module):
+      def __init__(self, rngs: nnx.Rngs):
+        self.linear = nnx.Linear(din, dout, rngs=rngs)
+        self.dropout = nnx.Dropout(0.5, deterministic=False, rngs=rngs)
+
+      def __call__(self, x: jax.Array) -> jax.Array:
+        return self.dropout(nnx.relu(self.linear(x)))
+
+    def create_block(rngs: nnx.Rngs):
+      return Block(rngs)
+
+    @partial(
+      nnx.vmap,
+      state_axes={},  # replicate all state
+      split_rngs=True,  # different rngs for each replica
+    )
+    def forward_block(module: Block, x):
+      return module(x)
+
+    rngs = nnx.Rngs(0)
+    initial_key = rngs.default.key.value
+    module = create_block(rngs)
+
+    assert rngs.default.count.value == 2
+    assert module.linear.kernel.value.shape == (din, dout)
+    assert module.linear.bias.value.shape == (dout,)
+
+    x = jnp.ones((5, 1, din))
+
+    y = forward_block(module, x)
+
+    assert y.shape == (5, 1, dout)
+    assert rngs.default.count.value == 3
+
+    assert not jnp.allclose(y[0], y[1])
+
+    y2 = forward_block(module, x)
+
+    # dropout is working!
+    assert not jnp.allclose(y, y2)
+
+    assert rngs.default.key.value == initial_key
+
+  def test_combinator(self):
+    class Block(nnx.Module):
       def __init__(self, *, rngs: nnx.Rngs):
         self.linear = nnx.Linear(3, 3, rngs=rngs)
 
       def __call__(self, x: jax.Array) -> jax.Array:
         x = self.linear(x)
         x = nnx.gelu(x)
         return x
 
-    MLP = nnx.Vmap(Block, variable_axes={nnx.Param: 0}, axis_size=5)
+    MLP = nnx.Vmap(Block, state_axes={nnx.Param: 0}, axis_size=5)
 
     module = MLP(rngs=nnx.Rngs(0))
 
     assert not jnp.allclose(
       module.vmap_module.linear.kernel.value[0],
       module.vmap_module.linear.kernel.value[1],
     )
     assert module.vmap_module.linear.kernel.value.shape == (5, 3, 3)
     assert module.vmap_module.linear.bias.value.shape == (5, 3)
 
     x = jnp.ones((5, 1, 3))
     y = module(x)
 
     assert y.shape == (5, 1, 3)
+
+  def test_combinator_init(self):
+    class Block(nnx.Module):
+      def __init__(self, *, graphdef: str, rngs: nnx.Rngs):
+        self.linear = nnx.Linear(3, 3, rngs=rngs)
+        self.graphdef = graphdef
+
+      def __call__(self, x: jax.Array) -> jax.Array:
+        x = self.linear(x)
+        x = nnx.gelu(x)
+        return x
+
+    MLP = nnx.Vmap(Block, state_axes={nnx.Param: 0}, axis_size=5)
+
+    module = MLP(graphdef='hello', rngs=nnx.Rngs(0))
+
+    assert module.vmap_module.graphdef == 'hello'
```

### Comparing `flax-0.8.2/flax/ids.py` & `flax-0.8.3/flax/ids.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/io.py` & `flax-0.8.3/flax/io.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/jax_utils.py` & `flax-0.8.3/flax/jax_utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/linen/README.md` & `flax-0.8.3/flax/linen/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/linen/__init__.py` & `flax-0.8.3/flax/linen/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,21 +27,21 @@
 # limitations under the License.
 
 """The Flax Module system."""
 
 
 # pylint: disable=g-multiple-import,useless-import-alias
 # re-export commonly used modules and functions
-from ..core import (
+from flax.core import (
     DenyList as DenyList,
     FrozenDict as FrozenDict,
     broadcast as broadcast,
     meta as meta,
 )
-from ..core.meta import (
+from flax.core.meta import (
     PARTITION_NAME as PARTITION_NAME,
     Partitioned as Partitioned,
     get_partition_spec as get_partition_spec,
     get_sharding as get_sharding,
     unbox as unbox,
     with_partitioning as with_partitioning,
 )
```

### Comparing `flax-0.8.2/flax/linen/activation.py` & `flax-0.8.3/flax/linen/activation.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/linen/attention.py` & `flax-0.8.3/flax/linen/attention.py`

 * *Files 6% similar despite different names*

```diff
@@ -38,35 +38,37 @@
   Initializer,
   PrecisionLike,
   DotGeneralT,
 )
 
 
 def dot_product_attention_weights(
-  query: Array,
-  key: Array,
-  bias: Optional[Array] = None,
-  mask: Optional[Array] = None,
-  broadcast_dropout: bool = True,
-  dropout_rng: Optional[PRNGKey] = None,
-  dropout_rate: float = 0.0,
-  deterministic: bool = False,
-  dtype: Optional[Dtype] = None,
-  precision: PrecisionLike = None,
-  module: Optional[Module] = None,
+    query: Array,
+    key: Array,
+    bias: Optional[Array] = None,
+    mask: Optional[Array] = None,
+    broadcast_dropout: bool = True,
+    dropout_rng: Optional[PRNGKey] = None,
+    dropout_rate: float = 0.0,
+    deterministic: bool = False,
+    dtype: Optional[Dtype] = None,
+    precision: PrecisionLike = None,
+    module: Optional[Module] = None,
+    force_fp32_for_softmax: bool = False,
+    einsum_dot_general: Callable[..., Array] = jax.lax.dot_general,
 ):
   """Computes dot-product attention weights given query and key.
 
   Used by :func:`dot_product_attention`, which is what you'll most likely use.
   But if you want access to the attention weights for introspection, then
   you can directly call this function and call einsum yourself.
 
   Args:
-    query: queries for calculating attention with shape of ``[batch..., q_length,
-      num_heads, qk_depth_per_head]``.
+    query: queries for calculating attention with shape of ``[batch...,
+      q_length, num_heads, qk_depth_per_head]``.
     key: keys for calculating attention with shape of ``[batch..., kv_length,
       num_heads, qk_depth_per_head]``.
     bias: bias for the attention weights. This should be broadcastable to the
       shape ``[batch..., num_heads, q_length, kv_length]``. This can be used for
       incorporating causal masks, padding masks, proximity bias, etc.
     mask: mask for the attention weights. This should be broadcastable to the
       shape ``[batch..., num_heads, q_length, kv_length]``. This can be used for
@@ -76,17 +78,21 @@
     dropout_rng: JAX PRNGKey: to be used for dropout
     dropout_rate: dropout rate
     deterministic: bool, deterministic or not (to apply dropout)
     dtype: the dtype of the computation (default: infer from inputs and params)
     precision: numerical precision of the computation see ``jax.lax.Precision``
       for details.
     module: the Module that will sow the attention weights into the
-      'intermediates' collection. Remember to mark 'intermediates' as mutable via
-      ``mutable=['intermediates']`` in order to have that collection returned.
-      If ``module`` is None, the attention weights will not be sowed.
+      'intermediates' collection. Remember to mark 'intermediates' as mutable
+      via ``mutable=['intermediates']`` in order to have that collection
+      returned. If ``module`` is None, the attention weights will not be sowed.
+    force_fp32_for_softmax: bool, whether to force the softmax to be computed in
+      fp32. This is useful for mixed-precision training where higher precision
+      is desired for numerical stability.
+    einsum_dot_general: the dot_general to use in einsum.
 
   Returns:
     Output of shape ``[batch..., num_heads, q_length, kv_length]``.
   """
   query, key = promote_dtype(query, key, dtype=dtype)
   dtype = query.dtype
 
@@ -96,27 +102,34 @@
   assert query.shape[-1] == key.shape[-1], 'q, k depths must match.'
 
   # calculate attention matrix
   depth = query.shape[-1]
   query = query / jnp.sqrt(depth).astype(dtype)
   # attn weight shape is (batch..., num_heads, q_length, kv_length)
   attn_weights = jnp.einsum(
-    '...qhd,...khd->...hqk', query, key, precision=precision
+      '...qhd,...khd->...hqk',
+      query,
+      key,
+      precision=precision,
+      _dot_general=einsum_dot_general,
   )
 
   # apply attention bias: masking, dropout, proximity bias, etc.
   if bias is not None:
     attn_weights = attn_weights + bias
   # apply attention mask
   if mask is not None:
     big_neg = jnp.finfo(dtype).min
     attn_weights = jnp.where(mask, attn_weights, big_neg)
 
   # normalize the attention weights
-  attn_weights = jax.nn.softmax(attn_weights).astype(dtype)
+  if force_fp32_for_softmax and dtype != jnp.float32:
+    attn_weights = jax.nn.softmax(attn_weights.astype(jnp.float32))
+  else:
+    attn_weights = jax.nn.softmax(attn_weights).astype(dtype)
 
   if module:
     module.sow('intermediates', 'attention_weights', attn_weights)
 
   # apply attention dropout
   if not deterministic and dropout_rate > 0.0:
     keep_prob = 1.0 - dropout_rate
@@ -129,38 +142,41 @@
     multiplier = keep.astype(dtype) / jnp.asarray(keep_prob, dtype=dtype)
     attn_weights = attn_weights * multiplier
 
   return attn_weights
 
 
 def dot_product_attention(
-  query: Array,
-  key: Array,
-  value: Array,
-  bias: Optional[Array] = None,
-  mask: Optional[Array] = None,
-  broadcast_dropout: bool = True,
-  dropout_rng: Optional[PRNGKey] = None,
-  dropout_rate: float = 0.0,
-  deterministic: bool = False,
-  dtype: Optional[Dtype] = None,
-  precision: PrecisionLike = None,
-  module: Optional[Module] = None,
+    query: Array,
+    key: Array,
+    value: Array,
+    bias: Optional[Array] = None,
+    mask: Optional[Array] = None,
+    broadcast_dropout: bool = True,
+    dropout_rng: Optional[PRNGKey] = None,
+    dropout_rate: float = 0.0,
+    deterministic: bool = False,
+    dtype: Optional[Dtype] = None,
+    precision: PrecisionLike = None,
+    module: Optional[Module] = None,
+    force_fp32_for_softmax: bool = False,
+    einsum_dot_general: Callable[..., Array] = jax.lax.dot_general,
 ):
   """Computes dot-product attention given query, key, and value.
 
   This is the core function for applying attention based on
   https://arxiv.org/abs/1706.03762. It calculates the attention weights given
   query and key and combines the values using the attention weights.
 
-  Note: query, key, value needn't have any batch dimensions.
+  .. note::
+    ``query``, ``key``, ``value`` needn't have any batch dimensions.
 
   Args:
-    query: queries for calculating attention with shape of ``[batch..., q_length,
-      num_heads, qk_depth_per_head]``.
+    query: queries for calculating attention with shape of ``[batch...,
+      q_length, num_heads, qk_depth_per_head]``.
     key: keys for calculating attention with shape of ``[batch..., kv_length,
       num_heads, qk_depth_per_head]``.
     value: values to be used in attention with shape of ``[batch..., kv_length,
       num_heads, v_depth_per_head]``.
     bias: bias for the attention weights. This should be broadcastable to the
       shape ``[batch..., num_heads, q_length, kv_length]``. This can be used for
       incorporating causal masks, padding masks, proximity bias, etc.
@@ -172,17 +188,21 @@
     dropout_rng: JAX PRNGKey: to be used for dropout
     dropout_rate: dropout rate
     deterministic: bool, deterministic or not (to apply dropout)
     dtype: the dtype of the computation (default: infer from inputs)
     precision: numerical precision of the computation see ``jax.lax.Precision`
       for details.
     module: the Module that will sow the attention weights into the
-      'intermediates' collection. Remember to mark 'intermediates' as mutable via
-      ``mutable=['intermediates']`` in order to have that collection returned.
-      If ``module`` is None, the attention weights will not be sowed.
+      'intermediates' collection. Remember to mark 'intermediates' as mutable
+      via ``mutable=['intermediates']`` in order to have that collection
+      returned. If ``module`` is None, the attention weights will not be sowed.
+    force_fp32_for_softmax: bool, whether to force the softmax to be computed in
+      fp32. This is useful for mixed-precision training where higher precision
+      is desired for numerical stability.
+    einsum_dot_general: the dot_general to use in einsum.
 
   Returns:
     Output of shape ``[batch..., q_length, num_heads, v_depth_per_head]``.
   """
   query, key, value = promote_dtype(query, key, value, dtype=dtype)
   dtype = query.dtype
   assert key.ndim == query.ndim == value.ndim, 'q, k, v must have same rank.'
@@ -192,30 +212,36 @@
   assert (
     query.shape[-2] == key.shape[-2] == value.shape[-2]
   ), 'q, k, v num_heads must match.'
   assert key.shape[-3] == value.shape[-3], 'k, v lengths must match.'
 
   # compute attention weights
   attn_weights = dot_product_attention_weights(
-    query,
-    key,
-    bias,
-    mask,
-    broadcast_dropout,
-    dropout_rng,
-    dropout_rate,
-    deterministic,
-    dtype,
-    precision,
-    module,
+      query,
+      key,
+      bias,
+      mask,
+      broadcast_dropout,
+      dropout_rng,
+      dropout_rate,
+      deterministic,
+      dtype,
+      precision,
+      module,
+      force_fp32_for_softmax,
+      einsum_dot_general=einsum_dot_general,
   )
 
   # return weighted sum over values for each query position
   return jnp.einsum(
-    '...hqk,...khd->...qhd', attn_weights, value, precision=precision
+      '...hqk,...khd->...qhd',
+      attn_weights,
+      value,
+      precision=precision,
+      _dot_general=einsum_dot_general,
   )
 
 
 class MultiHeadDotProductAttention(Module):
   """Multi-head dot-product attention.
 
   Example usage::
@@ -263,34 +289,34 @@
     >>> # out1 and out2 are the same.
     >>> out1, out2 = module.apply(variables, q, dropout_rng=key5)
     >>> # out1 and out2 are the same as out3 and out4.
     >>> # providing a `dropout_rng` arg will take precedence over the `rngs` arg in `.apply`
     >>> out3, out4 = module.apply(variables, q, rngs={'dropout': key6}, dropout_rng=key5)
 
   Attributes:
-    num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])
+    num_heads: Number of attention heads. Features (i.e. inputs_q.shape[-1])
       should be divisible by the number of heads.
-    dtype: the dtype of the computation (default: infer from inputs and params)
-    param_dtype: the dtype passed to parameter initializers (default: float32)
-    qkv_features: dimension of the key, query, and value.
-    out_features: dimension of the last projection
-    broadcast_dropout: bool: use a broadcasted dropout along batch dims.
-    dropout_rate: dropout rate
-    deterministic: if false, the attention weight is masked randomly using
-      dropout, whereas if true, the attention weights are deterministic.
-    precision: numerical precision of the computation see ``jax.lax.Precision``
+    dtype: The dtype of the computation (default: infer from inputs and params)
+    param_dtype: The dtype passed to parameter initializers (default: float32)
+    qkv_features: Dimension of the key, query, and value.
+    out_features: Dimension of the last projection
+    broadcast_dropout: Use a broadcasted dropout along batch dims.
+    dropout_rate: Dropout rate.
+    deterministic: If False, the attention weight is masked randomly using
+      dropout, whereas if True, the attention weights are deterministic.
+    precision: Numerical precision of the computation see ``jax.lax.Precision``
       for details.
-    kernel_init: initializer for the kernel of the Dense layers.
-    bias_init: initializer for the bias of the Dense layers.
-    use_bias: bool: whether pointwise QKVO dense transforms use bias.
+    kernel_init: Initializer for the kernel of the Dense layers.
+    bias_init: Initializer for the bias of the Dense layers.
+    use_bias: Whether pointwise QKVO dense transforms use bias.
     attention_fn: dot_product_attention or compatible function. Accepts query,
       key, value, and returns output of shape ``[bs, dim1, dim2, ..., dimN,,
       num_heads, value_channels]``
-    decode: whether to prepare and use an autoregressive cache.
-    normalize_qk: should QK normalization be applied (arxiv.org/abs/2302.05442).
+    decode: Whether to prepare and use an autoregressive cache.
+    normalize_qk: Should QK normalization be applied (arxiv.org/abs/2302.05442).
   """
 
   num_heads: int
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   qkv_features: Optional[int] = None
   out_features: Optional[int] = None
@@ -300,14 +326,15 @@
   precision: PrecisionLike = None
   kernel_init: Initializer = default_kernel_init
   bias_init: Initializer = initializers.zeros_init()
   use_bias: bool = True
   attention_fn: Callable[..., Array] = dot_product_attention
   decode: bool = False
   normalize_qk: bool = False
+  force_fp32_for_softmax: bool = False
   # Deprecated, will be removed.
   qkv_dot_general: Optional[DotGeneralT] = None
   out_dot_general: Optional[DotGeneralT] = None
   qkv_dot_general_cls: Any = None
   out_dot_general_cls: Any = None
 
   @overload
```

### Comparing `flax-0.8.2/flax/linen/batch_apply.py` & `flax-0.8.3/flax/linen/batch_apply.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/linen/combinators.py` & `flax-0.8.3/flax/linen/combinators.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/linen/dtypes.py` & `flax-0.8.3/flax/linen/dtypes.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/linen/experimental/layers_with_named_axes.py` & `flax-0.8.3/flax/linen/experimental/layers_with_named_axes.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/linen/fp8_ops.py` & `flax-0.8.3/flax/linen/fp8_ops.py`

 * *Files 12% similar despite different names*

```diff
@@ -18,25 +18,37 @@
 from functools import partial
 
 from jax import custom_jvp, custom_vjp, lax, random
 from jax import numpy as jnp
 from jax._src import core
 from jax._src import dtypes
 
+try:
+  from jax._src import earray
+  from jax._src.interpreters import pxla
+  CAN_USE_EARRAY = True
+except (ModuleNotFoundError, ImportError):
+  CAN_USE_EARRAY = False
+
 from flax.linen import initializers, module
 
 OVERWRITE_WITH_GRADIENT = '_overwrite_with_gradient'
 
 # Define a custom dtype for FP8 meta params.
 class Fp8MetaTyRules:
   # tell JAX how to lower this dtype to an HLO dtype
   @staticmethod
   def physical_element_aval(dtype) -> core.ShapedArray:
     return core.ShapedArray((), dtype.float_dtype)
 
+  @staticmethod
+  def replicate_trailing_dims(ctx, val, aval):
+    del ctx, aval
+    return val
+
   # allow conversions to and from the corresponding float type
   @staticmethod
   def convert_from(fp8_meta_dtype, other_dtype) -> bool:
     return fp8_meta_dtype.float_dtype == other_dtype
 
   @staticmethod
   def convert_to(other_dtype, fp8_meta_dtype) -> bool:
@@ -61,19 +73,32 @@
 
   @staticmethod
   def full(shape, fill_value, dtype):
     fill_value = lax.convert_element_type(fill_value, dtype.float_dtype)
     out_raw = lax.full(shape, fill_value, dtype.float_dtype)
     return lax.convert_element_type(out_raw, dtype)
 
-  # NOTE: by skipping some rules, this dtype can only be used underneath jit
   @staticmethod
-  def global_sharded_result_handler(aval, sharding, committed, is_from_xla):
-    raise NotImplementedError("convert back under the jit")
+  def logical_sharding(aval, phys_sharding):
+    return phys_sharding
 
+  @staticmethod
+  def global_sharded_result_handler(aval, out_sharding, committed):
+    if not CAN_USE_EARRAY:
+      raise NotImplementedError("convert back under the jit")
+
+    phys_sharding = out_sharding  # unlike KeyTyRules, assume same shape
+    phys_aval = core.physical_aval(aval)
+    phys_handler_maker = pxla.global_result_handlers[core.ShapedArray]
+    phys_handler = phys_handler_maker(phys_aval, phys_sharding, committed)
+    return lambda bufs: earray.EArray(aval, phys_handler(bufs))
+
+  @staticmethod
+  def physical_sharding(aval, sharding):
+    return sharding  # unlike KeyTyRules, assume same shape
 
 # class to use as second argument to jax.dtypes.issubdtype
 class fp8_meta_dtype(dtypes.extended): pass
 
 # parameterized datatype for use in e.g. lax.convert_element_type
 @dataclasses.dataclass(frozen=True)
 class fp8_meta_dtype_wrapper(dtypes.ExtendedDType):
```

### Comparing `flax-0.8.2/flax/linen/initializers.py` & `flax-0.8.3/flax/linen/initializers.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/linen/kw_only_dataclasses.py` & `flax-0.8.3/flax/linen/kw_only_dataclasses.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/linen/linear.py` & `flax-0.8.3/flax/linen/linear.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Linear modules."""
 
-import dataclasses
 from typing import (
   Any,
   Iterable,
   List,
   Optional,
   Sequence,
   Tuple,
@@ -291,15 +290,15 @@
   Example usage::
 
     >>> import flax.linen as nn
     >>> import jax, jax.numpy as jnp
 
     >>> layer = nn.Einsum((5, 6, 7), 'abc,cde->abde')
     >>> variables = layer.init(jax.random.key(0), jnp.ones((3, 4, 5)))
-    >>> jax.tree_map(jnp.shape, variables)
+    >>> jax.tree_util.tree_map(jnp.shape, variables)
     {'params': {'bias': (6, 7), 'kernel': (5, 6, 7)}}
 
   Attributes:
     shape: the shape of the kernel.
     einsum_str: a string to denote the einsum equation. The equation must
       have exactly two operands, the lhs being the input passed in, and
       the rhs being the learnable kernel. Exactly one of ``einsum_str``
@@ -1101,16 +1100,14 @@
 
   num_embeddings: int
   features: int
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   embedding_init: Initializer = default_embed_init
 
-  embedding: Array = dataclasses.field(init=False)
-
   def setup(self):
     self.embedding = self.param(
       'embedding',
       self.embedding_init,
       (self.num_embeddings, self.features),
       self.param_dtype,
     )
```

### Comparing `flax-0.8.2/flax/linen/module.py` & `flax-0.8.3/flax/linen/module.py`

 * *Files 2% similar despite different names*

```diff
@@ -308,6532 +308,6758 @@
 00001330: 7920 7369 6e67 6c65 746f 6e20 7365 6e74  y singleton sent
 00001340: 696e 656c 2e0a 0a20 2064 6566 205f 5f64  inel...  def __d
 00001350: 6565 7063 6f70 795f 5f28 7365 6c66 2c20  eepcopy__(self, 
 00001360: 6d65 6d6f 293a 0a20 2020 2064 656c 206d  memo):.    del m
 00001370: 656d 6f0a 2020 2020 7265 7475 726e 2073  emo.    return s
 00001380: 656c 6620 2023 2044 6f20 6e6f 7420 636f  elf  # Do not co
 00001390: 7079 2073 696e 676c 6574 6f6e 2073 656e  py singleton sen
-000013a0: 7469 6e65 6c2e 0a0a 0a5f 756e 7370 6563  tinel...._unspec
-000013b0: 6966 6965 645f 7061 7265 6e74 203d 205f  ified_parent = _
-000013c0: 5365 6e74 696e 656c 2829 0a0a 0a23 2045  Sentinel()...# E
-000013d0: 6e61 626c 6520 6175 746f 6d61 7469 6320  nable automatic 
-000013e0: 6e61 6d65 645f 6361 6c6c 2077 7261 7070  named_call wrapp
-000013f0: 696e 6720 666f 7220 6c61 6265 6c6c 696e  ing for labellin
-00001400: 6720 7072 6f66 696c 6520 7472 6163 6573  g profile traces
-00001410: 2e0a 2320 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ..# ------------
-00001420: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001430: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001440: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001450: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001460: 2d0a 5f75 7365 5f6e 616d 6564 5f63 616c  -._use_named_cal
-00001470: 6c20 3d20 636f 6e66 6967 2e66 6c61 785f  l = config.flax_
-00001480: 7072 6f66 696c 650a 0a0a 6465 6620 5f64  profile...def _d
-00001490: 6572 6976 655f 7072 6f66 696c 696e 675f  erive_profiling_
-000014a0: 6e61 6d65 286d 6f64 756c 652c 2066 6e29  name(module, fn)
-000014b0: 3a0a 2020 666e 5f6e 616d 6520 3d20 5f67  :.  fn_name = _g
-000014c0: 6574 5f66 6e5f 6e61 6d65 2866 6e29 0a20  et_fn_name(fn). 
-000014d0: 206d 6574 686f 645f 7375 6666 6978 203d   method_suffix =
-000014e0: 2066 272e 7b66 6e5f 6e61 6d65 7d27 2069   f'.{fn_name}' i
-000014f0: 6620 666e 5f6e 616d 6520 213d 2027 5f5f  f fn_name != '__
-00001500: 6361 6c6c 5f5f 2720 656c 7365 2027 270a  call__' else ''.
-00001510: 2020 6d6f 6475 6c65 5f6e 616d 6520 3d20    module_name = 
-00001520: 6d6f 6475 6c65 2e6e 616d 6520 6f72 206d  module.name or m
-00001530: 6f64 756c 652e 5f5f 636c 6173 735f 5f2e  odule.__class__.
-00001540: 5f5f 6e61 6d65 5f5f 0a20 2072 6574 7572  __name__.  retur
-00001550: 6e20 6627 7b6d 6f64 756c 655f 6e61 6d65  n f'{module_name
-00001560: 7d7b 6d65 7468 6f64 5f73 7566 6669 787d  }{method_suffix}
-00001570: 270a 0a0a 6465 6620 656e 6162 6c65 5f6e  '...def enable_n
-00001580: 616d 6564 5f63 616c 6c28 293a 0a20 2022  amed_call():.  "
-00001590: 2222 456e 6162 6c65 7320 6e61 6d65 6420  ""Enables named 
-000015a0: 6361 6c6c 2077 7261 7070 696e 6720 666f  call wrapping fo
-000015b0: 7220 6c61 6265 6c6c 696e 6720 7072 6f66  r labelling prof
-000015c0: 696c 6520 7472 6163 6573 2e0a 0a20 2057  ile traces...  W
-000015d0: 6865 6e20 6e61 6d65 6420 6361 6c6c 2077  hen named call w
-000015e0: 7261 7070 696e 6720 6973 2065 6e61 626c  rapping is enabl
-000015f0: 6564 2061 6c6c 204a 4158 206f 7073 2065  ed all JAX ops e
-00001600: 7865 6375 7465 6420 696e 2061 204d 6f64  xecuted in a Mod
-00001610: 756c 650a 2020 7769 6c6c 2062 6520 7275  ule.  will be ru
-00001620: 6e20 756e 6465 7220 6060 6a61 782e 6e61  n under ``jax.na
-00001630: 6d65 645f 7363 6f70 6560 602e 2054 6865  med_scope``. The
-00001640: 2060 604d 6f64 756c 6560 6020 636c 6173   ``Module`` clas
-00001650: 7320 6e61 6d65 2077 696c 6c0a 2020 7368  s name will.  sh
-00001660: 6f77 2075 7020 6172 6f75 6e64 2074 6865  ow up around the
-00001670: 206f 7065 7261 7469 6f6e 7320 6265 6c6f   operations belo
-00001680: 6e67 696e 6720 746f 2074 6861 7420 4d6f  nging to that Mo
-00001690: 6475 6c65 2069 6e20 7468 650a 2020 5465  dule in the.  Te
-000016a0: 6e73 6f72 626f 6172 6420 7072 6f66 696c  nsorboard profil
-000016b0: 696e 6720 5549 2c20 7369 6d70 6c69 6679  ing UI, simplify
-000016c0: 696e 6720 7468 6520 7072 6f66 696c 696e  ing the profilin
-000016d0: 6720 7072 6f63 6573 732e 0a0a 2020 4e6f  g process...  No
-000016e0: 7465 2074 6861 7420 6060 6a61 782e 6e61  te that ``jax.na
-000016f0: 6d65 645f 7363 6f70 6560 6020 6f6e 6c79  med_scope`` only
-00001700: 2077 6f72 6b73 2066 6f72 0a20 2063 6f6d   works for.  com
-00001710: 7069 6c65 6420 6675 6e63 7469 6f6e 7320  piled functions 
-00001720: 2865 2e67 2e3a 2075 7369 6e67 206a 6178  (e.g.: using jax
-00001730: 2e6a 6974 206f 7220 6a61 782e 706d 6170  .jit or jax.pmap
-00001740: 292e 0a20 2022 2222 0a20 2067 6c6f 6261  )..  """.  globa
-00001750: 6c20 5f75 7365 5f6e 616d 6564 5f63 616c  l _use_named_cal
-00001760: 6c0a 2020 5f75 7365 5f6e 616d 6564 5f63  l.  _use_named_c
-00001770: 616c 6c20 3d20 5472 7565 0a0a 0a64 6566  all = True...def
-00001780: 2064 6973 6162 6c65 5f6e 616d 6564 5f63   disable_named_c
-00001790: 616c 6c28 293a 0a20 2022 2222 4469 7361  all():.  """Disa
-000017a0: 626c 6573 206e 616d 6564 2063 616c 6c20  bles named call 
-000017b0: 7772 6170 7069 6e67 2e0a 0a20 2053 6565  wrapping...  See
-000017c0: 2060 6065 6e61 626c 655f 6e61 6d65 645f   ``enable_named_
-000017d0: 6361 6c6c 6060 0a20 2022 2222 0a20 2067  call``.  """.  g
-000017e0: 6c6f 6261 6c20 5f75 7365 5f6e 616d 6564  lobal _use_named
-000017f0: 5f63 616c 6c0a 2020 5f75 7365 5f6e 616d  _call.  _use_nam
-00001800: 6564 5f63 616c 6c20 3d20 4661 6c73 650a  ed_call = False.
-00001810: 0a0a 4063 6f6e 7465 7874 6c69 622e 636f  ..@contextlib.co
-00001820: 6e74 6578 746d 616e 6167 6572 0a64 6566  ntextmanager.def
-00001830: 206f 7665 7272 6964 655f 6e61 6d65 645f   override_named_
-00001840: 6361 6c6c 2865 6e61 626c 653a 2062 6f6f  call(enable: boo
-00001850: 6c20 3d20 5472 7565 293a 0a20 2023 2070  l = True):.  # p
-00001860: 796c 696e 743a 2064 6973 6162 6c65 3d67  ylint: disable=g
-00001870: 2d64 6f63 2d72 6574 7572 6e2d 6f72 2d79  -doc-return-or-y
-00001880: 6965 6c64 0a20 2022 2222 5265 7475 726e  ield.  """Return
-00001890: 7320 6120 636f 6e74 6578 7420 6d61 6e61  s a context mana
-000018a0: 6765 7220 7468 6174 2065 6e61 626c 6573  ger that enables
-000018b0: 2f64 6973 6162 6c65 7320 6e61 6d65 6420  /disables named 
-000018c0: 6361 6c6c 2077 7261 7070 696e 672e 0a0a  call wrapping...
-000018d0: 2020 4172 6773 3a0a 2020 2020 656e 6162    Args:.    enab
-000018e0: 6c65 3a20 4966 2074 7275 652c 2065 6e61  le: If true, ena
-000018f0: 626c 6573 206e 616d 6564 2063 616c 6c20  bles named call 
-00001900: 7772 6170 7069 6e67 2066 6f72 206c 6162  wrapping for lab
-00001910: 656c 6c69 6e67 2070 726f 6669 6c65 2074  elling profile t
-00001920: 7261 6365 732e 0a20 2020 2020 2028 7365  races..      (se
-00001930: 6520 6060 656e 6162 6c65 645f 6e61 6d65  e ``enabled_name
-00001940: 645f 6361 6c6c 6060 292e 0a20 2022 2222  d_call``)..  """
-00001950: 0a20 2023 2070 796c 696e 743a 2065 6e61  .  # pylint: ena
-00001960: 626c 653d 672d 646f 632d 7265 7475 726e  ble=g-doc-return
-00001970: 2d6f 722d 7969 656c 640a 2020 676c 6f62  -or-yield.  glob
-00001980: 616c 205f 7573 655f 6e61 6d65 645f 6361  al _use_named_ca
-00001990: 6c6c 0a20 2075 7365 5f6e 616d 6564 5f63  ll.  use_named_c
-000019a0: 616c 6c5f 7072 6576 203d 205f 7573 655f  all_prev = _use_
-000019b0: 6e61 6d65 645f 6361 6c6c 0a20 205f 7573  named_call.  _us
-000019c0: 655f 6e61 6d65 645f 6361 6c6c 203d 2065  e_named_call = e
-000019d0: 6e61 626c 650a 2020 7472 793a 0a20 2020  nable.  try:.   
-000019e0: 2079 6965 6c64 0a20 2066 696e 616c 6c79   yield.  finally
-000019f0: 3a0a 2020 2020 5f75 7365 5f6e 616d 6564  :.    _use_named
-00001a00: 5f63 616c 6c20 3d20 7573 655f 6e61 6d65  _call = use_name
-00001a10: 645f 6361 6c6c 5f70 7265 760a 0a0a 2320  d_call_prev...# 
-00001a20: 496e 7465 7263 6570 7420 6d6f 6475 6c65  Intercept module
-00001a30: 206d 6574 686f 6473 2e0a 2320 2d2d 2d2d   methods..# ----
-00001a40: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001a50: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001a60: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001a70: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00001a80: 2d2d 2d2d 2d2d 2d2d 2d0a 4064 6174 6163  ---------.@datac
-00001a90: 6c61 7373 6573 2e64 6174 6163 6c61 7373  lasses.dataclass
-00001aa0: 2866 726f 7a65 6e3d 5472 7565 290a 636c  (frozen=True).cl
-00001ab0: 6173 7320 496e 7465 7263 6570 746f 7243  ass InterceptorC
-00001ac0: 6f6e 7465 7874 3a0a 2020 2222 2252 6561  ontext:.  """Rea
-00001ad0: 6420 6f6e 6c79 2073 7461 7465 2073 686f  d only state sho
-00001ae0: 7769 6e67 2074 6865 2063 616c 6c69 6e67  wing the calling
-00001af0: 2063 6f6e 7465 7874 2066 6f72 206d 6574   context for met
-00001b00: 686f 6420 696e 7465 7263 6570 746f 7273  hod interceptors
-00001b10: 2e0a 0a20 2041 7474 7269 6275 7465 733a  ...  Attributes:
-00001b20: 0a20 2020 206d 6f64 756c 653a 2054 6865  .    module: The
-00001b30: 204d 6f64 756c 6520 696e 7374 616e 6365   Module instance
-00001b40: 2077 686f 7365 206d 6574 686f 6420 6973   whose method is
-00001b50: 2062 6569 6e67 2063 616c 6c65 642e 0a20   being called.. 
-00001b60: 2020 206d 6574 686f 645f 6e61 6d65 3a20     method_name: 
-00001b70: 5468 6520 6e61 6d65 206f 6620 7468 6520  The name of the 
-00001b80: 6d65 7468 6f64 2062 6569 6e67 2063 616c  method being cal
-00001b90: 6c65 6420 6f6e 2074 6865 206d 6f64 756c  led on the modul
-00001ba0: 652e 0a20 2020 206f 7269 675f 6d65 7468  e..    orig_meth
-00001bb0: 6f64 3a20 5468 6520 6f72 6967 696e 616c  od: The original
-00001bc0: 206d 6574 686f 6420 6465 6669 6e65 6420   method defined 
-00001bd0: 6f6e 2074 6865 206d 6f64 756c 652e 2043  on the module. C
-00001be0: 616c 6c69 6e67 2069 7420 7769 6c6c 0a20  alling it will. 
-00001bf0: 2020 2020 2073 686f 7274 2063 6972 6375       short circu
-00001c00: 6974 2061 6c6c 206f 7468 6572 2069 6e74  it all other int
-00001c10: 6572 6365 7074 6f72 732e 0a20 2022 2222  erceptors..  """
-00001c20: 0a0a 2020 6d6f 6475 6c65 3a20 274d 6f64  ..  module: 'Mod
-00001c30: 756c 6527 0a20 206d 6574 686f 645f 6e61  ule'.  method_na
-00001c40: 6d65 3a20 7374 720a 2020 6f72 6967 5f6d  me: str.  orig_m
-00001c50: 6574 686f 643a 2043 616c 6c61 626c 655b  ethod: Callable[
-00001c60: 2e2e 2e2c 2041 6e79 5d0a 0a0a 636c 6173  ..., Any]...clas
-00001c70: 7320 5468 7265 6164 4c6f 6361 6c53 7461  s ThreadLocalSta
-00001c80: 636b 2874 6872 6561 6469 6e67 2e6c 6f63  ck(threading.loc
-00001c90: 616c 293a 0a20 2022 2222 5468 7265 6164  al):.  """Thread
-00001ca0: 2d6c 6f63 616c 2073 7461 636b 2e22 2222  -local stack."""
-00001cb0: 0a0a 2020 6465 6620 5f5f 696e 6974 5f5f  ..  def __init__
-00001cc0: 2873 656c 6629 3a0a 2020 2020 7365 6c66  (self):.    self
-00001cd0: 2e5f 7374 6f72 6167 6520 3d20 5b5d 0a0a  ._storage = []..
-00001ce0: 2020 6465 6620 7075 7368 2873 656c 662c    def push(self,
-00001cf0: 2065 6c65 6d3a 2041 6e79 2920 2d3e 204e   elem: Any) -> N
-00001d00: 6f6e 653a 0a20 2020 2073 656c 662e 5f73  one:.    self._s
-00001d10: 746f 7261 6765 2e61 7070 656e 6428 656c  torage.append(el
-00001d20: 656d 290a 0a20 2064 6566 2070 6f70 2873  em)..  def pop(s
-00001d30: 656c 6629 202d 3e20 416e 793a 0a20 2020  elf) -> Any:.   
-00001d40: 2072 6574 7572 6e20 7365 6c66 2e5f 7374   return self._st
-00001d50: 6f72 6167 652e 706f 7028 290a 0a20 2064  orage.pop()..  d
-00001d60: 6566 205f 5f69 7465 725f 5f28 7365 6c66  ef __iter__(self
-00001d70: 2920 2d3e 2049 7465 7261 746f 725b 416e  ) -> Iterator[An
-00001d80: 795d 3a0a 2020 2020 7265 7475 726e 2069  y]:.    return i
-00001d90: 7465 7228 7265 7665 7273 6564 2873 656c  ter(reversed(sel
-00001da0: 662e 5f73 746f 7261 6765 2929 0a0a 2020  f._storage))..  
-00001db0: 6465 6620 5f5f 6c65 6e5f 5f28 7365 6c66  def __len__(self
-00001dc0: 2920 2d3e 2069 6e74 3a0a 2020 2020 7265  ) -> int:.    re
-00001dd0: 7475 726e 206c 656e 2873 656c 662e 5f73  turn len(self._s
-00001de0: 746f 7261 6765 290a 0a20 2064 6566 205f  torage)..  def _
-00001df0: 5f72 6570 725f 5f28 7365 6c66 2920 2d3e  _repr__(self) ->
-00001e00: 2073 7472 3a0a 2020 2020 7265 7475 726e   str:.    return
-00001e10: 2066 277b 7365 6c66 2e5f 5f63 6c61 7373   f'{self.__class
-00001e20: 5f5f 2e5f 5f6e 616d 655f 5f7d 287b 7365  __.__name__}({se
-00001e30: 6c66 2e5f 7374 6f72 6167 657d 2927 0a0a  lf._storage})'..
-00001e40: 0a41 7267 7320 3d20 5475 706c 655b 416e  .Args = Tuple[An
-00001e50: 795d 0a4b 7761 7267 7320 3d20 4469 6374  y].Kwargs = Dict
-00001e60: 5b73 7472 2c20 416e 795d 0a4e 6578 7447  [str, Any].NextG
-00001e70: 6574 7465 7220 3d20 4361 6c6c 6162 6c65  etter = Callable
-00001e80: 5b2e 2e2e 2c20 416e 795d 0a49 6e74 6572  [..., Any].Inter
-00001e90: 6365 7074 6f72 203d 2043 616c 6c61 626c  ceptor = Callabl
-00001ea0: 655b 5b4e 6578 7447 6574 7465 722c 2041  e[[NextGetter, A
-00001eb0: 7267 732c 204b 7761 7267 732c 2049 6e74  rgs, Kwargs, Int
-00001ec0: 6572 6365 7074 6f72 436f 6e74 6578 745d  erceptorContext]
-00001ed0: 2c20 416e 795d 0a5f 676c 6f62 616c 5f69  , Any]._global_i
-00001ee0: 6e74 6572 6365 7074 6f72 5f73 7461 636b  nterceptor_stack
-00001ef0: 203d 2054 6872 6561 644c 6f63 616c 5374   = ThreadLocalSt
-00001f00: 6163 6b28 290a 0a0a 4063 6f6e 7465 7874  ack()...@context
-00001f10: 6c69 622e 636f 6e74 6578 746d 616e 6167  lib.contextmanag
-00001f20: 6572 0a64 6566 2069 6e74 6572 6365 7074  er.def intercept
-00001f30: 5f6d 6574 686f 6473 2869 6e74 6572 6365  _methods(interce
-00001f40: 7074 6f72 3a20 496e 7465 7263 6570 746f  ptor: Intercepto
-00001f50: 7229 3a0a 2020 2320 7079 6c69 6e74 3a20  r):.  # pylint: 
-00001f60: 6469 7361 626c 653d 672d 646f 632d 7265  disable=g-doc-re
-00001f70: 7475 726e 2d6f 722d 7969 656c 640a 2020  turn-or-yield.  
-00001f80: 7222 2222 5265 6769 7374 6572 7320 6120  r"""Registers a 
-00001f90: 6e65 7720 6d65 7468 6f64 2069 6e74 6572  new method inter
-00001fa0: 6365 7074 6f72 2e0a 0a20 204d 6574 686f  ceptor...  Metho
-00001fb0: 6420 696e 7465 7263 6570 746f 7273 2061  d interceptors a
-00001fc0: 6c6c 6f77 2079 6f75 2074 6f20 2861 7420  llow you to (at 
-00001fd0: 6120 6469 7374 616e 6365 2920 696e 7465  a distance) inte
-00001fe0: 7263 6570 7420 6d65 7468 6f64 2063 616c  rcept method cal
-00001ff0: 6c73 2074 6f0a 2020 6d6f 6475 6c65 732e  ls to.  modules.
-00002000: 2049 7420 776f 726b 7320 7369 6d69 6c61   It works simila
-00002010: 726c 7920 746f 2064 6563 6f72 6174 6f72  rly to decorator
-00002020: 732e 2059 6f75 2063 6f75 6c64 206d 6f64  s. You could mod
-00002030: 6966 7920 6172 6773 2f6b 7761 7267 7320  ify args/kwargs 
-00002040: 6265 666f 7265 0a20 2063 616c 6c69 6e67  before.  calling
-00002050: 2074 6865 2075 6e64 6572 6c79 696e 6720   the underlying 
-00002060: 6d65 7468 6f64 2061 6e64 2f6f 7220 6d6f  method and/or mo
-00002070: 6469 6679 2074 6865 2072 6573 756c 7420  dify the result 
-00002080: 7265 7475 726e 696e 6720 6672 6f6d 2063  returning from c
-00002090: 616c 6c69 6e67 0a20 2074 6865 2075 6e64  alling.  the und
-000020a0: 6572 6c79 696e 6720 6d65 7468 6f64 2e20  erlying method. 
-000020b0: 4f72 2079 6f75 2063 6f75 6c64 2063 6f6d  Or you could com
-000020c0: 706c 6574 656c 7920 736b 6970 2063 616c  pletely skip cal
-000020d0: 6c69 6e67 2074 6865 2075 6e64 6572 6c79  ling the underly
-000020e0: 696e 670a 2020 6d65 7468 6f64 2061 6e64  ing.  method and
-000020f0: 2064 6563 6964 6520 746f 2064 6f20 736f   decide to do so
-00002100: 6d65 7468 696e 6720 6469 6666 6572 656e  mething differen
-00002110: 746c 792e 2020 466f 7220 6578 616d 706c  tly.  For exampl
-00002120: 653a 3a0a 0a20 2020 203e 3e3e 2069 6d70  e::..    >>> imp
-00002130: 6f72 7420 666c 6178 2e6c 696e 656e 2061  ort flax.linen a
-00002140: 7320 6e6e 0a20 2020 203e 3e3e 2069 6d70  s nn.    >>> imp
-00002150: 6f72 7420 6a61 782e 6e75 6d70 7920 6173  ort jax.numpy as
-00002160: 206a 6e70 0a20 2020 202e 2e2e 0a20 2020   jnp.    ....   
-00002170: 203e 3e3e 2063 6c61 7373 2046 6f6f 286e   >>> class Foo(n
-00002180: 6e2e 4d6f 6475 6c65 293a 0a20 2020 202e  n.Module):.    .
-00002190: 2e2e 2020 2064 6566 205f 5f63 616c 6c5f  ..   def __call_
-000021a0: 5f28 7365 6c66 2c20 7829 3a0a 2020 2020  _(self, x):.    
-000021b0: 2e2e 2e20 2020 2020 7265 7475 726e 2078  ...     return x
-000021c0: 0a20 2020 202e 2e2e 0a20 2020 203e 3e3e  .    ....    >>>
-000021d0: 2064 6566 206d 795f 696e 7465 7263 6570   def my_intercep
-000021e0: 746f 7231 286e 6578 745f 6675 6e2c 2061  tor1(next_fun, a
-000021f0: 7267 732c 206b 7761 7267 732c 2063 6f6e  rgs, kwargs, con
-00002200: 7465 7874 293a 0a20 2020 202e 2e2e 2020  text):.    ...  
-00002210: 2070 7269 6e74 2827 6361 6c6c 696e 6720   print('calling 
-00002220: 6d79 5f69 6e74 6572 6365 7074 6f72 3127  my_interceptor1'
-00002230: 290a 2020 2020 2e2e 2e20 2020 7265 7475  ).    ...   retu
-00002240: 726e 206e 6578 745f 6675 6e28 2a61 7267  rn next_fun(*arg
-00002250: 732c 202a 2a6b 7761 7267 7329 0a20 2020  s, **kwargs).   
-00002260: 202e 2e2e 0a20 2020 203e 3e3e 2066 6f6f   ....    >>> foo
-00002270: 203d 2046 6f6f 2829 0a20 2020 203e 3e3e   = Foo().    >>>
-00002280: 2077 6974 6820 6e6e 2e69 6e74 6572 6365   with nn.interce
-00002290: 7074 5f6d 6574 686f 6473 286d 795f 696e  pt_methods(my_in
-000022a0: 7465 7263 6570 746f 7231 293a 0a20 2020  terceptor1):.   
-000022b0: 202e 2e2e 2020 205f 203d 2066 6f6f 286a   ...   _ = foo(j
-000022c0: 6e70 2e6f 6e65 7328 5b31 5d29 290a 2020  np.ones([1])).  
-000022d0: 2020 6361 6c6c 696e 6720 6d79 5f69 6e74    calling my_int
-000022e0: 6572 6365 7074 6f72 310a 0a20 2059 6f75  erceptor1..  You
-000022f0: 2063 6f75 6c64 2061 6c73 6f20 7265 6769   could also regi
-00002300: 7374 6572 206d 756c 7469 706c 6520 696e  ster multiple in
-00002310: 7465 7263 6570 746f 7273 206f 6e20 7468  terceptors on th
-00002320: 6520 7361 6d65 206d 6574 686f 642e 2049  e same method. I
-00002330: 6e74 6572 6365 7074 6f72 730a 2020 7769  nterceptors.  wi
-00002340: 6c6c 2072 756e 2069 6e20 6f72 6465 722e  ll run in order.
-00002350: 2046 6f72 2065 7861 6d70 6c65 3a3a 0a0a   For example::..
-00002360: 2020 2020 3e3e 3e20 6465 6620 6d79 5f69      >>> def my_i
-00002370: 6e74 6572 6365 7074 6f72 3228 6e65 7874  nterceptor2(next
-00002380: 5f66 756e 2c20 6172 6773 2c20 6b77 6172  _fun, args, kwar
-00002390: 6773 2c20 636f 6e74 6578 7429 3a0a 2020  gs, context):.  
-000023a0: 2020 2e2e 2e20 2020 7072 696e 7428 2763    ...   print('c
-000023b0: 616c 6c69 6e67 206d 795f 696e 7465 7263  alling my_interc
-000023c0: 6570 746f 7232 2729 0a20 2020 202e 2e2e  eptor2').    ...
-000023d0: 2020 2072 6574 7572 6e20 6e65 7874 5f66     return next_f
-000023e0: 756e 282a 6172 6773 2c20 2a2a 6b77 6172  un(*args, **kwar
-000023f0: 6773 290a 2020 2020 2e2e 2e0a 2020 2020  gs).    ....    
-00002400: 3e3e 3e20 7769 7468 206e 6e2e 696e 7465  >>> with nn.inte
-00002410: 7263 6570 745f 6d65 7468 6f64 7328 6d79  rcept_methods(my
-00002420: 5f69 6e74 6572 6365 7074 6f72 3129 2c20  _interceptor1), 
-00002430: 5c0a 2020 2020 2e2e 2e20 2020 2020 206e  \.    ...      n
-00002440: 6e2e 696e 7465 7263 6570 745f 6d65 7468  n.intercept_meth
-00002450: 6f64 7328 6d79 5f69 6e74 6572 6365 7074  ods(my_intercept
-00002460: 6f72 3229 3a0a 2020 2020 2e2e 2e20 2020  or2):.    ...   
-00002470: 5f20 3d20 666f 6f28 6a6e 702e 6f6e 6573  _ = foo(jnp.ones
-00002480: 285b 315d 2929 0a20 2020 2063 616c 6c69  ([1])).    calli
-00002490: 6e67 206d 795f 696e 7465 7263 6570 746f  ng my_intercepto
-000024a0: 7231 0a20 2020 2063 616c 6c69 6e67 206d  r1.    calling m
-000024b0: 795f 696e 7465 7263 6570 746f 7232 0a0a  y_interceptor2..
-000024c0: 2020 596f 7520 636f 756c 6420 736b 6970    You could skip
-000024d0: 206f 7468 6572 2069 6e74 6572 6365 7074   other intercept
-000024e0: 6f72 7320 6279 2064 6972 6563 746c 7920  ors by directly 
-000024f0: 6361 6c6c 696e 6720 7468 650a 2020 6060  calling the.  ``
-00002500: 636f 6e74 6578 742e 6f72 6967 5f6d 6574  context.orig_met
-00002510: 686f 6460 602e 2046 6f72 2065 7861 6d70  hod``. For examp
-00002520: 6c65 3a3a 0a0a 2020 2020 3e3e 3e20 6465  le::..    >>> de
-00002530: 6620 6d79 5f69 6e74 6572 6365 7074 6f72  f my_interceptor
-00002540: 3328 6e65 7874 5f66 756e 2c20 6172 6773  3(next_fun, args
-00002550: 2c20 6b77 6172 6773 2c20 636f 6e74 6578  , kwargs, contex
-00002560: 7429 3a0a 2020 2020 2e2e 2e20 2020 7072  t):.    ...   pr
-00002570: 696e 7428 2763 616c 6c69 6e67 206d 795f  int('calling my_
-00002580: 696e 7465 7263 6570 746f 7233 2729 0a20  interceptor3'). 
-00002590: 2020 202e 2e2e 2020 2072 6574 7572 6e20     ...   return 
-000025a0: 636f 6e74 6578 742e 6f72 6967 5f6d 6574  context.orig_met
-000025b0: 686f 6428 2a61 7267 732c 202a 2a6b 7761  hod(*args, **kwa
-000025c0: 7267 7329 0a20 2020 203e 3e3e 2077 6974  rgs).    >>> wit
-000025d0: 6820 6e6e 2e69 6e74 6572 6365 7074 5f6d  h nn.intercept_m
-000025e0: 6574 686f 6473 286d 795f 696e 7465 7263  ethods(my_interc
-000025f0: 6570 746f 7233 292c 205c 0a20 2020 202e  eptor3), \.    .
-00002600: 2e2e 2020 2020 2020 6e6e 2e69 6e74 6572  ..      nn.inter
-00002610: 6365 7074 5f6d 6574 686f 6473 286d 795f  cept_methods(my_
-00002620: 696e 7465 7263 6570 746f 7231 292c 205c  interceptor1), \
-00002630: 0a20 2020 202e 2e2e 2020 2020 2020 6e6e  .    ...      nn
-00002640: 2e69 6e74 6572 6365 7074 5f6d 6574 686f  .intercept_metho
-00002650: 6473 286d 795f 696e 7465 7263 6570 746f  ds(my_intercepto
-00002660: 7232 293a 0a20 2020 202e 2e2e 2020 205f  r2):.    ...   _
-00002670: 203d 2066 6f6f 286a 6e70 2e6f 6e65 7328   = foo(jnp.ones(
-00002680: 5b31 5d29 290a 2020 2020 6361 6c6c 696e  [1])).    callin
-00002690: 6720 6d79 5f69 6e74 6572 6365 7074 6f72  g my_interceptor
-000026a0: 330a 0a20 2054 6865 2066 6f6c 6c6f 7769  3..  The followi
-000026b0: 6e67 206d 6574 686f 6473 2063 6f75 6c64  ng methods could
-000026c0: 6e27 7420 6265 2069 6e74 6572 6365 7074  n't be intercept
-000026d0: 6564 3a0a 0a20 2031 2e20 4d65 7468 6f64  ed:..  1. Method
-000026e0: 7320 6465 636f 7261 746f 7265 6420 7769  s decoratored wi
-000026f0: 7468 2060 606e 6e2e 6e6f 7772 6170 6060  th ``nn.nowrap``
-00002700: 2e0a 2020 322e 2044 756e 6465 7220 6d65  ..  2. Dunder me
-00002710: 7468 6f64 7320 696e 636c 7564 696e 6720  thods including 
-00002720: 6060 5f5f 6571 5f5f 6060 2c20 6060 5f5f  ``__eq__``, ``__
-00002730: 7265 7072 5f5f 6060 2c20 6060 5f5f 696e  repr__``, ``__in
-00002740: 6974 5f5f 6060 2c20 6060 5f5f 6861 7368  it__``, ``__hash
-00002750: 5f5f 6060 2c20 616e 6420 6060 5f5f 706f  __``, and ``__po
-00002760: 7374 5f69 6e69 745f 5f60 602e 0a20 2033  st_init__``..  3
-00002770: 2e20 4d6f 6475 6c65 2064 6174 6163 6c61  . Module datacla
-00002780: 7373 2066 6965 6c64 732e 0a20 2034 2e20  ss fields..  4. 
-00002790: 4d6f 6475 6c65 2064 6573 6372 6970 746f  Module descripto
-000027a0: 7273 2e0a 0a20 2041 7267 733a 0a20 2020  rs...  Args:.   
-000027b0: 2069 6e74 6572 6365 7074 6f72 3a20 4120   interceptor: A 
-000027c0: 6d65 7468 6f64 2069 6e74 6572 6365 7074  method intercept
-000027d0: 6f72 2e0a 2020 2222 220a 2020 5f67 6c6f  or..  """.  _glo
-000027e0: 6261 6c5f 696e 7465 7263 6570 746f 725f  bal_interceptor_
-000027f0: 7374 6163 6b2e 7075 7368 2869 6e74 6572  stack.push(inter
-00002800: 6365 7074 6f72 290a 2020 7472 793a 0a20  ceptor).  try:. 
-00002810: 2020 2079 6965 6c64 0a20 2066 696e 616c     yield.  final
-00002820: 6c79 3a0a 2020 2020 6173 7365 7274 205f  ly:.    assert _
-00002830: 676c 6f62 616c 5f69 6e74 6572 6365 7074  global_intercept
-00002840: 6f72 5f73 7461 636b 2e70 6f70 2829 2069  or_stack.pop() i
-00002850: 7320 696e 7465 7263 6570 746f 720a 0a0a  s interceptor...
-00002860: 6465 6620 7275 6e5f 696e 7465 7263 6570  def run_intercep
-00002870: 746f 7273 280a 2020 6f72 6967 5f6d 6574  tors(.  orig_met
-00002880: 686f 643a 2043 616c 6c61 626c 655b 2e2e  hod: Callable[..
-00002890: 2e2c 2041 6e79 5d2c 0a20 206d 6f64 756c  ., Any],.  modul
-000028a0: 653a 2027 4d6f 6475 6c65 272c 0a20 202a  e: 'Module',.  *
-000028b0: 6172 6773 2c0a 2020 2a2a 6b77 6172 6773  args,.  **kwargs
-000028c0: 2c0a 2920 2d3e 2041 6e79 3a0a 2020 2222  ,.) -> Any:.  ""
-000028d0: 2252 756e 7320 6d65 7468 6f64 2069 6e74  "Runs method int
-000028e0: 6572 6365 7074 6f72 732e 2222 220a 2020  erceptors.""".  
-000028f0: 6d65 7468 6f64 5f6e 616d 6520 3d20 5f67  method_name = _g
-00002900: 6574 5f66 6e5f 6e61 6d65 286f 7269 675f  et_fn_name(orig_
-00002910: 6d65 7468 6f64 290a 2020 6675 6e20 3d20  method).  fun = 
-00002920: 6675 6e63 746f 6f6c 732e 7061 7274 6961  functools.partia
-00002930: 6c28 6f72 6967 5f6d 6574 686f 642c 206d  l(orig_method, m
-00002940: 6f64 756c 6529 0a20 2063 6f6e 7465 7874  odule).  context
-00002950: 203d 2049 6e74 6572 6365 7074 6f72 436f   = InterceptorCo
-00002960: 6e74 6578 7428 6d6f 6475 6c65 2c20 6d65  ntext(module, me
-00002970: 7468 6f64 5f6e 616d 652c 2066 756e 290a  thod_name, fun).
-00002980: 0a20 2064 6566 2077 7261 705f 696e 7465  .  def wrap_inte
-00002990: 7263 6570 746f 7228 696e 7465 7263 6570  rceptor(intercep
-000029a0: 746f 722c 2066 756e 293a 0a20 2020 2022  tor, fun):.    "
-000029b0: 2222 5772 6170 7320 6066 756e 6020 7769  ""Wraps `fun` wi
-000029c0: 7468 2060 696e 7465 7263 6570 746f 7260  th `interceptor`
-000029d0: 2e22 2222 0a0a 2020 2020 4066 756e 6374  ."""..    @funct
-000029e0: 6f6f 6c73 2e77 7261 7073 2866 756e 290a  ools.wraps(fun).
-000029f0: 2020 2020 6465 6620 7772 6170 7065 6428      def wrapped(
-00002a00: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
-00002a10: 3a0a 2020 2020 2020 7265 7475 726e 2069  :.      return i
-00002a20: 6e74 6572 6365 7074 6f72 2866 756e 2c20  nterceptor(fun, 
-00002a30: 6172 6773 2c20 6b77 6172 6773 2c20 636f  args, kwargs, co
-00002a40: 6e74 6578 7429 0a0a 2020 2020 7265 7475  ntext)..    retu
-00002a50: 726e 2077 7261 7070 6564 0a0a 2020 2320  rn wrapped..  # 
-00002a60: 5772 6170 7320 696e 7465 7263 6570 746f  Wraps intercepto
-00002a70: 7273 2061 726f 756e 6420 7468 6520 6f72  rs around the or
-00002a80: 6967 696e 616c 206d 6574 686f 642e 2054  iginal method. T
-00002a90: 6865 2069 6e6e 6572 6d6f 7374 2069 6e74  he innermost int
-00002aa0: 6572 6365 7074 6f72 2069 730a 2020 2320  erceptor is.  # 
-00002ab0: 7468 6520 6c61 7374 206f 6e65 2061 6464  the last one add
-00002ac0: 6564 2061 6e64 2064 6972 6563 746c 7920  ed and directly 
-00002ad0: 7772 6170 7065 6420 6172 6f75 6e64 2074  wrapped around t
-00002ae0: 6865 206f 7269 6769 6e61 6c20 626f 756e  he original boun
-00002af0: 6420 6d65 7468 6f64 2e0a 2020 666f 7220  d method..  for 
-00002b00: 696e 7465 7263 6570 746f 7220 696e 205f  interceptor in _
-00002b10: 676c 6f62 616c 5f69 6e74 6572 6365 7074  global_intercept
-00002b20: 6f72 5f73 7461 636b 3a0a 2020 2020 6675  or_stack:.    fu
-00002b30: 6e20 3d20 7772 6170 5f69 6e74 6572 6365  n = wrap_interce
-00002b40: 7074 6f72 2869 6e74 6572 6365 7074 6f72  ptor(interceptor
-00002b50: 2c20 6675 6e29 0a20 2072 6574 7572 6e20  , fun).  return 
-00002b60: 6675 6e28 2a61 7267 732c 202a 2a6b 7761  fun(*args, **kwa
-00002b70: 7267 7329 0a0a 0a23 2055 7469 6c69 7469  rgs)...# Utiliti
-00002b80: 6573 2066 6f72 2070 7974 7265 6573 206f  es for pytrees o
-00002b90: 6620 4d6f 6475 6c65 7320 6465 6669 6e65  f Modules define
-00002ba0: 6420 696e 7369 6465 2073 6574 7570 2829  d inside setup()
-00002bb0: 0a23 202d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  .# -------------
-00002bc0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00002bd0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00002be0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00002bf0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00002c00: 0a0a 0a64 6566 205f 736f 7274 6564 5f69  ...def _sorted_i
-00002c10: 7465 6d73 2878 293a 0a20 2022 2222 5265  tems(x):.  """Re
-00002c20: 7475 726e 7320 6974 656d 7320 6f66 2061  turns items of a
-00002c30: 2064 6963 7420 6f72 6465 7265 6420 6279   dict ordered by
-00002c40: 206b 6579 732e 2222 220a 2020 7265 7475   keys.""".  retu
-00002c50: 726e 2073 6f72 7465 6428 782e 6974 656d  rn sorted(x.item
-00002c60: 7328 292c 206b 6579 3d6c 616d 6264 6120  s(), key=lambda 
-00002c70: 783a 2078 5b30 5d29 0a0a 0a64 6566 205f  x: x[0])...def _
-00002c80: 6765 745f 7375 6666 6978 5f76 616c 7565  get_suffix_value
-00002c90: 5f70 6169 7273 280a 2020 7472 6565 5f6f  _pairs(.  tree_o
-00002ca0: 725f 6c65 6166 3a20 416e 792c 0a29 202d  r_leaf: Any,.) -
-00002cb0: 3e20 4c69 7374 5b54 7570 6c65 5b73 7472  > List[Tuple[str
-00002cc0: 2c20 5479 7065 5b27 4d6f 6475 6c65 275d  , Type['Module']
-00002cd0: 5d5d 3a0a 2020 2222 2248 656c 7065 7220  ]]:.  """Helper 
-00002ce0: 666f 7220 6e61 6d69 6e67 2070 7974 7265  for naming pytre
-00002cf0: 6573 206f 6620 7375 626d 6f64 756c 6573  es of submodules
-00002d00: 2e22 2222 0a20 2064 6963 745f 6f72 5f6c  .""".  dict_or_l
-00002d10: 6561 6620 3d20 7365 7269 616c 697a 6174  eaf = serializat
-00002d20: 696f 6e2e 746f 5f73 7461 7465 5f64 6963  ion.to_state_dic
-00002d30: 7428 7472 6565 5f6f 725f 6c65 6166 290a  t(tree_or_leaf).
-00002d40: 2020 6966 206e 6f74 2069 7369 6e73 7461    if not isinsta
-00002d50: 6e63 6528 6469 6374 5f6f 725f 6c65 6166  nce(dict_or_leaf
-00002d60: 2c20 6469 6374 2920 6f72 206e 6f74 2064  , dict) or not d
-00002d70: 6963 745f 6f72 5f6c 6561 663a 0a20 2020  ict_or_leaf:.   
-00002d80: 2072 6574 7572 6e20 5b28 2727 2c20 7472   return [('', tr
-00002d90: 6565 5f6f 725f 6c65 6166 295d 0a20 2065  ee_or_leaf)].  e
-00002da0: 6c73 653a 0a20 2020 2066 6c61 745f 6469  lse:.    flat_di
-00002db0: 6374 203d 2074 7261 7665 7273 655f 7574  ct = traverse_ut
-00002dc0: 696c 2e66 6c61 7474 656e 5f64 6963 7428  il.flatten_dict(
-00002dd0: 6469 6374 5f6f 725f 6c65 6166 290a 2020  dict_or_leaf).  
-00002de0: 2020 7265 7475 726e 205b 2827 5f27 202b    return [('_' +
-00002df0: 2027 5f27 2e6a 6f69 6e28 6b29 2c20 7629   '_'.join(k), v)
-00002e00: 2066 6f72 206b 2c20 7620 696e 205f 736f   for k, v in _so
-00002e10: 7274 6564 5f69 7465 6d73 2866 6c61 745f  rted_items(flat_
-00002e20: 6469 6374 295d 0a0a 0a64 6566 205f 6d61  dict)]...def _ma
-00002e30: 705f 6f76 6572 5f6d 6f64 756c 6573 5f69  p_over_modules_i
-00002e40: 6e5f 7472 6565 2866 6e2c 2074 7265 655f  n_tree(fn, tree_
-00002e50: 6f72 5f6c 6561 6629 3a0a 2020 2222 2248  or_leaf):.  """H
-00002e60: 656c 7065 7220 666f 7220 6d61 7070 696e  elper for mappin
-00002e70: 6720 6675 6e63 7469 6f6e 206f 7665 7220  g function over 
-00002e80: 7375 626d 6f64 756c 6573 2e22 2222 0a20  submodules.""". 
-00002e90: 2064 6963 745f 6f72 5f6c 6561 6620 3d20   dict_or_leaf = 
-00002ea0: 7365 7269 616c 697a 6174 696f 6e2e 746f  serialization.to
-00002eb0: 5f73 7461 7465 5f64 6963 7428 7472 6565  _state_dict(tree
-00002ec0: 5f6f 725f 6c65 6166 290a 2020 6966 206e  _or_leaf).  if n
-00002ed0: 6f74 2069 7369 6e73 7461 6e63 6528 6469  ot isinstance(di
-00002ee0: 6374 5f6f 725f 6c65 6166 2c20 6469 6374  ct_or_leaf, dict
-00002ef0: 2920 6f72 206e 6f74 2064 6963 745f 6f72  ) or not dict_or
-00002f00: 5f6c 6561 663a 0a20 2020 2072 6574 7572  _leaf:.    retur
-00002f10: 6e20 666e 2827 272c 2074 7265 655f 6f72  n fn('', tree_or
-00002f20: 5f6c 6561 6629 0a20 2065 6c73 653a 0a20  _leaf).  else:. 
-00002f30: 2020 2066 6c61 745f 6469 6374 203d 2074     flat_dict = t
-00002f40: 7261 7665 7273 655f 7574 696c 2e66 6c61  raverse_util.fla
-00002f50: 7474 656e 5f64 6963 7428 6469 6374 5f6f  tten_dict(dict_o
-00002f60: 725f 6c65 6166 2c20 6b65 6570 5f65 6d70  r_leaf, keep_emp
-00002f70: 7479 5f6e 6f64 6573 3d54 7275 6529 0a20  ty_nodes=True). 
-00002f80: 2020 206d 6170 7065 645f 666c 6174 5f64     mapped_flat_d
-00002f90: 6963 7420 3d20 7b0a 2020 2020 2020 6b3a  ict = {.      k:
-00002fa0: 2066 6e28 275f 2720 2b20 275f 272e 6a6f   fn('_' + '_'.jo
-00002fb0: 696e 286b 292c 2076 2920 666f 7220 6b2c  in(k), v) for k,
-00002fc0: 2076 2069 6e20 5f73 6f72 7465 645f 6974   v in _sorted_it
-00002fd0: 656d 7328 666c 6174 5f64 6963 7429 0a20  ems(flat_dict). 
-00002fe0: 2020 207d 0a20 2020 2072 6574 7572 6e20     }.    return 
-00002ff0: 7365 7269 616c 697a 6174 696f 6e2e 6672  serialization.fr
-00003000: 6f6d 5f73 7461 7465 5f64 6963 7428 0a20  om_state_dict(. 
-00003010: 2020 2020 2074 7265 655f 6f72 5f6c 6561       tree_or_lea
-00003020: 662c 2074 7261 7665 7273 655f 7574 696c  f, traverse_util
-00003030: 2e75 6e66 6c61 7474 656e 5f64 6963 7428  .unflatten_dict(
-00003040: 6d61 7070 6564 5f66 6c61 745f 6469 6374  mapped_flat_dict
-00003050: 290a 2020 2020 290a 0a0a 6465 6620 5f66  ).    )...def _f
-00003060: 7265 657a 655f 6174 7472 2876 616c 3a20  reeze_attr(val: 
-00003070: 416e 7929 202d 3e20 416e 793a 0a20 2022  Any) -> Any:.  "
-00003080: 2222 5265 6375 7273 6976 656c 7920 7772  ""Recursively wr
-00003090: 6170 2074 6865 2067 6976 656e 2061 7474  ap the given att
-000030a0: 7269 6275 7465 2060 7661 7260 2069 6e20  ribute `var` in 
-000030b0: 6060 4672 6f7a 656e 4469 6374 6060 2e22  ``FrozenDict``."
-000030c0: 2222 0a20 2069 6620 6973 696e 7374 616e  "".  if isinstan
-000030d0: 6365 2876 616c 2c20 2864 6963 742c 2046  ce(val, (dict, F
-000030e0: 726f 7a65 6e44 6963 7429 293a 0a20 2020  rozenDict)):.   
-000030f0: 2072 6574 7572 6e20 4672 6f7a 656e 4469   return FrozenDi
-00003100: 6374 287b 6b3a 205f 6672 6565 7a65 5f61  ct({k: _freeze_a
-00003110: 7474 7228 7629 2066 6f72 206b 2c20 7620  ttr(v) for k, v 
-00003120: 696e 2076 616c 2e69 7465 6d73 2829 7d29  in val.items()})
-00003130: 0a20 2065 6c69 6620 6973 696e 7374 616e  .  elif isinstan
-00003140: 6365 2876 616c 2c20 7475 706c 6529 3a0a  ce(val, tuple):.
-00003150: 2020 2020 2320 5370 6563 6961 6c20 6361      # Special ca
-00003160: 7365 206e 616d 6564 7475 706c 6573 2061  se namedtuples a
-00003170: 6e64 2073 7065 6369 616c 204a 4158 2074  nd special JAX t
-00003180: 7570 6c65 2073 7472 7563 7475 7265 7320  uple structures 
-00003190: 6f74 6865 7277 6973 6520 7468 6579 0a20  otherwise they. 
-000031a0: 2020 2023 2077 6f75 6c64 2062 6520 646f     # would be do
-000031b0: 776e 6772 6164 6564 2074 6f20 6e6f 726d  wngraded to norm
-000031c0: 616c 2074 7570 6c65 732e 0a20 2020 2069  al tuples..    i
-000031d0: 6620 6861 7361 7474 7228 7661 6c2c 2027  f hasattr(val, '
-000031e0: 5f66 6965 6c64 7327 2920 6f72 2074 7970  _fields') or typ
-000031f0: 6528 7661 6c29 2e5f 5f6e 616d 655f 5f20  e(val).__name__ 
-00003200: 3d3d 2027 5061 7274 6974 696f 6e53 7065  == 'PartitionSpe
-00003210: 6327 3a0a 2020 2020 2020 7265 7475 726e  c':.      return
-00003220: 2074 7970 6528 7661 6c29 282a 5b5f 6672   type(val)(*[_fr
-00003230: 6565 7a65 5f61 7474 7228 7629 2066 6f72  eeze_attr(v) for
-00003240: 2076 2069 6e20 7661 6c5d 290a 2020 2020   v in val]).    
-00003250: 656c 7365 3a0a 2020 2020 2020 7265 7475  else:.      retu
-00003260: 726e 2074 7570 6c65 285f 6672 6565 7a65  rn tuple(_freeze
-00003270: 5f61 7474 7228 7629 2066 6f72 2076 2069  _attr(v) for v i
-00003280: 6e20 7661 6c29 0a20 2065 6c69 6620 6973  n val).  elif is
-00003290: 696e 7374 616e 6365 2876 616c 2c20 6c69  instance(val, li
-000032a0: 7374 293a 0a20 2020 2072 6574 7572 6e20  st):.    return 
-000032b0: 7475 706c 6528 5f66 7265 657a 655f 6174  tuple(_freeze_at
-000032c0: 7472 2876 2920 666f 7220 7620 696e 2076  tr(v) for v in v
-000032d0: 616c 290a 2020 656c 7365 3a0a 2020 2020  al).  else:.    
-000032e0: 7265 7475 726e 2076 616c 0a0a 0a23 204d  return val...# M
-000032f0: 6574 686f 6420 7772 6170 7069 6e67 206f  ethod wrapping o
-00003300: 6620 2263 6f6d 7061 6374 206d 6574 686f  f "compact metho
-00003310: 6473 2220 616e 6420 7365 7475 7028 290a  ds" and setup().
-00003320: 2320 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  # --------------
-00003330: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00003340: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00003350: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00003360: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a  ---------------.
-00003370: 6465 6620 636f 6d70 6163 7428 6675 6e3a  def compact(fun:
-00003380: 205f 4361 6c6c 6162 6c65 5429 202d 3e20   _CallableT) -> 
-00003390: 5f43 616c 6c61 626c 6554 3a0a 2020 2222  _CallableT:.  ""
-000033a0: 224d 6172 6b73 2074 6865 2067 6976 656e  "Marks the given
-000033b0: 206d 6f64 756c 6520 6d65 7468 6f64 2061   module method a
-000033c0: 6c6c 6f77 696e 6720 696e 6c69 6e65 6420  llowing inlined 
-000033d0: 7375 626d 6f64 756c 6573 2e0a 0a20 204d  submodules...  M
-000033e0: 6574 686f 6473 2077 7261 7070 6564 2069  ethods wrapped i
-000033f0: 6e20 4063 6f6d 7061 6374 2063 616e 2064  n @compact can d
-00003400: 6566 696e 6520 7375 626d 6f64 756c 6573  efine submodules
-00003410: 2064 6972 6563 746c 7920 7769 7468 696e   directly within
-00003420: 2074 6865 206d 6574 686f 642e 0a0a 2020   the method...  
-00003430: 466f 7220 696e 7374 616e 6365 3a3a 0a0a  For instance::..
-00003440: 2020 2020 3e3e 3e20 696d 706f 7274 2066      >>> import f
-00003450: 6c61 782e 6c69 6e65 6e20 6173 206e 6e0a  lax.linen as nn.
-00003460: 0a20 2020 203e 3e3e 2063 6c61 7373 2046  .    >>> class F
-00003470: 6f6f 286e 6e2e 4d6f 6475 6c65 293a 0a20  oo(nn.Module):. 
-00003480: 2020 202e 2e2e 2020 2040 6e6e 2e63 6f6d     ...   @nn.com
-00003490: 7061 6374 0a20 2020 202e 2e2e 2020 2064  pact.    ...   d
-000034a0: 6566 205f 5f63 616c 6c5f 5f28 7365 6c66  ef __call__(self
-000034b0: 2c20 782c 2066 6561 7475 7265 7329 3a0a  , x, features):.
-000034c0: 2020 2020 2e2e 2e20 2020 2020 7820 3d20      ...     x = 
-000034d0: 6e6e 2e44 656e 7365 2866 6561 7475 7265  nn.Dense(feature
-000034e0: 7329 2878 290a 2020 2020 2e2e 2e20 2020  s)(x).    ...   
-000034f0: 2020 2e2e 2e0a 2020 2020 2e2e 2e20 2020    ....    ...   
-00003500: 2020 7265 7475 726e 2078 0a0a 2020 4174    return x..  At
-00003510: 206d 6f73 7420 6f6e 6520 6d65 7468 6f64   most one method
-00003520: 2069 6e20 6561 6368 204d 6f64 756c 6520   in each Module 
-00003530: 6d61 7920 6265 2077 7261 7070 6564 2077  may be wrapped w
-00003540: 6974 6820 4063 6f6d 7061 6374 2e0a 0a20  ith @compact... 
-00003550: 2041 7267 733a 0a20 2020 2066 756e 3a20   Args:.    fun: 
-00003560: 5468 6520 4d6f 6475 6c65 206d 6574 686f  The Module metho
-00003570: 6420 746f 206d 6172 6b20 6173 2063 6f6d  d to mark as com
-00003580: 7061 6374 2e0a 0a20 2052 6574 7572 6e73  pact...  Returns
-00003590: 3a0a 2020 2020 5468 6520 6769 7665 6e20  :.    The given 
-000035a0: 6675 6e63 7469 6f6e 2060 6066 756e 6060  function ``fun``
-000035b0: 206d 6172 6b65 6420 6173 2063 6f6d 7061   marked as compa
-000035c0: 6374 2e0a 2020 2222 220a 2020 6675 6e2e  ct..  """.  fun.
-000035d0: 636f 6d70 6163 7420 3d20 5472 7565 2020  compact = True  
-000035e0: 2320 7479 7065 3a20 6967 6e6f 7265 5b61  # type: ignore[a
-000035f0: 7474 722d 6465 6669 6e65 645d 0a20 2072  ttr-defined].  r
-00003600: 6574 7572 6e20 6675 6e0a 0a0a 6465 6620  eturn fun...def 
-00003610: 6e6f 7772 6170 2866 756e 3a20 5f43 616c  nowrap(fun: _Cal
-00003620: 6c61 626c 6554 2920 2d3e 205f 4361 6c6c  lableT) -> _Call
-00003630: 6162 6c65 543a 0a20 2022 2222 4d61 726b  ableT:.  """Mark
-00003640: 7320 7468 6520 6769 7665 6e20 6d6f 6475  s the given modu
-00003650: 6c65 206d 6574 686f 6420 6173 2061 2068  le method as a h
-00003660: 656c 7065 7220 6d65 7468 6f64 2074 6861  elper method tha
-00003670: 7420 6e65 6564 6e27 7420 6265 2077 7261  t needn't be wra
-00003680: 7070 6564 2e0a 0a20 204d 6574 686f 6473  pped...  Methods
-00003690: 2077 7261 7070 6564 2069 6e20 6060 406e   wrapped in ``@n
-000036a0: 6f77 7261 7060 6020 6172 6520 7072 6976  owrap`` are priv
-000036b0: 6174 6520 6865 6c70 6572 206d 6574 686f  ate helper metho
-000036c0: 6473 2074 6861 7420 6e65 6564 6e27 7420  ds that needn't 
-000036d0: 6265 2077 7261 7070 6564 0a20 2077 6974  be wrapped.  wit
-000036e0: 6820 7468 6520 7374 6174 6520 6861 6e64  h the state hand
-000036f0: 6c65 7220 6f72 2061 2073 6570 6172 6174  ler or a separat
-00003700: 6520 6e61 6d65 645f 6361 6c6c 2074 7261  e named_call tra
-00003710: 6e73 666f 726d 2e0a 0a20 2054 6869 7320  nsform...  This 
-00003720: 6973 206e 6565 6465 6420 696e 2073 6576  is needed in sev
-00003730: 6572 616c 2063 6f6e 6372 6574 6520 696e  eral concrete in
-00003740: 7374 616e 6365 733a 0a20 2020 2d20 6966  stances:.   - if
-00003750: 2079 6f75 2772 6520 7375 6263 6c61 7373   you're subclass
-00003760: 696e 6720 6120 6d65 7468 6f64 206c 696b  ing a method lik
-00003770: 6520 4d6f 6475 6c65 2e70 6172 616d 2061  e Module.param a
-00003780: 6e64 2064 6f6e 2774 2077 616e 7420 7468  nd don't want th
-00003790: 6973 0a20 2020 2020 6f76 6572 7269 6465  is.     override
-000037a0: 6e20 636f 7265 2066 756e 6374 696f 6e20  n core function 
-000037b0: 6465 636f 7261 7465 6420 7769 7468 2074  decorated with t
-000037c0: 6865 2073 7461 7465 206d 616e 6167 656d  he state managem
-000037d0: 656e 7420 7772 6170 7065 722e 0a20 2020  ent wrapper..   
-000037e0: 2d20 4966 2079 6f75 2077 616e 7420 6120  - If you want a 
-000037f0: 6d65 7468 6f64 2074 6f20 6265 2063 616c  method to be cal
-00003800: 6c61 626c 6520 6672 6f6d 2061 6e20 756e  lable from an un
-00003810: 626f 756e 6420 4d6f 6475 6c65 2028 652e  bound Module (e.
-00003820: 672e 3a20 610a 2020 2020 2066 756e 6374  g.: a.     funct
-00003830: 696f 6e20 6f66 2063 6f6e 7374 7275 6374  ion of construct
-00003840: 696f 6e20 6f66 2061 7267 756d 656e 7473  ion of arguments
-00003850: 2074 6861 7420 646f 6573 6e27 7420 6465   that doesn't de
-00003860: 7065 6e64 206f 6e20 7061 7261 6d73 2f52  pend on params/R
-00003870: 4e47 7329 2e0a 2020 2020 2049 6620 796f  NGs)..     If yo
-00003880: 7520 7761 6e74 2074 6f20 6c65 6172 6e20  u want to learn 
-00003890: 6d6f 7265 2061 626f 7574 2068 6f77 2046  more about how F
-000038a0: 6c61 7820 4d6f 6475 6c65 7320 6d61 6e61  lax Modules mana
-000038b0: 6765 2074 6865 6972 2073 7461 7465 2072  ge their state r
-000038c0: 6561 6420 7468 650a 2020 2020 205b 5468  ead the.     [Th
-000038d0: 6520 466c 6178 204d 6f64 756c 6520 6c69  e Flax Module li
-000038e0: 6665 6379 636c 655d 2868 7474 7073 3a2f  fecycle](https:/
-000038f0: 2f66 6c61 782e 7265 6164 7468 6564 6f63  /flax.readthedoc
-00003900: 732e 696f 2f65 6e2f 6c61 7465 7374 2f64  s.io/en/latest/d
-00003910: 6576 656c 6f70 6572 5f6e 6f74 6573 2f6d  eveloper_notes/m
-00003920: 6f64 756c 655f 6c69 6665 6379 636c 652e  odule_lifecycle.
-00003930: 6874 6d6c 290a 2020 2020 2067 7569 6465  html).     guide
-00003940: 2e0a 0a20 2046 6f72 2069 6e73 7461 6e63  ...  For instanc
-00003950: 653a 3a0a 0a20 2020 203e 3e3e 2069 6d70  e::..    >>> imp
-00003960: 6f72 7420 666c 6178 2e6c 696e 656e 2061  ort flax.linen a
-00003970: 7320 6e6e 0a20 2020 203e 3e3e 2069 6d70  s nn.    >>> imp
-00003980: 6f72 7420 6a61 782c 206a 6178 2e6e 756d  ort jax, jax.num
-00003990: 7079 2061 7320 6a6e 700a 0a20 2020 203e  py as jnp..    >
-000039a0: 3e3e 2063 6c61 7373 2046 6f6f 286e 6e2e  >> class Foo(nn.
-000039b0: 4d6f 6475 6c65 293a 0a20 2020 202e 2e2e  Module):.    ...
-000039c0: 2020 206e 756d 5f66 6561 7475 7265 733a     num_features:
-000039d0: 2069 6e74 0a0a 2020 2020 2e2e 2e20 2020   int..    ...   
-000039e0: 406e 6e2e 6e6f 7772 6170 0a20 2020 202e  @nn.nowrap.    .
-000039f0: 2e2e 2020 2064 6566 205f 6d61 6b65 5f64  ..   def _make_d
-00003a00: 656e 7365 2873 656c 662c 206e 756d 5f66  ense(self, num_f
-00003a10: 6561 7475 7265 7329 3a0a 2020 2020 2e2e  eatures):.    ..
-00003a20: 2e20 2020 2020 7265 7475 726e 206e 6e2e  .     return nn.
-00003a30: 4465 6e73 6528 6e75 6d5f 6665 6174 7572  Dense(num_featur
-00003a40: 6573 290a 0a20 2020 202e 2e2e 2020 2040  es)..    ...   @
-00003a50: 6e6e 2e63 6f6d 7061 6374 0a20 2020 202e  nn.compact.    .
-00003a60: 2e2e 2020 2064 6566 205f 5f63 616c 6c5f  ..   def __call_
-00003a70: 5f28 7365 6c66 2c20 7829 3a0a 2020 2020  _(self, x):.    
-00003a80: 2e2e 2e20 2020 2020 2320 6e6f 7720 7361  ...     # now sa
-00003a90: 6665 2074 6f20 7573 6520 636f 6e73 7472  fe to use constr
-00003aa0: 7563 746f 7220 6865 6c70 6572 2065 7665  uctor helper eve
-00003ab0: 6e20 6966 2075 7369 6e67 206e 616d 6564  n if using named
-00003ac0: 5f63 616c 6c0a 2020 2020 2e2e 2e20 2020  _call.    ...   
-00003ad0: 2020 6465 6e73 6520 3d20 7365 6c66 2e5f    dense = self._
-00003ae0: 6d61 6b65 5f64 656e 7365 2873 656c 662e  make_dense(self.
-00003af0: 6e75 6d5f 6665 6174 7572 6573 290a 2020  num_features).  
-00003b00: 2020 2e2e 2e20 2020 2020 7265 7475 726e    ...     return
-00003b10: 2064 656e 7365 2878 290a 0a20 2041 7267   dense(x)..  Arg
-00003b20: 733a 0a20 2020 2066 756e 3a20 5468 6520  s:.    fun: The 
-00003b30: 4d6f 6475 6c65 206d 6574 686f 6420 746f  Module method to
-00003b40: 206d 6172 6b20 6173 206e 6f77 7261 702e   mark as nowrap.
-00003b50: 0a0a 2020 5265 7475 726e 733a 0a20 2020  ..  Returns:.   
-00003b60: 2054 6865 2067 6976 656e 2066 756e 6374   The given funct
-00003b70: 696f 6e20 6060 6675 6e60 6020 6d61 726b  ion ``fun`` mark
-00003b80: 6564 2061 7320 6e6f 7772 6170 2e0a 2020  ed as nowrap..  
-00003b90: 2222 220a 2020 6675 6e2e 6e6f 7772 6170  """.  fun.nowrap
-00003ba0: 203d 2054 7275 6520 2023 2074 7970 653a   = True  # type:
-00003bb0: 2069 676e 6f72 655b 6174 7472 2d64 6566   ignore[attr-def
-00003bc0: 696e 6564 5d0a 2020 7265 7475 726e 2066  ined].  return f
-00003bd0: 756e 0a0a 0a64 6566 2063 6f6d 7061 6374  un...def compact
-00003be0: 5f6e 616d 655f 7363 6f70 6528 6675 6e3a  _name_scope(fun:
-00003bf0: 205f 4361 6c6c 6162 6c65 5429 202d 3e20   _CallableT) -> 
-00003c00: 5f43 616c 6c61 626c 6554 3a0a 2020 2222  _CallableT:.  ""
-00003c10: 2243 7265 6174 6573 2063 6f6d 7061 6374  "Creates compact
-00003c20: 2073 7562 6d6f 6475 6c65 7320 6672 6f6d   submodules from
-00003c30: 2061 206d 6574 686f 642e 0a0a 2020 5468   a method...  Th
-00003c40: 6973 2069 7320 6120 6465 636f 7261 746f  is is a decorato
-00003c50: 7220 7468 6174 2061 6c6c 6f77 7320 796f  r that allows yo
-00003c60: 7520 746f 2064 6566 696e 6520 636f 6d70  u to define comp
-00003c70: 6163 7420 7375 626d 6f64 756c 6573 2066  act submodules f
-00003c80: 726f 6d20 610a 2020 6d65 7468 6f64 2e20  rom a.  method. 
-00003c90: 4974 2773 2069 6e74 656e 7469 6f6e 2069  It's intention i
-00003ca0: 7320 746f 206d 616b 6520 6974 2065 6173  s to make it eas
-00003cb0: 6965 7220 746f 2070 6f72 7420 636f 6465  ier to port code
-00003cc0: 2048 6169 6b75 2063 6f64 6520 746f 2046   Haiku code to F
-00003cd0: 6c61 780a 2020 6279 2070 726f 7669 6469  lax.  by providi
-00003ce0: 6e67 2074 6865 2073 616d 6520 6675 6e63  ng the same func
-00003cf0: 7469 6f6e 616c 6974 792e 0a0a 2020 4578  tionality...  Ex
-00003d00: 616d 706c 653a 3a0a 0a20 2020 203e 3e3e  ample::..    >>>
-00003d10: 2069 6d70 6f72 7420 666c 6178 2e6c 696e   import flax.lin
-00003d20: 656e 2061 7320 6e6e 0a20 2020 203e 3e3e  en as nn.    >>>
-00003d30: 2069 6d70 6f72 7420 6a61 780a 2020 2020   import jax.    
-00003d40: 3e3e 3e20 696d 706f 7274 206a 6178 2e6e  >>> import jax.n
-00003d50: 756d 7079 2061 7320 6a6e 700a 2020 2020  umpy as jnp.    
-00003d60: 3e3e 3e20 6672 6f6d 2066 6c61 782e 636f  >>> from flax.co
-00003d70: 7265 2069 6d70 6f72 7420 7072 6574 7479  re import pretty
-00003d80: 5f72 6570 720a 2020 2020 2e2e 2e0a 2020  _repr.    ....  
-00003d90: 2020 3e3e 3e20 636c 6173 7320 466f 6f28    >>> class Foo(
-00003da0: 6e6e 2e4d 6f64 756c 6529 3a0a 2020 2020  nn.Module):.    
-00003db0: 2e2e 2e20 2020 406e 6e2e 636f 6d70 6163  ...   @nn.compac
-00003dc0: 745f 6e61 6d65 5f73 636f 7065 0a20 2020  t_name_scope.   
-00003dd0: 202e 2e2e 2020 2064 6566 2075 7028 7365   ...   def up(se
-00003de0: 6c66 2c20 7829 3a0a 2020 2020 2e2e 2e20  lf, x):.    ... 
-00003df0: 2020 2020 7265 7475 726e 206e 6e2e 4465      return nn.De
-00003e00: 6e73 6528 3329 2878 290a 2020 2020 2e2e  nse(3)(x).    ..
-00003e10: 2e0a 2020 2020 2e2e 2e20 2020 406e 6e2e  ..    ...   @nn.
-00003e20: 636f 6d70 6163 745f 6e61 6d65 5f73 636f  compact_name_sco
-00003e30: 7065 0a20 2020 202e 2e2e 2020 2064 6566  pe.    ...   def
-00003e40: 2064 6f77 6e28 7365 6c66 2c20 7829 3a0a   down(self, x):.
-00003e50: 2020 2020 2e2e 2e20 2020 2020 7265 7475      ...     retu
-00003e60: 726e 206e 6e2e 4465 6e73 6528 3329 2878  rn nn.Dense(3)(x
-00003e70: 290a 2020 2020 2e2e 2e0a 2020 2020 2e2e  ).    ....    ..
-00003e80: 2e20 2020 6465 6620 5f5f 6361 6c6c 5f5f  .   def __call__
-00003e90: 2873 656c 662c 2078 293a 0a20 2020 202e  (self, x):.    .
-00003ea0: 2e2e 2020 2020 2072 6574 7572 6e20 7365  ..     return se
-00003eb0: 6c66 2e75 7028 7829 202b 2073 656c 662e  lf.up(x) + self.
-00003ec0: 646f 776e 2878 290a 2020 2020 2e2e 2e0a  down(x).    ....
-00003ed0: 2020 2020 3e3e 3e20 6d6f 6475 6c65 203d      >>> module =
-00003ee0: 2046 6f6f 2829 0a20 2020 203e 3e3e 2076   Foo().    >>> v
-00003ef0: 6172 6961 626c 6573 203d 206d 6f64 756c  ariables = modul
-00003f00: 652e 696e 6974 286a 6178 2e72 616e 646f  e.init(jax.rando
-00003f10: 6d2e 5052 4e47 4b65 7928 3029 2c20 6a6e  m.PRNGKey(0), jn
-00003f20: 702e 6f6e 6573 2828 312c 2032 2929 290a  p.ones((1, 2))).
-00003f30: 2020 2020 3e3e 3e20 7061 7261 6d73 203d      >>> params =
-00003f40: 2076 6172 6961 626c 6573 5b27 7061 7261   variables['para
-00003f50: 6d73 275d 0a20 2020 203e 3e3e 2070 7269  ms'].    >>> pri
-00003f60: 6e74 2870 7265 7474 795f 7265 7072 286a  nt(pretty_repr(j
-00003f70: 6178 2e74 7265 655f 7574 696c 2e74 7265  ax.tree_util.tre
-00003f80: 655f 6d61 7028 6a6e 702e 7368 6170 652c  e_map(jnp.shape,
-00003f90: 2070 6172 616d 7329 2929 0a20 2020 207b   params))).    {
-00003fa0: 0a20 2020 2020 2020 2064 6f77 6e3a 207b  .        down: {
-00003fb0: 0a20 2020 2020 2020 2020 2020 2044 656e  .            Den
-00003fc0: 7365 5f30 3a20 7b0a 2020 2020 2020 2020  se_0: {.        
-00003fd0: 2020 2020 2020 2020 6269 6173 3a20 2833          bias: (3
-00003fe0: 2c29 2c0a 2020 2020 2020 2020 2020 2020  ,),.            
-00003ff0: 2020 2020 6b65 726e 656c 3a20 2832 2c20      kernel: (2, 
-00004000: 3329 2c0a 2020 2020 2020 2020 2020 2020  3),.            
-00004010: 7d2c 0a20 2020 2020 2020 207d 2c0a 2020  },.        },.  
-00004020: 2020 2020 2020 7570 3a20 7b0a 2020 2020        up: {.    
-00004030: 2020 2020 2020 2020 4465 6e73 655f 303a          Dense_0:
-00004040: 207b 0a20 2020 2020 2020 2020 2020 2020   {.             
-00004050: 2020 2062 6961 733a 2028 332c 292c 0a20     bias: (3,),. 
-00004060: 2020 2020 2020 2020 2020 2020 2020 206b                 k
-00004070: 6572 6e65 6c3a 2028 322c 2033 292c 0a20  ernel: (2, 3),. 
-00004080: 2020 2020 2020 2020 2020 207d 2c0a 2020             },.  
-00004090: 2020 2020 2020 7d2c 0a20 2020 207d 0a0a        },.    }..
-000040a0: 2020 596f 7520 6361 6e20 616c 736f 2075    You can also u
-000040b0: 7365 2060 6063 6f6d 7061 6374 5f6e 616d  se ``compact_nam
-000040c0: 655f 7363 6f70 6560 6020 696e 7369 6465  e_scope`` inside
-000040d0: 2060 6040 636f 6d70 6163 7460 6020 6d65   ``@compact`` me
-000040e0: 7468 6f64 7320 6f72 2065 7665 6e0a 2020  thods or even.  
-000040f0: 6f74 6865 720a 2020 6060 636f 6d70 6163  other.  ``compac
-00004100: 745f 6e61 6d65 5f73 636f 7065 6060 206d  t_name_scope`` m
-00004110: 6574 686f 6473 2e20 4d65 7468 6f64 7320  ethods. Methods 
-00004120: 7468 6174 2061 7265 2064 6563 6f72 6174  that are decorat
-00004130: 6564 2077 6974 680a 2020 6060 636f 6d70  ed with.  ``comp
-00004140: 6163 745f 6e61 6d65 5f73 636f 7065 6060  act_name_scope``
-00004150: 0a20 2063 616e 2061 6c73 6f20 6265 2063  .  can also be c
-00004160: 616c 6c65 6420 6469 7265 6374 6c79 2066  alled directly f
-00004170: 726f 6d20 6060 696e 6974 6060 206f 7220  rom ``init`` or 
-00004180: 6060 6170 706c 7960 6020 7669 6120 7468  ``apply`` via th
-00004190: 6520 6060 6d65 7468 6f64 6060 0a20 2061  e ``method``.  a
-000041a0: 7267 756d 656e 743a 3a0a 0a20 2020 203e  rgument::..    >
-000041b0: 3e3e 2079 5f64 6f77 6e20 3d20 6d6f 6475  >> y_down = modu
-000041c0: 6c65 2e61 7070 6c79 287b 2770 6172 616d  le.apply({'param
-000041d0: 7327 3a20 7061 7261 6d73 7d2c 206a 6e70  s': params}, jnp
-000041e0: 2e6f 6e65 7328 2831 2c20 3229 292c 206d  .ones((1, 2)), m
-000041f0: 6574 686f 643d 2764 6f77 6e27 290a 2020  ethod='down').  
-00004200: 2020 3e3e 3e20 795f 646f 776e 2e73 6861    >>> y_down.sha
-00004210: 7065 0a20 2020 2028 312c 2033 290a 0a20  pe.    (1, 3).. 
-00004220: 2041 7267 733a 0a20 2020 2066 756e 3a20   Args:.    fun: 
-00004230: 5468 6520 4d6f 6475 6c65 206d 6574 686f  The Module metho
-00004240: 6420 746f 206d 6172 6b20 6173 2063 6f6d  d to mark as com
-00004250: 7061 6374 5f6e 616d 655f 7363 6f70 652e  pact_name_scope.
-00004260: 0a0a 2020 5265 7475 726e 733a 0a20 2020  ..  Returns:.   
-00004270: 2054 6865 2067 6976 656e 2066 756e 6374   The given funct
-00004280: 696f 6e20 6060 6675 6e60 6020 6d61 726b  ion ``fun`` mark
-00004290: 6564 2061 7320 636f 6d70 6163 745f 6e61  ed as compact_na
-000042a0: 6d65 5f73 636f 7065 2e0a 2020 2222 220a  me_scope..  """.
-000042b0: 0a20 2040 6675 6e63 746f 6f6c 732e 7772  .  @functools.wr
-000042c0: 6170 7328 6675 6e29 0a20 2064 6566 2063  aps(fun).  def c
-000042d0: 6f6d 7061 6374 5f6e 616d 655f 7363 6f70  ompact_name_scop
-000042e0: 655f 7772 6170 7065 7228 7365 6c66 3a20  e_wrapper(self: 
-000042f0: 6e6e 2e4d 6f64 756c 652c 202a 6172 6773  nn.Module, *args
-00004300: 2c20 2a2a 6b77 6172 6773 293a 0a20 2020  , **kwargs):.   
-00004310: 206e 616d 6520 3d20 6675 6e2e 5f5f 6e61   name = fun.__na
-00004320: 6d65 5f5f 0a20 2020 2069 6620 6e6f 7420  me__.    if not 
-00004330: 6861 7361 7474 7228 7365 6c66 2c20 275f  hasattr(self, '_
-00004340: 636f 6d70 6163 745f 6e61 6d65 5f73 636f  compact_name_sco
-00004350: 7065 5f6d 6f64 756c 6573 2729 3a0a 2020  pe_modules'):.  
-00004360: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-00004370: 7272 6f72 280a 2020 2020 2020 2020 6627  rror(.        f'
-00004380: 4361 6e6e 6f74 2063 616c 6c20 636f 6d70  Cannot call comp
-00004390: 6163 745f 6e61 6d65 5f73 636f 7065 206d  act_name_scope m
-000043a0: 6574 686f 6420 7b6e 616d 6521 727d 206f  ethod {name!r} o
-000043b0: 6e20 6120 4d6f 6475 6c65 2074 6861 7420  n a Module that 
-000043c0: 6861 7320 6e6f 7420 6265 656e 2027 0a20  has not been '. 
-000043d0: 2020 2020 2020 2066 2773 6574 7570 2e20         f'setup. 
-000043e0: 5468 6973 2069 7320 6c69 6b65 6c79 2062  This is likely b
-000043f0: 6563 6175 7365 2079 6f75 2061 7265 2063  ecause you are c
-00004400: 616c 6c69 6e67 207b 6e61 6d65 2172 7d20  alling {name!r} 
-00004410: 270a 2020 2020 2020 2020 2766 726f 6d20  '.        'from 
-00004420: 6f75 7473 6964 6520 6f66 2069 6e69 7420  outside of init 
-00004430: 6f72 2061 7070 6c79 2e27 0a20 2020 2020  or apply.'.     
-00004440: 2029 0a20 2020 206d 6f64 756c 6520 3d20   ).    module = 
-00004450: 7365 6c66 2e5f 636f 6d70 6163 745f 6e61  self._compact_na
-00004460: 6d65 5f73 636f 7065 5f6d 6f64 756c 6573  me_scope_modules
-00004470: 5b6e 616d 655d 0a20 2020 2072 6574 7572  [name].    retur
-00004480: 6e20 6d6f 6475 6c65 282a 6172 6773 2c20  n module(*args, 
-00004490: 2a2a 6b77 6172 6773 290a 0a20 2063 6f6d  **kwargs)..  com
-000044a0: 7061 6374 5f6e 616d 655f 7363 6f70 655f  pact_name_scope_
-000044b0: 7772 6170 7065 722e 636f 6d70 6163 745f  wrapper.compact_
-000044c0: 6e61 6d65 5f73 636f 7065 203d 2054 7275  name_scope = Tru
-000044d0: 6520 2023 2074 7970 653a 2069 676e 6f72  e  # type: ignor
-000044e0: 655b 6174 7472 2d64 6566 696e 6564 5d0a  e[attr-defined].
-000044f0: 2020 636f 6d70 6163 745f 6e61 6d65 5f73    compact_name_s
-00004500: 636f 7065 5f77 7261 7070 6572 2e69 6e6e  cope_wrapper.inn
-00004510: 6572 5f66 756e 203d 2066 756e 2020 2320  er_fun = fun  # 
-00004520: 7479 7065 3a20 6967 6e6f 7265 5b61 7474  type: ignore[att
-00004530: 722d 6465 6669 6e65 645d 0a20 2063 6f6d  r-defined].  com
-00004540: 7061 6374 5f6e 616d 655f 7363 6f70 655f  pact_name_scope_
-00004550: 7772 6170 7065 722e 6e6f 7772 6170 203d  wrapper.nowrap =
-00004560: 2054 7275 6520 2023 2074 7970 653a 2069   True  # type: i
-00004570: 676e 6f72 655b 6174 7472 2d64 6566 696e  gnore[attr-defin
-00004580: 6564 5d0a 2020 7265 7475 726e 2063 6f6d  ed].  return com
-00004590: 7061 6374 5f6e 616d 655f 7363 6f70 655f  pact_name_scope_
-000045a0: 7772 6170 7065 7220 2023 2074 7970 653a  wrapper  # type:
-000045b0: 2069 676e 6f72 655b 7265 7475 726e 2d76   ignore[return-v
-000045c0: 616c 7565 5d0a 0a0a 6465 6620 5f67 6574  alue]...def _get
-000045d0: 5f6c 6f63 616c 5f6d 6574 686f 645f 6e61  _local_method_na
-000045e0: 6d65 7328 0a20 2063 6c73 3a20 416e 792c  mes(.  cls: Any,
-000045f0: 2065 7863 6c75 6465 3a20 4974 6572 6162   exclude: Iterab
-00004600: 6c65 5b73 7472 5d20 3d20 2829 0a29 202d  le[str] = ().) -
-00004610: 3e20 5475 706c 655b 7374 722c 202e 2e2e  > Tuple[str, ...
-00004620: 5d3a 0a20 2022 2222 4765 7473 206d 6574  ]:.  """Gets met
-00004630: 686f 6420 6e61 6d65 7320 6f66 2061 2063  hod names of a c
-00004640: 6c61 7373 2c20 6578 636c 7564 696e 6720  lass, excluding 
-00004650: 636c 6173 7320 616e 6420 7374 6174 6963  class and static
-00004660: 206d 6574 686f 6473 2e0a 0a20 2041 7267   methods...  Arg
-00004670: 733a 0a20 2020 2063 6c73 3a20 5468 6520  s:.    cls: The 
-00004680: 636c 6173 7320 746f 2067 6574 206d 6574  class to get met
-00004690: 686f 6420 6e61 6d65 7320 666f 722e 0a20  hod names for.. 
-000046a0: 2020 2065 7863 6c75 6465 3a20 4e61 6d65     exclude: Name
-000046b0: 7320 746f 2065 7863 6c75 6465 2066 726f  s to exclude fro
-000046c0: 6d20 6f75 7470 7574 2e0a 0a20 2052 6574  m output...  Ret
-000046d0: 7572 6e73 3a0a 2020 2020 4120 6c69 7374  urns:.    A list
-000046e0: 206f 6620 6d65 7468 6f64 206e 616d 6573   of method names
-000046f0: 2e0a 2020 2222 220a 2020 7472 7565 5f6d  ..  """.  true_m
-00004700: 6574 686f 6473 203d 2073 6574 2829 0a20  ethods = set(). 
-00004710: 2066 6f72 206d 2069 6e20 636c 732e 5f5f   for m in cls.__
-00004720: 6469 6374 5f5f 3a0a 2020 2020 6966 2063  dict__:.    if c
-00004730: 616c 6c61 626c 6528 636c 732e 5f5f 6469  allable(cls.__di
-00004740: 6374 5f5f 5b6d 5d29 2061 6e64 206e 6f74  ct__[m]) and not
-00004750: 2069 6e73 7065 6374 2e69 7363 6c61 7373   inspect.isclass
-00004760: 280a 2020 2020 2020 636c 732e 5f5f 6469  (.      cls.__di
-00004770: 6374 5f5f 5b6d 5d0a 2020 2020 293a 2020  ct__[m].    ):  
-00004780: 2320 7079 7479 7065 3a20 6469 7361 626c  # pytype: disabl
-00004790: 653d 6e6f 742d 7375 7070 6f72 7465 642d  e=not-supported-
-000047a0: 7965 740a 2020 2020 2020 6d74 7970 6520  yet.      mtype 
-000047b0: 3d20 7479 7065 2863 6c73 2e5f 5f64 6963  = type(cls.__dic
-000047c0: 745f 5f5b 6d5d 290a 2020 2020 2020 6966  t__[m]).      if
-000047d0: 206d 7479 7065 2021 3d20 7374 6174 6963   mtype != static
-000047e0: 6d65 7468 6f64 2061 6e64 206d 7479 7065  method and mtype
-000047f0: 2021 3d20 636c 6173 736d 6574 686f 643a   != classmethod:
-00004800: 0a20 2020 2020 2020 2074 7275 655f 6d65  .        true_me
-00004810: 7468 6f64 732e 6164 6428 6d29 0a20 2072  thods.add(m).  r
-00004820: 6574 7572 6e20 7475 706c 6528 7472 7565  eturn tuple(true
-00004830: 5f6d 6574 686f 6473 2e64 6966 6665 7265  _methods.differe
-00004840: 6e63 6528 7365 7428 6578 636c 7564 6529  nce(set(exclude)
-00004850: 2929 0a0a 0a64 6566 205f 6765 745f 6c6f  ))...def _get_lo
-00004860: 6361 6c5f 6465 7363 7269 7074 6f72 5f6e  cal_descriptor_n
-00004870: 616d 6573 280a 2020 636c 733a 2041 6e79  ames(.  cls: Any
-00004880: 2c20 6578 636c 7564 653a 2049 7465 7261  , exclude: Itera
-00004890: 626c 655b 7374 725d 203d 2028 290a 2920  ble[str] = ().) 
-000048a0: 2d3e 2054 7570 6c65 5b73 7472 2c20 2e2e  -> Tuple[str, ..
-000048b0: 2e5d 3a0a 2020 2222 2247 6574 7320 6465  .]:.  """Gets de
-000048c0: 7363 7269 7074 6f72 206e 616d 6573 206f  scriptor names o
-000048d0: 6620 6120 636c 6173 732e 0a0a 2020 4172  f a class...  Ar
-000048e0: 6773 3a0a 2020 2020 636c 733a 2054 6865  gs:.    cls: The
-000048f0: 2063 6c61 7373 2074 6f20 6765 7420 7072   class to get pr
-00004900: 6f70 6572 7479 206e 616d 6573 2066 6f72  operty names for
-00004910: 2e0a 2020 2020 6578 636c 7564 653a 204e  ..    exclude: N
-00004920: 616d 6573 2074 6f20 6578 636c 7564 6520  ames to exclude 
-00004930: 6672 6f6d 206f 7574 7075 742e 0a0a 2020  from output...  
-00004940: 5265 7475 726e 733a 0a20 2020 2041 206c  Returns:.    A l
-00004950: 6973 7420 6f66 2070 726f 7065 7274 7920  ist of property 
-00004960: 6e61 6d65 732e 0a20 2022 2222 0a20 2074  names..  """.  t
-00004970: 7275 655f 7072 6f70 6572 7469 6573 203d  rue_properties =
-00004980: 2073 6574 2829 0a20 2066 6f72 206d 2c20   set().  for m, 
-00004990: 6174 7472 2069 6e20 636c 732e 5f5f 6469  attr in cls.__di
-000049a0: 6374 5f5f 2e69 7465 6d73 2829 3a0a 2020  ct__.items():.  
-000049b0: 2020 6966 206e 6f74 2063 616c 6c61 626c    if not callabl
-000049c0: 6528 6174 7472 2920 616e 6420 280a 2020  e(attr) and (.  
-000049d0: 2020 2020 6861 7361 7474 7228 6174 7472      hasattr(attr
-000049e0: 2c20 275f 5f67 6574 5f5f 2729 0a20 2020  , '__get__').   
-000049f0: 2020 206f 7220 6861 7361 7474 7228 6174     or hasattr(at
-00004a00: 7472 2c20 275f 5f73 6574 5f5f 2729 0a20  tr, '__set__'). 
-00004a10: 2020 2020 206f 7220 6861 7361 7474 7228       or hasattr(
-00004a20: 6174 7472 2c20 275f 5f64 656c 6574 655f  attr, '__delete_
-00004a30: 5f27 290a 2020 2020 293a 0a20 2020 2020  _').    ):.     
-00004a40: 206d 7479 7065 203d 2074 7970 6528 6174   mtype = type(at
-00004a50: 7472 290a 2020 2020 2020 6966 206d 7479  tr).      if mty
-00004a60: 7065 2021 3d20 7374 6174 6963 6d65 7468  pe != staticmeth
-00004a70: 6f64 2061 6e64 206d 7479 7065 2021 3d20  od and mtype != 
-00004a80: 636c 6173 736d 6574 686f 643a 0a20 2020  classmethod:.   
-00004a90: 2020 2020 2074 7275 655f 7072 6f70 6572       true_proper
-00004aa0: 7469 6573 2e61 6464 286d 290a 2020 7265  ties.add(m).  re
-00004ab0: 7475 726e 2074 7570 6c65 2874 7275 655f  turn tuple(true_
-00004ac0: 7072 6f70 6572 7469 6573 2e64 6966 6665  properties.diffe
-00004ad0: 7265 6e63 6528 7365 7428 6578 636c 7564  rence(set(exclud
-00004ae0: 6529 2929 0a0a 0a64 6566 2077 7261 705f  e)))...def wrap_
-00004af0: 6d65 7468 6f64 5f6f 6e63 6528 6675 6e3a  method_once(fun:
-00004b00: 2043 616c 6c61 626c 655b 2e2e 2e2c 2041   Callable[..., A
-00004b10: 6e79 5d29 202d 3e20 4361 6c6c 6162 6c65  ny]) -> Callable
-00004b20: 5b2e 2e2e 2c20 416e 795d 3a0a 2020 2222  [..., Any]:.  ""
-00004b30: 224d 616e 6167 6573 204d 6f64 756c 6520  "Manages Module 
-00004b40: 7374 6174 6520 666f 7220 6120 6769 7665  state for a give
-00004b50: 6e20 7573 6572 2d64 6566 696e 6564 206d  n user-defined m
-00004b60: 6574 686f 642e 0a0a 2020 4172 6773 3a0a  ethod...  Args:.
-00004b70: 2020 2020 6675 6e3a 2055 7365 722d 6465      fun: User-de
-00004b80: 6669 6e65 6420 4d6f 6475 6c65 206d 6574  fined Module met
-00004b90: 686f 6420 746f 206d 616e 6167 6520 7374  hod to manage st
-00004ba0: 6174 6520 666f 722e 0a0a 2020 5265 7475  ate for...  Retu
-00004bb0: 726e 733a 0a20 2020 2057 7261 7070 6564  rns:.    Wrapped
-00004bc0: 206d 6574 686f 642e 0a20 2022 2222 0a20   method..  """. 
-00004bd0: 2023 2044 6f6e 2774 2072 6577 7261 7020   # Don't rewrap 
-00004be0: 6d65 7468 6f64 7320 7468 6174 2068 6176  methods that hav
-00004bf0: 6520 616c 7265 6164 7920 6861 6420 7468  e already had th
-00004c00: 6520 7374 6174 6520 6d61 6e61 6765 6d65  e state manageme
-00004c10: 6e74 2077 7261 7070 6572 0a20 2023 2061  nt wrapper.  # a
-00004c20: 7070 6c69 6564 2069 6e20 7468 6520 6465  pplied in the de
-00004c30: 636f 7261 746f 7220 7374 6163 6b2e 2020  corator stack.  
-00004c40: 5468 6973 2077 7261 7070 6572 2073 686f  This wrapper sho
-00004c50: 756c 6420 616c 7761 7973 2062 6520 6170  uld always be ap
-00004c60: 706c 6965 640a 2020 2320 6265 666f 7265  plied.  # before
-00004c70: 2074 7261 6e73 666f 726d 6174 696f 6e20   transformation 
-00004c80: 7772 6170 7065 7273 2e0a 2020 6966 2068  wrappers..  if h
-00004c90: 6173 6174 7472 2866 756e 2c20 276d 6574  asattr(fun, 'met
-00004ca0: 686f 645f 6861 6e64 6c65 725f 7772 6170  hod_handler_wrap
-00004cb0: 7065 6427 293a 0a20 2020 2072 6574 7572  ped'):.    retur
-00004cc0: 6e20 6675 6e0a 0a20 2040 6675 6e63 746f  n fun..  @functo
-00004cd0: 6f6c 732e 7772 6170 7328 6675 6e29 0a20  ols.wraps(fun). 
-00004ce0: 2064 6566 2077 7261 7070 6564 5f6d 6f64   def wrapped_mod
-00004cf0: 756c 655f 6d65 7468 6f64 282a 6172 6773  ule_method(*args
-00004d00: 2c20 2a2a 6b77 6172 6773 293a 0a20 2020  , **kwargs):.   
-00004d10: 2023 2057 6520 6d69 6768 7420 6861 7665   # We might have
-00004d20: 2069 6e63 6f72 7265 6374 6c79 2077 7261   incorrectly wra
-00004d30: 7070 7065 6420 6120 6361 6c6c 6162 6c65  ppped a callable
-00004d40: 0a20 2020 2023 2074 6861 7420 6973 206e  .    # that is n
-00004d50: 6f74 2061 206d 6574 686f 642e 2043 6865  ot a method. Che
-00004d60: 636b 2077 6865 7468 6572 2074 6865 2066  ck whether the f
-00004d70: 6972 7374 2061 7267 2069 7320 7365 6c66  irst arg is self
-00004d80: 2c0a 2020 2020 2320 6f74 6865 7277 6973  ,.    # otherwis
-00004d90: 6520 6361 6c6c 2074 6865 2077 7261 7070  e call the wrapp
-00004da0: 6564 2066 756e 6374 696f 6e20 6173 2069  ed function as i
-00004db0: 732e 0a20 2020 2069 6620 6172 6773 2061  s..    if args a
-00004dc0: 6e64 2069 7369 6e73 7461 6e63 6528 6172  nd isinstance(ar
-00004dd0: 6773 5b30 5d2c 204d 6f64 756c 6529 3a0a  gs[0], Module):.
-00004de0: 2020 2020 2020 7365 6c66 2c20 6172 6773        self, args
-00004df0: 203d 2061 7267 735b 305d 2c20 6172 6773   = args[0], args
-00004e00: 5b31 3a5d 0a20 2020 2020 2072 6574 7572  [1:].      retur
-00004e10: 6e20 7365 6c66 2e5f 6361 6c6c 5f77 7261  n self._call_wra
-00004e20: 7070 6564 5f6d 6574 686f 6428 6675 6e2c  pped_method(fun,
-00004e30: 2061 7267 732c 206b 7761 7267 7329 0a20   args, kwargs). 
-00004e40: 2020 2065 6c73 653a 0a20 2020 2020 2072     else:.      r
-00004e50: 6574 7572 6e20 6675 6e28 2a61 7267 732c  eturn fun(*args,
-00004e60: 202a 2a6b 7761 7267 7329 0a0a 2020 7772   **kwargs)..  wr
-00004e70: 6170 7065 645f 6d6f 6475 6c65 5f6d 6574  apped_module_met
-00004e80: 686f 642e 6d65 7468 6f64 5f68 616e 646c  hod.method_handl
-00004e90: 6572 5f77 7261 7070 6564 203d 2054 7275  er_wrapped = Tru
-00004ea0: 6520 2023 2074 7970 653a 2069 676e 6f72  e  # type: ignor
-00004eb0: 655b 6174 7472 2d64 6566 696e 6564 5d0a  e[attr-defined].
-00004ec0: 2020 7265 7475 726e 2077 7261 7070 6564    return wrapped
-00004ed0: 5f6d 6f64 756c 655f 6d65 7468 6f64 0a0a  _module_method..
-00004ee0: 0a64 6566 2077 7261 705f 6465 7363 7269  .def wrap_descri
-00004ef0: 7074 6f72 5f6f 6e63 6528 6465 7363 7269  ptor_once(descri
-00004f00: 7074 6f72 2920 2d3e 2027 4465 7363 7269  ptor) -> 'Descri
-00004f10: 7074 6f72 5772 6170 7065 7227 3a0a 2020  ptorWrapper':.  
-00004f20: 2222 2257 7261 7073 2061 2064 6573 6372  """Wraps a descr
-00004f30: 6970 746f 7220 746f 2067 6976 6520 6265  iptor to give be
-00004f40: 7474 6572 2065 7272 6f72 206d 6573 7361  tter error messa
-00004f50: 6765 732e 0a0a 2020 4172 6773 3a0a 2020  ges...  Args:.  
-00004f60: 2020 6465 7363 7269 7074 6f72 3a20 5573    descriptor: Us
-00004f70: 6572 2d64 6566 696e 6564 204d 6f64 756c  er-defined Modul
-00004f80: 6520 6174 7472 6962 7574 6520 6465 7363  e attribute desc
-00004f90: 7269 7074 6f72 2e0a 0a20 2052 6574 7572  riptor...  Retur
-00004fa0: 6e73 3a0a 2020 2020 5772 6170 7065 6420  ns:.    Wrapped 
-00004fb0: 6465 7363 7269 7074 6f72 2e0a 2020 2222  descriptor..  ""
-00004fc0: 220a 2020 2320 446f 6e27 7420 7265 7772  ".  # Don't rewr
-00004fd0: 6170 2064 6573 6372 6970 746f 7273 2e0a  ap descriptors..
-00004fe0: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
-00004ff0: 6465 7363 7269 7074 6f72 2c20 4465 7363  descriptor, Desc
-00005000: 7269 7074 6f72 5772 6170 7065 7229 3a0a  riptorWrapper):.
-00005010: 2020 2020 7265 7475 726e 2064 6573 6372      return descr
-00005020: 6970 746f 720a 0a20 2072 6574 7572 6e20  iptor..  return 
-00005030: 6372 6561 7465 5f64 6573 6372 6970 746f  create_descripto
-00005040: 725f 7772 6170 7065 7228 6465 7363 7269  r_wrapper(descri
-00005050: 7074 6f72 290a 0a0a 6465 6620 5f77 7261  ptor)...def _wra
-00005060: 705f 6861 7368 2868 6173 685f 666e 3a20  p_hash(hash_fn: 
-00005070: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
-00005080: 795d 2920 2d3e 2043 616c 6c61 626c 655b  y]) -> Callable[
-00005090: 2e2e 2e2c 2041 6e79 5d3a 0a20 2022 2222  ..., Any]:.  """
-000050a0: 5772 6170 7320 6120 6861 7368 2066 756e  Wraps a hash fun
-000050b0: 6374 696f 6e20 7769 7468 2073 6f6d 6520  ction with some 
-000050c0: 6368 6563 6b20 666f 7220 466c 6178 204d  check for Flax M
-000050d0: 6f64 756c 6573 2e22 2222 0a0a 2020 4066  odules."""..  @f
-000050e0: 756e 6374 6f6f 6c73 2e77 7261 7073 2868  unctools.wraps(h
-000050f0: 6173 685f 666e 290a 2020 6465 6620 7772  ash_fn).  def wr
-00005100: 6170 7065 6428 7365 6c66 293a 0a20 2020  apped(self):.   
-00005110: 2069 6620 7365 6c66 2e73 636f 7065 2069   if self.scope i
-00005120: 7320 6e6f 7420 4e6f 6e65 3a0a 2020 2020  s not None:.    
-00005130: 2020 7261 6973 6520 5479 7065 4572 726f    raise TypeErro
-00005140: 7228 2243 616e 2774 2063 616c 6c20 5f5f  r("Can't call __
-00005150: 6861 7368 5f5f 206f 6e20 6d6f 6475 6c65  hash__ on module
-00005160: 7320 7468 6174 2068 6f6c 6420 7661 7269  s that hold vari
-00005170: 6162 6c65 732e 2229 0a20 2020 2074 7279  ables.").    try
-00005180: 3a0a 2020 2020 2020 6861 7368 5f76 616c  :.      hash_val
-00005190: 7565 203d 2068 6173 685f 666e 2873 656c  ue = hash_fn(sel
-000051a0: 6629 0a20 2020 2065 7863 6570 7420 5479  f).    except Ty
-000051b0: 7065 4572 726f 7220 6173 2065 7863 3a0a  peError as exc:.
-000051c0: 2020 2020 2020 7261 6973 6520 5479 7065        raise Type
-000051d0: 4572 726f 7228 0a20 2020 2020 2020 2027  Error(.        '
-000051e0: 4661 696c 6564 2074 6f20 6861 7368 2046  Failed to hash F
-000051f0: 6c61 7820 4d6f 6475 6c65 2e20 2027 0a20  lax Module.  '. 
-00005200: 2020 2020 2020 2027 5468 6520 6d6f 6475         'The modu
-00005210: 6c65 2070 726f 6261 626c 7920 636f 6e74  le probably cont
-00005220: 6169 6e73 2075 6e68 6173 6861 626c 6520  ains unhashable 
-00005230: 6174 7472 6962 7574 6573 2e20 2027 0a20  attributes.  '. 
-00005240: 2020 2020 2020 2066 274d 6f64 756c 653d         f'Module=
-00005250: 7b73 656c 667d 270a 2020 2020 2020 2920  {self}'.      ) 
-00005260: 6672 6f6d 2065 7863 0a20 2020 2072 6574  from exc.    ret
-00005270: 7572 6e20 6861 7368 5f76 616c 7565 0a0a  urn hash_value..
-00005280: 2020 7265 7475 726e 2077 7261 7070 6564    return wrapped
-00005290: 0a0a 0a64 6566 205f 6765 745f 756e 626f  ...def _get_unbo
-000052a0: 756e 645f 666e 286d 6574 686f 645f 6f72  und_fn(method_or
-000052b0: 5f66 6e3a 2043 616c 6c61 626c 655b 2e2e  _fn: Callable[..
-000052c0: 2e2c 2041 6e79 5d29 202d 3e20 4361 6c6c  ., Any]) -> Call
-000052d0: 6162 6c65 5b2e 2e2e 2c20 416e 795d 3a0a  able[..., Any]:.
-000052e0: 2020 2222 2252 6574 7572 6e73 2061 6e20    """Returns an 
-000052f0: 756e 626f 756e 6420 6675 6e63 7469 6f6e  unbound function
-00005300: 2066 726f 6d20 6120 6d65 7468 6f64 2074   from a method t
-00005310: 6861 7420 6973 2070 6f73 7369 626c 7920  hat is possibly 
-00005320: 626f 756e 642e 0a0a 2020 5468 6973 206d  bound...  This m
-00005330: 6561 6e73 2074 6861 7420 6966 2074 6865  eans that if the
-00005340: 2070 6173 7365 6420 6675 6e63 7469 6f6e   passed function
-00005350: 2062 656c 6f6e 6773 206f 6620 616e 2069   belongs of an i
-00005360: 6e73 7461 6e63 6520 6f66 2061 2063 6c61  nstance of a cla
-00005370: 7373 2c20 7468 656e 0a20 2074 6865 2072  ss, then.  the r
-00005380: 6574 7572 6e65 6420 6675 6e63 7469 6f6e  eturned function
-00005390: 2064 6f65 7320 6e6f 206c 6f6e 6765 7220   does no longer 
-000053a0: 6465 7065 6e64 206f 6e20 7468 6520 696e  depend on the in
-000053b0: 7374 616e 6365 2c20 7768 6963 6820 6973  stance, which is
-000053c0: 2070 6173 7365 640a 2020 6173 2074 6865   passed.  as the
-000053d0: 2066 6972 7374 2061 7267 756d 656e 7420   first argument 
-000053e0: 746f 2074 6865 2066 756e 6374 696f 6e2e  to the function.
-000053f0: 0a0a 2020 4172 6773 3a0a 2020 2020 6d65  ..  Args:.    me
-00005400: 7468 6f64 5f6f 725f 666e 3a20 4120 636c  thod_or_fn: A cl
-00005410: 6173 7320 6d65 7468 6f64 206f 7220 6675  ass method or fu
-00005420: 6e63 7469 6f6e 2e0a 0a20 2052 6574 7572  nction...  Retur
-00005430: 6e73 3a0a 2020 2020 416e 2075 6e62 6f75  ns:.    An unbou
-00005440: 6e64 2076 6572 7369 6f6e 206f 6620 696e  nd version of in
-00005450: 7075 7420 6675 6e63 7469 6f6e 2e0a 2020  put function..  
-00005460: 2222 220a 2020 6966 2069 6e73 7065 6374  """.  if inspect
-00005470: 2e69 736d 6574 686f 6428 6d65 7468 6f64  .ismethod(method
-00005480: 5f6f 725f 666e 2920 616e 6420 6973 696e  _or_fn) and isin
-00005490: 7374 616e 6365 280a 2020 2020 6d65 7468  stance(.    meth
-000054a0: 6f64 5f6f 725f 666e 2e5f 5f73 656c 665f  od_or_fn.__self_
-000054b0: 5f2c 204d 6f64 756c 650a 2020 293a 2020  _, Module.  ):  
-000054c0: 2320 7079 7479 7065 3a20 6469 7361 626c  # pytype: disabl
-000054d0: 653d 6174 7472 6962 7574 652d 6572 726f  e=attribute-erro
-000054e0: 720a 2020 2020 6d65 7468 6f64 5f6f 725f  r.    method_or_
-000054f0: 666e 203d 206d 6574 686f 645f 6f72 5f66  fn = method_or_f
-00005500: 6e2e 5f5f 6675 6e63 5f5f 2020 2320 7079  n.__func__  # py
-00005510: 7479 7065 3a20 6469 7361 626c 653d 6174  type: disable=at
-00005520: 7472 6962 7574 652d 6572 726f 720a 0a20  tribute-error.. 
-00005530: 2023 2054 6865 206d 6574 686f 6420 7368   # The method sh
-00005540: 6f75 6c64 2062 6520 6361 6c6c 6162 6c65  ould be callable
-00005550: 2c20 616e 6420 6974 2073 686f 756c 6420  , and it should 
-00005560: 6861 7665 2061 7420 6c65 6173 7420 6f6e  have at least on
-00005570: 6520 6172 6775 6d65 6e74 0a20 2023 2072  e argument.  # r
-00005580: 6570 7265 7365 6e74 696e 6720 7468 6520  epresenting the 
-00005590: 636c 6173 7320 7468 6174 2069 7320 7061  class that is pa
-000055a0: 7373 6564 2069 6e2e 0a20 2069 6620 280a  ssed in..  if (.
-000055b0: 2020 2020 6e6f 7420 6361 6c6c 6162 6c65      not callable
-000055c0: 286d 6574 686f 645f 6f72 5f66 6e29 0a20  (method_or_fn). 
-000055d0: 2020 206f 7220 6c65 6e28 696e 7370 6563     or len(inspec
-000055e0: 742e 7369 676e 6174 7572 6528 6d65 7468  t.signature(meth
-000055f0: 6f64 5f6f 725f 666e 292e 7061 7261 6d65  od_or_fn).parame
-00005600: 7465 7273 2920 3c20 310a 2020 293a 0a20  ters) < 1.  ):. 
-00005610: 2020 2072 6169 7365 2065 7272 6f72 732e     raise errors.
-00005620: 4170 706c 794d 6f64 756c 6549 6e76 616c  ApplyModuleInval
-00005630: 6964 4d65 7468 6f64 4572 726f 7228 6d65  idMethodError(me
-00005640: 7468 6f64 5f6f 725f 666e 290a 0a20 2072  thod_or_fn)..  r
-00005650: 6574 7572 6e20 6d65 7468 6f64 5f6f 725f  eturn method_or_
-00005660: 666e 0a0a 0a64 6566 205f 6d61 705f 7375  fn...def _map_su
-00005670: 626d 6f64 756c 6573 2866 6e3a 2043 616c  bmodules(fn: Cal
-00005680: 6c61 626c 655b 5b27 4d6f 6475 6c65 275d  lable[['Module']
-00005690: 2c20 416e 795d 2c20 7472 6565 293a 0a20  , Any], tree):. 
-000056a0: 2022 2222 4d61 7020 6120 6675 6e63 7469   """Map a functi
-000056b0: 6f6e 206f 7665 7220 616c 6c20 7375 626d  on over all subm
-000056c0: 6f64 756c 6573 2069 6e20 6120 7472 6565  odules in a tree
-000056d0: 2e22 2222 0a20 2067 203d 206c 616d 6264  .""".  g = lambd
-000056e0: 6120 5f2c 2078 3a20 666e 2878 2920 6966  a _, x: fn(x) if
-000056f0: 2069 7369 6e73 7461 6e63 6528 782c 204d   isinstance(x, M
-00005700: 6f64 756c 6529 2065 6c73 6520 780a 2020  odule) else x.  
-00005710: 7265 7475 726e 205f 6672 6565 7a65 5f61  return _freeze_a
-00005720: 7474 7228 5f6d 6170 5f6f 7665 725f 6d6f  ttr(_map_over_mo
-00005730: 6475 6c65 735f 696e 5f74 7265 6528 672c  dules_in_tree(g,
-00005740: 2074 7265 6529 290a 0a0a 636c 6173 7320   tree))...class 
-00005750: 5365 7475 7053 7461 7465 2865 6e75 6d2e  SetupState(enum.
-00005760: 496e 7445 6e75 6d29 3a0a 2020 2320 7365  IntEnum):.  # se
-00005770: 7475 7028 2920 6861 7320 6e6f 7420 6265  tup() has not be
-00005780: 656e 2063 616c 6c65 642e 0a20 204e 4557  en called..  NEW
-00005790: 203d 2030 0a20 2023 2073 6574 7570 2829   = 0.  # setup()
-000057a0: 2068 6173 2062 6565 6e20 6361 6c6c 6564   has been called
-000057b0: 206f 7574 7369 6465 2061 2074 7261 6e73   outside a trans
-000057c0: 666f 726d 2062 6f75 6e64 6172 792e 0a20  form boundary.. 
-000057d0: 2054 5241 4e53 464f 524d 4544 203d 2031   TRANSFORMED = 1
-000057e0: 0a20 2023 2073 6574 7570 2829 2068 6173  .  # setup() has
-000057f0: 2062 6565 6e20 6361 6c6c 6564 2e0a 2020   been called..  
-00005800: 444f 4e45 203d 2032 0a0a 0a40 6461 7461  DONE = 2...@data
-00005810: 636c 6173 7365 732e 6461 7461 636c 6173  classes.dataclas
-00005820: 730a 636c 6173 7320 5f4d 6f64 756c 6549  s.class _ModuleI
-00005830: 6e74 6572 6e61 6c53 7461 7465 3a0a 2020  nternalState:.  
-00005840: 2222 2245 7068 656d 6572 616c 204d 6f64  """Ephemeral Mod
-00005850: 756c 6520 4576 616c 7561 7469 6f6e 2053  ule Evaluation S
-00005860: 7461 7465 2e0a 0a20 2046 6f72 2063 6c61  tate...  For cla
-00005870: 7269 7479 2c20 7765 2063 6f6c 6c65 6374  rity, we collect
-00005880: 2061 6c6c 206f 6620 7468 6520 7465 6d70   all of the temp
-00005890: 6f72 6172 7920 666c 6167 7320 616e 6420  orary flags and 
-000058a0: 6570 6865 6d65 7261 6c20 7374 6174 6520  ephemeral state 
-000058b0: 7573 6564 2062 790a 2020 4d6f 6475 6c65  used by.  Module
-000058c0: 7320 666f 7220 6175 746f 6e61 6d69 6e67  s for autonaming
-000058d0: 2061 6e64 2065 7272 6f72 206d 6573 7361   and error messa
-000058e0: 6765 7320 6865 7265 2c20 616c 6f6e 6773  ges here, alongs
-000058f0: 6964 6520 7468 6520 7275 6c65 7320 7573  ide the rules us
-00005900: 6564 0a20 2074 6f20 7061 7373 2074 6869  ed.  to pass thi
-00005910: 7320 6570 6865 6d65 7261 6c20 7374 6174  s ephemeral stat
-00005920: 6520 6163 726f 7373 2074 7261 6e73 666f  e across transfo
-00005930: 726d 2062 6f75 6e64 6172 6965 732e 0a20  rm boundaries.. 
-00005940: 2022 2222 0a0a 2020 696e 5f63 6f6d 7061   """..  in_compa
-00005950: 6374 5f6d 6574 686f 643a 2062 6f6f 6c20  ct_method: bool 
-00005960: 3d20 4661 6c73 650a 2020 696e 5f73 6574  = False.  in_set
-00005970: 7570 3a20 626f 6f6c 203d 2046 616c 7365  up: bool = False
-00005980: 0a20 2073 6574 7570 5f63 616c 6c65 643a  .  setup_called:
-00005990: 2053 6574 7570 5374 6174 6520 3d20 5365   SetupState = Se
-000059a0: 7475 7053 7461 7465 2e4e 4557 0a20 2069  tupState.NEW.  i
-000059b0: 735f 696e 6974 6961 6c69 7a65 643a 2062  s_initialized: b
-000059c0: 6f6f 6c20 3d20 4661 6c73 650a 2020 6175  ool = False.  au
-000059d0: 746f 6e61 6d65 5f63 7572 736f 723a 2044  toname_cursor: D
-000059e0: 6963 745b 7374 722c 2069 6e74 5d20 3d20  ict[str, int] = 
-000059f0: 6461 7461 636c 6173 7365 732e 6669 656c  dataclasses.fiel
-00005a00: 6428 6465 6661 756c 745f 6661 6374 6f72  d(default_factor
-00005a10: 793d 6469 6374 290a 2020 6368 696c 6472  y=dict).  childr
-00005a20: 656e 3a20 4469 6374 5b73 7472 2c20 556e  en: Dict[str, Un
-00005a30: 696f 6e5b 7374 722c 2027 4d6f 6475 6c65  ion[str, 'Module
-00005a40: 275d 5d20 3d20 6461 7461 636c 6173 7365  ']] = dataclasse
-00005a50: 732e 6669 656c 6428 0a20 2020 2064 6566  s.field(.    def
-00005a60: 6175 6c74 5f66 6163 746f 7279 3d64 6963  ault_factory=dic
-00005a70: 740a 2020 290a 0a20 2064 6566 2072 6573  t.  )..  def res
-00005a80: 6574 2873 656c 6629 202d 3e20 4e6f 6e65  et(self) -> None
-00005a90: 3a0a 2020 2020 2222 2252 6573 6574 7320  :.    """Resets 
-00005aa0: 7472 616e 7369 656e 7420 7374 6174 652e  transient state.
-00005ab0: 0a0a 2020 2020 5468 6973 2066 756e 6374  ..    This funct
-00005ac0: 696f 6e20 6973 2063 616c 6c65 6420 6166  ion is called af
-00005ad0: 7465 7220 6561 6368 206d 6f64 756c 6520  ter each module 
-00005ae0: 6d65 7468 6f64 2c20 736f 206f 6e6c 7920  method, so only 
-00005af0: 6174 7472 6962 7574 6573 2074 6861 740a  attributes that.
-00005b00: 2020 2020 6172 6520 6d65 7468 6f64 2d64      are method-d
-00005b10: 6570 656e 6465 6e74 2061 7265 2072 6573  ependent are res
-00005b20: 6574 2e0a 2020 2020 2222 220a 2020 2020  et..    """.    
-00005b30: 7365 6c66 2e69 6e5f 636f 6d70 6163 745f  self.in_compact_
-00005b40: 6d65 7468 6f64 203d 2046 616c 7365 0a20  method = False. 
-00005b50: 2020 2073 656c 662e 696e 5f73 6574 7570     self.in_setup
-00005b60: 203d 2046 616c 7365 0a20 2020 2073 656c   = False.    sel
-00005b70: 662e 6175 746f 6e61 6d65 5f63 7572 736f  f.autoname_curso
-00005b80: 7220 3d20 6469 6374 2829 0a0a 2020 6465  r = dict()..  de
-00005b90: 6620 6578 706f 7274 2873 656c 6629 202d  f export(self) -
-00005ba0: 3e20 275f 4d6f 6475 6c65 496e 7465 726e  > '_ModuleIntern
-00005bb0: 616c 5374 6174 6527 3a0a 2020 2020 2222  alState':.    ""
-00005bc0: 2245 7870 6f72 7473 2074 7261 6e73 666f  "Exports transfo
-00005bd0: 726d 2d70 7265 7365 7276 6564 2073 7461  rm-preserved sta
-00005be0: 7465 2061 6372 6f73 7320 7472 616e 7366  te across transf
-00005bf0: 6f72 6d20 626f 756e 6461 7279 2e22 2222  orm boundary."""
-00005c00: 0a20 2020 2073 6574 7570 5f73 7461 7465  .    setup_state
-00005c10: 203d 2028 0a20 2020 2020 2053 6574 7570   = (.      Setup
-00005c20: 5374 6174 652e 5452 414e 5346 4f52 4d45  State.TRANSFORME
-00005c30: 4420 6966 2073 656c 662e 7365 7475 705f  D if self.setup_
-00005c40: 6361 6c6c 6564 2065 6c73 6520 5365 7475  called else Setu
-00005c50: 7053 7461 7465 2e4e 4557 0a20 2020 2029  pState.NEW.    )
-00005c60: 0a20 2020 2063 6c6f 6e65 6420 3d20 5f4d  .    cloned = _M
-00005c70: 6f64 756c 6549 6e74 6572 6e61 6c53 7461  oduleInternalSta
-00005c80: 7465 280a 2020 2020 2020 696e 5f63 6f6d  te(.      in_com
-00005c90: 7061 6374 5f6d 6574 686f 643d 7365 6c66  pact_method=self
-00005ca0: 2e69 6e5f 636f 6d70 6163 745f 6d65 7468  .in_compact_meth
-00005cb0: 6f64 2c0a 2020 2020 2020 696e 5f73 6574  od,.      in_set
-00005cc0: 7570 3d73 656c 662e 696e 5f73 6574 7570  up=self.in_setup
-00005cd0: 2c0a 2020 2020 2020 7365 7475 705f 6361  ,.      setup_ca
-00005ce0: 6c6c 6564 3d73 6574 7570 5f73 7461 7465  lled=setup_state
-00005cf0: 2c0a 2020 2020 2020 6973 5f69 6e69 7469  ,.      is_initi
-00005d00: 616c 697a 6564 3d73 656c 662e 6973 5f69  alized=self.is_i
-00005d10: 6e69 7469 616c 697a 6564 2c0a 2020 2020  nitialized,.    
-00005d20: 2020 6175 746f 6e61 6d65 5f63 7572 736f    autoname_curso
-00005d30: 723d 6469 6374 2873 656c 662e 6175 746f  r=dict(self.auto
-00005d40: 6e61 6d65 5f63 7572 736f 7229 2c0a 2020  name_cursor),.  
-00005d50: 2020 290a 2020 2020 7265 7475 726e 2063    ).    return c
-00005d60: 6c6f 6e65 640a 0a20 2064 6566 2072 6569  loned..  def rei
-00005d70: 6d70 6f72 7428 7365 6c66 2c20 6f74 6865  mport(self, othe
-00005d80: 723a 2027 5f4d 6f64 756c 6549 6e74 6572  r: '_ModuleInter
-00005d90: 6e61 6c53 7461 7465 2729 202d 3e20 4e6f  nalState') -> No
-00005da0: 6e65 3a0a 2020 2020 2222 2252 652d 696d  ne:.    """Re-im
-00005db0: 706f 7274 7320 7472 616e 7366 6f72 6d2d  ports transform-
-00005dc0: 7072 6573 6572 7665 6420 7374 6174 6520  preserved state 
-00005dd0: 6672 6f6d 2061 6372 6f73 7320 7472 616e  from across tran
-00005de0: 7366 6f72 6d20 626f 756e 6461 7279 2e22  sform boundary."
-00005df0: 2222 0a20 2020 2073 656c 662e 696e 5f63  "".    self.in_c
-00005e00: 6f6d 7061 6374 5f6d 6574 686f 6420 3d20  ompact_method = 
-00005e10: 6f74 6865 722e 696e 5f63 6f6d 7061 6374  other.in_compact
-00005e20: 5f6d 6574 686f 640a 2020 2020 7365 6c66  _method.    self
-00005e30: 2e69 6e5f 7365 7475 7020 3d20 6f74 6865  .in_setup = othe
-00005e40: 722e 696e 5f73 6574 7570 0a20 2020 2073  r.in_setup.    s
-00005e50: 656c 662e 6973 5f69 6e69 7469 616c 697a  elf.is_initializ
-00005e60: 6564 203d 206f 7468 6572 2e69 735f 696e  ed = other.is_in
-00005e70: 6974 6961 6c69 7a65 640a 2020 2020 7365  itialized.    se
-00005e80: 6c66 2e61 7574 6f6e 616d 655f 6375 7273  lf.autoname_curs
-00005e90: 6f72 203d 2064 6963 7428 6f74 6865 722e  or = dict(other.
-00005ea0: 6175 746f 6e61 6d65 5f63 7572 736f 7229  autoname_cursor)
-00005eb0: 0a0a 0a5f 756e 696e 6974 6961 6c69 7a65  ..._uninitialize
-00005ec0: 645f 6d6f 6475 6c65 5f69 6e74 6572 6e61  d_module_interna
-00005ed0: 6c5f 7374 6174 6520 3d20 5f4d 6f64 756c  l_state = _Modul
-00005ee0: 6549 6e74 6572 6e61 6c53 7461 7465 2829  eInternalState()
-00005ef0: 0a0a 0a5f 554e 4445 4649 4e45 445f 434f  ..._UNDEFINED_CO
-00005f00: 5059 5f50 4943 4b4c 455f 4d45 5448 4f44  PY_PICKLE_METHOD
-00005f10: 5320 3d20 280a 2020 275f 5f67 6574 7374  S = (.  '__getst
-00005f20: 6174 655f 5f27 2c0a 2020 275f 5f73 6574  ate__',.  '__set
-00005f30: 7374 6174 655f 5f27 2c0a 2020 275f 5f67  state__',.  '__g
-00005f40: 6574 6e65 7761 7267 735f 6578 5f5f 272c  etnewargs_ex__',
-00005f50: 0a20 2027 5f5f 7265 6475 6365 5f5f 272c  .  '__reduce__',
-00005f60: 0a20 2027 5f5f 7265 6475 6365 5f65 785f  .  '__reduce_ex_
-00005f70: 5f27 2c0a 2020 275f 5f63 6f70 795f 5f27  _',.  '__copy__'
-00005f80: 2c0a 2020 275f 5f64 6565 7063 6f70 795f  ,.  '__deepcopy_
-00005f90: 5f27 2c0a 290a 0a0a 5f63 6163 6865 733a  _',.)..._caches:
-00005fa0: 2027 7765 616b 7265 662e 5765 616b 4b65   'weakref.WeakKe
-00005fb0: 7944 6963 7469 6f6e 6172 795b 5363 6f70  yDictionary[Scop
-00005fc0: 652c 2077 6561 6b72 6566 2e57 6561 6b56  e, weakref.WeakV
-00005fd0: 616c 7565 4469 6374 696f 6e61 7279 5b46  alueDictionary[F
-00005fe0: 6c61 7849 642c 204d 6f64 756c 655d 5d27  laxId, Module]]'
-00005ff0: 203d 2077 6561 6b72 6566 2e57 6561 6b4b   = weakref.WeakK
-00006000: 6579 4469 6374 696f 6e61 7279 2829 0a0a  eyDictionary()..
-00006010: 0a74 7570 6c65 5f72 6564 7563 6520 3d20  .tuple_reduce = 
-00006020: 6c61 6d62 6461 2078 732c 2078 3a20 7873  lambda xs, x: xs
-00006030: 202b 2028 782c 290a 7475 706c 655f 696e   + (x,).tuple_in
-00006040: 6974 203d 206c 616d 6264 613a 2028 290a  it = lambda: ().
-00006050: 0a0a 6361 7074 7572 655f 6361 6c6c 5f69  ..capture_call_i
-00006060: 6e74 6572 6d65 6469 6174 6573 203d 206c  ntermediates = l
-00006070: 616d 6264 6120 5f2c 206d 6574 686f 645f  ambda _, method_
-00006080: 6e61 6d65 3a20 6d65 7468 6f64 5f6e 616d  name: method_nam
-00006090: 6520 3d3d 2027 5f5f 6361 6c6c 5f5f 270a  e == '__call__'.
-000060a0: 0a0a 636c 6173 7320 5061 7265 6e74 4465  ..class ParentDe
-000060b0: 7363 7269 7074 6f72 3a0a 2020 2222 2257  scriptor:.  """W
-000060c0: 7261 7073 2070 6172 656e 7420 6d6f 6475  raps parent modu
-000060d0: 6c65 2072 6566 6572 656e 6365 7320 696e  le references in
-000060e0: 2077 6561 6b20 7265 6673 2e0a 0a20 2054   weak refs...  T
-000060f0: 6869 7320 7072 6576 656e 7473 2072 6566  his prevents ref
-00006100: 6572 656e 6365 2063 7963 6c65 7320 6672  erence cycles fr
-00006110: 6f6d 2066 6f72 6d69 6e67 2076 6961 2070  om forming via p
-00006120: 6172 656e 7420 6c69 6e6b 7320 7768 6963  arent links whic
-00006130: 6820 6361 6e20 6c65 6164 0a20 2074 6f20  h can lead.  to 
-00006140: 6163 6369 6465 6e74 616c 204f 4f4d 7320  accidental OOMs 
-00006150: 696e 2065 6167 6572 206d 6f64 6520 6475  in eager mode du
-00006160: 6520 746f 2073 6c6f 7720 6761 7262 6167  e to slow garbag
-00006170: 6520 636f 6c6c 6563 7469 6f6e 2061 7320  e collection as 
-00006180: 7765 6c6c 2061 730a 2020 7370 7572 696f  well as.  spurio
-00006190: 7573 2074 7261 6365 7220 6c65 616b 7320  us tracer leaks 
-000061a0: 6475 7269 6e67 206a 6974 2063 6f6d 7069  during jit compi
-000061b0: 6c61 7469 6f6e 2e0a 0a20 204e 6f74 653a  lation...  Note:
-000061c0: 2022 6465 7363 7269 7074 6f72 7322 2061   "descriptors" a
-000061d0: 7265 2074 6865 2075 6e64 6572 6c79 696e  re the underlyin
-000061e0: 6720 7079 7468 6f6e 206d 6563 6861 6e69  g python mechani
-000061f0: 736d 2066 6f72 2069 6d70 6c65 6d65 6e74  sm for implement
-00006200: 696e 670a 2020 6479 6e61 6d69 6320 4070  ing.  dynamic @p
-00006210: 726f 7065 7274 7920 6465 636f 7261 746f  roperty decorato
-00006220: 7273 2e20 2057 6520 6e65 6564 2074 6f20  rs.  We need to 
-00006230: 7573 6520 6120 7261 7720 6465 7363 7269  use a raw descri
-00006240: 7074 6f72 2069 6e73 7465 6164 206f 6620  ptor instead of 
-00006250: 7468 650a 2020 6d6f 7265 2063 6f6d 6d6f  the.  more commo
-00006260: 6e20 6465 636f 7261 746f 7220 696e 206f  n decorator in o
-00006270: 7264 6572 2074 6f20 666f 7263 6520 7468  rder to force th
-00006280: 6174 2074 6865 2061 7070 726f 7072 6961  at the appropria
-00006290: 7465 2067 6574 7465 722f 7365 7474 6572  te getter/setter
-000062a0: 0a20 206c 6f67 6963 2061 7070 6c69 6573  .  logic applies
-000062b0: 2069 6e20 7375 6263 6c61 7373 6573 2065   in subclasses e
-000062c0: 7665 6e20 6166 7465 7220 7661 7269 6f75  ven after variou
-000062d0: 7320 6461 7461 636c 6173 7320 7472 616e  s dataclass tran
-000062e0: 7366 6f72 6d73 2e0a 2020 2222 220a 0a20  sforms..  """.. 
-000062f0: 2064 6566 205f 5f67 6574 5f5f 2873 656c   def __get__(sel
-00006300: 662c 206f 626a 2c20 6f62 6a74 7970 653d  f, obj, objtype=
-00006310: 4e6f 6e65 293a 0a20 2020 2023 2063 6865  None):.    # che
-00006320: 636b 2069 6620 6f62 6a20 6973 204e 6f6e  ck if obj is Non
-00006330: 652c 2068 6170 7065 6e73 2064 7572 696e  e, happens durin
-00006340: 6720 2561 7574 6f72 656c 6f61 640a 2020  g %autoreload.  
-00006350: 2020 6966 206f 626a 2069 7320 4e6f 6e65    if obj is None
-00006360: 3a0a 2020 2020 2020 7265 7475 726e 204e  :.      return N
-00006370: 6f6e 650a 2020 2020 7061 7265 6e74 203d  one.    parent =
-00006380: 206f 626a 6563 742e 5f5f 6765 7461 7474   object.__getatt
-00006390: 7269 6275 7465 5f5f 286f 626a 2c20 275f  ribute__(obj, '_
-000063a0: 7061 7265 6e74 5f72 6566 2729 0a20 2020  parent_ref').   
-000063b0: 2072 6574 7572 6e20 7061 7265 6e74 2829   return parent()
-000063c0: 2069 6620 6973 696e 7374 616e 6365 2870   if isinstance(p
-000063d0: 6172 656e 742c 2077 6561 6b72 6566 2e52  arent, weakref.R
-000063e0: 6566 6572 656e 6365 5479 7065 2920 656c  eferenceType) el
-000063f0: 7365 2070 6172 656e 740a 0a20 2064 6566  se parent..  def
-00006400: 205f 5f73 6574 5f5f 2873 656c 662c 206f   __set__(self, o
-00006410: 626a 2c20 7661 6c75 6529 3a0a 2020 2020  bj, value):.    
-00006420: 6d61 7962 655f 7765 616b 203d 2077 6561  maybe_weak = wea
-00006430: 6b72 6566 2e72 6566 2876 616c 7565 2920  kref.ref(value) 
-00006440: 6966 2069 7369 6e73 7461 6e63 6528 7661  if isinstance(va
-00006450: 6c75 652c 204d 6f64 756c 6529 2065 6c73  lue, Module) els
-00006460: 6520 7661 6c75 650a 2020 2020 6f62 6a65  e value.    obje
-00006470: 6374 2e5f 5f73 6574 6174 7472 5f5f 286f  ct.__setattr__(o
-00006480: 626a 2c20 275f 7061 7265 6e74 5f72 6566  bj, '_parent_ref
-00006490: 272c 206d 6179 6265 5f77 6561 6b29 0a0a  ', maybe_weak)..
-000064a0: 0a63 6c61 7373 2044 6573 6372 6970 746f  .class Descripto
-000064b0: 7228 7470 652e 5072 6f74 6f63 6f6c 293a  r(tpe.Protocol):
-000064c0: 0a20 205f 5f69 7361 6273 7472 6163 746d  .  __isabstractm
-000064d0: 6574 686f 645f 5f3a 2062 6f6f 6c0a 0a20  ethod__: bool.. 
-000064e0: 2064 6566 205f 5f67 6574 5f5f 2873 656c   def __get__(sel
-000064f0: 662c 206f 626a 2c20 6f62 6a74 7970 653d  f, obj, objtype=
-00006500: 4e6f 6e65 2920 2d3e 2041 6e79 3a0a 2020  None) -> Any:.  
-00006510: 2020 2e2e 2e0a 0a20 2064 6566 205f 5f73    .....  def __s
-00006520: 6574 5f5f 2873 656c 662c 206f 626a 2c20  et__(self, obj, 
-00006530: 7661 6c75 6529 202d 3e20 4e6f 6e65 3a0a  value) -> None:.
-00006540: 2020 2020 2e2e 2e0a 0a20 2064 6566 205f      .....  def _
-00006550: 5f64 656c 6574 655f 5f28 7365 6c66 2c20  _delete__(self, 
-00006560: 6f62 6a29 202d 3e20 4e6f 6e65 3a0a 2020  obj) -> None:.  
-00006570: 2020 2e2e 2e0a 0a20 2064 6566 205f 5f73    .....  def __s
-00006580: 6574 5f6e 616d 655f 5f28 7365 6c66 2c20  et_name__(self, 
-00006590: 6f77 6e65 722c 206e 616d 6529 202d 3e20  owner, name) -> 
-000065a0: 4e6f 6e65 3a0a 2020 2020 2e2e 2e0a 0a0a  None:.    ......
-000065b0: 636c 6173 7320 4465 7363 7269 7074 6f72  class Descriptor
-000065c0: 5772 6170 7065 723a 0a20 2070 6173 730a  Wrapper:.  pass.
-000065d0: 0a0a 6465 6620 6372 6561 7465 5f64 6573  ..def create_des
-000065e0: 6372 6970 746f 725f 7772 6170 7065 7228  criptor_wrapper(
-000065f0: 6465 7363 7269 7074 6f72 3a20 4465 7363  descriptor: Desc
-00006600: 7269 7074 6f72 293a 0a20 2022 2222 4372  riptor):.  """Cr
-00006610: 6561 7465 7320 6120 6465 7363 7269 7074  eates a descript
-00006620: 6f72 2077 7261 7070 6572 2074 6861 7420  or wrapper that 
-00006630: 6361 6c6c 7320 6120 6765 745f 666e 206f  calls a get_fn o
-00006640: 6e20 7468 6520 6465 7363 7269 7074 6f72  n the descriptor
-00006650: 2e22 2222 0a0a 2020 636c 6173 7320 5f44  ."""..  class _D
-00006660: 6573 6372 6970 746f 7257 7261 7070 6572  escriptorWrapper
-00006670: 2844 6573 6372 6970 746f 7257 7261 7070  (DescriptorWrapp
-00006680: 6572 293a 0a20 2020 2022 2222 4120 6465  er):.    """A de
-00006690: 7363 7269 7074 6f72 2074 6861 7420 6361  scriptor that ca
-000066a0: 6e20 7772 6170 2061 6e79 2064 6573 6372  n wrap any descr
-000066b0: 6970 746f 722e 2222 220a 0a20 2020 2069  iptor."""..    i
-000066c0: 6620 6861 7361 7474 7228 6465 7363 7269  f hasattr(descri
-000066d0: 7074 6f72 2c20 275f 5f69 7361 6273 7472  ptor, '__isabstr
-000066e0: 6163 746d 6574 686f 645f 5f27 293a 0a20  actmethod__'):. 
-000066f0: 2020 2020 205f 5f69 7361 6273 7472 6163       __isabstrac
-00006700: 746d 6574 686f 645f 5f20 3d20 6465 7363  tmethod__ = desc
-00006710: 7269 7074 6f72 2e5f 5f69 7361 6273 7472  riptor.__isabstr
-00006720: 6163 746d 6574 686f 645f 5f0a 0a20 2020  actmethod__..   
-00006730: 2064 6566 205f 5f69 6e69 745f 5f28 7365   def __init__(se
-00006740: 6c66 2c20 7772 6170 7065 643a 2044 6573  lf, wrapped: Des
-00006750: 6372 6970 746f 7229 3a0a 2020 2020 2020  criptor):.      
-00006760: 7365 6c66 2e77 7261 7070 6564 203d 2077  self.wrapped = w
-00006770: 7261 7070 6564 0a0a 2020 2020 2320 636f  rapped..    # co
-00006780: 6e64 6974 696f 6e61 6c6c 7920 6465 6669  nditionally defi
-00006790: 6e65 2064 6573 6372 6970 746f 7220 6d65  ne descriptor me
-000067a0: 7468 6f64 730a 2020 2020 6966 2068 6173  thods.    if has
-000067b0: 6174 7472 2864 6573 6372 6970 746f 722c  attr(descriptor,
-000067c0: 2027 5f5f 6765 745f 5f27 293a 0a0a 2020   '__get__'):..  
-000067d0: 2020 2020 6465 6620 5f5f 6765 745f 5f28      def __get__(
-000067e0: 7365 6c66 2c20 2a61 7267 732c 202a 2a6b  self, *args, **k
-000067f0: 7761 7267 7329 3a0a 2020 2020 2020 2020  wargs):.        
-00006800: 2320 6865 7265 2077 6520 7769 6c6c 2063  # here we will c
-00006810: 6174 6368 2069 6e74 6572 6e61 6c20 4174  atch internal At
-00006820: 7472 6962 7574 6545 7272 6f72 2061 6e64  tributeError and
-00006830: 2072 652d 7261 6973 6520 6974 2061 7320   re-raise it as 
-00006840: 610a 2020 2020 2020 2020 2320 6d6f 7265  a.        # more
-00006850: 2069 6e66 6f72 6d61 7469 7665 2061 6e64   informative and
-00006860: 2063 6f72 7265 6374 2065 7272 6f72 206d   correct error m
-00006870: 6573 7361 6765 2e0a 2020 2020 2020 2020  essage..        
-00006880: 7472 793a 0a20 2020 2020 2020 2020 2072  try:.          r
-00006890: 6574 7572 6e20 7365 6c66 2e77 7261 7070  eturn self.wrapp
-000068a0: 6564 2e5f 5f67 6574 5f5f 282a 6172 6773  ed.__get__(*args
-000068b0: 2c20 2a2a 6b77 6172 6773 290a 2020 2020  , **kwargs).    
-000068c0: 2020 2020 6578 6365 7074 2041 7474 7269      except Attri
-000068d0: 6275 7465 4572 726f 7220 6173 2065 3a0a  buteError as e:.
-000068e0: 2020 2020 2020 2020 2020 7261 6973 6520            raise 
-000068f0: 6572 726f 7273 2e44 6573 6372 6970 746f  errors.Descripto
-00006900: 7241 7474 7269 6275 7465 4572 726f 7228  rAttributeError(
-00006910: 2920 6672 6f6d 2065 0a0a 2020 2020 6966  ) from e..    if
-00006920: 2068 6173 6174 7472 2864 6573 6372 6970   hasattr(descrip
-00006930: 746f 722c 2027 5f5f 7365 745f 5f27 293a  tor, '__set__'):
-00006940: 0a0a 2020 2020 2020 6465 6620 5f5f 7365  ..      def __se
-00006950: 745f 5f28 7365 6c66 2c20 2a61 7267 732c  t__(self, *args,
-00006960: 202a 2a6b 7761 7267 7329 3a0a 2020 2020   **kwargs):.    
-00006970: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-00006980: 7772 6170 7065 642e 5f5f 7365 745f 5f28  wrapped.__set__(
-00006990: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
-000069a0: 0a0a 2020 2020 6966 2068 6173 6174 7472  ..    if hasattr
-000069b0: 2864 6573 6372 6970 746f 722c 2027 5f5f  (descriptor, '__
-000069c0: 6465 6c65 7465 5f5f 2729 3a0a 0a20 2020  delete__'):..   
-000069d0: 2020 2064 6566 205f 5f64 656c 6574 655f     def __delete_
-000069e0: 5f28 7365 6c66 2c20 2a61 7267 732c 202a  _(self, *args, *
-000069f0: 2a6b 7761 7267 7329 3a0a 2020 2020 2020  *kwargs):.      
-00006a00: 2020 7265 7475 726e 2073 656c 662e 7772    return self.wr
-00006a10: 6170 7065 642e 5f5f 6465 6c65 7465 5f5f  apped.__delete__
-00006a20: 282a 6172 6773 2c20 2a2a 6b77 6172 6773  (*args, **kwargs
-00006a30: 290a 0a20 2020 2069 6620 6861 7361 7474  )..    if hasatt
-00006a40: 7228 6465 7363 7269 7074 6f72 2c20 275f  r(descriptor, '_
-00006a50: 5f73 6574 5f6e 616d 655f 5f27 293a 0a0a  _set_name__'):..
-00006a60: 2020 2020 2020 6465 6620 5f5f 7365 745f        def __set_
-00006a70: 6e61 6d65 5f5f 2873 656c 662c 202a 6172  name__(self, *ar
-00006a80: 6773 2c20 2a2a 6b77 6172 6773 293a 0a20  gs, **kwargs):. 
-00006a90: 2020 2020 2020 2073 656c 662e 7772 6170         self.wrap
-00006aa0: 7065 642e 5f5f 7365 745f 6e61 6d65 5f5f  ped.__set_name__
-00006ab0: 282a 6172 6773 2c20 2a2a 6b77 6172 6773  (*args, **kwargs
-00006ac0: 290a 0a20 2020 2064 6566 205f 5f67 6574  )..    def __get
-00006ad0: 6174 7472 5f5f 2873 656c 662c 206e 616d  attr__(self, nam
-00006ae0: 6529 3a0a 2020 2020 2020 6966 2027 7772  e):.      if 'wr
-00006af0: 6170 7065 6427 206e 6f74 2069 6e20 7661  apped' not in va
-00006b00: 7273 2873 656c 6629 3a0a 2020 2020 2020  rs(self):.      
-00006b10: 2020 7261 6973 6520 4174 7472 6962 7574    raise Attribut
-00006b20: 6545 7272 6f72 2829 0a20 2020 2020 2072  eError().      r
-00006b30: 6574 7572 6e20 6765 7461 7474 7228 7365  eturn getattr(se
-00006b40: 6c66 2e77 7261 7070 6564 2c20 6e61 6d65  lf.wrapped, name
-00006b50: 290a 0a20 2072 6574 7572 6e20 5f44 6573  )..  return _Des
-00006b60: 6372 6970 746f 7257 7261 7070 6572 2864  criptorWrapper(d
-00006b70: 6573 6372 6970 746f 7229 0a0a 0a23 2042  escriptor)...# B
-00006b80: 6173 6520 4d6f 6475 6c65 2064 6566 696e  ase Module defin
-00006b90: 6974 696f 6e2e 0a23 202d 2d2d 2d2d 2d2d  ition..# -------
-00006ba0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00006bb0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00006bc0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00006bd0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
-00006be0: 2d2d 2d2d 2d2d 0a0a 0a64 6566 206d 6f64  ------...def mod
-00006bf0: 756c 655f 6669 656c 6428 2a2c 206b 775f  ule_field(*, kw_
-00006c00: 6f6e 6c79 3a20 626f 6f6c 203d 2046 616c  only: bool = Fal
-00006c10: 7365 2c20 6465 6661 756c 743a 204f 7074  se, default: Opt
-00006c20: 696f 6e61 6c5b 416e 795d 203d 202e 2e2e  ional[Any] = ...
-00006c30: 2920 2d3e 2041 6e79 3a0a 2020 2e2e 2e0a  ) -> Any:.  ....
-00006c40: 0a0a 2320 5468 6520 4d6f 6475 6c65 4261  ..# The ModuleBa
-00006c50: 7365 2063 6c61 7373 2069 7320 6372 6561  se class is crea
-00006c60: 7465 6420 6f6e 6c79 2074 6f20 6d61 6b65  ted only to make
-00006c70: 2073 7461 7469 6320 616e 616c 797a 6572   static analyzer
-00006c80: 7320 6861 7070 790a 2320 6d61 696e 6c79  s happy.# mainly
-00006c90: 2070 7974 7970 6520 616e 6420 7079 7269   pytype and pyri
-00006ca0: 6768 742e 2053 6f6d 6520 6e6f 7465 733a  ght. Some notes:
-00006cb0: 0a23 202a 2070 7972 6967 6874 2028 636f  .# * pyright (co
-00006cc0: 7272 6563 746c 7929 2063 6f6d 706c 6169  rrectly) complai
-00006cd0: 6e73 2074 6861 7420 4d6f 6475 6c65 2069  ns that Module i
-00006ce0: 7473 656c 6620 6973 206e 6f74 2061 2064  tself is not a d
-00006cf0: 6174 6163 6c61 7373 2c20 6576 656e 0a23  ataclass, even.#
-00006d00: 2020 2074 686f 7567 6820 616c 6c20 6974     though all it
-00006d10: 7320 7375 6263 6c61 7373 6573 2061 6e64  s subclasses and
-00006d20: 2069 6e74 616e 6365 7320 4152 4520 6461   intances ARE da
-00006d30: 7461 636c 6173 7365 732e 2042 6563 6175  taclasses. Becau
-00006d40: 7365 2074 6865 7265 2069 7320 6e6f 0a23  se there is no.#
-00006d50: 2020 2077 6179 2074 6f20 616e 6e6f 7461     way to annota
-00006d60: 7465 2074 6869 7320 696e 2061 2077 6179  te this in a way
-00006d70: 2074 6861 7420 7079 7269 6768 7420 756e   that pyright un
-00006d80: 6465 7273 7461 6e64 732c 2077 6520 6372  derstands, we cr
-00006d90: 6561 7465 2061 0a23 2020 204d 6f64 756c  eate a.#   Modul
-00006da0: 6542 6173 6520 636c 6173 7320 6465 636f  eBase class deco
-00006db0: 7261 7465 6420 7769 7468 2060 6461 7461  rated with `data
-00006dc0: 636c 6173 735f 7472 616e 7366 6f72 6d60  class_transform`
-00006dd0: 2073 7563 6820 7468 6174 2070 7972 6967   such that pyrig
-00006de0: 6874 0a23 2020 2074 6869 6e6b 7320 4d6f  ht.#   thinks Mo
-00006df0: 6475 6c65 2069 7320 6120 6461 7461 636c  dule is a datacl
-00006e00: 6173 7320 2869 6e20 7265 616c 6974 7920  ass (in reality 
-00006e10: 6f6e 6c79 2073 7562 636c 6173 7365 7320  only subclasses 
-00006e20: 6172 6520 696e 7374 616e 7469 6174 6564  are instantiated
-00006e30: 0a23 2020 2073 6f20 7468 6973 2069 7320  .#   so this is 
-00006e40: 6669 6e65 292e 0a23 202a 2054 6865 2060  fine)..# * The `
-00006e50: 5f5f 6461 7461 636c 6173 735f 6669 656c  __dataclass_fiel
-00006e60: 6473 5f5f 6020 6174 7472 6962 7574 6520  ds__` attribute 
-00006e70: 6973 206e 6565 6465 6420 6265 6361 7573  is needed becaus
-00006e80: 6520 7079 7479 7065 2073 6565 6d73 2074  e pytype seems t
-00006e90: 6f0a 2320 2020 6e6f 7420 756e 6465 7273  o.#   not unders
-00006ea0: 7461 6e64 2074 6865 2060 6461 7461 636c  tand the `datacl
-00006eb0: 6173 735f 7472 616e 7366 6f72 6d60 2064  ass_transform` d
-00006ec0: 6563 6f72 6174 6f72 2c20 7468 6572 6566  ecorator, theref
-00006ed0: 6f72 6520 7765 206e 6565 640a 2320 2020  ore we need.#   
-00006ee0: 746f 2061 6464 2074 6865 2061 7474 7269  to add the attri
-00006ef0: 6275 7465 206d 616e 7561 6c6c 792e 0a23  bute manually..#
-00006f00: 202a 204f 7468 6572 2061 7474 7269 6275   * Other attribu
-00006f10: 7465 7320 6172 6520 616e 6e6f 7461 7465  tes are annotate
-00006f20: 6420 666f 7220 636f 6d70 6c65 7465 6e65  d for completene
-00006f30: 7373 2e20 4265 6361 7573 6520 7765 2061  ss. Because we a
-00006f40: 7265 2075 7369 6e67 0a23 2020 2074 6865  re using.#   the
-00006f50: 2060 6966 2074 7970 696e 672e 5459 5045   `if typing.TYPE
-00006f60: 5f43 4845 434b 494e 4760 2070 6174 7465  _CHECKING` patte
-00006f70: 726e 2c20 7468 6573 6520 616e 6e6f 7461  rn, these annota
-00006f80: 7469 6f6e 7320 6172 6520 6e6f 7420 7072  tions are not pr
-00006f90: 6573 656e 740a 2320 2020 6174 2072 756e  esent.#   at run
-00006fa0: 7469 6d65 2073 6f20 7468 6579 2064 6f6e  time so they don
-00006fb0: 2774 2061 6666 6563 7420 7468 6520 6461  't affect the da
-00006fc0: 7461 636c 6173 7320 6265 6861 7669 6f72  taclass behavior
-00006fd0: 2e0a 4074 7065 2e64 6174 6163 6c61 7373  ..@tpe.dataclass
-00006fe0: 5f74 7261 6e73 666f 726d 2866 6965 6c64  _transform(field
-00006ff0: 5f73 7065 6369 6669 6572 733d 286d 6f64  _specifiers=(mod
-00007000: 756c 655f 6669 656c 642c 2929 2020 2320  ule_field,))  # 
-00007010: 7479 7065 3a20 6967 6e6f 7265 5b6c 6974  type: ignore[lit
-00007020: 6572 616c 2d72 6571 7569 7265 645d 0a63  eral-required].c
-00007030: 6c61 7373 204d 6f64 756c 6542 6173 653a  lass ModuleBase:
-00007040: 0a20 2069 6620 7479 7069 6e67 2e54 5950  .  if typing.TYP
-00007050: 455f 4348 4543 4b49 4e47 3a0a 2020 2020  E_CHECKING:.    
-00007060: 7363 6f70 653a 204f 7074 696f 6e61 6c5b  scope: Optional[
-00007070: 5363 6f70 655d 0a20 2020 205f 7374 6174  Scope].    _stat
-00007080: 653a 205f 4d6f 6475 6c65 496e 7465 726e  e: _ModuleIntern
-00007090: 616c 5374 6174 650a 2020 2020 5f70 6172  alState.    _par
-000070a0: 656e 745f 7265 663a 2055 6e69 6f6e 5b27  ent_ref: Union['
-000070b0: 4d6f 6475 6c65 272c 2077 6561 6b72 6566  Module', weakref
-000070c0: 2e52 6566 6572 656e 6365 5479 7065 5b27  .ReferenceType['
-000070d0: 4d6f 6475 6c65 275d 2c20 4e6f 6e65 5d0a  Module'], None].
-000070e0: 2020 2020 5f5f 6461 7461 636c 6173 735f      __dataclass_
-000070f0: 6669 656c 6473 5f5f 3a20 4469 6374 5b73  fields__: Dict[s
-00007100: 7472 2c20 6461 7461 636c 6173 7365 732e  tr, dataclasses.
-00007110: 4669 656c 645d 0a0a 0a63 6c61 7373 204d  Field]...class M
-00007120: 6f64 756c 6528 4d6f 6475 6c65 4261 7365  odule(ModuleBase
-00007130: 293a 0a20 2022 2222 4261 7365 2063 6c61  ):.  """Base cla
-00007140: 7373 2066 6f72 2061 6c6c 206e 6575 7261  ss for all neura
-00007150: 6c20 6e65 7477 6f72 6b20 6d6f 6475 6c65  l network module
-00007160: 732e 0a0a 2020 4c61 7965 7273 2061 6e64  s...  Layers and
-00007170: 206d 6f64 656c 7320 7368 6f75 6c64 2073   models should s
-00007180: 7562 636c 6173 7320 7468 6973 2063 6c61  ubclass this cla
-00007190: 7373 2e0a 0a20 2041 6c6c 2046 6c61 7820  ss...  All Flax 
-000071a0: 4d6f 6475 6c65 7320 6172 6520 5079 7468  Modules are Pyth
-000071b0: 6f6e 2033 2e37 0a20 2060 6461 7461 636c  on 3.7.  `datacl
-000071c0: 6173 7365 7320 3c68 7474 7073 3a2f 2f64  asses <https://d
-000071d0: 6f63 732e 7079 7468 6f6e 2e6f 7267 2f33  ocs.python.org/3
-000071e0: 2f6c 6962 7261 7279 2f64 6174 6163 6c61  /library/datacla
-000071f0: 7373 6573 2e68 746d 6c3e 605f 2e20 5369  sses.html>`_. Si
-00007200: 6e63 650a 2020 6461 7461 636c 6173 7365  nce.  dataclasse
-00007210: 7320 7461 6b65 206f 7665 7220 6060 5f5f  s take over ``__
-00007220: 696e 6974 5f5f 6060 2c20 796f 7520 7368  init__``, you sh
-00007230: 6f75 6c64 2069 6e73 7465 6164 206f 7665  ould instead ove
-00007240: 7272 6964 6520 3a6d 6574 683a 6073 6574  rride :meth:`set
-00007250: 7570 602c 0a20 2077 6869 6368 2069 7320  up`,.  which is 
-00007260: 6175 746f 6d61 7469 6361 6c6c 7920 6361  automatically ca
-00007270: 6c6c 6564 2074 6f20 696e 6974 6961 6c69  lled to initiali
-00007280: 7a65 2074 6865 206d 6f64 756c 652e 0a0a  ze the module...
-00007290: 2020 4d6f 6475 6c65 7320 6361 6e20 636f    Modules can co
-000072a0: 6e74 6169 6e20 7375 626d 6f64 756c 6573  ntain submodules
-000072b0: 2c20 616e 6420 696e 2074 6869 7320 7761  , and in this wa
-000072c0: 7920 6361 6e20 6265 206e 6573 7465 6420  y can be nested 
-000072d0: 696e 2061 2074 7265 650a 2020 7374 7275  in a tree.  stru
-000072e0: 6374 7572 652e 2053 7562 6d6f 6465 6c73  cture. Submodels
-000072f0: 2063 616e 2062 6520 6173 7369 676e 6564   can be assigned
-00007300: 2061 7320 7265 6775 6c61 7220 6174 7472   as regular attr
-00007310: 6962 7574 6573 2069 6e73 6964 6520 7468  ibutes inside th
-00007320: 650a 2020 3a6d 6574 683a 6073 6574 7570  e.  :meth:`setup
-00007330: 6020 6d65 7468 6f64 2e0a 0a20 2059 6f75  ` method...  You
-00007340: 2063 616e 2064 6566 696e 6520 6172 6269   can define arbi
-00007350: 7472 6172 7920 2266 6f72 7761 7264 2070  trary "forward p
-00007360: 6173 7322 206d 6574 686f 6473 206f 6e20  ass" methods on 
-00007370: 796f 7572 204d 6f64 756c 6520 7375 6263  your Module subc
-00007380: 6c61 7373 2e0a 2020 5768 696c 6520 6e6f  lass..  While no
-00007390: 206d 6574 686f 6473 2061 7265 2073 7065   methods are spe
-000073a0: 6369 616c 2d63 6173 6564 2c20 6060 5f5f  cial-cased, ``__
-000073b0: 6361 6c6c 5f5f 6060 2069 7320 6120 706f  call__`` is a po
-000073c0: 7075 6c61 7220 6368 6f69 6365 2062 6563  pular choice bec
-000073d0: 6175 7365 0a20 2069 7420 616c 6c6f 7773  ause.  it allows
-000073e0: 2079 6f75 2074 6f20 7573 6520 6d6f 6475   you to use modu
-000073f0: 6c65 2069 6e73 7461 6e63 6573 2061 7320  le instances as 
-00007400: 6966 2074 6865 7920 6172 6520 6675 6e63  if they are func
-00007410: 7469 6f6e 733a 3a0a 0a20 2020 203e 3e3e  tions::..    >>>
-00007420: 2066 726f 6d20 666c 6178 2069 6d70 6f72   from flax impor
-00007430: 7420 6c69 6e65 6e20 6173 206e 6e0a 2020  t linen as nn.  
-00007440: 2020 3e3e 3e20 6672 6f6d 2074 7970 696e    >>> from typin
-00007450: 6720 696d 706f 7274 2054 7570 6c65 0a0a  g import Tuple..
-00007460: 2020 2020 3e3e 3e20 636c 6173 7320 4d6f      >>> class Mo
-00007470: 6475 6c65 286e 6e2e 4d6f 6475 6c65 293a  dule(nn.Module):
-00007480: 0a20 2020 202e 2e2e 2020 2066 6561 7475  .    ...   featu
-00007490: 7265 733a 2054 7570 6c65 5b69 6e74 2c20  res: Tuple[int, 
-000074a0: 2e2e 2e5d 203d 2028 3136 2c20 3429 0a0a  ...] = (16, 4)..
-000074b0: 2020 2020 2e2e 2e20 2020 6465 6620 7365      ...   def se
-000074c0: 7475 7028 7365 6c66 293a 0a20 2020 202e  tup(self):.    .
-000074d0: 2e2e 2020 2020 2073 656c 662e 6465 6e73  ..     self.dens
-000074e0: 6531 203d 206e 6e2e 4465 6e73 6528 7365  e1 = nn.Dense(se
-000074f0: 6c66 2e66 6561 7475 7265 735b 305d 290a  lf.features[0]).
-00007500: 2020 2020 2e2e 2e20 2020 2020 7365 6c66      ...     self
-00007510: 2e64 656e 7365 3220 3d20 6e6e 2e44 656e  .dense2 = nn.Den
-00007520: 7365 2873 656c 662e 6665 6174 7572 6573  se(self.features
-00007530: 5b31 5d29 0a0a 2020 2020 2e2e 2e20 2020  [1])..    ...   
-00007540: 6465 6620 5f5f 6361 6c6c 5f5f 2873 656c  def __call__(sel
-00007550: 662c 2078 293a 0a20 2020 202e 2e2e 2020  f, x):.    ...  
-00007560: 2020 2072 6574 7572 6e20 7365 6c66 2e64     return self.d
-00007570: 656e 7365 3228 6e6e 2e72 656c 7528 7365  ense2(nn.relu(se
-00007580: 6c66 2e64 656e 7365 3128 7829 2929 0a0a  lf.dense1(x)))..
-00007590: 2020 4f70 7469 6f6e 616c 6c79 2c20 666f    Optionally, fo
-000075a0: 7220 6d6f 7265 2063 6f6e 6369 7365 206d  r more concise m
-000075b0: 6f64 756c 6520 696d 706c 656d 656e 7461  odule implementa
-000075c0: 7469 6f6e 7320 7768 6572 6520 7375 626d  tions where subm
-000075d0: 6f64 756c 6573 0a20 2064 6566 696e 6974  odules.  definit
-000075e0: 696f 6e73 2061 7265 2063 6f2d 6c6f 6361  ions are co-loca
-000075f0: 7465 6420 7769 7468 2074 6865 6972 2075  ted with their u
-00007600: 7361 6765 2c20 796f 7520 6361 6e20 7573  sage, you can us
-00007610: 6520 7468 650a 2020 3a6d 6574 683a 6063  e the.  :meth:`c
-00007620: 6f6d 7061 6374 6020 7772 6170 7065 722e  ompact` wrapper.
-00007630: 0a20 2022 2222 0a0a 2020 6966 2074 7970  .  """..  if typ
-00007640: 696e 672e 5459 5045 5f43 4845 434b 494e  ing.TYPE_CHECKIN
-00007650: 473a 0a20 2020 206e 616d 653a 204f 7074  G:.    name: Opt
-00007660: 696f 6e61 6c5b 7374 725d 203d 206d 6f64  ional[str] = mod
-00007670: 756c 655f 6669 656c 6428 6b77 5f6f 6e6c  ule_field(kw_onl
-00007680: 793d 5472 7565 2c20 6465 6661 756c 743d  y=True, default=
-00007690: 4e6f 6e65 290a 2020 2020 7061 7265 6e74  None).    parent
-000076a0: 3a20 556e 696f 6e5b 274d 6f64 756c 6527  : Union['Module'
-000076b0: 2c20 5f53 656e 7469 6e65 6c2c 204e 6f6e  , _Sentinel, Non
-000076c0: 655d 203d 206d 6f64 756c 655f 6669 656c  e] = module_fiel
-000076d0: 6428 0a20 2020 2020 206b 775f 6f6e 6c79  d(.      kw_only
-000076e0: 3d54 7275 652c 2064 6566 6175 6c74 3d4e  =True, default=N
-000076f0: 6f6e 650a 2020 2020 290a 0a20 2020 2064  one.    )..    d
-00007700: 6566 205f 5f69 6e69 745f 5f28 7365 6c66  ef __init__(self
-00007710: 2c20 2a61 7267 732c 202a 2a6b 7761 7267  , *args, **kwarg
-00007720: 7329 3a0a 2020 2020 2020 2320 7468 6973  s):.      # this
-00007730: 2073 7475 6220 6d61 6b65 7320 7375 7265   stub makes sure
-00007740: 2070 7974 7970 6520 6163 6365 7074 7320   pytype accepts 
-00007750: 636f 6e73 7472 7563 746f 7220 6172 6775  constructor argu
-00007760: 6d65 6e74 732e 0a20 2020 2020 2070 6173  ments..      pas
-00007770: 730a 0a20 2020 2064 6566 205f 5f63 616c  s..    def __cal
-00007780: 6c5f 5f28 7365 6c66 2c20 2a61 7267 732c  l__(self, *args,
-00007790: 202a 2a6b 7761 7267 7329 202d 3e20 416e   **kwargs) -> An
-000077a0: 793a 0a20 2020 2020 2023 2074 6869 7320  y:.      # this 
-000077b0: 7374 7562 2061 6c6c 6f77 7320 7079 7479  stub allows pyty
-000077c0: 7065 2074 6f20 6163 6365 7074 204d 6f64  pe to accept Mod
-000077d0: 756c 6573 2061 7320 4361 6c6c 6162 6c65  ules as Callable
-000077e0: 732e 0a20 2020 2020 2070 6173 730a 0a20  s..      pass.. 
-000077f0: 2040 636c 6173 736d 6574 686f 640a 2020   @classmethod.  
-00007800: 6465 6620 5f5f 696e 6974 5f73 7562 636c  def __init_subcl
-00007810: 6173 735f 5f28 636c 732c 206b 775f 6f6e  ass__(cls, kw_on
-00007820: 6c79 3a20 626f 6f6c 203d 2046 616c 7365  ly: bool = False
-00007830: 2c20 2a2a 6b77 6172 6773 3a20 416e 7929  , **kwargs: Any)
-00007840: 202d 3e20 4e6f 6e65 3a0a 2020 2020 2222   -> None:.    ""
-00007850: 2241 7574 6f6d 6174 6963 616c 6c79 2069  "Automatically i
-00007860: 6e69 7469 616c 697a 6573 2061 6c6c 2073  nitializes all s
-00007870: 7562 636c 6173 7365 7320 6173 2063 7573  ubclasses as cus
-00007880: 746f 6d20 6461 7461 636c 6173 7365 732e  tom dataclasses.
-00007890: 2222 220a 2020 2020 7375 7065 7228 292e  """.    super().
-000078a0: 5f5f 696e 6974 5f73 7562 636c 6173 735f  __init_subclass_
-000078b0: 5f28 2a2a 6b77 6172 6773 290a 2020 2020  _(**kwargs).    
-000078c0: 2320 416c 6c20 466c 6178 204d 6f64 756c  # All Flax Modul
-000078d0: 6573 2061 7265 2064 6174 6163 6c61 7373  es are dataclass
-000078e0: 6573 2e20 2057 6520 666f 7263 6520 7468  es.  We force th
-000078f0: 6973 2063 6f6e 7665 6e74 696f 6e20 7369  is convention si
-00007900: 6e63 650a 2020 2020 2320 6974 2065 6e63  nce.    # it enc
-00007910: 6f75 7261 6765 7320 7468 6520 7374 6174  ourages the stat
-00007920: 656c 6573 7320 6265 6861 7669 6f72 206e  eless behavior n
-00007930: 6565 6465 6420 746f 2063 6c6f 6e65 206d  eeded to clone m
-00007940: 6f64 756c 6520 696e 7374 616e 6365 7320  odule instances 
-00007950: 666f 720a 2020 2020 2320 6675 6e63 7469  for.    # functi
-00007960: 6f6e 616c 2074 7261 6e73 666f 726d 6174  onal transformat
-00007970: 696f 6e2e 2020 496e 7374 6561 6420 6f66  ion.  Instead of
-00007980: 2075 7369 6e67 2061 2070 7974 686f 6e20   using a python 
-00007990: 6d65 7461 636c 6173 732c 2077 650a 2020  metaclass, we.  
-000079a0: 2020 2320 6175 746f 6d61 7469 6361 6c6c    # automaticall
-000079b0: 7920 7472 616e 7366 6f72 6d20 4d6f 6475  y transform Modu
-000079c0: 6c65 7320 696e 746f 2064 6174 6163 6c61  les into datacla
-000079d0: 7373 6573 2061 7420 7375 6263 6c61 7373  sses at subclass
-000079e0: 2063 7265 6174 696f 6e0a 2020 2020 2320   creation.    # 
-000079f0: 7469 6d65 2c20 616e 6420 7765 2073 6574  time, and we set
-00007a00: 2074 6865 206c 6173 7420 6461 7461 636c   the last datacl
-00007a10: 6173 7320 6172 6775 6d65 6e74 7320 746f  ass arguments to
-00007a20: 2060 7061 7265 6e74 6020 616e 6420 606e   `parent` and `n
-00007a30: 616d 6560 2e0a 2020 2020 636c 732e 5f63  ame`..    cls._c
-00007a40: 7573 746f 6d69 7a65 645f 6461 7461 636c  ustomized_datacl
-00007a50: 6173 735f 7472 616e 7366 6f72 6d28 6b77  ass_transform(kw
-00007a60: 5f6f 6e6c 7929 0a20 2020 2023 2057 6520  _only).    # We 
-00007a70: 7772 6170 2075 7365 722d 6465 6669 6e65  wrap user-define
-00007a80: 6420 6d65 7468 6f64 7320 696e 636c 7564  d methods includ
-00007a90: 696e 6720 7365 7475 7020 616e 6420 5f5f  ing setup and __
-00007aa0: 6361 6c6c 5f5f 2074 6f20 656e 666f 7263  call__ to enforc
-00007ab0: 650a 2020 2020 2320 6120 6e75 6d62 6572  e.    # a number
-00007ac0: 206f 6620 6469 6666 6572 656e 7420 6368   of different ch
-00007ad0: 6563 6b73 2061 6e64 2074 6f20 7072 6f76  ecks and to prov
-00007ae0: 6964 6520 636c 6561 7220 6572 726f 7220  ide clear error 
-00007af0: 6d65 7373 6167 6573 2e0a 2020 2020 636c  messages..    cl
-00007b00: 732e 5f76 6572 6966 795f 7369 6e67 6c65  s._verify_single
-00007b10: 5f6f 725f 6e6f 5f63 6f6d 7061 6374 2829  _or_no_compact()
-00007b20: 0a20 2020 2063 6c73 2e5f 6669 6e64 5f63  .    cls._find_c
-00007b30: 6f6d 7061 6374 5f6e 616d 655f 7363 6f70  ompact_name_scop
-00007b40: 655f 6d65 7468 6f64 7328 290a 2020 2020  e_methods().    
-00007b50: 636c 732e 5f77 7261 705f 6d6f 6475 6c65  cls._wrap_module
-00007b60: 5f61 7474 7269 6275 7465 7328 290a 2020  _attributes().  
-00007b70: 2020 2320 5365 7420 656d 7074 7920 636c    # Set empty cl
-00007b80: 6173 7320 6465 6661 756c 7473 2e0a 2020  ass defaults..  
-00007b90: 2020 636c 732e 5f73 7461 7465 203d 205f    cls._state = _
-00007ba0: 756e 696e 6974 6961 6c69 7a65 645f 6d6f  uninitialized_mo
-00007bb0: 6475 6c65 5f69 6e74 6572 6e61 6c5f 7374  dule_internal_st
-00007bc0: 6174 6520 2023 2074 7970 653a 2069 676e  ate  # type: ign
-00007bd0: 6f72 655b 6174 7472 2d64 6566 696e 6564  ore[attr-defined
-00007be0: 5d0a 2020 2020 636c 732e 7363 6f70 653a  ].    cls.scope:
-00007bf0: 204f 7074 696f 6e61 6c5b 5363 6f70 655d   Optional[Scope]
-00007c00: 203d 204e 6f6e 6520 2023 2074 7970 653a   = None  # type:
-00007c10: 2069 676e 6f72 650a 2020 2020 2320 4861   ignore.    # Ha
-00007c20: 6e64 6c65 7320 7765 616b 2072 6566 6572  ndles weak refer
-00007c30: 656e 6369 6e67 206f 6620 7061 7265 6e74  encing of parent
-00007c40: 204d 6f64 756c 6573 2074 6f20 7072 6576   Modules to prev
-00007c50: 656e 7420 7265 6665 7265 6e63 6520 6379  ent reference cy
-00007c60: 636c 6573 2e0a 2020 2020 636c 732e 5f70  cles..    cls._p
-00007c70: 6172 656e 745f 7265 6620 3d20 4e6f 6e65  arent_ref = None
-00007c80: 2020 2320 7479 7065 3a20 6967 6e6f 7265    # type: ignore
-00007c90: 5b61 7474 722d 6465 6669 6e65 645d 0a20  [attr-defined]. 
-00007ca0: 2020 2063 6c73 2e70 6172 656e 7420 3d20     cls.parent = 
-00007cb0: 5061 7265 6e74 4465 7363 7269 7074 6f72  ParentDescriptor
-00007cc0: 2829 2020 2320 7479 7065 3a20 6967 6e6f  ()  # type: igno
-00007cd0: 7265 5b61 7373 6967 6e6d 656e 745d 0a0a  re[assignment]..
-00007ce0: 2020 4063 6c61 7373 6d65 7468 6f64 0a20    @classmethod. 
-00007cf0: 2064 6566 205f 6375 7374 6f6d 697a 6564   def _customized
-00007d00: 5f64 6174 6163 6c61 7373 5f74 7261 6e73  _dataclass_trans
-00007d10: 666f 726d 2863 6c73 2c20 6b77 5f6f 6e6c  form(cls, kw_onl
-00007d20: 793a 2062 6f6f 6c29 3a0a 2020 2020 2222  y: bool):.    ""
-00007d30: 2254 7261 6e73 666f 726d 7320 6063 6c73  "Transforms `cls
-00007d40: 6020 696e 746f 2061 2064 6174 6163 6c61  ` into a datacla
-00007d50: 7373 2c20 7769 7468 2063 7573 746f 6d20  ss, with custom 
-00007d60: 6164 6469 7469 6f6e 616c 2062 6568 6176  additional behav
-00007d70: 696f 722e 0a0a 2020 2020 312e 2049 6e6a  ior...    1. Inj
-00007d80: 6563 7420 6070 6172 656e 7460 2061 6e64  ect `parent` and
-00007d90: 2060 6e61 6d65 6020 6669 656c 6473 2e20   `name` fields. 
-00007da0: 2028 4966 2074 6865 7920 6172 6520 616c   (If they are al
-00007db0: 7265 6164 7920 7072 6573 656e 742c 0a20  ready present,. 
-00007dc0: 2020 2020 2020 7468 656e 2063 6865 636b        then check
-00007dd0: 2074 6861 7420 7468 6579 2068 6176 6520   that they have 
-00007de0: 7468 6520 6578 7065 6374 6564 2074 7970  the expected typ
-00007df0: 6573 2e29 0a20 2020 2032 2e20 5365 7420  es.).    2. Set 
-00007e00: 636f 6d70 6172 652c 2068 6173 682c 2061  compare, hash, a
-00007e10: 6e64 2072 6570 7220 746f 2046 616c 7365  nd repr to False
-00007e20: 2066 6f72 206e 6f6e 2d69 6e69 7420 6669   for non-init fi
-00007e30: 656c 6473 2e0a 2020 2020 332e 2047 656e  elds..    3. Gen
-00007e40: 6572 6174 6520 6120 6861 7368 2066 756e  erate a hash fun
-00007e50: 6374 696f 6e20 2869 6620 6e6f 7420 7072  ction (if not pr
-00007e60: 6f76 6964 6564 2062 7920 636c 7329 2e0a  ovided by cls)..
-00007e70: 2020 2020 2222 220a 2020 2020 2320 4368      """.    # Ch
-00007e80: 6563 6b20 7265 7365 7276 6564 2061 7474  eck reserved att
-00007e90: 7269 6275 7465 7320 6861 7665 2065 7870  ributes have exp
-00007ea0: 6563 7465 6420 7479 7065 2061 6e6e 6f74  ected type annot
-00007eb0: 6174 696f 6e73 2e0a 2020 2020 616e 6e6f  ations..    anno
-00007ec0: 7461 7469 6f6e 7320 3d20 6469 6374 2863  tations = dict(c
-00007ed0: 6c73 2e5f 5f64 6963 745f 5f2e 6765 7428  ls.__dict__.get(
-00007ee0: 275f 5f61 6e6e 6f74 6174 696f 6e73 5f5f  '__annotations__
-00007ef0: 272c 207b 7d29 290a 2020 2020 6966 2061  ', {})).    if a
-00007f00: 6e6e 6f74 6174 696f 6e73 2e67 6574 2827  nnotations.get('
-00007f10: 7061 7265 6e74 272c 205f 5061 7265 6e74  parent', _Parent
-00007f20: 5479 7065 2920 213d 205f 5061 7265 6e74  Type) != _Parent
-00007f30: 5479 7065 3a0a 2020 2020 2020 7261 6973  Type:.      rais
-00007f40: 6520 6572 726f 7273 2e52 6573 6572 7665  e errors.Reserve
-00007f50: 644d 6f64 756c 6541 7474 7269 6275 7465  dModuleAttribute
-00007f60: 4572 726f 7228 616e 6e6f 7461 7469 6f6e  Error(annotation
-00007f70: 7329 0a20 2020 2069 6620 616e 6e6f 7461  s).    if annota
-00007f80: 7469 6f6e 732e 6765 7428 276e 616d 6527  tions.get('name'
-00007f90: 2c20 7374 7229 206e 6f74 2069 6e20 2827  , str) not in ('
-00007fa0: 7374 7227 2c20 7374 722c 204f 7074 696f  str', str, Optio
-00007fb0: 6e61 6c5b 7374 725d 293a 0a20 2020 2020  nal[str]):.     
-00007fc0: 2072 6169 7365 2065 7272 6f72 732e 5265   raise errors.Re
-00007fd0: 7365 7276 6564 4d6f 6475 6c65 4174 7472  servedModuleAttr
-00007fe0: 6962 7574 6545 7272 6f72 2861 6e6e 6f74  ibuteError(annot
-00007ff0: 6174 696f 6e73 290a 0a20 2020 2023 2061  ations)..    # a
-00008000: 6e79 206e 6f6e 2d69 6e69 7420 6669 656c  ny non-init fiel
-00008010: 6420 7769 6c6c 206f 6e6c 7920 6265 2073  d will only be s
-00008020: 6574 2069 6e20 7365 7475 700a 2020 2020  et in setup.    
-00008030: 2320 4475 7269 6e67 205f 5f68 6173 685f  # During __hash_
-00008040: 5f20 616e 6420 5f5f 6571 5f5f 2074 6865  _ and __eq__ the
-00008050: 2066 6965 6c64 2069 7320 6e6f 7420 7365   field is not se
-00008060: 7420 7965 740a 2020 2020 2320 736f 2069  t yet.    # so i
-00008070: 7420 7368 6f75 6c64 206e 6f74 2062 6520  t should not be 
-00008080: 7573 6564 2069 6e20 636f 6d70 6172 652c  used in compare,
-00008090: 2068 6173 6820 6f72 2072 6570 722e 0a20   hash or repr.. 
-000080a0: 2020 2066 6f72 2066 6965 6c64 2069 6e20     for field in 
-000080b0: 616e 6e6f 7461 7469 6f6e 733a 0a20 2020  annotations:.   
-000080c0: 2020 2066 6965 6c64 5f6d 6574 6120 3d20     field_meta = 
-000080d0: 6765 7461 7474 7228 636c 732c 2066 6965  getattr(cls, fie
-000080e0: 6c64 2c20 4e6f 6e65 290a 2020 2020 2020  ld, None).      
-000080f0: 6966 2069 7369 6e73 7461 6e63 6528 6669  if isinstance(fi
-00008100: 656c 645f 6d65 7461 2c20 6461 7461 636c  eld_meta, datacl
-00008110: 6173 7365 732e 4669 656c 6429 2061 6e64  asses.Field) and
-00008120: 206e 6f74 2066 6965 6c64 5f6d 6574 612e   not field_meta.
-00008130: 696e 6974 3a0a 2020 2020 2020 2020 6669  init:.        fi
-00008140: 656c 645f 6d65 7461 2e63 6f6d 7061 7265  eld_meta.compare
-00008150: 203d 2046 616c 7365 0a20 2020 2020 2020   = False.       
-00008160: 2066 6965 6c64 5f6d 6574 612e 6861 7368   field_meta.hash
-00008170: 203d 2046 616c 7365 0a20 2020 2020 2020   = False.       
-00008180: 2066 6965 6c64 5f6d 6574 612e 7265 7072   field_meta.repr
-00008190: 203d 2046 616c 7365 0a0a 2020 2020 6578   = False..    ex
-000081a0: 7472 615f 6669 656c 6473 203d 205b 0a20  tra_fields = [. 
-000081b0: 2020 2020 2028 0a20 2020 2020 2020 2027       (.        '
-000081c0: 7061 7265 6e74 272c 0a20 2020 2020 2020  parent',.       
-000081d0: 205f 5061 7265 6e74 5479 7065 2c0a 2020   _ParentType,.  
-000081e0: 2020 2020 2020 6b77 5f6f 6e6c 795f 6461        kw_only_da
-000081f0: 7461 636c 6173 7365 732e 6669 656c 6428  taclasses.field(
-00008200: 0a20 2020 2020 2020 2020 2072 6570 723d  .          repr=
-00008210: 4661 6c73 652c 2064 6566 6175 6c74 3d5f  False, default=_
-00008220: 756e 7370 6563 6966 6965 645f 7061 7265  unspecified_pare
-00008230: 6e74 2c20 6b77 5f6f 6e6c 793d 5472 7565  nt, kw_only=True
-00008240: 0a20 2020 2020 2020 2029 2c0a 2020 2020  .        ),.    
-00008250: 2020 292c 0a20 2020 2020 2028 0a20 2020    ),.      (.   
-00008260: 2020 2020 2027 6e61 6d65 272c 0a20 2020       'name',.   
-00008270: 2020 2020 204f 7074 696f 6e61 6c5b 7374       Optional[st
-00008280: 725d 2c0a 2020 2020 2020 2020 6b77 5f6f  r],.        kw_o
-00008290: 6e6c 795f 6461 7461 636c 6173 7365 732e  nly_dataclasses.
-000082a0: 6669 656c 6428 6465 6661 756c 743d 4e6f  field(default=No
-000082b0: 6e65 2c20 6b77 5f6f 6e6c 793d 5472 7565  ne, kw_only=True
-000082c0: 292c 0a20 2020 2020 2029 2c0a 2020 2020  ),.      ),.    
-000082d0: 5d0a 0a20 2020 2069 6620 6b77 5f6f 6e6c  ]..    if kw_onl
-000082e0: 793a 0a20 2020 2020 2069 6620 7475 706c  y:.      if tupl
-000082f0: 6528 7379 732e 7665 7273 696f 6e5f 696e  e(sys.version_in
-00008300: 666f 295b 3a33 5d20 3e3d 2028 332c 2031  fo)[:3] >= (3, 1
-00008310: 302c 2030 293a 0a20 2020 2020 2020 2066  0, 0):.        f
-00008320: 6f72 2028 0a20 2020 2020 2020 2020 206e  or (.          n
-00008330: 616d 652c 0a20 2020 2020 2020 2020 2061  ame,.          a
-00008340: 6e6e 6f74 6174 696f 6e2c 2020 2320 7079  nnotation,  # py
-00008350: 7479 7065 3a20 6469 7361 626c 653d 696e  type: disable=in
-00008360: 7661 6c69 642d 616e 6e6f 7461 7469 6f6e  valid-annotation
-00008370: 0a20 2020 2020 2020 2020 2064 6566 6175  .          defau
-00008380: 6c74 2c0a 2020 2020 2020 2020 2920 696e  lt,.        ) in
-00008390: 2065 7874 7261 5f66 6965 6c64 733a 0a20   extra_fields:. 
-000083a0: 2020 2020 2020 2020 2073 6574 6174 7472           setattr
-000083b0: 2863 6c73 2c20 6e61 6d65 2c20 6465 6661  (cls, name, defa
-000083c0: 756c 7429 0a20 2020 2020 2020 2020 2063  ult).          c
-000083d0: 6c73 2e5f 5f61 6e6e 6f74 6174 696f 6e73  ls.__annotations
-000083e0: 5f5f 5b6e 616d 655d 203d 2061 6e6e 6f74  __[name] = annot
-000083f0: 6174 696f 6e0a 2020 2020 2020 2020 6461  ation.        da
-00008400: 7461 636c 6173 7365 732e 6461 7461 636c  taclasses.datacl
-00008410: 6173 7328 2020 2320 7479 7065 3a20 6967  ass(  # type: ig
-00008420: 6e6f 7265 5b63 616c 6c2d 6f76 6572 6c6f  nore[call-overlo
-00008430: 6164 5d0a 2020 2020 2020 2020 2020 756e  ad].          un
-00008440: 7361 6665 5f68 6173 683d 275f 5f68 6173  safe_hash='__has
-00008450: 685f 5f27 206e 6f74 2069 6e20 636c 732e  h__' not in cls.
-00008460: 5f5f 6469 6374 5f5f 2c0a 2020 2020 2020  __dict__,.      
-00008470: 2020 2020 7265 7072 3d46 616c 7365 2c0a      repr=False,.
-00008480: 2020 2020 2020 2020 2020 6b77 5f6f 6e6c            kw_onl
-00008490: 793d 5472 7565 2c0a 2020 2020 2020 2020  y=True,.        
-000084a0: 2928 636c 7329 0a20 2020 2020 2065 6c73  )(cls).      els
-000084b0: 653a 0a20 2020 2020 2020 2072 6169 7365  e:.        raise
-000084c0: 2054 7970 6545 7272 6f72 2827 606b 775f   TypeError('`kw_
-000084d0: 6f6e 6c79 6020 6973 206e 6f74 2061 7661  only` is not ava
-000084e0: 696c 6162 6c65 2062 6566 6f72 6520 5079  ilable before Py
-000084f0: 2033 2e31 302e 2729 0a20 2020 2065 6c73   3.10.').    els
-00008500: 653a 0a20 2020 2020 2023 204e 6f77 2061  e:.      # Now a
-00008510: 7070 6c79 2064 6174 6163 6c61 7373 2074  pply dataclass t
-00008520: 7261 6e73 666f 726d 2028 7768 6963 6820  ransform (which 
-00008530: 6f70 6572 6174 6573 2069 6e2d 706c 6163  operates in-plac
-00008540: 6529 2e0a 2020 2020 2020 2320 446f 2067  e)..      # Do g
-00008550: 656e 6572 6174 6520 6120 6861 7368 2066  enerate a hash f
-00008560: 756e 6374 696f 6e20 6f6e 6c79 2069 6620  unction only if 
-00008570: 6e6f 7420 7072 6f76 6964 6564 2062 7920  not provided by 
-00008580: 7468 6520 636c 6173 732e 0a20 2020 2020  the class..     
-00008590: 206b 775f 6f6e 6c79 5f64 6174 6163 6c61   kw_only_datacla
-000085a0: 7373 6573 2e64 6174 6163 6c61 7373 280a  sses.dataclass(.
-000085b0: 2020 2020 2020 2020 636c 732c 0a20 2020          cls,.   
-000085c0: 2020 2020 2075 6e73 6166 655f 6861 7368       unsafe_hash
-000085d0: 3d27 5f5f 6861 7368 5f5f 2720 6e6f 7420  ='__hash__' not 
-000085e0: 696e 2063 6c73 2e5f 5f64 6963 745f 5f2c  in cls.__dict__,
-000085f0: 0a20 2020 2020 2020 2072 6570 723d 4661  .        repr=Fa
-00008600: 6c73 652c 0a20 2020 2020 2020 2065 7874  lse,.        ext
-00008610: 7261 5f66 6965 6c64 733d 6578 7472 615f  ra_fields=extra_
-00008620: 6669 656c 6473 2c0a 2020 2020 2020 2920  fields,.      ) 
-00008630: 2023 2070 7974 7970 653a 2064 6973 6162   # pytype: disab
-00008640: 6c65 3d77 726f 6e67 2d6b 6579 776f 7264  le=wrong-keyword
-00008650: 2d61 7267 730a 0a20 2020 2063 6c73 2e5f  -args..    cls._
-00008660: 5f68 6173 685f 5f20 3d20 5f77 7261 705f  _hash__ = _wrap_
-00008670: 6861 7368 2863 6c73 2e5f 5f68 6173 685f  hash(cls.__hash_
-00008680: 5f29 2020 2320 7479 7065 3a20 6967 6e6f  _)  # type: igno
-00008690: 7265 5b6d 6574 686f 642d 6173 7369 676e  re[method-assign
-000086a0: 5d0a 0a20 2040 636c 6173 736d 6574 686f  ]..  @classmetho
-000086b0: 640a 2020 6465 6620 5f76 6572 6966 795f  d.  def _verify_
-000086c0: 7369 6e67 6c65 5f6f 725f 6e6f 5f63 6f6d  single_or_no_com
-000086d0: 7061 6374 2863 6c73 293a 0a20 2020 2022  pact(cls):.    "
-000086e0: 2222 5374 6174 6963 616c 6c79 2076 6572  ""Statically ver
-000086f0: 6966 6965 7320 7468 6174 2061 7420 6d6f  ifies that at mo
-00008700: 7374 2061 2073 696e 676c 6520 6d65 7468  st a single meth
-00008710: 6f64 2069 7320 6c61 6265 6c6c 6564 2063  od is labelled c
-00008720: 6f6d 7061 6374 2e22 2222 0a20 2020 206d  ompact.""".    m
-00008730: 6574 686f 6473 203d 205b 6d5b 305d 2066  ethods = [m[0] f
-00008740: 6f72 206d 2069 6e20 696e 7370 6563 742e  or m in inspect.
-00008750: 6765 746d 656d 6265 7273 2863 6c73 2c20  getmembers(cls, 
-00008760: 7072 6564 6963 6174 653d 6361 6c6c 6162  predicate=callab
-00008770: 6c65 295d 0a20 2020 206e 5f63 6f6d 7061  le)].    n_compa
-00008780: 6374 5f66 6e73 203d 206c 656e 280a 2020  ct_fns = len(.  
-00008790: 2020 2020 5b0a 2020 2020 2020 2020 6d65      [.        me
-000087a0: 7468 6f64 5f6e 616d 650a 2020 2020 2020  thod_name.      
-000087b0: 2020 666f 7220 6d65 7468 6f64 5f6e 616d    for method_nam
-000087c0: 6520 696e 206d 6574 686f 6473 0a20 2020  e in methods.   
-000087d0: 2020 2020 2069 6620 6861 7361 7474 7228       if hasattr(
-000087e0: 6765 7461 7474 7228 636c 732c 206d 6574  getattr(cls, met
-000087f0: 686f 645f 6e61 6d65 292c 2027 636f 6d70  hod_name), 'comp
-00008800: 6163 7427 290a 2020 2020 2020 5d0a 2020  act').      ].  
-00008810: 2020 290a 2020 2020 6966 206e 5f63 6f6d    ).    if n_com
-00008820: 7061 6374 5f66 6e73 203e 2031 3a0a 2020  pact_fns > 1:.  
-00008830: 2020 2020 7261 6973 6520 6572 726f 7273      raise errors
-00008840: 2e4d 756c 7469 706c 654d 6574 686f 6473  .MultipleMethods
-00008850: 436f 6d70 6163 7445 7272 6f72 2829 0a0a  CompactError()..
-00008860: 2020 4063 6c61 7373 6d65 7468 6f64 0a20    @classmethod. 
-00008870: 2064 6566 205f 6669 6e64 5f63 6f6d 7061   def _find_compa
-00008880: 6374 5f6e 616d 655f 7363 6f70 655f 6d65  ct_name_scope_me
-00008890: 7468 6f64 7328 636c 7329 3a0a 2020 2020  thods(cls):.    
-000088a0: 2222 2246 696e 6473 2061 6c6c 2063 6f6d  """Finds all com
-000088b0: 7061 6374 5f6e 616d 655f 7363 6f70 6520  pact_name_scope 
-000088c0: 6d65 7468 6f64 7320 696e 2074 6865 2063  methods in the c
-000088d0: 6c61 7373 2e22 2222 0a20 2020 206d 6574  lass.""".    met
-000088e0: 686f 6473 203d 205b 6d5b 305d 2066 6f72  hods = [m[0] for
-000088f0: 206d 2069 6e20 696e 7370 6563 742e 6765   m in inspect.ge
-00008900: 746d 656d 6265 7273 2863 6c73 2c20 7072  tmembers(cls, pr
-00008910: 6564 6963 6174 653d 6361 6c6c 6162 6c65  edicate=callable
-00008920: 295d 0a20 2020 2063 6f6d 7061 6374 5f6e  )].    compact_n
-00008930: 616d 655f 7363 6f70 655f 666e 7320 3d20  ame_scope_fns = 
-00008940: 7475 706c 6528 0a20 2020 2020 206d 6574  tuple(.      met
-00008950: 686f 645f 6e61 6d65 0a20 2020 2020 2066  hod_name.      f
-00008960: 6f72 206d 6574 686f 645f 6e61 6d65 2069  or method_name i
-00008970: 6e20 6d65 7468 6f64 730a 2020 2020 2020  n methods.      
-00008980: 6966 2068 6173 6174 7472 2867 6574 6174  if hasattr(getat
-00008990: 7472 2863 6c73 2c20 6d65 7468 6f64 5f6e  tr(cls, method_n
-000089a0: 616d 6529 2c20 2763 6f6d 7061 6374 5f6e  ame), 'compact_n
-000089b0: 616d 655f 7363 6f70 6527 290a 2020 2020  ame_scope').    
-000089c0: 290a 2020 2020 636c 732e 5f63 6f6d 7061  ).    cls._compa
-000089d0: 6374 5f6e 616d 655f 7363 6f70 655f 6d65  ct_name_scope_me
-000089e0: 7468 6f64 7320 3d20 636f 6d70 6163 745f  thods = compact_
-000089f0: 6e61 6d65 5f73 636f 7065 5f66 6e73 0a0a  name_scope_fns..
-00008a00: 2020 4063 6c61 7373 6d65 7468 6f64 0a20    @classmethod. 
-00008a10: 2064 6566 205f 7772 6170 5f6d 6f64 756c   def _wrap_modul
-00008a20: 655f 6174 7472 6962 7574 6573 2863 6c73  e_attributes(cls
-00008a30: 293a 0a20 2020 2022 2222 5772 6170 7320  ):.    """Wraps 
-00008a40: 7573 6572 2d64 6566 696e 6564 206e 6f6e  user-defined non
-00008a50: 2d69 6e68 6572 6974 6564 206d 6574 686f  -inherited metho
-00008a60: 6473 2061 6e64 2064 6573 6372 6970 746f  ds and descripto
-00008a70: 7273 2077 6974 6820 7374 6174 650a 0a20  rs with state.. 
-00008a80: 2020 206d 616e 6167 656d 656e 7420 6675     management fu
-00008a90: 6e63 7469 6f6e 732e 0a20 2020 2022 2222  nctions..    """
-00008aa0: 0a20 2020 2023 2077 7261 7020 6d65 7468  .    # wrap meth
-00008ab0: 6f64 730a 2020 2020 6d65 7468 6f64 5f65  ods.    method_e
-00008ac0: 7863 6c75 7369 6f6e 7320 3d20 5b66 2e6e  xclusions = [f.n
-00008ad0: 616d 6520 666f 7220 6620 696e 2064 6174  ame for f in dat
-00008ae0: 6163 6c61 7373 6573 2e66 6965 6c64 7328  aclasses.fields(
-00008af0: 636c 7329 5d20 2b20 5b0a 2020 2020 2020  cls)] + [.      
-00008b00: 275f 5f65 715f 5f27 2c0a 2020 2020 2020  '__eq__',.      
-00008b10: 275f 5f72 6570 725f 5f27 2c0a 2020 2020  '__repr__',.    
-00008b20: 2020 275f 5f69 6e69 745f 5f27 2c0a 2020    '__init__',.  
-00008b30: 2020 2020 275f 5f68 6173 685f 5f27 2c0a      '__hash__',.
-00008b40: 2020 2020 2020 275f 5f70 6f73 745f 696e        '__post_in
-00008b50: 6974 5f5f 272c 0a20 2020 205d 0a20 2020  it__',.    ].   
-00008b60: 2066 6f72 206b 6579 2069 6e20 5f67 6574   for key in _get
-00008b70: 5f6c 6f63 616c 5f6d 6574 686f 645f 6e61  _local_method_na
-00008b80: 6d65 7328 636c 732c 2065 7863 6c75 6465  mes(cls, exclude
-00008b90: 3d6d 6574 686f 645f 6578 636c 7573 696f  =method_exclusio
-00008ba0: 6e73 293a 0a20 2020 2020 206d 6574 686f  ns):.      metho
-00008bb0: 6420 3d20 6765 7461 7474 7228 636c 732c  d = getattr(cls,
-00008bc0: 206b 6579 290a 2020 2020 2020 6966 2068   key).      if h
-00008bd0: 6173 6174 7472 286d 6574 686f 642c 2027  asattr(method, '
-00008be0: 6e6f 7772 6170 2729 3a0a 2020 2020 2020  nowrap'):.      
-00008bf0: 2020 636f 6e74 696e 7565 0a20 2020 2020    continue.     
-00008c00: 2073 6574 6174 7472 2863 6c73 2c20 6b65   setattr(cls, ke
-00008c10: 792c 2077 7261 705f 6d65 7468 6f64 5f6f  y, wrap_method_o
-00008c20: 6e63 6528 6d65 7468 6f64 2929 0a0a 2020  nce(method))..  
-00008c30: 2020 2320 7772 6170 2064 6573 6372 6970    # wrap descrip
-00008c40: 746f 7273 0a20 2020 2064 6573 6372 6970  tors.    descrip
-00008c50: 746f 725f 6578 636c 7573 696f 6e73 203d  tor_exclusions =
-00008c60: 205b 662e 6e61 6d65 2066 6f72 2066 2069   [f.name for f i
-00008c70: 6e20 6461 7461 636c 6173 7365 732e 6669  n dataclasses.fi
-00008c80: 656c 6473 2863 6c73 295d 202b 205b 0a20  elds(cls)] + [. 
-00008c90: 2020 2020 2027 7061 7265 6e74 272c 0a20       'parent',. 
-00008ca0: 2020 2020 2027 5f5f 6469 6374 5f5f 272c       '__dict__',
-00008cb0: 0a20 2020 205d 0a20 2020 2066 6f72 206b  .    ].    for k
-00008cc0: 6579 2069 6e20 5f67 6574 5f6c 6f63 616c  ey in _get_local
-00008cd0: 5f64 6573 6372 6970 746f 725f 6e61 6d65  _descriptor_name
-00008ce0: 7328 636c 732c 2064 6573 6372 6970 746f  s(cls, descripto
-00008cf0: 725f 6578 636c 7573 696f 6e73 293a 0a20  r_exclusions):. 
-00008d00: 2020 2020 2023 2064 6f6e 2774 2075 7365       # don't use
-00008d10: 2067 6574 6174 7472 2068 6572 652c 2073   getattr here, s
-00008d20: 696e 6365 2069 7420 7769 6c6c 2063 616c  ince it will cal
-00008d30: 6c20 7468 6520 6465 7363 7269 7074 6f72  l the descriptor
-00008d40: 0a20 2020 2020 2064 6573 6372 6970 746f  .      descripto
-00008d50: 7220 3d20 636c 732e 5f5f 6469 6374 5f5f  r = cls.__dict__
-00008d60: 5b6b 6579 5d0a 2020 2020 2020 6966 2068  [key].      if h
-00008d70: 6173 6174 7472 2864 6573 6372 6970 746f  asattr(descripto
-00008d80: 722c 2027 6e6f 7772 6170 2729 3a0a 2020  r, 'nowrap'):.  
-00008d90: 2020 2020 2020 636f 6e74 696e 7565 0a20        continue. 
-00008da0: 2020 2020 2073 6574 6174 7472 2863 6c73       setattr(cls
-00008db0: 2c20 6b65 792c 2077 7261 705f 6465 7363  , key, wrap_desc
-00008dc0: 7269 7074 6f72 5f6f 6e63 6528 6465 7363  riptor_once(desc
-00008dd0: 7269 7074 6f72 2929 0a20 2020 2072 6574  riptor)).    ret
-00008de0: 7572 6e20 636c 730a 0a20 2064 6566 205f  urn cls..  def _
-00008df0: 6361 6c6c 5f77 7261 7070 6564 5f6d 6574  call_wrapped_met
-00008e00: 686f 6428 7365 6c66 2c20 6675 6e2c 2061  hod(self, fun, a
-00008e10: 7267 732c 206b 7761 7267 7329 3a0a 2020  rgs, kwargs):.  
-00008e20: 2020 2222 2243 616c 6c73 2061 2077 7261    """Calls a wra
-00008e30: 7070 6564 206d 6574 686f 642e 0a0a 2020  pped method...  
-00008e40: 2020 5468 6973 2066 756e 6374 696f 6e20    This function 
-00008e50: 6973 2072 6573 706f 6e73 6962 6c65 2066  is responsible f
-00008e60: 6f72 2073 6574 7469 6e67 2075 7020 7468  or setting up th
-00008e70: 6520 7468 7265 6164 206c 6f63 616c 2073  e thread local s
-00008e80: 7461 7465 0a20 2020 2063 6f72 7265 6374  tate.    correct
-00008e90: 6c79 2062 6566 6f72 6520 6361 6c6c 696e  ly before callin
-00008ea0: 6720 7468 6520 6d65 7468 6f64 2061 6e64  g the method and
-00008eb0: 2063 6c65 616e 696e 6720 7570 2061 6674   cleaning up aft
-00008ec0: 6572 7761 7264 732e 0a20 2020 2054 6869  erwards..    Thi
-00008ed0: 7320 696e 636c 7564 6573 2073 746f 7269  s includes stori
-00008ee0: 6e67 2069 6e74 6572 6d65 6469 6174 6573  ng intermediates
-00008ef0: 2c20 7365 7475 7020 6f66 2074 6865 2063  , setup of the c
-00008f00: 6f6d 7061 6374 2073 636f 7065 2c0a 2020  ompact scope,.  
-00008f10: 2020 616e 6420 6d61 6b69 6e67 2073 7572    and making sur
-00008f20: 6520 7365 7475 7020 6973 2063 616c 6c65  e setup is calle
-00008f30: 6420 6265 666f 7265 2061 6e79 206f 7468  d before any oth
-00008f40: 6572 206d 6574 686f 642e 0a0a 2020 2020  er method...    
-00008f50: 4172 6773 3a0a 2020 2020 2020 6675 6e3a  Args:.      fun:
-00008f60: 2054 6865 2077 7261 7070 6564 206d 6574   The wrapped met
-00008f70: 686f 642e 0a20 2020 2020 2061 7267 733a  hod..      args:
-00008f80: 204e 616d 6564 2061 7267 756d 656e 7473   Named arguments
-00008f90: 2070 6173 7365 6420 746f 2060 6066 756e   passed to ``fun
-00008fa0: 6060 2e0a 2020 2020 2020 6b77 6172 6773  ``..      kwargs
-00008fb0: 3a20 4b65 7977 6f72 6420 6172 6775 6d65  : Keyword argume
-00008fc0: 6e74 7320 7061 7373 6564 2074 6f20 6060  nts passed to ``
-00008fd0: 6675 6e60 602e 0a0a 2020 2020 5265 7475  fun``...    Retu
-00008fe0: 726e 733a 0a20 2020 2020 2054 6865 2072  rns:.      The r
-00008ff0: 6573 756c 7473 206f 6620 6361 6c6c 696e  esults of callin
-00009000: 6720 6060 6675 6e60 602e 0a20 2020 2022  g ``fun``..    "
-00009010: 2222 0a20 2020 2069 735f 636f 6d70 6163  "".    is_compac
-00009020: 745f 6d65 7468 6f64 203d 2068 6173 6174  t_method = hasat
-00009030: 7472 2866 756e 2c20 2763 6f6d 7061 6374  tr(fun, 'compact
-00009040: 2729 0a20 2020 2066 756e 5f6e 616d 6520  ').    fun_name 
-00009050: 3d20 5f67 6574 5f66 6e5f 6e61 6d65 2866  = _get_fn_name(f
-00009060: 756e 290a 2020 2020 6973 5f73 6574 7570  un).    is_setup
-00009070: 5f6d 6574 686f 6420 3d20 6675 6e5f 6e61  _method = fun_na
-00009080: 6d65 203d 3d20 2773 6574 7570 270a 2020  me == 'setup'.  
-00009090: 2020 6164 645f 6361 6c6c 5f69 6e66 6f20    add_call_info 
-000090a0: 3d20 6e6f 7420 6973 5f73 6574 7570 5f6d  = not is_setup_m
-000090b0: 6574 686f 6420 616e 6420 6c65 6e28 5f63  ethod and len(_c
-000090c0: 6f6e 7465 7874 2e63 616c 6c5f 696e 666f  ontext.call_info
-000090d0: 5f73 7461 636b 2920 3e20 300a 2020 2020  _stack) > 0.    
-000090e0: 2320 5765 206c 617a 696c 7920 6361 6c6c  # We lazily call
-000090f0: 2073 6574 7570 2829 206f 6e6c 7920 7768   setup() only wh
-00009100: 656e 206e 6565 6465 642e 0a20 2020 2069  en needed..    i
-00009110: 6620 6973 5f73 6574 7570 5f6d 6574 686f  f is_setup_metho
-00009120: 643a 0a20 2020 2020 2069 6620 7365 6c66  d:.      if self
-00009130: 2e73 636f 7065 2069 7320 4e6f 6e65 3a0a  .scope is None:.
-00009140: 2020 2020 2020 2020 7261 6973 6520 6572          raise er
-00009150: 726f 7273 2e43 616c 6c53 6574 7570 556e  rors.CallSetupUn
-00009160: 626f 756e 644d 6f64 756c 6545 7272 6f72  boundModuleError
-00009170: 2829 0a20 2020 2020 2069 735f 7265 6375  ().      is_recu
-00009180: 7272 656e 7420 3d20 7365 6c66 2e5f 7374  rrent = self._st
-00009190: 6174 652e 696e 5f73 6574 7570 0a20 2020  ate.in_setup.   
-000091a0: 2020 2073 656c 662e 5f73 7461 7465 2e69     self._state.i
-000091b0: 6e5f 7365 7475 7020 3d20 5472 7565 0a20  n_setup = True. 
-000091c0: 2020 2065 6c73 653a 0a20 2020 2020 2073     else:.      s
-000091d0: 656c 662e 5f74 7279 5f73 6574 7570 2829  elf._try_setup()
-000091e0: 0a0a 2020 2020 6966 2069 735f 636f 6d70  ..    if is_comp
-000091f0: 6163 745f 6d65 7468 6f64 3a0a 2020 2020  act_method:.    
-00009200: 2020 6966 2073 656c 662e 7363 6f70 6520    if self.scope 
-00009210: 6973 204e 6f6e 653a 0a20 2020 2020 2020  is None:.       
-00009220: 2072 6169 7365 2065 7272 6f72 732e 4361   raise errors.Ca
-00009230: 6c6c 436f 6d70 6163 7455 6e62 6f75 6e64  llCompactUnbound
-00009240: 4d6f 6475 6c65 4572 726f 7228 290a 2020  ModuleError().  
-00009250: 2020 2020 6973 5f72 6563 7572 7265 6e74      is_recurrent
-00009260: 203d 2073 656c 662e 5f73 7461 7465 2e69   = self._state.i
-00009270: 6e5f 636f 6d70 6163 745f 6d65 7468 6f64  n_compact_method
-00009280: 0a20 2020 2020 2073 656c 662e 5f73 7461  .      self._sta
-00009290: 7465 2e69 6e5f 636f 6d70 6163 745f 6d65  te.in_compact_me
-000092a0: 7468 6f64 203d 2054 7275 650a 2020 2020  thod = True.    
-000092b0: 5f63 6f6e 7465 7874 2e6d 6f64 756c 655f  _context.module_
-000092c0: 7374 6163 6b2e 6170 7065 6e64 2873 656c  stack.append(sel
-000092d0: 6629 0a20 2020 2074 7279 3a0a 2020 2020  f).    try:.    
-000092e0: 2020 2320 6765 7420 6361 6c6c 2069 6e66    # get call inf
-000092f0: 6f0a 2020 2020 2020 6966 2061 6464 5f63  o.      if add_c
-00009300: 616c 6c5f 696e 666f 3a0a 2020 2020 2020  all_info:.      
-00009310: 2020 6173 7365 7274 2073 656c 662e 7363    assert self.sc
-00009320: 6f70 6520 6973 206e 6f74 204e 6f6e 650a  ope is not None.
-00009330: 2020 2020 2020 2020 6361 6c6c 5f69 6e64          call_ind
-00009340: 6578 203d 205f 636f 6e74 6578 742e 6361  ex = _context.ca
-00009350: 6c6c 5f69 6e66 6f5f 7374 6163 6b5b 2d31  ll_info_stack[-1
-00009360: 5d2e 6765 745f 6361 6c6c 5f69 6e64 6578  ].get_call_index
-00009370: 2829 0a0a 2020 2020 2020 6966 205f 676c  ()..      if _gl
-00009380: 6f62 616c 5f69 6e74 6572 6365 7074 6f72  obal_interceptor
-00009390: 5f73 7461 636b 3a0a 2020 2020 2020 2020  _stack:.        
-000093a0: 7275 6e5f 6675 6e20 3d20 6675 6e63 746f  run_fun = functo
-000093b0: 6f6c 732e 7061 7274 6961 6c28 7275 6e5f  ols.partial(run_
-000093c0: 696e 7465 7263 6570 746f 7273 2c20 6675  interceptors, fu
-000093d0: 6e29 0a20 2020 2020 2065 6c73 653a 0a20  n).      else:. 
-000093e0: 2020 2020 2020 2072 756e 5f66 756e 203d         run_fun =
-000093f0: 2066 756e 0a0a 2020 2020 2020 2320 6361   fun..      # ca
-00009400: 6c6c 206d 6574 686f 640a 2020 2020 2020  ll method.      
-00009410: 6966 205f 7573 655f 6e61 6d65 645f 6361  if _use_named_ca
-00009420: 6c6c 3a0a 2020 2020 2020 2020 7769 7468  ll:.        with
-00009430: 206a 6178 2e6e 616d 6564 5f73 636f 7065   jax.named_scope
-00009440: 285f 6465 7269 7665 5f70 726f 6669 6c69  (_derive_profili
-00009450: 6e67 5f6e 616d 6528 7365 6c66 2c20 6675  ng_name(self, fu
-00009460: 6e29 293a 0a20 2020 2020 2020 2020 2079  n)):.          y
-00009470: 203d 2072 756e 5f66 756e 2873 656c 662c   = run_fun(self,
-00009480: 202a 6172 6773 2c20 2a2a 6b77 6172 6773   *args, **kwargs
-00009490: 290a 2020 2020 2020 656c 7365 3a0a 2020  ).      else:.  
-000094a0: 2020 2020 2020 7920 3d20 7275 6e5f 6675        y = run_fu
-000094b0: 6e28 7365 6c66 2c20 2a61 7267 732c 202a  n(self, *args, *
-000094c0: 2a6b 7761 7267 7329 0a0a 2020 2020 2020  *kwargs)..      
-000094d0: 6966 205f 636f 6e74 6578 742e 6361 7074  if _context.capt
-000094e0: 7572 655f 7374 6163 6b3a 0a20 2020 2020  ure_stack:.     
-000094f0: 2020 2066 696c 7465 725f 666e 203d 205f     filter_fn = _
-00009500: 636f 6e74 6578 742e 6361 7074 7572 655f  context.capture_
-00009510: 7374 6163 6b5b 2d31 5d0a 2020 2020 2020  stack[-1].      
-00009520: 2020 6966 2066 696c 7465 725f 666e 2061    if filter_fn a
-00009530: 6e64 2066 696c 7465 725f 666e 2873 656c  nd filter_fn(sel
-00009540: 662c 2066 756e 5f6e 616d 6529 3a0a 2020  f, fun_name):.  
-00009550: 2020 2020 2020 2020 7365 6c66 2e73 6f77          self.sow
-00009560: 2827 696e 7465 726d 6564 6961 7465 7327  ('intermediates'
-00009570: 2c20 6675 6e5f 6e61 6d65 2c20 7929 0a20  , fun_name, y). 
-00009580: 2020 2020 2069 6620 6164 645f 6361 6c6c       if add_call
-00009590: 5f69 6e66 6f3a 0a20 2020 2020 2020 205f  _info:.        _
-000095a0: 6172 6773 2c20 5f6b 7761 7267 732c 205f  args, _kwargs, _
-000095b0: 7920 3d20 666c 6178 2e6c 696e 656e 2e73  y = flax.linen.s
-000095c0: 756d 6d61 7279 2e5f 7265 7072 6573 656e  ummary._represen
-000095d0: 745f 7472 6565 280a 2020 2020 2020 2020  t_tree(.        
-000095e0: 2020 2861 7267 732c 206b 7761 7267 732c    (args, kwargs,
-000095f0: 2079 290a 2020 2020 2020 2020 290a 2020   y).        ).  
-00009600: 2020 2020 2020 5f63 6f6e 7465 7874 2e63        _context.c
-00009610: 616c 6c5f 696e 666f 5f73 7461 636b 5b2d  all_info_stack[-
-00009620: 315d 2e63 616c 6c73 2e61 7070 656e 6428  1].calls.append(
-00009630: 0a20 2020 2020 2020 2020 205f 4361 6c6c  .          _Call
-00009640: 496e 666f 280a 2020 2020 2020 2020 2020  Info(.          
-00009650: 2020 6361 6c6c 5f69 6e64 6578 2c0a 2020    call_index,.  
-00009660: 2020 2020 2020 2020 2020 7365 6c66 2e70            self.p
-00009670: 6174 682c 0a20 2020 2020 2020 2020 2020  ath,.           
-00009680: 2073 656c 662e 636c 6f6e 6528 292c 0a20   self.clone(),. 
-00009690: 2020 2020 2020 2020 2020 2073 656c 662e             self.
-000096a0: 7363 6f70 652e 726e 6773 2c0a 2020 2020  scope.rngs,.    
-000096b0: 2020 2020 2020 2020 7365 6c66 2e73 636f          self.sco
-000096c0: 7065 2e6d 7574 6162 6c65 2c0a 2020 2020  pe.mutable,.    
-000096d0: 2020 2020 2020 2020 6675 6e2e 5f5f 6e61          fun.__na
-000096e0: 6d65 5f5f 2c0a 2020 2020 2020 2020 2020  me__,.          
-000096f0: 2020 5f61 7267 732c 0a20 2020 2020 2020    _args,.       
-00009700: 2020 2020 205f 6b77 6172 6773 2c0a 2020       _kwargs,.  
-00009710: 2020 2020 2020 2020 2020 5f79 2c0a 2020            _y,.  
-00009720: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
-00009730: 2020 290a 2020 2020 2020 7265 7475 726e    ).      return
-00009740: 2079 0a20 2020 2066 696e 616c 6c79 3a0a   y.    finally:.
-00009750: 2020 2020 2020 5f63 6f6e 7465 7874 2e6d        _context.m
-00009760: 6f64 756c 655f 7374 6163 6b2e 706f 7028  odule_stack.pop(
-00009770: 290a 2020 2020 2020 6966 2069 735f 636f  ).      if is_co
-00009780: 6d70 6163 745f 6d65 7468 6f64 3a0a 2020  mpact_method:.  
-00009790: 2020 2020 2020 6f62 6a65 6374 2e5f 5f73        object.__s
-000097a0: 6574 6174 7472 5f5f 2873 656c 662c 2027  etattr__(self, '
-000097b0: 7363 6f70 6527 2c20 7365 6c66 2e73 636f  scope', self.sco
-000097c0: 7065 2e72 6577 6f75 6e64 2829 290a 2020  pe.rewound()).  
-000097d0: 2020 2020 2320 7365 7475 7020 6f72 2063      # setup or c
-000097e0: 6f6d 7061 6374 2063 616c 6c73 2063 616e  ompact calls can
-000097f0: 2062 6520 7265 6375 7272 656e 7420 666f   be recurrent fo
-00009800: 7220 6578 616d 706c 6520 6475 6520 746f  r example due to
-00009810: 2073 7570 6572 2063 616c 6c73 0a20 2020   super calls.   
-00009820: 2020 2023 2072 6573 6574 7469 6e67 2074     # resetting t
-00009830: 6865 2073 7461 7465 2077 6f75 6c64 2063  he state would c
-00009840: 6175 7365 2069 7320 636f 6d70 6163 742f  ause is compact/
-00009850: 7365 7475 7020 6d65 7468 6f64 0a20 2020  setup method.   
-00009860: 2020 2023 2074 6f20 6265 2073 6574 2074     # to be set t
-00009870: 6f20 4661 6c73 6520 7072 656d 6174 7572  o False prematur
-00009880: 656c 792e 0a20 2020 2020 2069 6620 2869  ely..      if (i
-00009890: 735f 636f 6d70 6163 745f 6d65 7468 6f64  s_compact_method
-000098a0: 206f 7220 6973 5f73 6574 7570 5f6d 6574   or is_setup_met
-000098b0: 686f 6429 2061 6e64 206e 6f74 2069 735f  hod) and not is_
-000098c0: 7265 6375 7272 656e 743a 0a20 2020 2020  recurrent:.     
-000098d0: 2020 2073 656c 662e 5f73 7461 7465 2e72     self._state.r
-000098e0: 6573 6574 2829 0a0a 2020 6465 6620 5f5f  eset()..  def __
-000098f0: 7365 7461 7474 725f 5f28 7365 6c66 2c20  setattr__(self, 
-00009900: 6e61 6d65 3a20 7374 722c 2076 616c 3a20  name: str, val: 
-00009910: 416e 7929 3a0a 2020 2020 2222 2253 6574  Any):.    """Set
-00009920: 7320 616e 2061 7474 7269 6275 7465 206f  s an attribute o
-00009930: 6e20 7468 6973 204d 6f64 756c 652e 0a0a  n this Module...
-00009940: 2020 2020 5765 206f 7665 726c 6f61 6420      We overload 
-00009950: 7365 7461 7474 7220 736f 6c65 6c79 2074  setattr solely t
-00009960: 6f20 7375 7070 6f72 7420 7079 7468 6f6e  o support python
-00009970: 6963 206e 616d 696e 6720 7669 6120 6173  ic naming via as
-00009980: 7369 676e 6d65 6e74 206f 660a 2020 2020  signment of.    
-00009990: 7375 626d 6f64 756c 6573 2069 6e20 7468  submodules in th
-000099a0: 6520 7370 6563 6961 6c20 3a6d 6574 683a  e special :meth:
-000099b0: 6073 6574 7570 6020 6675 6e63 7469 6f6e  `setup` function
-000099c0: 3a3a 0a0a 2020 2020 2020 7365 6c66 2e73  ::..      self.s
-000099d0: 7562 6d6f 6475 6c65 5f6e 616d 6520 3d20  ubmodule_name = 
-000099e0: 4d79 4d6f 6475 6c65 282e 2e2e 290a 0a20  MyModule(...).. 
-000099f0: 2020 2057 6520 616c 736f 2073 7570 706f     We also suppo
-00009a00: 7274 206c 6973 7473 2061 6e64 206f 7468  rt lists and oth
-00009a10: 6572 2067 656e 6572 616c 2070 7974 7265  er general pytre
-00009a20: 6573 2c20 652e 672e 3a3a 0a0a 2020 2020  es, e.g.::..    
-00009a30: 2020 7365 6c66 2e73 7562 6d6f 6475 6c65    self.submodule
-00009a40: 7320 3d20 5b4d 794d 6f64 756c 6530 282e  s = [MyModule0(.
-00009a50: 2e29 2c20 4d79 4d6f 6475 6c65 3128 2e2e  .), MyModule1(..
-00009a60: 292c 202e 2e2e 5d0a 0a20 2020 2041 7267  ), ...]..    Arg
-00009a70: 733a 0a20 2020 2020 206e 616d 653a 2041  s:.      name: A
-00009a80: 7474 7269 6275 7465 2074 6f20 7365 742e  ttribute to set.
-00009a90: 0a20 2020 2020 2076 616c 3a20 5661 6c75  .      val: Valu
-00009aa0: 6520 6f66 2074 6865 2061 7474 7269 6275  e of the attribu
-00009ab0: 7465 2e0a 2020 2020 2222 220a 2020 2020  te..    """.    
-00009ac0: 6669 656c 6473 203d 2073 656c 662e 5f5f  fields = self.__
-00009ad0: 6461 7461 636c 6173 735f 6669 656c 6473  dataclass_fields
-00009ae0: 5f5f 2020 2320 7079 7479 7065 3a20 6469  __  # pytype: di
-00009af0: 7361 626c 653d 6174 7472 6962 7574 652d  sable=attribute-
-00009b00: 6572 726f 720a 2020 2020 6973 5f64 6174  error.    is_dat
-00009b10: 6163 6c61 7373 5f61 7474 7220 3d20 6e61  aclass_attr = na
-00009b20: 6d65 2069 6e20 6669 656c 6473 2061 6e64  me in fields and
-00009b30: 2066 6965 6c64 735b 6e61 6d65 5d2e 696e   fields[name].in
-00009b40: 6974 0a0a 2020 2020 6966 206e 6f74 2073  it..    if not s
-00009b50: 656c 662e 5f73 7461 7465 2e69 6e5f 7365  elf._state.in_se
-00009b60: 7475 703a 0a20 2020 2020 2069 6620 6e6f  tup:.      if no
-00009b70: 7420 7365 6c66 2e5f 7374 6174 652e 6973  t self._state.is
-00009b80: 5f69 6e69 7469 616c 697a 6564 3a0a 2020  _initialized:.  
-00009b90: 2020 2020 2020 2320 5365 7474 696e 6720        # Setting 
-00009ba0: 6174 7472 6962 7574 6573 2062 6566 6f72  attributes befor
-00009bb0: 6520 656e 6420 6f66 204d 6f64 756c 652e  e end of Module.
-00009bc0: 5f5f 706f 7374 5f69 6e69 745f 5f28 290a  __post_init__().
-00009bd0: 2020 2020 2020 2020 6f62 6a65 6374 2e5f          object._
-00009be0: 5f73 6574 6174 7472 5f5f 2873 656c 662c  _setattr__(self,
-00009bf0: 206e 616d 652c 2076 616c 290a 2020 2020   name, val).    
-00009c00: 2020 2020 7265 7475 726e 0a20 2020 2020      return.     
-00009c10: 2065 6c73 653a 0a20 2020 2020 2020 2023   else:.        #
-00009c20: 2057 6527 7265 2070 6173 7420 616c 6c20   We're past all 
-00009c30: 696e 6974 6961 6c69 7a61 7469 6f6e 2061  initialization a
-00009c40: 6e64 2073 6574 7570 206c 6f67 6963 3a0a  nd setup logic:.
-00009c50: 2020 2020 2020 2020 2320 5261 6973 6573          # Raises
-00009c60: 2061 2054 7970 6545 7272 6f72 206a 7573   a TypeError jus
-00009c70: 7420 6c69 6b65 2066 726f 7a65 6e20 7079  t like frozen py
-00009c80: 7468 6f6e 2064 6174 6163 6c61 7373 6573  thon dataclasses
-00009c90: 2e0a 2020 2020 2020 2020 7261 6973 6520  ..        raise 
-00009ca0: 6572 726f 7273 2e53 6574 4174 7472 6962  errors.SetAttrib
-00009cb0: 7574 6546 726f 7a65 6e4d 6f64 756c 6545  uteFrozenModuleE
-00009cc0: 7272 6f72 280a 2020 2020 2020 2020 2020  rror(.          
-00009cd0: 7365 6c66 2e5f 5f63 6c61 7373 5f5f 2e5f  self.__class__._
-00009ce0: 5f6e 616d 655f 5f2c 206e 616d 652c 2076  _name__, name, v
-00009cf0: 616c 0a20 2020 2020 2020 2029 0a0a 2020  al.        )..  
-00009d00: 2020 2320 5765 2772 6520 696e 7369 6465    # We're inside
-00009d10: 2074 6865 2073 6574 7570 2829 206d 6574   the setup() met
-00009d20: 686f 643a 0a20 2020 2069 6620 6973 5f64  hod:.    if is_d
-00009d30: 6174 6163 6c61 7373 5f61 7474 723a 0a20  ataclass_attr:. 
-00009d40: 2020 2020 2023 2054 6865 7365 206e 616d       # These nam
-00009d50: 6573 2061 7265 2073 7065 6369 6669 6564  es are specified
-00009d60: 2061 7320 6461 7461 636c 6173 7320 6669   as dataclass fi
-00009d70: 656c 6473 2e20 5468 6579 2073 686f 756c  elds. They shoul
-00009d80: 6420 6e6f 7420 6265 0a20 2020 2020 2023  d not be.      #
-00009d90: 2069 6e69 7469 616c 697a 6564 2077 6974   initialized wit
-00009da0: 6869 6e20 7468 6520 7365 7475 7028 2920  hin the setup() 
-00009db0: 6d65 7468 6f64 2c20 6275 7420 6361 6e20  method, but can 
-00009dc0: 6265 206d 6f64 6966 6965 6420 6672 6565  be modified free
-00009dd0: 6c79 0a20 2020 2020 2023 2062 6566 6f72  ly.      # befor
-00009de0: 6520 6974 2e0a 2020 2020 2020 7261 6973  e it..      rais
-00009df0: 6520 6572 726f 7273 2e53 6574 4174 7472  e errors.SetAttr
-00009e00: 6962 7574 6549 6e4d 6f64 756c 6553 6574  ibuteInModuleSet
-00009e10: 7570 4572 726f 7228 290a 0a20 2020 2023  upError()..    #
-00009e20: 2056 616c 7565 7320 2874 6861 7420 6d61   Values (that ma
-00009e30: 7920 6265 2076 6172 6961 626c 6573 206f  y be variables o
-00009e40: 7220 7375 626d 6f64 756c 6573 2920 6172  r submodules) ar
-00009e50: 6520 6265 696e 6720 6465 6669 6e65 6420  e being defined 
-00009e60: 616e 640a 2020 2020 2320 6174 7461 6368  and.    # attach
-00009e70: 6564 2069 6e20 7365 7475 7028 292c 2077  ed in setup(), w
-00009e80: 6520 7275 6e20 736f 6d65 2065 7874 7261  e run some extra
-00009e90: 206c 6f67 6963 2069 6e20 7468 6174 2063   logic in that c
-00009ea0: 6173 652e 0a20 2020 2073 656c 662e 5f72  ase..    self._r
-00009eb0: 6567 6973 7465 725f 7375 626d 6f64 756c  egister_submodul
-00009ec0: 6573 286e 616d 652c 2076 616c 290a 0a20  es(name, val).. 
-00009ed0: 2064 6566 205f 5f67 6574 6174 7472 5f5f   def __getattr__
-00009ee0: 2873 656c 662c 206e 616d 653a 2073 7472  (self, name: str
-00009ef0: 2920 2d3e 2041 6e79 3a0a 2020 2020 2222  ) -> Any:.    ""
-00009f00: 2243 616c 6c20 7365 7475 7028 2920 6265  "Call setup() be
-00009f10: 666f 7265 2067 6574 7469 6e67 2061 6e79  fore getting any
-00009f20: 2073 6574 7570 2d64 6566 696e 6564 2061   setup-defined a
-00009f30: 7474 7269 6275 7465 732e 2222 220a 2020  ttributes.""".  
-00009f40: 2020 2320 5765 2064 6f6e 2774 2077 616e    # We don't wan
-00009f50: 7420 746f 2072 6574 7572 6e20 616e 7974  t to return anyt
-00009f60: 6869 6e67 2066 6f72 2070 7974 686f 6e20  hing for python 
-00009f70: 636f 7079 202f 2070 6963 6b6c 6520 6d65  copy / pickle me
-00009f80: 7468 6f64 732e 0a20 2020 2069 6620 6e61  thods..    if na
-00009f90: 6d65 2069 6e20 5f55 4e44 4546 494e 4544  me in _UNDEFINED
-00009fa0: 5f43 4f50 595f 5049 434b 4c45 5f4d 4554  _COPY_PICKLE_MET
-00009fb0: 484f 4453 3a0a 2020 2020 2020 7261 6973  HODS:.      rais
-00009fc0: 6520 4174 7472 6962 7574 6545 7272 6f72  e AttributeError
-00009fd0: 2829 0a20 2020 2073 656c 662e 5f74 7279  ().    self._try
-00009fe0: 5f73 6574 7570 2829 0a20 2020 2069 6620  _setup().    if 
-00009ff0: 6e61 6d65 2069 6e20 7365 6c66 2e5f 5f64  name in self.__d
-0000a000: 6963 745f 5f3a 0a20 2020 2020 2072 6574  ict__:.      ret
-0000a010: 7572 6e20 7365 6c66 2e5f 5f64 6963 745f  urn self.__dict_
-0000a020: 5f5b 6e61 6d65 5d0a 2020 2020 656c 7365  _[name].    else
-0000a030: 3a0a 2020 2020 2020 6d73 6720 3d20 6627  :.      msg = f'
-0000a040: 227b 7365 6c66 2e5f 5f63 6c61 7373 5f5f  "{self.__class__
-0000a050: 2e5f 5f6e 616d 655f 5f7d 2220 6f62 6a65  .__name__}" obje
-0000a060: 6374 2068 6173 206e 6f20 6174 7472 6962  ct has no attrib
-0000a070: 7574 6520 227b 6e61 6d65 7d22 2e27 0a20  ute "{name}".'. 
-0000a080: 2020 2020 2069 6620 7365 6c66 2e73 636f       if self.sco
-0000a090: 7065 2069 7320 4e6f 6e65 3a0a 2020 2020  pe is None:.    
-0000a0a0: 2020 2020 6d73 6720 2b3d 2028 0a20 2020      msg += (.   
-0000a0b0: 2020 2020 2020 2066 2720 4966 2022 7b6e         f' If "{n
-0000a0c0: 616d 657d 2220 6973 2064 6566 696e 6564  ame}" is defined
-0000a0d0: 2069 6e20 5c27 2e73 6574 7570 2829 5c27   in \'.setup()\'
-0000a0e0: 2c20 7265 6d65 6d62 6572 2074 6865 7365  , remember these
-0000a0f0: 2066 6965 6c64 7320 270a 2020 2020 2020   fields '.      
-0000a100: 2020 2020 2261 7265 206f 6e6c 7920 6163      "are only ac
-0000a110: 6365 7373 6962 6c65 2066 726f 6d20 696e  cessible from in
-0000a120: 7369 6465 2027 696e 6974 2720 6f72 2027  side 'init' or '
-0000a130: 6170 706c 7927 2e22 0a20 2020 2020 2020  apply'.".       
-0000a140: 2029 0a20 2020 2020 2072 6169 7365 2041   ).      raise A
-0000a150: 7474 7269 6275 7465 4572 726f 7228 6d73  ttributeError(ms
-0000a160: 6729 0a0a 2020 6465 6620 5f5f 6469 725f  g)..  def __dir_
-0000a170: 5f28 7365 6c66 2920 2d3e 204c 6973 745b  _(self) -> List[
-0000a180: 7374 725d 3a0a 2020 2020 2222 2243 616c  str]:.    """Cal
-0000a190: 6c20 7365 7475 7028 2920 6265 666f 7265  l setup() before
-0000a1a0: 206c 6973 7469 6e67 2061 7474 7269 6275   listing attribu
-0000a1b0: 7465 732e 2222 220a 2020 2020 7365 6c66  tes.""".    self
-0000a1c0: 2e5f 7472 795f 7365 7475 7028 290a 2020  ._try_setup().  
-0000a1d0: 2020 7265 7475 726e 206f 626a 6563 742e    return object.
-0000a1e0: 5f5f 6469 725f 5f28 7365 6c66 2920 2023  __dir__(self)  #
-0000a1f0: 2074 7970 653a 2069 676e 6f72 650a 0a20   type: ignore.. 
-0000a200: 2064 6566 205f 5f70 6f73 745f 696e 6974   def __post_init
-0000a210: 5f5f 2873 656c 6629 202d 3e20 4e6f 6e65  __(self) -> None
-0000a220: 3a0a 2020 2020 2320 444f 204e 4f54 2052  :.    # DO NOT R
-0000a230: 454d 4f56 4520 2d20 4d61 726b 6572 2066  EMOVE - Marker f
-0000a240: 6f72 2069 6e74 6572 6e61 6c20 6c6f 6767  or internal logg
-0000a250: 696e 672e 0a20 2020 2023 2049 6e20 6461  ing..    # In da
-0000a260: 7461 636c 6173 7365 732c 205f 5f69 6e69  taclasses, __ini
-0000a270: 745f 5f20 6973 206f 7665 7272 6964 6465  t__ is overridde
-0000a280: 6e20 746f 2070 726f 6365 7373 2064 6174  n to process dat
-0000a290: 6163 6c61 7373 2061 7267 756d 656e 7473  aclass arguments
-0000a2a0: 2c0a 2020 2020 2320 616e 6420 5f5f 706f  ,.    # and __po
-0000a2b0: 7374 5f69 6e69 745f 5f20 6973 2063 616c  st_init__ is cal
-0000a2c0: 6c65 6420 696d 6d65 6469 6174 656c 7920  led immediately 
-0000a2d0: 6166 7465 7277 6172 6473 2e20 4865 7265  afterwards. Here
-0000a2e0: 2c20 6465 7065 6e64 696e 6720 6f6e 2074  , depending on t
-0000a2f0: 6865 0a20 2020 2023 2074 7970 6520 6f66  he.    # type of
-0000a300: 2060 7061 7265 6e74 6020 7061 7373 6564   `parent` passed
-0000a310: 2074 6f20 696e 6974 6961 6c69 7a65 2074   to initialize t
-0000a320: 6865 204d 6f64 756c 652c 2077 6520 6569  he Module, we ei
-0000a330: 7468 6572 2064 6566 6572 0a20 2020 2023  ther defer.    #
-0000a340: 2069 6e69 7469 616c 697a 6174 696f 6e2c   initialization,
-0000a350: 2061 7474 6163 6820 7468 6973 204d 6f64   attach this Mod
-0000a360: 756c 6520 6173 2061 2073 7562 6d6f 6475  ule as a submodu
-0000a370: 6c65 206f 6620 6120 7061 7265 6e74 2c20  le of a parent, 
-0000a380: 6f72 2062 696e 640a 2020 2020 2320 7468  or bind.    # th
-0000a390: 6973 204d 6f64 756c 6520 6174 2074 6865  is Module at the
-0000a3a0: 2074 6f70 2d6c 6576 656c 2074 6f20 7661   top-level to va
-0000a3b0: 7269 6162 6c65 7320 616e 6420 726e 6773  riables and rngs
-0000a3c0: 2e0a 0a20 2020 206f 626a 6563 742e 5f5f  ...    object.__
-0000a3d0: 7365 7461 7474 725f 5f28 7365 6c66 2c20  setattr__(self, 
-0000a3e0: 275f 6964 272c 2075 7569 6428 2929 0a20  '_id', uuid()). 
-0000a3f0: 2020 206f 626a 6563 742e 5f5f 7365 7461     object.__seta
-0000a400: 7474 725f 5f28 7365 6c66 2c20 275f 7374  ttr__(self, '_st
-0000a410: 6174 6527 2c20 5f4d 6f64 756c 6549 6e74  ate', _ModuleInt
-0000a420: 6572 6e61 6c53 7461 7465 2829 290a 0a20  ernalState()).. 
-0000a430: 2020 2023 2054 7970 6963 616c 6c79 2077     # Typically w
-0000a440: 6520 7365 7420 7468 6520 7061 7265 6e74  e set the parent
-0000a450: 2062 6173 6564 206f 6e20 7468 6520 6479   based on the dy
-0000a460: 6e61 6d69 6320 6d6f 6475 6c65 2063 6f6e  namic module con
-0000a470: 7465 7874 2e0a 2020 2020 6966 2073 656c  text..    if sel
-0000a480: 662e 7061 7265 6e74 2069 7320 5f75 6e73  f.parent is _uns
-0000a490: 7065 6369 6669 6564 5f70 6172 656e 743a  pecified_parent:
-0000a4a0: 2020 2320 7079 7479 7065 3a20 6469 7361    # pytype: disa
-0000a4b0: 626c 653d 6174 7472 6962 7574 652d 6572  ble=attribute-er
-0000a4c0: 726f 720a 2020 2020 2020 6f62 6a65 6374  ror.      object
-0000a4d0: 2e5f 5f73 6574 6174 7472 5f5f 2873 656c  .__setattr__(sel
-0000a4e0: 662c 2027 7061 7265 6e74 272c 205f 636f  f, 'parent', _co
-0000a4f0: 6e74 6578 742e 6d6f 6475 6c65 5f73 7461  ntext.module_sta
-0000a500: 636b 5b2d 315d 290a 0a20 2020 2023 2049  ck[-1])..    # I
-0000a510: 6e69 7469 616c 697a 6174 696f 6e20 6973  nitialization is
-0000a520: 2064 6566 6572 7265 6420 666f 7220 746f   deferred for to
-0000a530: 7020 6c65 7665 6c20 4d6f 6475 6c65 7320  p level Modules 
-0000a540: 6f72 2061 6e79 206f 7468 6572 2022 6f72  or any other "or
-0000a550: 7068 616e 220a 2020 2020 2320 4d6f 6475  phan".    # Modu
-0000a560: 6c65 7320 756e 7469 6c20 6174 7461 6368  les until attach
-0000a570: 6d65 6e74 2062 7920 5f5f 7365 7461 7474  ment by __setatt
-0000a580: 725f 5f20 692e 652e 204d 794d 6f64 756c  r__ i.e. MyModul
-0000a590: 6528 2e2e 2e2c 2070 6172 656e 743d 4e6f  e(..., parent=No
-0000a5a0: 6e65 290a 2020 2020 6966 2073 656c 662e  ne).    if self.
-0000a5b0: 7061 7265 6e74 2069 7320 4e6f 6e65 3a0a  parent is None:.
-0000a5c0: 2020 2020 2020 7265 7475 726e 0a0a 2020        return..  
-0000a5d0: 2020 2320 5265 6769 7374 6572 2073 7562    # Register sub
-0000a5e0: 6d6f 6475 6c65 206f 6e20 7061 7265 6e74  module on parent
-0000a5f0: 204d 6f64 756c 652e 0a20 2020 2069 6620   Module..    if 
-0000a600: 6973 696e 7374 616e 6365 2873 656c 662e  isinstance(self.
-0000a610: 7061 7265 6e74 2c20 4d6f 6475 6c65 293a  parent, Module):
-0000a620: 0a20 2020 2020 2023 2057 6865 6e20 696e  .      # When in
-0000a630: 6974 6961 6c69 7a69 6e67 2061 6e20 756e  itializing an un
-0000a640: 6e61 6d65 6420 4d6f 6475 6c65 2069 6e73  named Module ins
-0000a650: 6964 6520 7365 7475 7028 290a 2020 2020  ide setup().    
-0000a660: 2020 2320 696e 6974 6961 6c69 7a61 7469    # initializati
-0000a670: 6f6e 2069 7320 6465 6665 7272 6564 2075  on is deferred u
-0000a680: 6e74 696c 2061 7474 6163 686d 656e 7420  ntil attachment 
-0000a690: 6279 205f 5f73 6574 6174 7472 5f5f 0a20  by __setattr__. 
-0000a6a0: 2020 2020 2023 2069 2e65 2e20 7365 6c66       # i.e. self
-0000a6b0: 2e6d 796d 6f64 756c 6520 3d20 4d79 4d6f  .mymodule = MyMo
-0000a6c0: 6475 6c65 282e 2e2e 290a 2020 2020 2020  dule(...).      
-0000a6d0: 7365 6c66 2e6e 616d 653a 204f 7074 696f  self.name: Optio
-0000a6e0: 6e61 6c5b 7374 725d 0a20 2020 2020 2069  nal[str].      i
-0000a6f0: 6620 280a 2020 2020 2020 2020 7365 6c66  f (.        self
-0000a700: 2e70 6172 656e 742e 5f73 7461 7465 2e69  .parent._state.i
-0000a710: 6e5f 7365 7475 7020 616e 6420 7365 6c66  n_setup and self
-0000a720: 2e6e 616d 6520 6973 204e 6f6e 650a 2020  .name is None.  
-0000a730: 2020 2020 293a 2020 2320 7079 7479 7065      ):  # pytype
-0000a740: 3a20 6469 7361 626c 653d 6174 7472 6962  : disable=attrib
-0000a750: 7574 652d 6572 726f 720a 2020 2020 2020  ute-error.      
-0000a760: 2020 7265 7475 726e 0a20 2020 2020 2069    return.      i
-0000a770: 6620 6e6f 7420 7365 6c66 2e70 6172 656e  f not self.paren
-0000a780: 742e 5f69 6e69 7469 616c 697a 6174 696f  t._initializatio
-0000a790: 6e5f 616c 6c6f 7765 643a 0a20 2020 2020  n_allowed:.     
-0000a7a0: 2020 2072 6169 7365 2065 7272 6f72 732e     raise errors.
-0000a7b0: 4173 7369 676e 5375 624d 6f64 756c 6545  AssignSubModuleE
-0000a7c0: 7272 6f72 2873 656c 662e 5f5f 636c 6173  rror(self.__clas
-0000a7d0: 735f 5f2e 5f5f 6e61 6d65 5f5f 290a 2020  s__.__name__).  
-0000a7e0: 2020 2020 2320 4175 746f 6e61 6d69 6e67      # Autonaming
-0000a7f0: 206f 6620 7375 626d 6f64 756c 6573 2e0a   of submodules..
-0000a800: 2020 2020 2020 6966 2073 656c 662e 6e61        if self.na
-0000a810: 6d65 2069 7320 4e6f 6e65 3a20 2023 2070  me is None:  # p
-0000a820: 7974 7970 653a 2064 6973 6162 6c65 3d61  ytype: disable=a
-0000a830: 7474 7269 6275 7465 2d65 7272 6f72 0a20  ttribute-error. 
-0000a840: 2020 2020 2020 2070 7265 6669 7820 3d20         prefix = 
-0000a850: 6627 7b73 656c 662e 5f5f 636c 6173 735f  f'{self.__class_
-0000a860: 5f2e 5f5f 6e61 6d65 5f5f 7d27 0a20 2020  _.__name__}'.   
-0000a870: 2020 2020 2063 7572 736f 7220 3d20 7365       cursor = se
-0000a880: 6c66 2e70 6172 656e 742e 5f73 7461 7465  lf.parent._state
-0000a890: 2e61 7574 6f6e 616d 655f 6375 7273 6f72  .autoname_cursor
-0000a8a0: 2e67 6574 2870 7265 6669 782c 2030 290a  .get(prefix, 0).
-0000a8b0: 2020 2020 2020 2020 7365 6c66 2e6e 616d          self.nam
-0000a8c0: 6520 3d20 6627 7b70 7265 6669 787d 5f7b  e = f'{prefix}_{
-0000a8d0: 6375 7273 6f72 7d27 0a20 2020 2020 2020  cursor}'.       
-0000a8e0: 2073 656c 662e 7061 7265 6e74 2e5f 7374   self.parent._st
-0000a8f0: 6174 652e 6175 746f 6e61 6d65 5f63 7572  ate.autoname_cur
-0000a900: 736f 725b 7072 6566 6978 5d20 3d20 6375  sor[prefix] = cu
-0000a910: 7273 6f72 202b 2031 0a20 2020 2020 2023  rsor + 1.      #
-0000a920: 2041 6c6c 6f77 2073 636f 7065 2061 6c69   Allow scope ali
-0000a930: 6173 696e 6720 756e 6465 7220 7472 616e  asing under tran
-0000a940: 7366 6f72 6d73 2066 6f72 2073 7562 6d6f  sforms for submo
-0000a950: 6475 6c65 7320 6465 6669 6e65 6420 696e  dules defined in
-0000a960: 2073 6574 7570 2e0a 2020 2020 2020 7265   setup..      re
-0000a970: 7573 655f 7363 6f70 6573 203d 2028 0a20  use_scopes = (. 
-0000a980: 2020 2020 2020 2073 656c 662e 7061 7265         self.pare
-0000a990: 6e74 2e5f 7374 6174 652e 696e 5f73 6574  nt._state.in_set
-0000a9a0: 7570 0a20 2020 2020 2020 2061 6e64 2073  up.        and s
-0000a9b0: 656c 662e 7061 7265 6e74 2e5f 7374 6174  elf.parent._stat
-0000a9c0: 652e 7365 7475 705f 6361 6c6c 6564 203d  e.setup_called =
-0000a9d0: 3d20 5365 7475 7053 7461 7465 2e54 5241  = SetupState.TRA
-0000a9e0: 4e53 464f 524d 4544 0a20 2020 2020 2029  NSFORMED.      )
-0000a9f0: 0a20 2020 2020 2023 2050 6572 666f 726d  .      # Perform
-0000aa00: 206e 616d 652d 636f 6c6c 6973 696f 6e20   name-collision 
-0000aa10: 6368 6563 6b2e 0a20 2020 2020 2069 6620  check..      if 
-0000aa20: 7365 6c66 2e70 6172 656e 742e 5f6e 616d  self.parent._nam
-0000aa30: 655f 7461 6b65 6e28 7365 6c66 2e6e 616d  e_taken(self.nam
-0000aa40: 652c 2072 6575 7365 5f73 636f 7065 733d  e, reuse_scopes=
-0000aa50: 7265 7573 655f 7363 6f70 6573 293a 0a20  reuse_scopes):. 
-0000aa60: 2020 2020 2020 2070 6172 656e 745f 636c         parent_cl
-0000aa70: 6173 7320 3d20 7365 6c66 2e70 6172 656e  ass = self.paren
-0000aa80: 742e 5f5f 636c 6173 735f 5f2e 5f5f 6e61  t.__class__.__na
-0000aa90: 6d65 5f5f 0a20 2020 2020 2020 2072 6169  me__.        rai
-0000aaa0: 7365 2065 7272 6f72 732e 4e61 6d65 496e  se errors.NameIn
-0000aab0: 5573 6545 7272 6f72 2827 7375 626d 6f64  UseError('submod
-0000aac0: 756c 6527 2c20 7365 6c66 2e6e 616d 652c  ule', self.name,
-0000aad0: 2070 6172 656e 745f 636c 6173 7329 0a20   parent_class). 
-0000aae0: 2020 2020 2023 2046 696e 616c 697a 6520       # Finalize 
-0000aaf0: 6174 7461 6368 6d65 6e74 2074 6f20 7061  attachment to pa
-0000ab00: 7265 6e74 2061 6e64 2073 636f 7065 2069  rent and scope i
-0000ab10: 6e69 7469 616c 697a 6174 696f 6e2e 0a20  nitialization.. 
-0000ab20: 2020 2020 2073 656c 662e 7061 7265 6e74       self.parent
-0000ab30: 2e5f 7374 6174 652e 6368 696c 6472 656e  ._state.children
-0000ab40: 5b73 656c 662e 6e61 6d65 5d20 3d20 7365  [self.name] = se
-0000ab50: 6c66 0a20 2020 2020 2061 7373 6572 7420  lf.      assert 
-0000ab60: 7365 6c66 2e70 6172 656e 742e 7363 6f70  self.parent.scop
-0000ab70: 6520 6973 206e 6f74 204e 6f6e 650a 2020  e is not None.  
-0000ab80: 2020 2020 6f62 6a65 6374 2e5f 5f73 6574      object.__set
-0000ab90: 6174 7472 5f5f 280a 2020 2020 2020 2020  attr__(.        
-0000aba0: 7365 6c66 2c20 2773 636f 7065 272c 2073  self, 'scope', s
-0000abb0: 656c 662e 7061 7265 6e74 2e73 636f 7065  elf.parent.scope
-0000abc0: 2e70 7573 6828 7365 6c66 2e6e 616d 652c  .push(self.name,
-0000abd0: 2072 6575 7365 3d72 6575 7365 5f73 636f   reuse=reuse_sco
-0000abe0: 7065 7329 0a20 2020 2020 2029 0a0a 2020  pes).      )..  
-0000abf0: 2020 2320 546f 702d 6c65 7665 6c20 696e    # Top-level in
-0000ac00: 766f 6361 7469 6f6e 2077 6974 6820 6120  vocation with a 
-0000ac10: 6675 6e63 7469 6f6e 616c 2053 636f 7065  functional Scope
-0000ac20: 2e0a 2020 2020 656c 6966 2069 7369 6e73  ..    elif isins
-0000ac30: 7461 6e63 6528 7365 6c66 2e70 6172 656e  tance(self.paren
-0000ac40: 742c 2053 636f 7065 293a 0a20 2020 2020  t, Scope):.     
-0000ac50: 206f 626a 6563 742e 5f5f 7365 7461 7474   object.__setatt
-0000ac60: 725f 5f28 7365 6c66 2c20 2773 636f 7065  r__(self, 'scope
-0000ac70: 272c 2073 656c 662e 7061 7265 6e74 290a  ', self.parent).
-0000ac80: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      
-0000ac90: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-0000aca0: 2827 7061 7265 6e74 206d 7573 7420 6265  ('parent must be
-0000acb0: 204e 6f6e 652c 204d 6f64 756c 6520 6f72   None, Module or
-0000acc0: 2053 636f 7065 2729 0a0a 2020 2020 2320   Scope')..    # 
-0000acd0: 6561 6765 726c 7920 6269 6e64 2073 7562  eagerly bind sub
-0000ace0: 6d6f 6475 6c65 7320 6966 2073 636f 7065  modules if scope
-0000acf0: 2069 7320 6176 6169 6c61 626c 650a 2020   is available.  
-0000ad00: 2020 6966 2073 656c 662e 7363 6f70 6520    if self.scope 
-0000ad10: 6973 206e 6f74 204e 6f6e 653a 0a20 2020  is not None:.   
-0000ad20: 2020 2066 6f72 2066 6965 6c64 2069 6e20     for field in 
-0000ad30: 6461 7461 636c 6173 7365 732e 6669 656c  dataclasses.fiel
-0000ad40: 6473 2873 656c 6629 3a0a 2020 2020 2020  ds(self):.      
-0000ad50: 2020 6966 2066 6965 6c64 2e6e 616d 6520    if field.name 
-0000ad60: 6e6f 7420 696e 2028 2770 6172 656e 7427  not in ('parent'
-0000ad70: 2c20 276e 616d 6527 2920 616e 6420 6669  , 'name') and fi
-0000ad80: 656c 642e 696e 6974 3a0a 2020 2020 2020  eld.init:.      
-0000ad90: 2020 2020 7365 6c66 2e5f 7265 6769 7374      self._regist
-0000ada0: 6572 5f73 7562 6d6f 6475 6c65 7328 6669  er_submodules(fi
-0000adb0: 656c 642e 6e61 6d65 2c20 6765 7461 7474  eld.name, getatt
-0000adc0: 7228 7365 6c66 2c20 6669 656c 642e 6e61  r(self, field.na
-0000add0: 6d65 2929 0a0a 2020 2020 7365 6c66 2e5f  me))..    self._
-0000ade0: 7374 6174 652e 6973 5f69 6e69 7469 616c  state.is_initial
-0000adf0: 697a 6564 203d 2054 7275 650a 0a20 2064  ized = True..  d
-0000ae00: 6566 205f 5f72 6570 725f 5f28 7365 6c66  ef __repr__(self
-0000ae10: 2920 2d3e 2073 7472 3a0a 2020 2020 7265  ) -> str:.    re
-0000ae20: 7475 726e 205f 6d6f 6475 6c65 5f72 6570  turn _module_rep
-0000ae30: 7228 7365 6c66 290a 0a20 2064 6566 2073  r(self)..  def s
-0000ae40: 6574 7570 2873 656c 6629 202d 3e20 4e6f  etup(self) -> No
-0000ae50: 6e65 3a0a 2020 2020 2222 2249 6e69 7469  ne:.    """Initi
-0000ae60: 616c 697a 6573 2061 204d 6f64 756c 6520  alizes a Module 
-0000ae70: 6c61 7a69 6c79 2028 7369 6d69 6c61 7220  lazily (similar 
-0000ae80: 746f 2061 206c 617a 7920 6060 5f5f 696e  to a lazy ``__in
-0000ae90: 6974 5f5f 6060 292e 0a0a 2020 2020 6060  it__``)...    ``
-0000aea0: 7365 7475 7060 6020 6973 2063 616c 6c65  setup`` is calle
-0000aeb0: 6420 6f6e 6365 206c 617a 696c 7920 6f6e  d once lazily on
-0000aec0: 2061 206d 6f64 756c 6520 696e 7374 616e   a module instan
-0000aed0: 6365 2077 6865 6e20 6120 6d6f 6475 6c65  ce when a module
-0000aee0: 0a20 2020 2069 7320 626f 756e 642c 2069  .    is bound, i
-0000aef0: 6d6d 6564 6961 7465 6c79 2062 6566 6f72  mmediately befor
-0000af00: 6520 616e 7920 6f74 6865 7220 6d65 7468  e any other meth
-0000af10: 6f64 7320 6c69 6b65 2060 605f 5f63 616c  ods like ``__cal
-0000af20: 6c5f 5f60 6020 6172 650a 2020 2020 696e  l__`` are.    in
-0000af30: 766f 6b65 642c 206f 7220 6265 666f 7265  voked, or before
-0000af40: 2061 2060 6073 6574 7570 6060 2d64 6566   a ``setup``-def
-0000af50: 696e 6564 2061 7474 7269 6275 7465 206f  ined attribute o
-0000af60: 6e20 6060 7365 6c66 6060 2069 7320 6163  n ``self`` is ac
-0000af70: 6365 7373 6564 2e0a 0a20 2020 2054 6869  cessed...    Thi
-0000af80: 7320 6361 6e20 6861 7070 656e 2069 6e20  s can happen in 
-0000af90: 7468 7265 6520 6361 7365 733a 0a0a 2020  three cases:..  
-0000afa0: 2020 2020 312e 2049 6d6d 6564 6961 7465      1. Immediate
-0000afb0: 6c79 2077 6865 6e20 696e 766f 6b69 6e67  ly when invoking
-0000afc0: 203a 6d65 7468 3a60 6170 706c 7960 2c20   :meth:`apply`, 
-0000afd0: 3a6d 6574 683a 6069 6e69 7460 206f 720a  :meth:`init` or.
-0000afe0: 2020 2020 2020 2020 203a 6d65 7468 3a60           :meth:`
-0000aff0: 696e 6974 5f61 6e64 5f6f 7574 7075 7460  init_and_output`
-0000b000: 2e0a 0a20 2020 2020 2032 2e20 4f6e 6365  ...      2. Once
-0000b010: 2074 6865 206d 6f64 756c 6520 6973 2067   the module is g
-0000b020: 6976 656e 2061 206e 616d 6520 6279 2062  iven a name by b
-0000b030: 6569 6e67 2061 7373 6967 6e65 6420 746f  eing assigned to
-0000b040: 2061 6e20 6174 7472 6962 7574 6520 6f66   an attribute of
-0000b050: 0a20 2020 2020 2020 2020 616e 6f74 6865  .         anothe
-0000b060: 7220 6d6f 6475 6c65 2069 6e73 6964 6520  r module inside 
-0000b070: 7468 6520 6f74 6865 7220 6d6f 6475 6c65  the other module
-0000b080: 2773 2060 6073 6574 7570 6060 206d 6574  's ``setup`` met
-0000b090: 686f 640a 2020 2020 2020 2020 2028 7365  hod.         (se
-0000b0a0: 6520 3a6d 6574 683a 605f 5f73 6574 6174  e :meth:`__setat
-0000b0b0: 7472 5f5f 6029 3a3a 0a0a 2020 2020 2020  tr__`)::..      
-0000b0c0: 2020 2020 2020 3e3e 3e20 636c 6173 7320        >>> class 
-0000b0d0: 4d79 4d6f 6475 6c65 286e 6e2e 4d6f 6475  MyModule(nn.Modu
-0000b0e0: 6c65 293a 0a20 2020 2020 2020 2020 2020  le):.           
-0000b0f0: 202e 2e2e 2020 2064 6566 2073 6574 7570   ...   def setup
-0000b100: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
-0000b110: 2020 2020 2e2e 2e20 2020 2020 7375 626d      ...     subm
-0000b120: 6f64 756c 6520 3d20 6e6e 2e43 6f6e 7628  odule = nn.Conv(
-0000b130: 2e2e 2e29 0a0a 2020 2020 2020 2020 2020  ...)..          
-0000b140: 2020 2e2e 2e20 2020 2020 2320 4163 6365    ...     # Acce
-0000b150: 7373 696e 6720 6073 7562 6d6f 6475 6c65  ssing `submodule
-0000b160: 6020 6174 7472 6962 7574 6573 2064 6f65  ` attributes doe
-0000b170: 7320 6e6f 7420 7965 7420 776f 726b 2068  s not yet work h
-0000b180: 6572 652e 0a0a 2020 2020 2020 2020 2020  ere...          
-0000b190: 2020 2e2e 2e20 2020 2020 2320 5468 6520    ...     # The 
-0000b1a0: 666f 6c6c 6f77 696e 6720 6c69 6e65 2069  following line i
-0000b1b0: 6e76 6f6b 6573 2060 7365 6c66 2e5f 5f73  nvokes `self.__s
-0000b1c0: 6574 6174 7472 5f5f 602c 2077 6869 6368  etattr__`, which
-0000b1d0: 2067 6976 6573 0a20 2020 2020 2020 2020   gives.         
-0000b1e0: 2020 202e 2e2e 2020 2020 2023 2060 7375     ...     # `su
-0000b1f0: 626d 6f64 756c 6560 2074 6865 206e 616d  bmodule` the nam
-0000b200: 6520 2263 6f6e 7631 222e 0a20 2020 2020  e "conv1"..     
-0000b210: 2020 2020 2020 202e 2e2e 2020 2020 2073         ...     s
-0000b220: 656c 662e 636f 6e76 3120 3d20 7375 626d  elf.conv1 = subm
-0000b230: 6f64 756c 650a 0a20 2020 2020 2020 2020  odule..         
-0000b240: 2020 202e 2e2e 2020 2020 2023 2041 6363     ...     # Acc
-0000b250: 6573 7369 6e67 2060 7375 626d 6f64 756c  essing `submodul
-0000b260: 6560 2061 7474 7269 6275 7465 7320 6f72  e` attributes or
-0000b270: 206d 6574 686f 6473 2069 7320 6e6f 7720   methods is now 
-0000b280: 7361 6665 2061 6e64 0a20 2020 2020 2020  safe and.       
-0000b290: 2020 2020 202e 2e2e 2020 2020 2023 2065       ...     # e
-0000b2a0: 6974 6865 7220 6361 7573 6573 2073 6574  ither causes set
-0000b2b0: 7570 2829 2074 6f20 6265 2063 616c 6c65  up() to be calle
-0000b2c0: 6420 6f6e 6365 2e0a 0a20 2020 2020 2033  d once...      3
-0000b2d0: 2e20 4f6e 6365 2061 206d 6f64 756c 6520  . Once a module 
-0000b2e0: 6973 2063 6f6e 7374 7275 6374 6564 2069  is constructed i
-0000b2f0: 6e73 6964 6520 6120 6d65 7468 6f64 2077  nside a method w
-0000b300: 7261 7070 6564 2077 6974 680a 2020 2020  rapped with.    
-0000b310: 2020 2020 203a 6d65 7468 3a60 636f 6d70       :meth:`comp
-0000b320: 6163 7460 2c20 696d 6d65 6469 6174 656c  act`, immediatel
-0000b330: 7920 6265 666f 7265 2061 6e6f 7468 6572  y before another
-0000b340: 206d 6574 686f 6420 6973 2063 616c 6c65   method is calle
-0000b350: 6420 6f72 0a20 2020 2020 2020 2020 6060  d or.         ``
-0000b360: 7365 7475 7060 6020 6465 6669 6e65 6420  setup`` defined 
-0000b370: 6174 7472 6962 7574 6520 6973 2061 6363  attribute is acc
-0000b380: 6573 7365 642e 0a20 2020 2022 2222 0a20  essed..    """. 
-0000b390: 2020 2070 6173 730a 0a20 2064 6566 205f     pass..  def _
-0000b3a0: 7265 6769 7374 6572 5f73 7562 6d6f 6475  register_submodu
-0000b3b0: 6c65 7328 7365 6c66 2c20 6e61 6d65 2c20  les(self, name, 
-0000b3c0: 7661 6c29 3a0a 2020 2020 2222 2252 6567  val):.    """Reg
-0000b3d0: 6973 7465 7273 2061 2073 7562 6d6f 6475  isters a submodu
-0000b3e0: 6c65 2e22 2222 0a20 2020 2061 7373 6572  le.""".    asser
-0000b3f0: 7420 7365 6c66 2e73 636f 7065 2c20 2754  t self.scope, 'T
-0000b400: 7279 696e 6720 746f 2072 6567 6973 7465  rying to registe
-0000b410: 7220 7375 626d 6f64 756c 6573 206f 6e20  r submodules on 
-0000b420: 756e 626f 756e 6420 7363 6f70 652e 270a  unbound scope.'.
-0000b430: 2020 2020 726f 6f74 203d 2073 656c 662e      root = self.
-0000b440: 7363 6f70 652e 726f 6f74 0a20 2020 2063  scope.root.    c
-0000b450: 6163 6865 203d 205f 6361 6368 6573 2e67  ache = _caches.g
-0000b460: 6574 2872 6f6f 742c 2077 6561 6b72 6566  et(root, weakref
-0000b470: 2e57 6561 6b56 616c 7565 4469 6374 696f  .WeakValueDictio
-0000b480: 6e61 7279 2829 290a 2020 2020 5f63 6163  nary()).    _cac
-0000b490: 6865 735b 726f 6f74 5d20 3d20 6361 6368  hes[root] = cach
-0000b4a0: 650a 2020 2020 7175 6575 6520 3d20 5b5d  e.    queue = []
-0000b4b0: 0a20 2020 2070 7265 7365 7276 655f 6164  .    preserve_ad
-0000b4c0: 6f70 7465 645f 6e61 6d65 7320 3d20 636f  opted_names = co
-0000b4d0: 6e66 6967 2e66 6c61 785f 7072 6573 6572  nfig.flax_preser
-0000b4e0: 7665 5f61 646f 7074 6564 5f6e 616d 6573  ve_adopted_names
-0000b4f0: 0a20 2020 2069 6620 6861 7361 7474 7228  .    if hasattr(
-0000b500: 7479 7065 2873 656c 6629 2c20 2770 7265  type(self), 'pre
-0000b510: 7365 7276 655f 6164 6f70 7465 645f 6e61  serve_adopted_na
-0000b520: 6d65 7327 293a 0a20 2020 2020 2070 7265  mes'):.      pre
-0000b530: 7365 7276 655f 6164 6f70 7465 645f 6e61  serve_adopted_na
-0000b540: 6d65 7320 3d20 7479 7065 2873 656c 6629  mes = type(self)
-0000b550: 2e70 7265 7365 7276 655f 6164 6f70 7465  .preserve_adopte
-0000b560: 645f 6e61 6d65 730a 0a20 2020 2064 6566  d_names..    def
-0000b570: 2061 646f 7074 5f61 7474 725f 6d6f 6475   adopt_attr_modu
-0000b580: 6c65 7328 6361 6368 652c 2071 7565 7565  les(cache, queue
-0000b590: 2c20 7375 6666 6978 2c20 7375 6276 616c  , suffix, subval
-0000b5a0: 7565 293a 0a20 2020 2020 2069 6620 6973  ue):.      if is
-0000b5b0: 696e 7374 616e 6365 2873 7562 7661 6c75  instance(subvalu
-0000b5c0: 652c 204d 6f64 756c 6529 3a0a 2020 2020  e, Module):.    
-0000b5d0: 2020 2020 6375 7272 656e 745f 6e61 6d65      current_name
-0000b5e0: 203d 2073 7562 7661 6c75 652e 6e61 6d65   = subvalue.name
-0000b5f0: 0a20 2020 2020 2020 2061 646f 7074 6564  .        adopted
-0000b600: 5f6e 616d 6520 3d20 4e6f 6e65 0a20 2020  _name = None.   
-0000b610: 2020 2020 2069 6620 7375 6276 616c 7565       if subvalue
-0000b620: 2e70 6172 656e 7420 6973 204e 6f6e 653a  .parent is None:
-0000b630: 0a20 2020 2020 2020 2020 2023 2050 7265  .          # Pre
-0000b640: 7365 7276 6520 7368 6172 696e 672d 6279  serve sharing-by
-0000b650: 2d72 6566 6572 656e 6365 2072 656c 6174  -reference relat
-0000b660: 696f 6e73 6869 7073 2064 7572 696e 6720  ionships during 
-0000b670: 6164 6f70 7469 6f6e 0a20 2020 2020 2020  adoption.       
-0000b680: 2020 2023 2076 6961 2063 6163 6865 206b     # via cache k
-0000b690: 6579 6564 206f 6e20 756e 6971 7565 2069  eyed on unique i
-0000b6a0: 6e73 7461 6e63 6520 6964 732e 0a20 2020  nstance ids..   
-0000b6b0: 2020 2020 2020 206b 6579 203d 2073 7562         key = sub
-0000b6c0: 7661 6c75 652e 5f69 640a 2020 2020 2020  value._id.      
-0000b6d0: 2020 2020 2320 4d6f 6475 6c65 2077 6173      # Module was
-0000b6e0: 2070 6173 7365 6420 6672 6f6d 206f 7574   passed from out
-0000b6f0: 7369 6465 2e20 4974 206e 6565 6473 2074  side. It needs t
-0000b700: 6f20 6265 2063 6c6f 6e65 642e 0a20 2020  o be cloned..   
-0000b710: 2020 2020 2020 2023 204f 7574 7369 6465         # Outside
-0000b720: 206d 6f64 756c 6573 2061 7265 206e 616d   modules are nam
-0000b730: 6564 2062 7920 6174 7461 6368 6d65 6e74  ed by attachment
-0000b740: 2c20 6e6f 7420 616e 206f 7574 6572 206e  , not an outer n
-0000b750: 616d 652c 0a20 2020 2020 2020 2020 2023  ame,.          #
-0000b760: 2055 4e4c 4553 5320 7765 2772 6520 7573   UNLESS we're us
-0000b770: 696e 6720 6e65 7720 6164 6f70 7465 6420  ing new adopted 
-0000b780: 6e61 6d65 2070 6f6c 6963 792c 2069 6e20  name policy, in 
-0000b790: 7768 6963 6820 6361 7365 2061 6e20 6578  which case an ex
-0000b7a0: 6973 7469 6e67 0a20 2020 2020 2020 2020  isting.         
-0000b7b0: 2023 206e 616d 6520 7769 6c6c 2062 6520   # name will be 
-0000b7c0: 7573 6564 2c20 6173 2069 7320 6f66 7465  used, as is ofte
-0000b7d0: 6e20 7375 7070 6c69 6564 2062 7920 636f  n supplied by co
-0000b7e0: 6e66 6967 2073 7973 7465 6d73 2e0a 2020  nfig systems..  
-0000b7f0: 2020 2020 2020 2020 6966 2070 7265 7365          if prese
-0000b800: 7276 655f 6164 6f70 7465 645f 6e61 6d65  rve_adopted_name
-0000b810: 733a 0a20 2020 2020 2020 2020 2020 2061  s:.            a
-0000b820: 646f 7074 6564 5f6e 616d 6520 3d20 6f62  dopted_name = ob
-0000b830: 6a65 6374 2e5f 5f67 6574 6174 7472 6962  ject.__getattrib
-0000b840: 7574 655f 5f28 7375 6276 616c 7565 2c20  ute__(subvalue, 
-0000b850: 276e 616d 6527 290a 2020 2020 2020 2020  'name').        
-0000b860: 2020 6966 206b 6579 2069 6e20 6361 6368    if key in cach
-0000b870: 653a 0a20 2020 2020 2020 2020 2020 2073  e:.            s
-0000b880: 7562 7661 6c75 6520 3d20 6361 6368 655b  ubvalue = cache[
-0000b890: 6b65 795d 0a20 2020 2020 2020 2020 2065  key].          e
-0000b8a0: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           
-0000b8b0: 2073 7562 7661 6c75 6520 3d20 7375 6276   subvalue = subv
-0000b8c0: 616c 7565 2e63 6c6f 6e65 286e 616d 653d  alue.clone(name=
-0000b8d0: 4e6f 6e65 290a 2020 2020 2020 2020 2020  None).          
-0000b8e0: 2020 6361 6368 655b 6b65 795d 203d 2073    cache[key] = s
-0000b8f0: 7562 7661 6c75 650a 2020 2020 2020 2020  ubvalue.        
-0000b900: 6966 2073 7562 7661 6c75 652e 6e61 6d65  if subvalue.name
-0000b910: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
-0000b920: 2020 2020 6f62 6a65 6374 2e5f 5f73 6574      object.__set
-0000b930: 6174 7472 5f5f 2873 7562 7661 6c75 652c  attr__(subvalue,
-0000b940: 2027 7061 7265 6e74 272c 2073 656c 6629   'parent', self)
-0000b950: 0a20 2020 2020 2020 2020 2069 6620 6164  .          if ad
-0000b960: 6f70 7465 645f 6e61 6d65 2069 7320 4e6f  opted_name is No
-0000b970: 6e65 3a0a 2020 2020 2020 2020 2020 2020  ne:.            
-0000b980: 6164 6f70 7465 645f 6e61 6d65 203d 2028  adopted_name = (
-0000b990: 0a20 2020 2020 2020 2020 2020 2020 2066  .              f
-0000b9a0: 277b 6e61 6d65 7d7b 7375 6666 6978 7d27  '{name}{suffix}'
-0000b9b0: 0a20 2020 2020 2020 2020 2020 2020 2069  .              i
-0000b9c0: 6620 6e6f 7420 6973 696e 7374 616e 6365  f not isinstance
-0000b9d0: 2873 7562 7661 6c75 652c 2043 6f6d 7061  (subvalue, Compa
-0000b9e0: 6374 4e61 6d65 5363 6f70 6529 0a20 2020  ctNameScope).   
-0000b9f0: 2020 2020 2020 2020 2020 2065 6c73 6520             else 
-0000ba00: 6375 7272 656e 745f 6e61 6d65 0a20 2020  current_name.   
-0000ba10: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     
-0000ba20: 2020 2020 206f 626a 6563 742e 5f5f 7365       object.__se
-0000ba30: 7461 7474 725f 5f28 7375 6276 616c 7565  tattr__(subvalue
-0000ba40: 2c20 276e 616d 6527 2c20 6164 6f70 7465  , 'name', adopte
-0000ba50: 645f 6e61 6d65 290a 2020 2020 2020 2020  d_name).        
-0000ba60: 2020 7175 6575 652e 6170 7065 6e64 2873    queue.append(s
-0000ba70: 7562 7661 6c75 6529 0a20 2020 2020 2072  ubvalue).      r
-0000ba80: 6574 7572 6e20 7375 6276 616c 7565 0a0a  eturn subvalue..
-0000ba90: 2020 2020 7661 6c20 3d20 5f66 7265 657a      val = _freez
-0000baa0: 655f 6174 7472 280a 2020 2020 2020 5f6d  e_attr(.      _m
-0000bab0: 6170 5f6f 7665 725f 6d6f 6475 6c65 735f  ap_over_modules_
-0000bac0: 696e 5f74 7265 6528 0a20 2020 2020 2020  in_tree(.       
-0000bad0: 2066 756e 6374 6f6f 6c73 2e70 6172 7469   functools.parti
-0000bae0: 616c 2861 646f 7074 5f61 7474 725f 6d6f  al(adopt_attr_mo
-0000baf0: 6475 6c65 732c 2063 6163 6865 2c20 7175  dules, cache, qu
-0000bb00: 6575 6529 2c20 7661 6c0a 2020 2020 2020  eue), val.      
-0000bb10: 290a 2020 2020 290a 2020 2020 6f62 6a65  ).    ).    obje
-0000bb20: 6374 2e5f 5f73 6574 6174 7472 5f5f 2873  ct.__setattr__(s
-0000bb30: 656c 662c 206e 616d 652c 2076 616c 290a  elf, name, val).
-0000bb40: 2020 2020 666f 7220 7820 696e 2071 7565      for x in que
-0000bb50: 7565 3a0a 2020 2020 2020 782e 5f5f 706f  ue:.      x.__po
-0000bb60: 7374 5f69 6e69 745f 5f28 290a 0a20 2064  st_init__()..  d
-0000bb70: 6566 205f 7472 795f 7365 7475 7028 7365  ef _try_setup(se
-0000bb80: 6c66 2c20 7368 616c 6c6f 773a 2062 6f6f  lf, shallow: boo
-0000bb90: 6c20 3d20 4661 6c73 6529 202d 3e20 4e6f  l = False) -> No
-0000bba0: 6e65 3a0a 2020 2020 2222 2254 7269 6573  ne:.    """Tries
-0000bbb0: 2074 6f20 7365 7475 7020 6d6f 6475 6c65   to setup module
-0000bbc0: 2069 6620 7363 6f70 6520 6973 2061 7661   if scope is ava
-0000bbd0: 696c 6162 6c65 2061 6e64 2073 6574 7570  ilable and setup
-0000bbe0: 2068 6173 206e 6f74 2062 6565 6e20 6361   has not been ca
-0000bbf0: 6c6c 6564 2079 6574 2e22 2222 0a20 2020  lled yet.""".   
-0000bc00: 2069 6620 280a 2020 2020 2020 7365 6c66   if (.      self
-0000bc10: 2e73 636f 7065 0a20 2020 2020 2061 6e64  .scope.      and
-0000bc20: 206e 6f74 2073 656c 662e 5f73 7461 7465   not self._state
-0000bc30: 2e69 6e5f 7365 7475 700a 2020 2020 2020  .in_setup.      
-0000bc40: 616e 6420 7365 6c66 2e5f 7374 6174 652e  and self._state.
-0000bc50: 7365 7475 705f 6361 6c6c 6564 2021 3d20  setup_called != 
-0000bc60: 5365 7475 7053 7461 7465 2e44 4f4e 450a  SetupState.DONE.
-0000bc70: 2020 2020 293a 0a20 2020 2020 2074 7279      ):.      try
-0000bc80: 3a0a 2020 2020 2020 2020 7365 6c66 2e5f  :.        self._
-0000bc90: 7374 6174 652e 696e 5f73 6574 7570 203d  state.in_setup =
-0000bca0: 2054 7275 650a 2020 2020 2020 2020 2320   True.        # 
-0000bcb0: 4120 7368 616c 6c6f 7720 7365 7475 7020  A shallow setup 
-0000bcc0: 7769 6c6c 206f 6e6c 7920 7265 6769 7374  will only regist
-0000bcd0: 6572 2061 7474 7269 6275 7465 2073 7562  er attribute sub
-0000bce0: 6d6f 6475 6c65 7320 6275 7420 6974 2064  modules but it d
-0000bcf0: 6f65 730a 2020 2020 2020 2020 2320 6e6f  oes.        # no
-0000bd00: 7420 6361 6c6c 2074 6865 2075 7365 7227  t call the user'
-0000bd10: 7320 7365 7475 702e 2054 6869 7320 6176  s setup. This av
-0000bd20: 6f69 6473 2072 756e 6e69 6e67 2062 6566  oids running bef
-0000bd30: 6f72 6520 610a 2020 2020 2020 2020 2320  ore a.        # 
-0000bd40: 7472 616e 7366 6f72 6d61 7469 6f6e 2e0a  transformation..
-0000bd50: 2020 2020 2020 2020 666f 7220 6669 656c          for fiel
-0000bd60: 6420 696e 2064 6174 6163 6c61 7373 6573  d in dataclasses
-0000bd70: 2e66 6965 6c64 7328 7365 6c66 293a 0a20  .fields(self):. 
-0000bd80: 2020 2020 2020 2020 2069 6620 6669 656c           if fiel
-0000bd90: 642e 6e61 6d65 206e 6f74 2069 6e20 2827  d.name not in ('
-0000bda0: 7061 7265 6e74 272c 2027 6e61 6d65 2729  parent', 'name')
-0000bdb0: 2061 6e64 2066 6965 6c64 2e69 6e69 743a   and field.init:
-0000bdc0: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel
-0000bdd0: 662e 5f72 6567 6973 7465 725f 7375 626d  f._register_subm
-0000bde0: 6f64 756c 6573 2866 6965 6c64 2e6e 616d  odules(field.nam
-0000bdf0: 652c 2067 6574 6174 7472 2873 656c 662c  e, getattr(self,
-0000be00: 2066 6965 6c64 2e6e 616d 6529 290a 2020   field.name)).  
-0000be10: 2020 2020 2020 6966 206e 6f74 2073 6861        if not sha
-0000be20: 6c6c 6f77 3a0a 2020 2020 2020 2020 2020  llow:.          
-0000be30: 7365 6c66 2e73 6574 7570 2829 0a20 2020  self.setup().   
-0000be40: 2020 2020 2020 2023 2063 7265 6174 6520         # create 
-0000be50: 4e6f 6e54 7261 6e73 7061 7265 6e74 204d  NonTransparent M
-0000be60: 6f64 756c 6573 0a20 2020 2020 2020 2020  odules.         
-0000be70: 2073 656c 662e 5f63 6f6d 7061 6374 5f6e   self._compact_n
-0000be80: 616d 655f 7363 6f70 655f 6d6f 6475 6c65  ame_scope_module
-0000be90: 7320 3d20 7b0a 2020 2020 2020 2020 2020  s = {.          
-0000bea0: 2020 6e61 6d65 3a20 436f 6d70 6163 744e    name: CompactN
-0000beb0: 616d 6553 636f 7065 280a 2020 2020 2020  ameScope(.      
-0000bec0: 2020 2020 2020 2020 6765 7461 7474 7228          getattr(
-0000bed0: 7479 7065 2873 656c 6629 2c20 6e61 6d65  type(self), name
-0000bee0: 292e 696e 6e65 725f 6675 6e2c 206c 616d  ).inner_fun, lam
-0000bef0: 6264 613a 2073 656c 662c 206e 616d 653d  bda: self, name=
-0000bf00: 6e61 6d65 0a20 2020 2020 2020 2020 2020  name.           
-0000bf10: 2029 0a20 2020 2020 2020 2020 2020 2066   ).            f
-0000bf20: 6f72 206e 616d 6520 696e 2073 656c 662e  or name in self.
-0000bf30: 5f63 6f6d 7061 6374 5f6e 616d 655f 7363  _compact_name_sc
-0000bf40: 6f70 655f 6d65 7468 6f64 730a 2020 2020  ope_methods.    
-0000bf50: 2020 2020 2020 7d0a 0a20 2020 2020 2020        }..       
-0000bf60: 2023 2057 6520 7275 6e20 7374 6174 6963   # We run static
-0000bf70: 2063 6865 636b 7320 6162 7374 7261 6374   checks abstract
-0000bf80: 6c79 206f 6e63 6520 666f 7220 7365 7475  ly once for setu
-0000bf90: 7020 6265 666f 7265 2061 6e79 2074 7261  p before any tra
-0000bfa0: 6e73 666f 726d 730a 2020 2020 2020 2020  nsforms.        
-0000bfb0: 2320 746f 2064 6574 6563 7420 6e61 6d65  # to detect name
-0000bfc0: 2063 6f6c 6c69 7369 6f6e 7320 616e 6420   collisions and 
-0000bfd0: 6f74 6865 7220 7079 7468 6f6e 2065 7272  other python err
-0000bfe0: 6f72 732e 0a20 2020 2020 2020 2065 6c69  ors..        eli
-0000bff0: 6620 7365 6c66 2e5f 7374 6174 652e 7365  f self._state.se
-0000c000: 7475 705f 6361 6c6c 6564 203d 3d20 5365  tup_called == Se
-0000c010: 7475 7053 7461 7465 2e4e 4557 3a0a 2020  tupState.NEW:.  
-0000c020: 2020 2020 2020 2020 7365 6c66 2e5f 7661          self._va
-0000c030: 6c69 6461 7465 5f73 6574 7570 2829 0a20  lidate_setup(). 
-0000c040: 2020 2020 2066 696e 616c 6c79 3a0a 2020       finally:.  
-0000c050: 2020 2020 2020 7365 6c66 2e5f 7374 6174        self._stat
-0000c060: 652e 696e 5f73 6574 7570 203d 2046 616c  e.in_setup = Fal
-0000c070: 7365 0a20 2020 2020 2020 2069 6620 6e6f  se.        if no
-0000c080: 7420 7368 616c 6c6f 773a 0a20 2020 2020  t shallow:.     
-0000c090: 2020 2020 2073 656c 662e 5f73 7461 7465       self._state
-0000c0a0: 2e73 6574 7570 5f63 616c 6c65 6420 3d20  .setup_called = 
-0000c0b0: 5365 7475 7053 7461 7465 2e44 4f4e 450a  SetupState.DONE.
-0000c0c0: 0a20 2064 6566 205f 7661 6c69 6461 7465  .  def _validate
-0000c0d0: 5f73 6574 7570 2873 656c 6629 202d 3e20  _setup(self) -> 
-0000c0e0: 4e6f 6e65 3a0a 2020 2020 2222 2241 6273  None:.    """Abs
-0000c0f0: 7472 6163 746c 7920 6576 616c 7561 7465  tractly evaluate
-0000c100: 7320 7365 7475 7020 6f6e 6c79 2074 6f20  s setup only to 
-0000c110: 7275 6e20 7374 6174 6963 2063 6865 636b  run static check
-0000c120: 732e 2222 220a 0a20 2020 2064 6566 2072  s."""..    def r
-0000c130: 756e 5f73 6574 7570 5f6f 6e6c 7928 7829  un_setup_only(x)
-0000c140: 3a0a 2020 2020 2020 7772 6170 7065 645f  :.      wrapped_
-0000c150: 6964 203d 2077 7261 705f 6d65 7468 6f64  id = wrap_method
-0000c160: 5f6f 6e63 6528 6c61 6d62 6461 206d 2c20  _once(lambda m, 
-0000c170: 783a 2078 290a 2020 2020 2020 7769 7468  x: x).      with
-0000c180: 2054 6573 7453 636f 7065 287b 7d2c 2072   TestScope({}, r
-0000c190: 6e67 733d 7b7d 2c20 6d75 7461 626c 653d  ngs={}, mutable=
-0000c1a0: 5472 7565 292e 7465 6d70 6f72 6172 7928  True).temporary(
-0000c1b0: 2920 6173 2072 6f6f 743a 0a20 2020 2020  ) as root:.     
-0000c1c0: 2020 2072 6574 7572 6e20 7772 6170 7065     return wrappe
-0000c1d0: 645f 6964 2873 656c 662e 636c 6f6e 6528  d_id(self.clone(
-0000c1e0: 7061 7265 6e74 3d72 6f6f 7429 2c20 7829  parent=root), x)
-0000c1f0: 0a0a 2020 2020 5f20 3d20 6a61 782e 6576  ..    _ = jax.ev
-0000c200: 616c 5f73 6861 7065 2872 756e 5f73 6574  al_shape(run_set
-0000c210: 7570 5f6f 6e6c 792c 2030 290a 0a20 2064  up_only, 0)..  d
-0000c220: 6566 205f 6e61 6d65 5f74 616b 656e 280a  ef _name_taken(.
-0000c230: 2020 2020 7365 6c66 2c0a 2020 2020 6e61      self,.    na
-0000c240: 6d65 3a20 7374 722c 0a20 2020 2072 6575  me: str,.    reu
-0000c250: 7365 5f73 636f 7065 733a 2062 6f6f 6c20  se_scopes: bool 
-0000c260: 3d20 4661 6c73 652c 0a20 2020 2063 6f6c  = False,.    col
-0000c270: 6c65 6374 696f 6e3a 204f 7074 696f 6e61  lection: Optiona
-0000c280: 6c5b 7374 725d 203d 204e 6f6e 652c 0a20  l[str] = None,. 
-0000c290: 2029 202d 3e20 626f 6f6c 3a0a 2020 2020   ) -> bool:.    
-0000c2a0: 6173 7365 7274 2073 656c 662e 7363 6f70  assert self.scop
-0000c2b0: 6520 6973 206e 6f74 204e 6f6e 650a 2020  e is not None.  
-0000c2c0: 2020 6966 2072 6575 7365 5f73 636f 7065    if reuse_scope
-0000c2d0: 733a 0a20 2020 2020 2072 6574 7572 6e20  s:.      return 
-0000c2e0: 4661 6c73 650a 2020 2020 7265 7475 726e  False.    return
-0000c2f0: 2073 656c 662e 7363 6f70 652e 6e61 6d65   self.scope.name
-0000c300: 5f72 6573 6572 7665 6428 6e61 6d65 2c20  _reserved(name, 
-0000c310: 636f 6c6c 6563 7469 6f6e 290a 0a20 2040  collection)..  @
-0000c320: 7072 6f70 6572 7479 0a20 2064 6566 205f  property.  def _
-0000c330: 696e 6974 6961 6c69 7a61 7469 6f6e 5f61  initialization_a
-0000c340: 6c6c 6f77 6564 2873 656c 6629 3a0a 2020  llowed(self):.  
-0000c350: 2020 7265 7475 726e 2028 0a20 2020 2020    return (.     
-0000c360: 206e 6f74 2073 656c 662e 5f73 7461 7465   not self._state
-0000c370: 2e69 735f 696e 6974 6961 6c69 7a65 6420  .is_initialized 
-0000c380: 2023 2061 6c6c 6f77 2065 6167 6572 2061   # allow eager a
-0000c390: 7474 6163 686d 656e 7420 696e 2070 6f73  ttachment in pos
-0000c3a0: 742d 696e 6974 0a20 2020 2020 206f 7220  t-init.      or 
-0000c3b0: 7365 6c66 2e5f 7374 6174 652e 696e 5f73  self._state.in_s
-0000c3c0: 6574 7570 0a20 2020 2020 206f 7220 7365  etup.      or se
-0000c3d0: 6c66 2e5f 7374 6174 652e 696e 5f63 6f6d  lf._state.in_com
-0000c3e0: 7061 6374 5f6d 6574 686f 640a 2020 2020  pact_method.    
-0000c3f0: 290a 0a20 2040 7072 6f70 6572 7479 0a20  )..  @property. 
-0000c400: 2064 6566 2070 6174 6828 7365 6c66 293a   def path(self):
-0000c410: 0a20 2020 2069 6620 7365 6c66 2e73 636f  .    if self.sco
-0000c420: 7065 2069 7320 4e6f 6e65 3a0a 2020 2020  pe is None:.    
-0000c430: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-0000c440: 6f72 2822 4361 6e27 7420 6163 6365 7373  or("Can't access
-0000c450: 206d 6f64 756c 6520 7061 7468 7320 6f6e   module paths on
-0000c460: 2075 6e62 6f75 6e64 206d 6f64 756c 6573   unbound modules
-0000c470: 2e22 290a 0a20 2020 2072 6574 7572 6e20  .")..    return 
-0000c480: 7365 6c66 2e73 636f 7065 2e70 6174 680a  self.scope.path.
-0000c490: 0a20 2064 6566 2063 6c6f 6e65 280a 2020  .  def clone(.  
-0000c4a0: 2020 7365 6c66 3a20 4d2c 0a20 2020 202a    self: M,.    *
-0000c4b0: 2c0a 2020 2020 7061 7265 6e74 3a20 4f70  ,.    parent: Op
-0000c4c0: 7469 6f6e 616c 5b55 6e69 6f6e 5b53 636f  tional[Union[Sco
-0000c4d0: 7065 2c20 274d 6f64 756c 6527 2c20 5f53  pe, 'Module', _S
-0000c4e0: 656e 7469 6e65 6c5d 5d20 3d20 4e6f 6e65  entinel]] = None
-0000c4f0: 2c0a 2020 2020 5f64 6565 705f 636c 6f6e  ,.    _deep_clon
-0000c500: 653a 2055 6e69 6f6e 5b62 6f6f 6c2c 2077  e: Union[bool, w
-0000c510: 6561 6b72 6566 2e57 6561 6b56 616c 7565  eakref.WeakValue
-0000c520: 4469 6374 696f 6e61 7279 5d20 3d20 4661  Dictionary] = Fa
-0000c530: 6c73 652c 0a20 2020 205f 7265 7365 745f  lse,.    _reset_
-0000c540: 6e61 6d65 733a 2062 6f6f 6c20 3d20 4661  names: bool = Fa
-0000c550: 6c73 652c 0a20 2020 202a 2a75 7064 6174  lse,.    **updat
-0000c560: 6573 2c0a 2020 2920 2d3e 204d 3a0a 2020  es,.  ) -> M:.  
-0000c570: 2020 2222 2243 7265 6174 6573 2061 2063    """Creates a c
-0000c580: 6c6f 6e65 206f 6620 7468 6973 204d 6f64  lone of this Mod
-0000c590: 756c 652c 2077 6974 6820 6f70 7469 6f6e  ule, with option
-0000c5a0: 616c 6c79 2075 7064 6174 6564 2061 7267  ally updated arg
-0000c5b0: 756d 656e 7473 2e0a 0a20 2020 204e 4f54  uments...    NOT
-0000c5c0: 453a 2065 6e64 2075 7365 7273 2061 7265  E: end users are
-0000c5d0: 2065 6e63 6f75 7261 6765 6420 746f 2075   encouraged to u
-0000c5e0: 7365 2074 6865 2060 6063 6f70 7960 6020  se the ``copy`` 
-0000c5f0: 6d65 7468 6f64 2e20 2060 6063 6c6f 6e65  method.  ``clone
-0000c600: 6060 2069 7320 7573 6564 0a20 2020 2020  `` is used.     
-0000c610: 2070 7269 6d61 7269 6c79 2066 6f72 2069   primarily for i
-0000c620: 6e74 6572 6e61 6c20 726f 7574 696e 6573  nternal routines
-0000c630: 2c20 616e 6420 6060 636f 7079 6060 206f  , and ``copy`` o
-0000c640: 6666 6572 7320 7369 6d70 6c65 7220 6172  ffers simpler ar
-0000c650: 6775 6d65 6e74 7320 616e 640a 2020 2020  guments and.    
-0000c660: 2020 6265 7474 6572 2064 6566 6175 6c74    better default
-0000c670: 732e 0a0a 2020 2020 4172 6773 3a0a 2020  s...    Args:.  
-0000c680: 2020 2020 7061 7265 6e74 3a20 5468 6520      parent: The 
-0000c690: 7061 7265 6e74 206f 6620 7468 6520 636c  parent of the cl
-0000c6a0: 6f6e 652e 2054 6865 2063 6c6f 6e65 2077  one. The clone w
-0000c6b0: 696c 6c20 6861 7665 206e 6f20 7061 7265  ill have no pare
-0000c6c0: 6e74 2069 6620 6e6f 0a20 2020 2020 2020  nt if no.       
-0000c6d0: 2065 7870 6c69 6369 7420 7061 7265 6e74   explicit parent
-0000c6e0: 2069 7320 7370 6563 6966 6965 642e 0a20   is specified.. 
-0000c6f0: 2020 2020 205f 6465 6570 5f63 6c6f 6e65       _deep_clone
-0000c700: 3a20 4120 626f 6f6c 6561 6e20 6f72 2061  : A boolean or a
-0000c710: 2077 6561 6b20 7661 6c75 6520 6469 6374   weak value dict
-0000c720: 696f 6e61 7279 2074 6f20 636f 6e74 726f  ionary to contro
-0000c730: 6c20 6465 6570 2063 6c6f 6e69 6e67 0a20  l deep cloning. 
-0000c740: 2020 2020 2020 206f 6620 7375 626d 6f64         of submod
-0000c750: 756c 6573 2e20 4966 2054 7275 652c 2073  ules. If True, s
-0000c760: 7562 6d6f 6475 6c65 7320 7769 6c6c 2062  ubmodules will b
-0000c770: 6520 636c 6f6e 6564 2072 6563 7572 7369  e cloned recursi
-0000c780: 7665 6c79 2e20 4966 2061 2077 6561 6b0a  vely. If a weak.
-0000c790: 2020 2020 2020 2020 7661 6c75 6520 6469          value di
-0000c7a0: 6374 696f 6e61 7279 2069 7320 7061 7373  ctionary is pass
-0000c7b0: 6564 2c20 6974 2077 696c 6c20 6265 2075  ed, it will be u
-0000c7c0: 7365 6420 746f 2063 6163 6865 2063 6c6f  sed to cache clo
-0000c7d0: 6e65 6420 7375 626d 6f64 756c 6573 2e0a  ned submodules..
-0000c7e0: 2020 2020 2020 2020 5468 6973 2066 6c61          This fla
-0000c7f0: 6720 6973 2075 7365 6420 6279 2069 6e69  g is used by ini
-0000c800: 742f 6170 706c 792f 6269 6e64 2074 6f20  t/apply/bind to 
-0000c810: 6176 6f69 6420 7363 6f70 6520 6c65 616b  avoid scope leak
-0000c820: 6167 652e 0a20 2020 2020 205f 7265 7365  age..      _rese
-0000c830: 745f 6e61 6d65 733a 2049 6620 5472 7565  t_names: If True
-0000c840: 2c20 6060 6e61 6d65 3d4e 6f6e 6560 6020  , ``name=None`` 
-0000c850: 6973 2061 6c73 6f20 7061 7373 6564 2074  is also passed t
-0000c860: 6f20 7375 626d 6f64 756c 6573 2077 6865  o submodules whe
-0000c870: 6e0a 2020 2020 2020 2020 636c 6f6e 696e  n.        clonin
-0000c880: 672e 2052 6573 6574 7469 6e67 206e 616d  g. Resetting nam
-0000c890: 6573 2069 6e20 7375 626d 6f64 756c 6573  es in submodules
-0000c8a0: 2069 7320 6e65 6365 7373 6172 7920 7768   is necessary wh
-0000c8b0: 656e 2063 616c 6c69 6e67 2060 602e 756e  en calling ``.un
-0000c8c0: 6269 6e64 6060 2e0a 2020 2020 2020 2a2a  bind``..      **
-0000c8d0: 7570 6461 7465 733a 2041 7474 7269 6275  updates: Attribu
-0000c8e0: 7465 2075 7064 6174 6573 2e0a 0a20 2020  te updates...   
-0000c8f0: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
-0000c900: 4120 636c 6f6e 6520 6f66 2074 6865 2074  A clone of the t
-0000c910: 6869 7320 4d6f 6475 6c65 2077 6974 6820  his Module with 
-0000c920: 7468 6520 7570 6461 7465 6420 6174 7472  the updated attr
-0000c930: 6962 7574 6573 2061 6e64 2070 6172 656e  ibutes and paren
-0000c940: 742e 0a20 2020 2022 2222 0a20 2020 2061  t..    """.    a
-0000c950: 7474 7273 203d 207b 0a20 2020 2020 2066  ttrs = {.      f
-0000c960: 2e6e 616d 653a 2067 6574 6174 7472 2873  .name: getattr(s
-0000c970: 656c 662c 2066 2e6e 616d 6529 2066 6f72  elf, f.name) for
-0000c980: 2066 2069 6e20 6461 7461 636c 6173 7365   f in dataclasse
-0000c990: 732e 6669 656c 6473 2873 656c 6629 2069  s.fields(self) i
-0000c9a0: 6620 662e 696e 6974 0a20 2020 207d 0a0a  f f.init.    }..
-0000c9b0: 2020 2020 6174 7472 732e 7570 6461 7465      attrs.update
-0000c9c0: 2870 6172 656e 743d 7061 7265 6e74 2c20  (parent=parent, 
-0000c9d0: 2a2a 7570 6461 7465 7329 0a0a 2020 2020  **updates)..    
-0000c9e0: 2320 4865 7265 2077 6520 696d 706c 656d  # Here we implem
-0000c9f0: 656e 7420 6465 6570 2063 6c6f 6e69 6e67  ent deep cloning
-0000ca00: 206f 6620 7375 626d 6f64 756c 6573 2c20   of submodules, 
-0000ca10: 7468 6973 2069 7320 6e65 6365 7373 6172  this is necessar
-0000ca20: 7920 746f 2061 766f 6964 2073 636f 7065  y to avoid scope
-0000ca30: 206c 6561 6b61 6765 0a20 2020 2023 2066   leakage.    # f
-0000ca40: 726f 6d20 6578 7465 726e 616c 2073 7562  rom external sub
-0000ca50: 6d6f 6475 6c65 7320 696e 746f 2069 6e69  modules into ini
-0000ca60: 742f 6170 706c 792f 6269 6e64 2077 6869  t/apply/bind whi
-0000ca70: 6c65 2070 7265 7365 7276 696e 6720 7368  le preserving sh
-0000ca80: 6172 696e 672d 6279 2d72 6566 6572 656e  aring-by-referen
-0000ca90: 6365 0a20 2020 2023 2072 656c 6174 696f  ce.    # relatio
-0000caa0: 6e73 6869 7073 2062 6574 7765 656e 2073  nships between s
-0000cab0: 7562 6d6f 6475 6c65 732e 0a20 2020 2069  ubmodules..    i
-0000cac0: 6620 5f64 6565 705f 636c 6f6e 6520 213d  f _deep_clone !=
-0000cad0: 2046 616c 7365 3a0a 2020 2020 2020 2320   False:.      # 
-0000cae0: 5765 2075 7365 2061 2077 6561 6b20 7661  We use a weak va
-0000caf0: 6c75 6520 6469 6374 696f 6e61 7279 2074  lue dictionary t
-0000cb00: 6f20 6361 6368 6520 636c 6f6e 6564 2073  o cache cloned s
-0000cb10: 7562 6d6f 6475 6c65 732e 2057 6865 6e20  ubmodules. When 
-0000cb20: 6120 7368 6172 6564 0a20 2020 2020 2023  a shared.      #
-0000cb30: 2073 7562 6d6f 6475 6c65 2069 7320 636c   submodule is cl
-0000cb40: 6f6e 6564 2c20 6974 7320 6f6e 6c79 2063  oned, its only c
-0000cb50: 6c6f 6e65 6420 6f6e 6365 2065 6c73 6520  loned once else 
-0000cb60: 6974 7320 6665 7463 6865 6420 6672 6f6d  its fetched from
-0000cb70: 2074 6865 2063 6163 6865 2e0a 2020 2020   the cache..    
-0000cb80: 2020 6361 6368 6520 3d20 280a 2020 2020    cache = (.    
-0000cb90: 2020 2020 7765 616b 7265 662e 5765 616b      weakref.Weak
-0000cba0: 5661 6c75 6544 6963 7469 6f6e 6172 7928  ValueDictionary(
-0000cbb0: 290a 2020 2020 2020 2020 6966 2069 7369  ).        if isi
-0000cbc0: 6e73 7461 6e63 6528 5f64 6565 705f 636c  nstance(_deep_cl
-0000cbd0: 6f6e 652c 2062 6f6f 6c29 0a20 2020 2020  one, bool).     
-0000cbe0: 2020 2065 6c73 6520 5f64 6565 705f 636c     else _deep_cl
-0000cbf0: 6f6e 650a 2020 2020 2020 290a 0a20 2020  one.      )..   
-0000cc00: 2020 2064 6566 2063 6c6f 6e65 5f66 6e28     def clone_fn(
-0000cc10: 6d3a 204d 6f64 756c 6529 202d 3e20 4d6f  m: Module) -> Mo
-0000cc20: 6475 6c65 3a0a 2020 2020 2020 2020 6966  dule:.        if
-0000cc30: 2068 6173 6174 7472 286d 2c20 275f 6964   hasattr(m, '_id
-0000cc40: 2729 3a0a 2020 2020 2020 2020 2020 6b65  '):.          ke
-0000cc50: 7920 3d20 6d2e 5f69 640a 2020 2020 2020  y = m._id.      
-0000cc60: 2020 2020 6966 206b 6579 2069 6e20 6361      if key in ca
-0000cc70: 6368 653a 0a20 2020 2020 2020 2020 2020  che:.           
-0000cc80: 2072 6574 7572 6e20 6361 6368 655b 6b65   return cache[ke
-0000cc90: 795d 0a20 2020 2020 2020 2020 2065 6c73  y].          els
-0000cca0: 653a 0a20 2020 2020 2020 2020 2020 2069  e:.            i
-0000ccb0: 6620 5f72 6573 6574 5f6e 616d 6573 3a0a  f _reset_names:.
-0000ccc0: 2020 2020 2020 2020 2020 2020 2020 636c                cl
-0000ccd0: 6f6e 6520 3d20 6d2e 636c 6f6e 6528 0a20  one = m.clone(. 
-0000cce0: 2020 2020 2020 2020 2020 2020 2020 205f                 _
-0000ccf0: 6465 6570 5f63 6c6f 6e65 3d63 6163 6865  deep_clone=cache
-0000cd00: 2c20 5f72 6573 6574 5f6e 616d 6573 3d5f  , _reset_names=_
-0000cd10: 7265 7365 745f 6e61 6d65 732c 206e 616d  reset_names, nam
-0000cd20: 653d 4e6f 6e65 0a20 2020 2020 2020 2020  e=None.         
-0000cd30: 2020 2020 2029 0a20 2020 2020 2020 2020       ).         
-0000cd40: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       
-0000cd50: 2020 2020 2020 2063 6c6f 6e65 203d 206d         clone = m
-0000cd60: 2e63 6c6f 6e65 285f 6465 6570 5f63 6c6f  .clone(_deep_clo
-0000cd70: 6e65 3d63 6163 6865 290a 2020 2020 2020  ne=cache).      
-0000cd80: 2020 2020 2020 6361 6368 655b 6b65 795d        cache[key]
-0000cd90: 203d 2063 6c6f 6e65 0a20 2020 2020 2020   = clone.       
-0000cda0: 2020 2020 2072 6574 7572 6e20 636c 6f6e       return clon
-0000cdb0: 650a 2020 2020 2020 2020 656c 7365 3a0a  e.        else:.
-0000cdc0: 2020 2020 2020 2020 2020 2320 4966 2074            # If t
-0000cdd0: 6865 206d 6f64 756c 6520 646f 6573 6e27  he module doesn'
-0000cde0: 7420 6861 7665 2061 6e20 5f69 6420 6174  t have an _id at
-0000cdf0: 7472 6962 7574 6520 6974 2063 6f75 6c64  tribute it could
-0000ce00: 2062 6520 6120 6d6f 636b 206f 626a 6563   be a mock objec
-0000ce10: 740a 2020 2020 2020 2020 2020 2320 736f  t.          # so
-0000ce20: 2077 6520 7265 7475 726e 2069 7420 6173   we return it as
-0000ce30: 2069 732e 0a20 2020 2020 2020 2020 2072   is..          r
-0000ce40: 6574 7572 6e20 6d0a 0a20 2020 2020 2023  eturn m..      #
-0000ce50: 205f 6d61 705f 7375 626d 6f64 756c 6573   _map_submodules
-0000ce60: 2077 696c 6c20 6d61 7020 6f76 6572 2061   will map over a
-0000ce70: 6c6c 2073 7562 6d6f 6475 6c65 7320 696e  ll submodules in
-0000ce80: 7369 6465 2061 7474 7273 0a20 2020 2020  side attrs.     
-0000ce90: 2023 2076 616c 7565 2068 6572 6520 6361   # value here ca
-0000cea0: 6e20 6265 2061 6e79 2070 7974 7265 652c  n be any pytree,
-0000ceb0: 206e 6f6e 2d6d 6f64 756c 6520 7661 6c75   non-module valu
-0000cec0: 6573 2061 7265 2069 676e 6f72 6564 0a20  es are ignored. 
-0000ced0: 2020 2020 2066 6f72 2066 6965 6c64 5f6e       for field_n
-0000cee0: 616d 652c 2076 616c 7565 2069 6e20 6174  ame, value in at
-0000cef0: 7472 732e 6974 656d 7328 293a 0a20 2020  trs.items():.   
-0000cf00: 2020 2020 2069 6620 6669 656c 645f 6e61       if field_na
-0000cf10: 6d65 203d 3d20 2770 6172 656e 7427 3a0a  me == 'parent':.
-0000cf20: 2020 2020 2020 2020 2020 636f 6e74 696e            contin
-0000cf30: 7565 0a20 2020 2020 2020 2061 7474 7273  ue.        attrs
-0000cf40: 5b66 6965 6c64 5f6e 616d 655d 203d 205f  [field_name] = _
-0000cf50: 6d61 705f 7375 626d 6f64 756c 6573 2863  map_submodules(c
-0000cf60: 6c6f 6e65 5f66 6e2c 2076 616c 7565 290a  lone_fn, value).
-0000cf70: 0a20 2020 206d 6f64 756c 6520 3d20 7365  .    module = se
-0000cf80: 6c66 2e5f 5f63 6c61 7373 5f5f 282a 2a61  lf.__class__(**a
-0000cf90: 7474 7273 290a 0a20 2020 2072 6574 7572  ttrs)..    retur
-0000cfa0: 6e20 6d6f 6475 6c65 0a0a 2020 6465 6620  n module..  def 
-0000cfb0: 636f 7079 280a 2020 2020 7365 6c66 3a20  copy(.    self: 
-0000cfc0: 4d2c 0a20 2020 202a 2c0a 2020 2020 7061  M,.    *,.    pa
-0000cfd0: 7265 6e74 3a20 4f70 7469 6f6e 616c 5b55  rent: Optional[U
-0000cfe0: 6e69 6f6e 5b53 636f 7065 2c20 274d 6f64  nion[Scope, 'Mod
-0000cff0: 756c 6527 2c20 5f53 656e 7469 6e65 6c5d  ule', _Sentinel]
-0000d000: 5d20 3d20 5f75 6e73 7065 6369 6669 6564  ] = _unspecified
-0000d010: 5f70 6172 656e 742c 0a20 2020 206e 616d  _parent,.    nam
-0000d020: 653a 204f 7074 696f 6e61 6c5b 7374 725d  e: Optional[str]
-0000d030: 203d 204e 6f6e 652c 0a20 2020 202a 2a75   = None,.    **u
-0000d040: 7064 6174 6573 2c0a 2020 2920 2d3e 204d  pdates,.  ) -> M
-0000d050: 3a0a 2020 2020 2222 2243 7265 6174 6573  :.    """Creates
-0000d060: 2061 2063 6f70 7920 6f66 2074 6869 7320   a copy of this 
-0000d070: 4d6f 6475 6c65 2c20 7769 7468 206f 7074  Module, with opt
-0000d080: 696f 6e61 6c6c 7920 7570 6461 7465 6420  ionally updated 
-0000d090: 6172 6775 6d65 6e74 732e 0a0a 2020 2020  arguments...    
-0000d0a0: 4172 6773 3a0a 2020 2020 2020 7061 7265  Args:.      pare
-0000d0b0: 6e74 3a20 5468 6520 7061 7265 6e74 206f  nt: The parent o
-0000d0c0: 6620 7468 6520 636f 7079 2e20 2042 7920  f the copy.  By 
-0000d0d0: 6465 6661 756c 7420 7468 6520 6375 7272  default the curr
-0000d0e0: 656e 7420 6d6f 6475 6c65 2069 7320 7461  ent module is ta
-0000d0f0: 6b65 6e0a 2020 2020 2020 2020 6173 2070  ken.        as p
-0000d100: 6172 656e 7420 6966 206e 6f74 2065 7870  arent if not exp
-0000d110: 6c69 6369 746c 7920 7370 6563 6966 6965  licitly specifie
-0000d120: 642e 0a20 2020 2020 206e 616d 653a 2041  d..      name: A
-0000d130: 206e 6577 206e 616d 6520 666f 7220 7468   new name for th
-0000d140: 6520 636f 7069 6564 204d 6f64 756c 652c  e copied Module,
-0000d150: 2062 7920 6465 6661 756c 7420 6120 6e65   by default a ne
-0000d160: 7720 6175 746f 6d61 7469 6320 6e61 6d65  w automatic name
-0000d170: 0a20 2020 2020 2020 2077 696c 6c20 6265  .        will be
-0000d180: 2067 6976 656e 2e0a 2020 2020 2020 2a2a   given..      **
-0000d190: 7570 6461 7465 733a 2041 7474 7269 6275  updates: Attribu
-0000d1a0: 7465 2075 7064 6174 6573 2e0a 0a20 2020  te updates...   
-0000d1b0: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
-0000d1c0: 4120 636f 7079 206f 6620 7468 6520 7468  A copy of the th
-0000d1d0: 6973 204d 6f64 756c 6520 7769 7468 2074  is Module with t
-0000d1e0: 6865 2075 7064 6174 6564 206e 616d 652c  he updated name,
-0000d1f0: 2070 6172 656e 742c 2061 6e64 2061 7474   parent, and att
-0000d200: 7269 6275 7465 732e 0a20 2020 2022 2222  ributes..    """
-0000d210: 0a20 2020 2072 6574 7572 6e20 7365 6c66  .    return self
-0000d220: 2e63 6c6f 6e65 280a 2020 2020 2020 7061  .clone(.      pa
-0000d230: 7265 6e74 3d70 6172 656e 742c 206e 616d  rent=parent, nam
-0000d240: 653d 6e61 6d65 2c20 5f64 6565 705f 636c  e=name, _deep_cl
-0000d250: 6f6e 653d 5472 7565 2c20 5f72 6573 6574  one=True, _reset
-0000d260: 5f6e 616d 6573 3d46 616c 7365 2c20 2a2a  _names=False, **
-0000d270: 7570 6461 7465 730a 2020 2020 290a 0a20  updates.    ).. 
-0000d280: 2040 6f76 6572 6c6f 6164 0a20 2064 6566   @overload.  def
-0000d290: 2076 6172 6961 626c 6528 0a20 2020 2073   variable(.    s
-0000d2a0: 656c 662c 0a20 2020 2063 6f6c 3a20 7374  elf,.    col: st
-0000d2b0: 722c 0a20 2020 206e 616d 653a 2073 7472  r,.    name: str
-0000d2c0: 2c0a 2020 2020 696e 6974 5f66 6e3a 204f  ,.    init_fn: O
-0000d2d0: 7074 696f 6e61 6c5b 4361 6c6c 6162 6c65  ptional[Callable
-0000d2e0: 5b2e 2e2e 2c20 545d 5d20 3d20 4e6f 6e65  [..., T]] = None
-0000d2f0: 2c0a 2020 2020 2a69 6e69 745f 6172 6773  ,.    *init_args
-0000d300: 2c0a 2020 2920 2d3e 2056 6172 6961 626c  ,.  ) -> Variabl
-0000d310: 655b 545d 3a0a 2020 2020 2e2e 2e0a 0a20  e[T]:.    ..... 
-0000d320: 2040 6f76 6572 6c6f 6164 0a20 2064 6566   @overload.  def
-0000d330: 2076 6172 6961 626c 6528 0a20 2020 2073   variable(.    s
-0000d340: 656c 662c 0a20 2020 2063 6f6c 3a20 7374  elf,.    col: st
-0000d350: 722c 0a20 2020 206e 616d 653a 2073 7472  r,.    name: str
-0000d360: 2c0a 2020 2020 696e 6974 5f66 6e3a 204f  ,.    init_fn: O
-0000d370: 7074 696f 6e61 6c5b 4361 6c6c 6162 6c65  ptional[Callable
-0000d380: 5b2e 2e2e 2c20 545d 5d20 3d20 4e6f 6e65  [..., T]] = None
-0000d390: 2c0a 2020 2020 2a69 6e69 745f 6172 6773  ,.    *init_args
-0000d3a0: 2c0a 2020 2020 756e 626f 783a 204c 6974  ,.    unbox: Lit
-0000d3b0: 6572 616c 5b54 7275 655d 2c0a 2020 2020  eral[True],.    
-0000d3c0: 2a2a 696e 6974 5f6b 7761 7267 732c 0a20  **init_kwargs,. 
-0000d3d0: 2029 202d 3e20 5661 7269 6162 6c65 5b54   ) -> Variable[T
-0000d3e0: 5d3a 0a20 2020 202e 2e2e 0a0a 2020 406f  ]:.    .....  @o
-0000d3f0: 7665 726c 6f61 640a 2020 6465 6620 7661  verload.  def va
-0000d400: 7269 6162 6c65 280a 2020 2020 7365 6c66  riable(.    self
-0000d410: 2c0a 2020 2020 636f 6c3a 2073 7472 2c0a  ,.    col: str,.
-0000d420: 2020 2020 6e61 6d65 3a20 7374 722c 0a20      name: str,. 
-0000d430: 2020 2069 6e69 745f 666e 3a20 4f70 7469     init_fn: Opti
-0000d440: 6f6e 616c 5b43 616c 6c61 626c 655b 2e2e  onal[Callable[..
-0000d450: 2e2c 2054 5d5d 203d 204e 6f6e 652c 0a20  ., T]] = None,. 
-0000d460: 2020 202a 696e 6974 5f61 7267 732c 0a20     *init_args,. 
-0000d470: 2020 2075 6e62 6f78 3a20 4c69 7465 7261     unbox: Litera
-0000d480: 6c5b 4661 6c73 655d 2c0a 2020 2020 2a2a  l[False],.    **
-0000d490: 696e 6974 5f6b 7761 7267 732c 0a20 2029  init_kwargs,.  )
-0000d4a0: 202d 3e20 5661 7269 6162 6c65 5b6d 6574   -> Variable[met
-0000d4b0: 612e 4178 6973 4d65 7461 6461 7461 5b54  a.AxisMetadata[T
-0000d4c0: 5d5d 3a0a 2020 2020 2e2e 2e0a 0a20 2040  ]]:.    .....  @
-0000d4d0: 6f76 6572 6c6f 6164 0a20 2064 6566 2076  overload.  def v
-0000d4e0: 6172 6961 626c 6528 0a20 2020 2073 656c  ariable(.    sel
-0000d4f0: 662c 0a20 2020 2063 6f6c 3a20 7374 722c  f,.    col: str,
-0000d500: 0a20 2020 206e 616d 653a 2073 7472 2c0a  .    name: str,.
-0000d510: 2020 2020 696e 6974 5f66 6e3a 204f 7074      init_fn: Opt
-0000d520: 696f 6e61 6c5b 4361 6c6c 6162 6c65 5b2e  ional[Callable[.
-0000d530: 2e2e 2c20 545d 5d20 3d20 4e6f 6e65 2c0a  .., T]] = None,.
-0000d540: 2020 2020 2a69 6e69 745f 6172 6773 2c0a      *init_args,.
-0000d550: 2020 2020 756e 626f 783a 2062 6f6f 6c20      unbox: bool 
-0000d560: 3d20 5472 7565 2c0a 2020 2020 2a2a 696e  = True,.    **in
-0000d570: 6974 5f6b 7761 7267 732c 0a20 2029 202d  it_kwargs,.  ) -
-0000d580: 3e20 556e 696f 6e5b 5661 7269 6162 6c65  > Union[Variable
-0000d590: 5b54 5d2c 2056 6172 6961 626c 655b 6d65  [T], Variable[me
-0000d5a0: 7461 2e41 7869 734d 6574 6164 6174 615b  ta.AxisMetadata[
-0000d5b0: 545d 5d5d 3a0a 2020 2020 2e2e 2e0a 0a20  T]]]:.    ..... 
-0000d5c0: 2064 6566 2076 6172 6961 626c 6528 0a20   def variable(. 
-0000d5d0: 2020 2073 656c 662c 0a20 2020 2063 6f6c     self,.    col
-0000d5e0: 3a20 7374 722c 0a20 2020 206e 616d 653a  : str,.    name:
-0000d5f0: 2073 7472 2c0a 2020 2020 696e 6974 5f66   str,.    init_f
-0000d600: 6e3a 204f 7074 696f 6e61 6c5b 4361 6c6c  n: Optional[Call
-0000d610: 6162 6c65 5b2e 2e2e 2c20 545d 5d20 3d20  able[..., T]] = 
-0000d620: 4e6f 6e65 2c0a 2020 2020 2a69 6e69 745f  None,.    *init_
-0000d630: 6172 6773 2c0a 2020 2020 756e 626f 783a  args,.    unbox:
-0000d640: 2062 6f6f 6c20 3d20 5472 7565 2c0a 2020   bool = True,.  
-0000d650: 2020 2a2a 696e 6974 5f6b 7761 7267 732c    **init_kwargs,
-0000d660: 0a20 2029 202d 3e20 556e 696f 6e5b 5661  .  ) -> Union[Va
-0000d670: 7269 6162 6c65 5b54 5d2c 2056 6172 6961  riable[T], Varia
-0000d680: 626c 655b 6d65 7461 2e41 7869 734d 6574  ble[meta.AxisMet
-0000d690: 6164 6174 615b 545d 5d5d 3a0a 2020 2020  adata[T]]]:.    
-0000d6a0: 2222 2244 6563 6c61 7265 7320 616e 6420  """Declares and 
-0000d6b0: 7265 7475 726e 7320 6120 7661 7269 6162  returns a variab
-0000d6c0: 6c65 2069 6e20 7468 6973 204d 6f64 756c  le in this Modul
-0000d6d0: 652e 0a0a 2020 2020 5365 6520 3a6d 6f64  e...    See :mod
-0000d6e0: 3a60 666c 6178 2e63 6f72 652e 7661 7269  :`flax.core.vari
-0000d6f0: 6162 6c65 7360 2066 6f72 206d 6f72 6520  ables` for more 
-0000d700: 696e 666f 726d 6174 696f 6e2e 2053 6565  information. See
-0000d710: 2061 6c73 6f20 3a6d 6574 683a 6070 6172   also :meth:`par
-0000d720: 616d 600a 2020 2020 666f 7220 6120 7368  am`.    for a sh
-0000d730: 6f72 7468 616e 6420 7761 7920 746f 2064  orthand way to d
-0000d740: 6566 696e 6520 7265 6164 2d6f 6e6c 7920  efine read-only 
-0000d750: 7661 7269 6162 6c65 7320 696e 2074 6865  variables in the
-0000d760: 2022 7061 7261 6d73 220a 2020 2020 636f   "params".    co
-0000d770: 6c6c 6563 7469 6f6e 2e0a 0a20 2020 2043  llection...    C
-0000d780: 6f6e 7472 6172 7920 746f 203a 6d65 7468  ontrary to :meth
-0000d790: 3a60 7061 7261 6d60 2c20 616c 6c20 6172  :`param`, all ar
-0000d7a0: 6775 6d65 6e74 7320 7061 7373 696e 6720  guments passing 
-0000d7b0: 7573 696e 6720 6060 696e 6974 5f66 6e60  using ``init_fn`
-0000d7c0: 6020 7368 6f75 6c64 2062 650a 2020 2020  ` should be.    
-0000d7d0: 7061 7373 6564 206f 6e20 6578 706c 6963  passed on explic
-0000d7e0: 6974 6c79 3a3a 0a0a 2020 2020 2020 3e3e  itly::..      >>
-0000d7f0: 3e20 636c 6173 7320 466f 6f28 6e6e 2e4d  > class Foo(nn.M
-0000d800: 6f64 756c 6529 3a0a 2020 2020 2020 2e2e  odule):.      ..
-0000d810: 2e20 2020 406e 6e2e 636f 6d70 6163 740a  .   @nn.compact.
-0000d820: 2020 2020 2020 2e2e 2e20 2020 6465 6620        ...   def 
-0000d830: 5f5f 6361 6c6c 5f5f 2873 656c 662c 2078  __call__(self, x
-0000d840: 293a 0a20 2020 2020 202e 2e2e 2020 2020  ):.      ...    
-0000d850: 2078 203d 206e 6e2e 4465 6e73 6528 3429   x = nn.Dense(4)
-0000d860: 2878 290a 2020 2020 2020 2e2e 2e20 2020  (x).      ...   
-0000d870: 2020 6b65 7920 3d20 7365 6c66 2e6d 616b    key = self.mak
-0000d880: 655f 726e 6728 2773 7461 7473 2729 0a20  e_rng('stats'). 
-0000d890: 2020 2020 202e 2e2e 2020 2020 206d 6561       ...     mea
-0000d8a0: 6e20 3d20 7365 6c66 2e76 6172 6961 626c  n = self.variabl
-0000d8b0: 6528 2773 7461 7473 272c 2027 6d65 616e  e('stats', 'mean
-0000d8c0: 272c 206e 6e2e 696e 6974 6961 6c69 7a65  ', nn.initialize
-0000d8d0: 7273 2e6c 6563 756e 5f6e 6f72 6d61 6c28  rs.lecun_normal(
-0000d8e0: 292c 206b 6579 2c20 782e 7368 6170 6529  ), key, x.shape)
-0000d8f0: 0a20 2020 2020 202e 2e2e 2020 2020 202e  .      ...     .
-0000d900: 2e2e 0a20 2020 2020 202e 2e2e 2020 2020  ...      ...    
-0000d910: 2072 6574 7572 6e20 7820 2a20 6d65 616e   return x * mean
-0000d920: 2e76 616c 7565 0a20 2020 2020 203e 3e3e  .value.      >>>
-0000d930: 2076 6172 6961 626c 6573 203d 2046 6f6f   variables = Foo
-0000d940: 2829 2e69 6e69 7428 7b27 7061 7261 6d73  ().init({'params
-0000d950: 273a 206a 6178 2e72 616e 646f 6d2e 6b65  ': jax.random.ke
-0000d960: 7928 3029 2c20 2773 7461 7473 273a 206a  y(0), 'stats': j
-0000d970: 6178 2e72 616e 646f 6d2e 6b65 7928 3129  ax.random.key(1)
-0000d980: 7d2c 206a 6e70 2e6f 6e65 7328 2832 2c20  }, jnp.ones((2, 
-0000d990: 3329 2929 0a20 2020 2020 203e 3e3e 206a  3))).      >>> j
-0000d9a0: 6178 2e74 7265 655f 7574 696c 2e74 7265  ax.tree_util.tre
-0000d9b0: 655f 6d61 7028 6a6e 702e 7368 6170 652c  e_map(jnp.shape,
-0000d9c0: 2076 6172 6961 626c 6573 290a 2020 2020   variables).    
-0000d9d0: 2020 7b27 7061 7261 6d73 273a 207b 2744    {'params': {'D
-0000d9e0: 656e 7365 5f30 273a 207b 2762 6961 7327  ense_0': {'bias'
-0000d9f0: 3a20 2834 2c29 2c20 276b 6572 6e65 6c27  : (4,), 'kernel'
-0000da00: 3a20 2833 2c20 3429 7d7d 2c20 2773 7461  : (3, 4)}}, 'sta
-0000da10: 7473 273a 207b 276d 6561 6e27 3a20 2832  ts': {'mean': (2
-0000da20: 2c20 3429 7d7d 0a0a 2020 2020 496e 2074  , 4)}}..    In t
-0000da30: 6865 2065 7861 6d70 6c65 2061 626f 7665  he example above
-0000da40: 2c20 7468 6520 6675 6e63 7469 6f6e 2060  , the function `
-0000da50: 606c 6563 756e 5f6e 6f72 6d61 6c60 6020  `lecun_normal`` 
-0000da60: 6578 7065 6374 7320 7477 6f20 6172 6775  expects two argu
-0000da70: 6d65 6e74 733a 0a20 2020 2060 606b 6579  ments:.    ``key
-0000da80: 6060 2061 6e64 2060 6073 6861 7065 6060  `` and ``shape``
-0000da90: 2c20 616e 6420 626f 7468 2068 6176 6520  , and both have 
-0000daa0: 746f 2062 6520 7061 7373 6564 206f 6e2e  to be passed on.
-0000dab0: 2054 6865 2050 524e 4720 666f 7220 6060   The PRNG for ``
-0000dac0: 7374 6174 7360 600a 2020 2020 6861 7320  stats``.    has 
-0000dad0: 746f 2062 6520 7072 6f76 6964 6564 2065  to be provided e
-0000dae0: 7870 6c69 6369 746c 7920 7768 656e 2063  xplicitly when c
-0000daf0: 616c 6c69 6e67 203a 6d65 7468 3a60 696e  alling :meth:`in
-0000db00: 6974 6020 616e 6420 3a6d 6574 683a 6061  it` and :meth:`a
-0000db10: 7070 6c79 602e 0a0a 2020 2020 4172 6773  pply`...    Args
-0000db20: 3a0a 2020 2020 2020 636f 6c3a 2054 6865  :.      col: The
-0000db30: 2076 6172 6961 626c 6520 636f 6c6c 6563   variable collec
-0000db40: 7469 6f6e 206e 616d 652e 0a20 2020 2020  tion name..     
-0000db50: 206e 616d 653a 2054 6865 2076 6172 6961   name: The varia
-0000db60: 626c 6520 6e61 6d65 2e0a 2020 2020 2020  ble name..      
-0000db70: 696e 6974 5f66 6e3a 2054 6865 2066 756e  init_fn: The fun
-0000db80: 6374 696f 6e20 7468 6174 2077 696c 6c20  ction that will 
-0000db90: 6265 2063 616c 6c65 6420 746f 2063 6f6d  be called to com
-0000dba0: 7075 7465 2074 6865 2069 6e69 7469 616c  pute the initial
-0000dbb0: 2076 616c 7565 206f 660a 2020 2020 2020   value of.      
-0000dbc0: 2020 7468 6973 2076 6172 6961 626c 652e    this variable.
-0000dbd0: 2054 6869 7320 6675 6e63 7469 6f6e 2077   This function w
-0000dbe0: 696c 6c20 6f6e 6c79 2062 6520 6361 6c6c  ill only be call
-0000dbf0: 6564 2074 6865 2066 6972 7374 2074 696d  ed the first tim
-0000dc00: 6520 7468 6973 0a20 2020 2020 2020 2076  e this.        v
-0000dc10: 6172 6961 626c 6520 6973 2075 7365 6420  ariable is used 
-0000dc20: 696e 2074 6869 7320 6d6f 6475 6c65 2e20  in this module. 
-0000dc30: 4966 204e 6f6e 652c 2074 6865 2076 6172  If None, the var
-0000dc40: 6961 626c 6520 6d75 7374 2061 6c72 6561  iable must alrea
-0000dc50: 6479 2062 650a 2020 2020 2020 2020 696e  dy be.        in
-0000dc60: 6974 6961 6c69 7a65 6420 6f74 6865 7277  itialized otherw
-0000dc70: 6973 6520 616e 2065 7272 6f72 2069 7320  ise an error is 
-0000dc80: 7261 6973 6564 2e0a 2020 2020 2020 2a69  raised..      *i
-0000dc90: 6e69 745f 6172 6773 3a20 5468 6520 706f  nit_args: The po
-0000dca0: 7369 7469 6f6e 616c 2061 7267 756d 656e  sitional argumen
-0000dcb0: 7473 2074 6f20 7061 7373 2074 6f20 696e  ts to pass to in
-0000dcc0: 6974 5f66 6e2e 0a20 2020 2020 2075 6e62  it_fn..      unb
-0000dcd0: 6f78 3a20 4966 2054 7275 652c 2060 6041  ox: If True, ``A
-0000dce0: 7869 734d 6574 6164 6174 6160 6020 696e  xisMetadata`` in
-0000dcf0: 7374 616e 6365 7320 6172 6520 7265 706c  stances are repl
-0000dd00: 6163 6564 2062 7920 7468 6569 7220 756e  aced by their un
-0000dd10: 626f 7865 640a 2020 2020 2020 2020 7661  boxed.        va
-0000dd20: 6c75 652c 2073 6565 2060 6066 6c61 782e  lue, see ``flax.
-0000dd30: 6e6e 2e6d 6574 612e 756e 626f 7860 6020  nn.meta.unbox`` 
-0000dd40: 2864 6566 6175 6c74 3a20 5472 7565 292e  (default: True).
-0000dd50: 0a20 2020 2020 202a 2a69 6e69 745f 6b77  .      **init_kw
-0000dd60: 6172 6773 3a20 5468 6520 6b65 792d 776f  args: The key-wo
-0000dd70: 7264 2061 7267 756d 656e 7473 2074 6f20  rd arguments to 
-0000dd80: 7061 7373 2074 6f20 696e 6974 5f66 6e0a  pass to init_fn.
-0000dd90: 0a20 2020 2052 6574 7572 6e73 3a0a 2020  .    Returns:.  
-0000dda0: 2020 2020 4120 3a63 6c61 7373 3a60 666c      A :class:`fl
-0000ddb0: 6178 2e63 6f72 652e 7661 7269 6162 6c65  ax.core.variable
-0000ddc0: 732e 5661 7269 6162 6c65 6020 7468 6174  s.Variable` that
-0000ddd0: 2063 616e 2062 6520 7265 6164 206f 7220   can be read or 
-0000dde0: 7365 7420 7669 610a 2020 2020 2020 222e  set via.      ".
-0000ddf0: 7661 6c75 6522 2061 7474 7269 6275 7465  value" attribute
-0000de00: 2e20 5468 726f 7773 2061 6e20 6572 726f  . Throws an erro
-0000de10: 7220 6966 2074 6865 2076 6172 6961 626c  r if the variabl
-0000de20: 6520 6578 6973 7473 2061 6c72 6561 6479  e exists already
-0000de30: 2e0a 2020 2020 2222 220a 2020 2020 6966  ..    """.    if
-0000de40: 206e 6f74 2073 656c 662e 5f69 6e69 7469   not self._initi
-0000de50: 616c 697a 6174 696f 6e5f 616c 6c6f 7765  alization_allowe
-0000de60: 643a 0a20 2020 2020 2072 6169 7365 2056  d:.      raise V
-0000de70: 616c 7565 4572 726f 7228 0a20 2020 2020  alueError(.     
-0000de80: 2020 2027 5661 7269 6162 6c65 7320 6d75     'Variables mu
-0000de90: 7374 2062 6520 696e 6974 6961 6c69 7a65  st be initialize
-0000dea0: 6420 696e 2060 7365 7475 7028 2960 206f  d in `setup()` o
-0000deb0: 7220 696e 2061 206d 6574 686f 6420 270a  r in a method '.
-0000dec0: 2020 2020 2020 2020 2777 7261 7070 6564          'wrapped
-0000ded0: 2069 6e20 6040 636f 6d70 6163 7460 270a   in `@compact`'.
-0000dee0: 2020 2020 2020 290a 2020 2020 6966 2073        ).    if s
-0000def0: 656c 662e 5f6e 616d 655f 7461 6b65 6e28  elf._name_taken(
-0000df00: 6e61 6d65 2c20 636f 6c6c 6563 7469 6f6e  name, collection
-0000df10: 3d63 6f6c 293a 0a20 2020 2020 2072 6169  =col):.      rai
-0000df20: 7365 2065 7272 6f72 732e 4e61 6d65 496e  se errors.NameIn
-0000df30: 5573 6545 7272 6f72 2827 7661 7269 6162  UseError('variab
-0000df40: 6c65 272c 206e 616d 652c 2073 656c 662e  le', name, self.
-0000df50: 5f5f 636c 6173 735f 5f2e 5f5f 6e61 6d65  __class__.__name
-0000df60: 5f5f 290a 2020 2020 6173 7365 7274 2073  __).    assert s
-0000df70: 656c 662e 7363 6f70 6520 6973 206e 6f74  elf.scope is not
-0000df80: 204e 6f6e 650a 2020 2020 7620 3d20 7365   None.    v = se
-0000df90: 6c66 2e73 636f 7065 2e76 6172 6961 626c  lf.scope.variabl
-0000dfa0: 6528 0a20 2020 2020 2063 6f6c 2c20 6e61  e(.      col, na
-0000dfb0: 6d65 2c20 696e 6974 5f66 6e2c 202a 696e  me, init_fn, *in
-0000dfc0: 6974 5f61 7267 732c 2075 6e62 6f78 3d75  it_args, unbox=u
-0000dfd0: 6e62 6f78 2c20 2a2a 696e 6974 5f6b 7761  nbox, **init_kwa
-0000dfe0: 7267 730a 2020 2020 290a 2020 2020 7365  rgs.    ).    se
-0000dff0: 6c66 2e5f 7374 6174 652e 6368 696c 6472  lf._state.childr
-0000e000: 656e 5b6e 616d 655d 203d 2063 6f6c 0a20  en[name] = col. 
-0000e010: 2020 2072 6574 7572 6e20 760a 0a20 2040     return v..  @
-0000e020: 6f76 6572 6c6f 6164 0a20 2064 6566 2070  overload.  def p
-0000e030: 6172 616d 280a 2020 2020 7365 6c66 2c20  aram(.    self, 
-0000e040: 6e61 6d65 3a20 7374 722c 2069 6e69 745f  name: str, init_
-0000e050: 666e 3a20 4361 6c6c 6162 6c65 5b2e 2e2e  fn: Callable[...
-0000e060: 2c20 545d 2c20 2a69 6e69 745f 6172 6773  , T], *init_args
-0000e070: 2c0a 2020 2920 2d3e 2054 3a0a 2020 2020  ,.  ) -> T:.    
-0000e080: 2e2e 2e0a 0a20 2040 6f76 6572 6c6f 6164  .....  @overload
-0000e090: 0a20 2064 6566 2070 6172 616d 280a 2020  .  def param(.  
-0000e0a0: 2020 7365 6c66 2c0a 2020 2020 6e61 6d65    self,.    name
-0000e0b0: 3a20 7374 722c 0a20 2020 2069 6e69 745f  : str,.    init_
-0000e0c0: 666e 3a20 4361 6c6c 6162 6c65 5b2e 2e2e  fn: Callable[...
-0000e0d0: 2c20 545d 2c0a 2020 2020 2a69 6e69 745f  , T],.    *init_
-0000e0e0: 6172 6773 2c0a 2020 2020 756e 626f 783a  args,.    unbox:
-0000e0f0: 204c 6974 6572 616c 5b54 7275 655d 2c0a   Literal[True],.
-0000e100: 2020 2020 2a2a 696e 6974 5f6b 7761 7267      **init_kwarg
-0000e110: 732c 0a20 2029 202d 3e20 543a 0a20 2020  s,.  ) -> T:.   
-0000e120: 202e 2e2e 0a0a 2020 406f 7665 726c 6f61   .....  @overloa
-0000e130: 640a 2020 6465 6620 7061 7261 6d28 0a20  d.  def param(. 
-0000e140: 2020 2073 656c 662c 0a20 2020 206e 616d     self,.    nam
-0000e150: 653a 2073 7472 2c0a 2020 2020 696e 6974  e: str,.    init
-0000e160: 5f66 6e3a 2043 616c 6c61 626c 655b 2e2e  _fn: Callable[..
-0000e170: 2e2c 2054 5d2c 0a20 2020 202a 696e 6974  ., T],.    *init
-0000e180: 5f61 7267 732c 0a20 2020 2075 6e62 6f78  _args,.    unbox
-0000e190: 3a20 4c69 7465 7261 6c5b 4661 6c73 655d  : Literal[False]
-0000e1a0: 2c0a 2020 2020 2a2a 696e 6974 5f6b 7761  ,.    **init_kwa
-0000e1b0: 7267 732c 0a20 2029 202d 3e20 6d65 7461  rgs,.  ) -> meta
-0000e1c0: 2e41 7869 734d 6574 6164 6174 615b 545d  .AxisMetadata[T]
-0000e1d0: 3a0a 2020 2020 2e2e 2e0a 0a20 2040 6f76  :.    .....  @ov
-0000e1e0: 6572 6c6f 6164 0a20 2064 6566 2070 6172  erload.  def par
-0000e1f0: 616d 280a 2020 2020 7365 6c66 2c0a 2020  am(.    self,.  
-0000e200: 2020 6e61 6d65 3a20 7374 722c 0a20 2020    name: str,.   
-0000e210: 2069 6e69 745f 666e 3a20 4361 6c6c 6162   init_fn: Callab
-0000e220: 6c65 5b2e 2e2e 2c20 545d 2c0a 2020 2020  le[..., T],.    
-0000e230: 2a69 6e69 745f 6172 6773 2c0a 2020 2020  *init_args,.    
-0000e240: 756e 626f 783a 2062 6f6f 6c2c 0a20 2020  unbox: bool,.   
-0000e250: 202a 2a69 6e69 745f 6b77 6172 6773 2c0a   **init_kwargs,.
-0000e260: 2020 2920 2d3e 2055 6e69 6f6e 5b54 2c20    ) -> Union[T, 
-0000e270: 6d65 7461 2e41 7869 734d 6574 6164 6174  meta.AxisMetadat
-0000e280: 615b 545d 5d3a 0a20 2020 202e 2e2e 0a0a  a[T]]:.    .....
-0000e290: 2020 6465 6620 7061 7261 6d28 0a20 2020    def param(.   
-0000e2a0: 2073 656c 662c 0a20 2020 206e 616d 653a   self,.    name:
-0000e2b0: 2073 7472 2c0a 2020 2020 696e 6974 5f66   str,.    init_f
-0000e2c0: 6e3a 2043 616c 6c61 626c 655b 2e2e 2e2c  n: Callable[...,
-0000e2d0: 2054 5d2c 0a20 2020 202a 696e 6974 5f61   T],.    *init_a
-0000e2e0: 7267 732c 0a20 2020 2075 6e62 6f78 3a20  rgs,.    unbox: 
-0000e2f0: 626f 6f6c 203d 2054 7275 652c 0a20 2020  bool = True,.   
-0000e300: 202a 2a69 6e69 745f 6b77 6172 6773 2c0a   **init_kwargs,.
-0000e310: 2020 2920 2d3e 2055 6e69 6f6e 5b54 2c20    ) -> Union[T, 
-0000e320: 6d65 7461 2e41 7869 734d 6574 6164 6174  meta.AxisMetadat
-0000e330: 615b 545d 5d3a 0a20 2020 2022 2222 4465  a[T]]:.    """De
-0000e340: 636c 6172 6573 2061 6e64 2072 6574 7572  clares and retur
-0000e350: 6e73 2061 2070 6172 616d 6574 6572 2069  ns a parameter i
-0000e360: 6e20 7468 6973 204d 6f64 756c 652e 0a0a  n this Module...
-0000e370: 2020 2020 5061 7261 6d65 7465 7273 2061      Parameters a
-0000e380: 7265 2072 6561 642d 6f6e 6c79 2076 6172  re read-only var
-0000e390: 6961 626c 6573 2069 6e20 7468 6520 636f  iables in the co
-0000e3a0: 6c6c 6563 7469 6f6e 206e 616d 6564 2022  llection named "
-0000e3b0: 7061 7261 6d73 222e 2053 6565 0a20 2020  params". See.   
-0000e3c0: 203a 6d6f 643a 6066 6c61 782e 636f 7265   :mod:`flax.core
-0000e3d0: 2e76 6172 6961 626c 6573 6020 666f 7220  .variables` for 
-0000e3e0: 6d6f 7265 2064 6574 6169 6c73 206f 6e20  more details on 
-0000e3f0: 7661 7269 6162 6c65 732e 0a0a 2020 2020  variables...    
-0000e400: 5468 6520 6669 7273 7420 6172 6775 6d65  The first argume
-0000e410: 6e74 206f 6620 6060 696e 6974 5f66 6e60  nt of ``init_fn`
-0000e420: 6020 6973 2061 7373 756d 6564 2074 6f20  ` is assumed to 
-0000e430: 6265 2061 2050 524e 4720 6b65 792c 2077  be a PRNG key, w
-0000e440: 6869 6368 2069 730a 2020 2020 7072 6f76  hich is.    prov
-0000e450: 6964 6564 2061 7574 6f6d 6174 6963 616c  ided automatical
-0000e460: 6c79 2061 6e64 2064 6f65 7320 6e6f 7420  ly and does not 
-0000e470: 6861 7665 2074 6f20 6265 2070 6173 7365  have to be passe
-0000e480: 6420 7573 696e 6720 6060 696e 6974 5f61  d using ``init_a
-0000e490: 7267 7360 600a 2020 2020 6f72 2060 6069  rgs``.    or ``i
-0000e4a0: 6e69 745f 6b77 6172 6773 6060 3a3a 0a0a  nit_kwargs``::..
-0000e4b0: 2020 2020 2020 3e3e 3e20 636c 6173 7320        >>> class 
-0000e4c0: 466f 6f28 6e6e 2e4d 6f64 756c 6529 3a0a  Foo(nn.Module):.
-0000e4d0: 2020 2020 2020 2e2e 2e20 2020 406e 6e2e        ...   @nn.
-0000e4e0: 636f 6d70 6163 740a 2020 2020 2020 2e2e  compact.      ..
-0000e4f0: 2e20 2020 6465 6620 5f5f 6361 6c6c 5f5f  .   def __call__
-0000e500: 2873 656c 662c 2078 293a 0a20 2020 2020  (self, x):.     
-0000e510: 202e 2e2e 2020 2020 2078 203d 206e 6e2e   ...     x = nn.
-0000e520: 4465 6e73 6528 3429 2878 290a 2020 2020  Dense(4)(x).    
-0000e530: 2020 2e2e 2e20 2020 2020 6d65 616e 203d    ...     mean =
-0000e540: 2073 656c 662e 7061 7261 6d28 276d 6561   self.param('mea
-0000e550: 6e27 2c20 6e6e 2e69 6e69 7469 616c 697a  n', nn.initializ
-0000e560: 6572 732e 6c65 6375 6e5f 6e6f 726d 616c  ers.lecun_normal
-0000e570: 2829 2c20 782e 7368 6170 6529 0a20 2020  (), x.shape).   
-0000e580: 2020 202e 2e2e 2020 2020 202e 2e2e 0a20     ...     .... 
-0000e590: 2020 2020 202e 2e2e 2020 2020 2072 6574       ...     ret
-0000e5a0: 7572 6e20 7820 2a20 6d65 616e 0a20 2020  urn x * mean.   
-0000e5b0: 2020 203e 3e3e 2076 6172 6961 626c 6573     >>> variables
-0000e5c0: 203d 2046 6f6f 2829 2e69 6e69 7428 7b27   = Foo().init({'
-0000e5d0: 7061 7261 6d73 273a 206a 6178 2e72 616e  params': jax.ran
-0000e5e0: 646f 6d2e 6b65 7928 3029 2c20 2773 7461  dom.key(0), 'sta
-0000e5f0: 7473 273a 206a 6178 2e72 616e 646f 6d2e  ts': jax.random.
-0000e600: 6b65 7928 3129 7d2c 206a 6e70 2e6f 6e65  key(1)}, jnp.one
-0000e610: 7328 2832 2c20 3329 2929 0a20 2020 2020  s((2, 3))).     
-0000e620: 203e 3e3e 206a 6178 2e74 7265 655f 7574   >>> jax.tree_ut
-0000e630: 696c 2e74 7265 655f 6d61 7028 6a6e 702e  il.tree_map(jnp.
-0000e640: 7368 6170 652c 2076 6172 6961 626c 6573  shape, variables
-0000e650: 290a 2020 2020 2020 7b27 7061 7261 6d73  ).      {'params
-0000e660: 273a 207b 2744 656e 7365 5f30 273a 207b  ': {'Dense_0': {
-0000e670: 2762 6961 7327 3a20 2834 2c29 2c20 276b  'bias': (4,), 'k
-0000e680: 6572 6e65 6c27 3a20 2833 2c20 3429 7d2c  ernel': (3, 4)},
-0000e690: 2027 6d65 616e 273a 2028 322c 2034 297d   'mean': (2, 4)}
-0000e6a0: 7d0a 0a20 2020 2049 6e20 7468 6520 6578  }..    In the ex
-0000e6b0: 616d 706c 6520 6162 6f76 652c 2074 6865  ample above, the
-0000e6c0: 2066 756e 6374 696f 6e20 6060 6c65 6375   function ``lecu
-0000e6d0: 6e5f 6e6f 726d 616c 6060 2065 7870 6563  n_normal`` expec
-0000e6e0: 7473 2074 776f 2061 7267 756d 656e 7473  ts two arguments
-0000e6f0: 3a0a 2020 2020 6060 6b65 7960 6020 616e  :.    ``key`` an
-0000e700: 6420 6060 7368 6170 6560 602c 2062 7574  d ``shape``, but
-0000e710: 206f 6e6c 7920 6060 7368 6170 6560 6020   only ``shape`` 
-0000e720: 6861 7320 746f 2062 6520 7072 6f76 6964  has to be provid
-0000e730: 6564 2065 7870 6c69 6369 746c 793b 0a20  ed explicitly;. 
-0000e740: 2020 2060 606b 6579 6060 2069 7320 7365     ``key`` is se
-0000e750: 7420 6175 746f 6d61 7469 6361 6c6c 7920  t automatically 
-0000e760: 7573 696e 6720 7468 6520 5052 4e47 2066  using the PRNG f
-0000e770: 6f72 2060 6070 6172 616d 7360 6020 7468  or ``params`` th
-0000e780: 6174 2069 7320 7061 7373 6564 0a20 2020  at is passed.   
-0000e790: 2077 6865 6e20 696e 6974 6961 6c69 7a69   when initializi
-0000e7a0: 6e67 2074 6865 206d 6f64 756c 6520 7573  ng the module us
-0000e7b0: 696e 6720 3a6d 6574 683a 6069 6e69 7460  ing :meth:`init`
-0000e7c0: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   
-0000e7d0: 2020 206e 616d 653a 2054 6865 2070 6172     name: The par
-0000e7e0: 616d 6574 6572 206e 616d 652e 0a20 2020  ameter name..   
-0000e7f0: 2020 2069 6e69 745f 666e 3a20 5468 6520     init_fn: The 
-0000e800: 6675 6e63 7469 6f6e 2074 6861 7420 7769  function that wi
-0000e810: 6c6c 2062 6520 6361 6c6c 6564 2074 6f20  ll be called to 
-0000e820: 636f 6d70 7574 6520 7468 6520 696e 6974  compute the init
-0000e830: 6961 6c20 7661 6c75 6520 6f66 0a20 2020  ial value of.   
-0000e840: 2020 2020 2074 6869 7320 7661 7269 6162       this variab
-0000e850: 6c65 2e20 5468 6973 2066 756e 6374 696f  le. This functio
-0000e860: 6e20 7769 6c6c 206f 6e6c 7920 6265 2063  n will only be c
-0000e870: 616c 6c65 6420 7468 6520 6669 7273 7420  alled the first 
-0000e880: 7469 6d65 2074 6869 730a 2020 2020 2020  time this.      
-0000e890: 2020 7061 7261 6d65 7465 7220 6973 2075    parameter is u
-0000e8a0: 7365 6420 696e 2074 6869 7320 6d6f 6475  sed in this modu
-0000e8b0: 6c65 2e0a 2020 2020 2020 2a69 6e69 745f  le..      *init_
-0000e8c0: 6172 6773 3a20 5468 6520 706f 7369 7469  args: The positi
-0000e8d0: 6f6e 616c 2061 7267 756d 656e 7473 2074  onal arguments t
-0000e8e0: 6f20 7061 7373 2074 6f20 696e 6974 5f66  o pass to init_f
-0000e8f0: 6e2e 0a20 2020 2020 2075 6e62 6f78 3a20  n..      unbox: 
-0000e900: 4966 2054 7275 652c 2060 6041 7869 734d  If True, ``AxisM
-0000e910: 6574 6164 6174 6160 6020 696e 7374 616e  etadata`` instan
-0000e920: 6365 7320 6172 6520 7265 706c 6163 6564  ces are replaced
-0000e930: 2062 7920 7468 6569 7220 756e 626f 7865   by their unboxe
-0000e940: 640a 2020 2020 2020 2020 7661 6c75 652c  d.        value,
-0000e950: 2073 6565 2060 6066 6c61 782e 6e6e 2e6d   see ``flax.nn.m
-0000e960: 6574 612e 756e 626f 7860 6020 2864 6566  eta.unbox`` (def
-0000e970: 6175 6c74 3a20 5472 7565 292e 0a20 2020  ault: True)..   
-0000e980: 2020 202a 2a69 6e69 745f 6b77 6172 6773     **init_kwargs
-0000e990: 3a20 5468 6520 6b65 792d 776f 7264 2061  : The key-word a
-0000e9a0: 7267 756d 656e 7473 2074 6f20 7061 7373  rguments to pass
-0000e9b0: 2074 6f20 696e 6974 5f66 6e2e 0a0a 2020   to init_fn...  
-0000e9c0: 2020 5265 7475 726e 733a 0a20 2020 2020    Returns:.     
-0000e9d0: 2054 6865 2076 616c 7565 206f 6620 7468   The value of th
-0000e9e0: 6520 696e 6974 6961 6c69 7a65 6420 7061  e initialized pa
-0000e9f0: 7261 6d65 7465 722e 2054 6872 6f77 7320  rameter. Throws 
-0000ea00: 616e 2065 7272 6f72 2069 6620 7468 6520  an error if the 
-0000ea10: 7061 7261 6d65 7465 720a 2020 2020 2020  parameter.      
-0000ea20: 6578 6973 7473 2061 6c72 6561 6479 2e0a  exists already..
-0000ea30: 2020 2020 2222 220a 2020 2020 6966 206e      """.    if n
-0000ea40: 6f74 2073 656c 662e 5f69 6e69 7469 616c  ot self._initial
-0000ea50: 697a 6174 696f 6e5f 616c 6c6f 7765 643a  ization_allowed:
-0000ea60: 0a20 2020 2020 2072 6169 7365 2056 616c  .      raise Val
-0000ea70: 7565 4572 726f 7228 0a20 2020 2020 2020  ueError(.       
-0000ea80: 2027 5061 7261 6d65 7465 7273 206d 7573   'Parameters mus
-0000ea90: 7420 6265 2069 6e69 7469 616c 697a 6564  t be initialized
-0000eaa0: 2069 6e20 6073 6574 7570 2829 6020 6f72   in `setup()` or
-0000eab0: 2069 6e20 6120 6d65 7468 6f64 2027 0a20   in a method '. 
-0000eac0: 2020 2020 2020 2027 7772 6170 7065 6420         'wrapped 
-0000ead0: 696e 2060 4063 6f6d 7061 6374 6027 0a20  in `@compact`'. 
-0000eae0: 2020 2020 2029 0a20 2020 2069 6620 7365       ).    if se
-0000eaf0: 6c66 2e5f 6e61 6d65 5f74 616b 656e 286e  lf._name_taken(n
-0000eb00: 616d 652c 2063 6f6c 6c65 6374 696f 6e3d  ame, collection=
-0000eb10: 2770 6172 616d 7327 293a 0a20 2020 2020  'params'):.     
-0000eb20: 2072 6169 7365 2065 7272 6f72 732e 4e61   raise errors.Na
-0000eb30: 6d65 496e 5573 6545 7272 6f72 2827 7061  meInUseError('pa
-0000eb40: 7261 6d27 2c20 6e61 6d65 2c20 7365 6c66  ram', name, self
-0000eb50: 2e5f 5f63 6c61 7373 5f5f 2e5f 5f6e 616d  .__class__.__nam
-0000eb60: 655f 5f29 0a20 2020 2061 7373 6572 7420  e__).    assert 
-0000eb70: 7365 6c66 2e73 636f 7065 2069 7320 6e6f  self.scope is no
-0000eb80: 7420 4e6f 6e65 0a20 2020 2076 203d 2073  t None.    v = s
-0000eb90: 656c 662e 7363 6f70 652e 7061 7261 6d28  elf.scope.param(
-0000eba0: 6e61 6d65 2c20 696e 6974 5f66 6e2c 202a  name, init_fn, *
-0000ebb0: 696e 6974 5f61 7267 732c 2075 6e62 6f78  init_args, unbox
-0000ebc0: 3d75 6e62 6f78 2c20 2a2a 696e 6974 5f6b  =unbox, **init_k
-0000ebd0: 7761 7267 7329 0a20 2020 2073 656c 662e  wargs).    self.
-0000ebe0: 5f73 7461 7465 2e63 6869 6c64 7265 6e5b  _state.children[
-0000ebf0: 6e61 6d65 5d20 3d20 2770 6172 616d 7327  name] = 'params'
-0000ec00: 0a20 2020 2072 6574 7572 6e20 760a 0a20  .    return v.. 
-0000ec10: 2064 6566 2068 6173 5f76 6172 6961 626c   def has_variabl
-0000ec20: 6528 7365 6c66 2c20 636f 6c3a 2073 7472  e(self, col: str
-0000ec30: 2c20 6e61 6d65 3a20 7374 7229 202d 3e20  , name: str) -> 
-0000ec40: 626f 6f6c 3a0a 2020 2020 2222 2243 6865  bool:.    """Che
-0000ec50: 636b 7320 6966 2061 2076 6172 6961 626c  cks if a variabl
-0000ec60: 6520 6f66 2067 6976 656e 2063 6f6c 6c65  e of given colle
-0000ec70: 6374 696f 6e20 616e 6420 6e61 6d65 2065  ction and name e
-0000ec80: 7869 7374 7320 696e 2074 6869 7320 4d6f  xists in this Mo
-0000ec90: 6475 6c65 2e0a 0a20 2020 2053 6565 203a  dule...    See :
-0000eca0: 6d6f 643a 6066 6c61 782e 636f 7265 2e76  mod:`flax.core.v
-0000ecb0: 6172 6961 626c 6573 6020 666f 7220 6d6f  ariables` for mo
-0000ecc0: 7265 2065 7870 6c61 6e61 7469 6f6e 206f  re explanation o
-0000ecd0: 6e20 7661 7269 6162 6c65 7320 616e 640a  n variables and.
-0000ece0: 2020 2020 636f 6c6c 6563 7469 6f6e 732e      collections.
-0000ecf0: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    
-0000ed00: 2020 636f 6c3a 2054 6865 2076 6172 6961    col: The varia
-0000ed10: 626c 6520 636f 6c6c 6563 7469 6f6e 206e  ble collection n
-0000ed20: 616d 652e 0a20 2020 2020 206e 616d 653a  ame..      name:
-0000ed30: 2054 6865 206e 616d 6520 6f66 2074 6865   The name of the
-0000ed40: 2076 6172 6961 626c 652e 0a0a 2020 2020   variable...    
-0000ed50: 5265 7475 726e 733a 0a20 2020 2020 2054  Returns:.      T
-0000ed60: 7275 6520 6966 2074 6865 2076 6172 6961  rue if the varia
-0000ed70: 626c 6520 6578 6973 7473 2e0a 2020 2020  ble exists..    
-0000ed80: 2222 220a 2020 2020 6966 2073 656c 662e  """.    if self.
-0000ed90: 7363 6f70 6520 6973 204e 6f6e 653a 0a20  scope is None:. 
-0000eda0: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-0000edb0: 4572 726f 7228 2243 616e 2774 2061 6363  Error("Can't acc
-0000edc0: 6573 7320 7661 7269 6162 6c65 7320 6f6e  ess variables on
-0000edd0: 2075 6e62 6f75 6e64 206d 6f64 756c 6573   unbound modules
-0000ede0: 2229 0a20 2020 2072 6574 7572 6e20 7365  ").    return se
-0000edf0: 6c66 2e73 636f 7065 2e68 6173 5f76 6172  lf.scope.has_var
-0000ee00: 6961 626c 6528 636f 6c2c 206e 616d 6529  iable(col, name)
-0000ee10: 0a0a 2020 6465 6620 6973 5f6d 7574 6162  ..  def is_mutab
-0000ee20: 6c65 5f63 6f6c 6c65 6374 696f 6e28 7365  le_collection(se
-0000ee30: 6c66 2c20 636f 6c3a 2073 7472 2920 2d3e  lf, col: str) ->
-0000ee40: 2062 6f6f 6c3a 0a20 2020 2022 2222 5265   bool:.    """Re
-0000ee50: 7475 726e 7320 7472 7565 2069 6620 7468  turns true if th
-0000ee60: 6520 636f 6c6c 6563 7469 6f6e 2060 6063  e collection ``c
-0000ee70: 6f6c 6060 2069 7320 6d75 7461 626c 652e  ol`` is mutable.
-0000ee80: 2222 220a 2020 2020 6966 2073 656c 662e  """.    if self.
-0000ee90: 7363 6f70 6520 6973 204e 6f6e 653a 0a20  scope is None:. 
-0000eea0: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-0000eeb0: 4572 726f 7228 2243 616e 2774 2063 6865  Error("Can't che
-0000eec0: 636b 206d 7574 6162 696c 6974 7920 6f6e  ck mutability on
-0000eed0: 2075 6e62 6f75 6e64 206d 6f64 756c 6573   unbound modules
-0000eee0: 2229 0a20 2020 2072 6574 7572 6e20 7365  ").    return se
-0000eef0: 6c66 2e73 636f 7065 2e69 735f 6d75 7461  lf.scope.is_muta
-0000ef00: 626c 655f 636f 6c6c 6563 7469 6f6e 2863  ble_collection(c
-0000ef10: 6f6c 290a 0a20 2064 6566 2068 6173 5f72  ol)..  def has_r
-0000ef20: 6e67 2873 656c 662c 206e 616d 653a 2073  ng(self, name: s
-0000ef30: 7472 2920 2d3e 2062 6f6f 6c3a 0a20 2020  tr) -> bool:.   
-0000ef40: 2022 2222 5265 7475 726e 7320 7472 7565   """Returns true
-0000ef50: 2069 6620 6120 5052 4e47 5365 7175 656e   if a PRNGSequen
-0000ef60: 6365 2077 6974 6820 6e61 6d65 2060 606e  ce with name ``n
-0000ef70: 616d 6560 6020 6578 6973 7473 2e22 2222  ame`` exists."""
-0000ef80: 0a20 2020 2069 6620 7365 6c66 2e73 636f  .    if self.sco
-0000ef90: 7065 2069 7320 4e6f 6e65 3a0a 2020 2020  pe is None:.    
-0000efa0: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-0000efb0: 6f72 2822 4361 6e27 7420 7175 6572 7920  or("Can't query 
-0000efc0: 666f 7220 524e 4773 206f 6e20 756e 626f  for RNGs on unbo
-0000efd0: 756e 6420 6d6f 6475 6c65 7322 290a 2020  und modules").  
-0000efe0: 2020 7265 7475 726e 2073 656c 662e 7363    return self.sc
-0000eff0: 6f70 652e 6861 735f 726e 6728 6e61 6d65  ope.has_rng(name
-0000f000: 290a 0a20 2064 6566 206d 616b 655f 726e  )..  def make_rn
-0000f010: 6728 7365 6c66 2c20 6e61 6d65 3a20 7374  g(self, name: st
-0000f020: 7220 3d20 2770 6172 616d 7327 2920 2d3e  r = 'params') ->
-0000f030: 2050 524e 474b 6579 3a0a 2020 2020 2222   PRNGKey:.    ""
-0000f040: 2252 6574 7572 6e73 2061 206e 6577 2052  "Returns a new R
-0000f050: 4e47 206b 6579 2066 726f 6d20 6120 6769  NG key from a gi
-0000f060: 7665 6e20 524e 4720 7365 7175 656e 6365  ven RNG sequence
-0000f070: 2066 6f72 2074 6869 7320 4d6f 6475 6c65   for this Module
-0000f080: 2e0a 0a20 2020 2054 6865 206e 6577 2052  ...    The new R
-0000f090: 4e47 206b 6579 2069 7320 7370 6c69 7420  NG key is split 
-0000f0a0: 6672 6f6d 2074 6865 2070 7265 7669 6f75  from the previou
-0000f0b0: 7320 6f6e 652e 2054 6875 732c 2065 7665  s one. Thus, eve
-0000f0c0: 7279 2063 616c 6c20 746f 0a20 2020 2060  ry call to.    `
-0000f0d0: 606d 616b 655f 726e 6760 6020 7265 7475  `make_rng`` retu
-0000f0e0: 726e 7320 6120 6e65 7720 524e 4720 6b65  rns a new RNG ke
-0000f0f0: 792c 2077 6869 6c65 2073 7469 6c6c 2067  y, while still g
-0000f100: 7561 7261 6e74 6565 696e 6720 6675 6c6c  uaranteeing full
-0000f110: 0a20 2020 2072 6570 726f 6475 6369 6269  .    reproducibi
-0000f120: 6c69 7479 2e0a 0a20 2020 204e 4f54 453a  lity...    NOTE:
-0000f130: 2069 6620 616e 2069 6e76 616c 6964 206e   if an invalid n
-0000f140: 616d 6520 6973 2070 6173 7365 6420 2869  ame is passed (i
-0000f150: 2e65 2e20 6e6f 2052 4e47 206b 6579 2077  .e. no RNG key w
-0000f160: 6173 2070 6173 7365 6420 6279 0a20 2020  as passed by.   
-0000f170: 2074 6865 2075 7365 7220 696e 2060 602e   the user in ``.
-0000f180: 696e 6974 6060 206f 7220 6060 2e61 7070  init`` or ``.app
-0000f190: 6c79 6060 2066 6f72 2074 6869 7320 6e61  ly`` for this na
-0000f1a0: 6d65 292c 2074 6865 6e20 6060 6e61 6d65  me), then ``name
-0000f1b0: 6060 0a20 2020 2077 696c 6c20 6465 6661  ``.    will defa
-0000f1c0: 756c 7420 746f 2060 6027 7061 7261 6d73  ult to ``'params
-0000f1d0: 2760 602e 0a0a 2020 2020 544f 444f 3a20  '``...    TODO: 
-0000f1e0: 4c69 6e6b 2074 6f20 466c 6178 2052 4e47  Link to Flax RNG
-0000f1f0: 2064 6573 6967 6e20 6e6f 7465 2e0a 0a20   design note... 
-0000f200: 2020 2041 7267 733a 0a20 2020 2020 206e     Args:.      n
-0000f210: 616d 653a 2054 6865 2052 4e47 2073 6571  ame: The RNG seq
-0000f220: 7565 6e63 6520 6e61 6d65 2e0a 0a20 2020  uence name...   
-0000f230: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
-0000f240: 5468 6520 6e65 776c 7920 6765 6e65 7261  The newly genera
-0000f250: 7465 6420 524e 4720 6b65 792e 0a20 2020  ted RNG key..   
-0000f260: 2022 2222 0a20 2020 2069 6620 7365 6c66   """.    if self
-0000f270: 2e73 636f 7065 2069 7320 4e6f 6e65 3a0a  .scope is None:.
-0000f280: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
-0000f290: 6545 7272 6f72 2822 4361 6e27 7420 7573  eError("Can't us
-0000f2a0: 6520 524e 4773 206f 6e20 756e 626f 756e  e RNGs on unboun
-0000f2b0: 6420 6d6f 6475 6c65 7322 290a 2020 2020  d modules").    
-0000f2c0: 7265 7475 726e 2073 656c 662e 7363 6f70  return self.scop
-0000f2d0: 652e 6d61 6b65 5f72 6e67 286e 616d 6529  e.make_rng(name)
-0000f2e0: 0a0a 2020 6465 6620 6973 5f69 6e69 7469  ..  def is_initi
-0000f2f0: 616c 697a 696e 6728 7365 6c66 2920 2d3e  alizing(self) ->
-0000f300: 2062 6f6f 6c3a 0a20 2020 2022 2222 5265   bool:.    """Re
-0000f310: 7475 726e 7320 5472 7565 2069 6620 7275  turns True if ru
-0000f320: 6e6e 696e 6720 756e 6465 7220 7365 6c66  nning under self
-0000f330: 2e69 6e69 7428 2e2e 2e29 206f 7220 6e6e  .init(...) or nn
-0000f340: 2e69 6e69 7428 2e2e 2e29 2829 2e0a 0a20  .init(...)()... 
-0000f350: 2020 2054 6869 7320 6973 2061 2068 656c     This is a hel
-0000f360: 7065 7220 6d65 7468 6f64 2074 6f20 6861  per method to ha
-0000f370: 6e64 6c65 2074 6865 2063 6f6d 6d6f 6e20  ndle the common 
-0000f380: 6361 7365 206f 6620 7369 6d70 6c65 2069  case of simple i
-0000f390: 6e69 7469 616c 697a 6174 696f 6e0a 2020  nitialization.  
-0000f3a0: 2020 7768 6572 6520 7765 2077 6973 6820    where we wish 
-0000f3b0: 746f 2068 6176 6520 7365 7475 7020 6c6f  to have setup lo
-0000f3c0: 6769 6320 6f63 6375 7220 7768 656e 206f  gic occur when o
-0000f3d0: 6e6c 7920 6361 6c6c 6564 2075 6e64 6572  nly called under
-0000f3e0: 0a20 2020 2060 606d 6f64 756c 652e 696e  .    ``module.in
-0000f3f0: 6974 6060 206f 7220 6060 6e6e 2e69 6e69  it`` or ``nn.ini
-0000f400: 7460 602e 2020 466f 7220 6d6f 7265 2063  t``.  For more c
-0000f410: 6f6d 706c 6963 6174 6564 206d 756c 7469  omplicated multi
-0000f420: 2d70 6861 7365 0a20 2020 2069 6e69 7469  -phase.    initi
-0000f430: 616c 697a 6174 696f 6e20 7363 656e 6172  alization scenar
-0000f440: 696f 7320 6974 2069 7320 6265 7474 6572  ios it is better
-0000f450: 2074 6f20 7465 7374 2066 6f72 2074 6865   to test for the
-0000f460: 206d 7574 6162 696c 6974 7920 6f66 0a20   mutability of. 
-0000f470: 2020 2070 6172 7469 6375 6c61 7220 7661     particular va
-0000f480: 7269 6162 6c65 2063 6f6c 6c65 6374 696f  riable collectio
-0000f490: 6e73 206f 7220 666f 7220 7468 6520 7072  ns or for the pr
-0000f4a0: 6573 656e 6365 206f 6620 7061 7274 6963  esence of partic
-0000f4b0: 756c 6172 0a20 2020 2076 6172 6961 626c  ular.    variabl
-0000f4c0: 6573 2074 6861 7420 706f 7465 6e74 6961  es that potentia
-0000f4d0: 6c6c 7920 6e65 6564 2074 6f20 6265 2069  lly need to be i
-0000f4e0: 6e69 7469 616c 697a 6564 2e0a 2020 2020  nitialized..    
-0000f4f0: 2222 220a 2020 2020 6966 2073 656c 662e  """.    if self.
-0000f500: 7363 6f70 6520 6973 204e 6f6e 653a 0a20  scope is None:. 
-0000f510: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-0000f520: 4572 726f 7228 2243 616e 2774 2063 6865  Error("Can't che
-0000f530: 636b 2069 6620 7275 6e6e 696e 6720 756e  ck if running un
-0000f540: 6465 7220 696e 6974 2829 206f 6e20 756e  der init() on un
-0000f550: 626f 756e 6420 6d6f 6475 6c65 7322 290a  bound modules").
-0000f560: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-0000f570: 7363 6f70 652e 6765 745f 666c 6167 2827  scope.get_flag('
-0000f580: 696e 6974 6961 6c69 7a69 6e67 272c 2046  initializing', F
-0000f590: 616c 7365 290a 0a20 2064 6566 205f 6d6f  alse)..  def _mo
-0000f5a0: 6475 6c65 5f63 6865 636b 7328 7365 6c66  dule_checks(self
-0000f5b0: 293a 0a20 2020 2022 2222 5275 6e20 7374  ):.    """Run st
-0000f5c0: 616e 6461 7264 2072 756e 7469 6d65 2063  andard runtime c
-0000f5d0: 6865 636b 732e 2222 220a 0a20 2020 2069  hecks."""..    i
-0000f5e0: 6620 6e6f 7420 6973 696e 7374 616e 6365  f not isinstance
-0000f5f0: 2873 656c 662c 204d 6f64 756c 6529 3a0a  (self, Module):.
-0000f600: 2020 2020 2020 7261 6973 6520 6572 726f        raise erro
-0000f610: 7273 2e49 6e76 616c 6964 496e 7374 616e  rs.InvalidInstan
-0000f620: 6365 4d6f 6475 6c65 4572 726f 7228 290a  ceModuleError().
-0000f630: 0a20 2020 206f 7665 7272 6964 6465 6e5f  .    overridden_
-0000f640: 706f 7374 5f69 6e69 7420 3d20 7365 6c66  post_init = self
-0000f650: 2e5f 5f70 6f73 745f 696e 6974 5f5f 2021  .__post_init__ !
-0000f660: 3d20 4d6f 6475 6c65 2e5f 5f70 6f73 745f  = Module.__post_
-0000f670: 696e 6974 5f5f 0a20 2020 2069 6620 6f76  init__.    if ov
-0000f680: 6572 7269 6464 656e 5f70 6f73 745f 696e  erridden_post_in
-0000f690: 6974 2061 6e64 206e 6f74 2068 6173 6174  it and not hasat
-0000f6a0: 7472 2873 656c 662c 2027 5f69 6427 293a  tr(self, '_id'):
-0000f6b0: 0a20 2020 2020 2072 6169 7365 2065 7272  .      raise err
-0000f6c0: 6f72 732e 496e 636f 7272 6563 7450 6f73  ors.IncorrectPos
-0000f6d0: 7449 6e69 744f 7665 7272 6964 6545 7272  tInitOverrideErr
-0000f6e0: 6f72 2829 0a0a 2020 4074 7261 6365 6261  or()..  @traceba
-0000f6f0: 636b 5f75 7469 6c2e 6170 695f 626f 756e  ck_util.api_boun
-0000f700: 6461 7279 0a20 2064 6566 2062 696e 6428  dary.  def bind(
-0000f710: 0a20 2020 2073 656c 663a 204d 2c0a 2020  .    self: M,.  
-0000f720: 2020 7661 7269 6162 6c65 733a 2056 6172    variables: Var
-0000f730: 6961 626c 6544 6963 742c 0a20 2020 202a  iableDict,.    *
-0000f740: 6172 6773 2c0a 2020 2020 726e 6773 3a20  args,.    rngs: 
-0000f750: 4f70 7469 6f6e 616c 5b52 4e47 5365 7175  Optional[RNGSequ
-0000f760: 656e 6365 735d 203d 204e 6f6e 652c 0a20  ences] = None,. 
-0000f770: 2020 206d 7574 6162 6c65 3a20 436f 6c6c     mutable: Coll
-0000f780: 6563 7469 6f6e 4669 6c74 6572 203d 2046  ectionFilter = F
-0000f790: 616c 7365 2c0a 2020 2920 2d3e 204d 3a0a  alse,.  ) -> M:.
-0000f7a0: 2020 2020 2222 2243 7265 6174 6573 2061      """Creates a
-0000f7b0: 6e20 696e 7465 7261 6374 6976 6520 4d6f  n interactive Mo
-0000f7c0: 6475 6c65 2069 6e73 7461 6e63 6520 6279  dule instance by
-0000f7d0: 2062 696e 6469 6e67 2076 6172 6961 626c   binding variabl
-0000f7e0: 6573 2061 6e64 2052 4e47 732e 0a0a 2020  es and RNGs...  
-0000f7f0: 2020 6060 6269 6e64 6060 2070 726f 7669    ``bind`` provi
-0000f800: 6465 7320 616e 2022 696e 7465 7261 6374  des an "interact
-0000f810: 6976 6522 2069 6e73 7461 6e63 6520 6f66  ive" instance of
-0000f820: 2061 204d 6f64 756c 6520 6469 7265 6374   a Module direct
-0000f830: 6c79 2077 6974 686f 7574 0a20 2020 2074  ly without.    t
-0000f840: 7261 6e73 666f 726d 696e 6720 6120 6675  ransforming a fu
-0000f850: 6e63 7469 6f6e 2077 6974 6820 6060 6170  nction with ``ap
-0000f860: 706c 7960 602e 2054 6869 7320 6973 2070  ply``. This is p
-0000f870: 6172 7469 6375 6c61 726c 7920 7573 6566  articularly usef
-0000f880: 756c 2066 6f72 0a20 2020 2064 6562 7567  ul for.    debug
-0000f890: 6769 6e67 2061 6e64 2069 6e74 6572 6163  ging and interac
-0000f8a0: 7469 7665 2075 7365 2063 6173 6573 206c  tive use cases l
-0000f8b0: 696b 6520 6e6f 7465 626f 6f6b 7320 7768  ike notebooks wh
-0000f8c0: 6572 6520 6120 6675 6e63 7469 6f6e 2077  ere a function w
-0000f8d0: 6f75 6c64 0a20 2020 206c 696d 6974 2074  ould.    limit t
-0000f8e0: 6865 2061 6269 6c69 7479 2074 6f20 7370  he ability to sp
-0000f8f0: 6c69 7420 7570 2063 6f64 6520 696e 746f  lit up code into
-0000f900: 2064 6966 6665 7265 6e74 2063 656c 6c73   different cells
-0000f910: 2e0a 0a20 2020 204f 6e63 6520 7468 6520  ...    Once the 
-0000f920: 7661 7269 6162 6c65 7320 2861 6e64 206f  variables (and o
-0000f930: 7074 696f 6e61 6c6c 7920 524e 4773 2920  ptionally RNGs) 
-0000f940: 6172 6520 626f 756e 6420 746f 2061 2060  are bound to a `
-0000f950: 604d 6f64 756c 6560 6020 6974 0a20 2020  `Module`` it.   
-0000f960: 2062 6563 6f6d 6573 2061 2073 7461 7465   becomes a state
-0000f970: 6675 6c20 6f62 6a65 6374 2e20 4e6f 7465  ful object. Note
-0000f980: 2074 6861 7420 6964 696f 6d61 7469 6320   that idiomatic 
-0000f990: 4a41 5820 6973 2066 756e 6374 696f 6e61  JAX is functiona
-0000f9a0: 6c20 616e 640a 2020 2020 7468 6572 6566  l and.    theref
-0000f9b0: 6f72 6520 616e 2069 6e74 6572 6163 7469  ore an interacti
-0000f9c0: 7665 2069 6e73 7461 6e63 6520 646f 6573  ve instance does
-0000f9d0: 206e 6f74 206d 6978 2077 656c 6c20 7769   not mix well wi
-0000f9e0: 7468 2076 616e 696c 6c61 204a 4158 2041  th vanilla JAX A
-0000f9f0: 5049 732e 0a20 2020 2060 6062 696e 6428  PIs..    ``bind(
-0000fa00: 2960 6020 7368 6f75 6c64 206f 6e6c 7920  )`` should only 
-0000fa10: 6265 2075 7365 6420 666f 7220 696e 7465  be used for inte
-0000fa20: 7261 6374 6976 6520 6578 7065 7269 6d65  ractive experime
-0000fa30: 6e74 6174 696f 6e2c 2061 6e64 2069 6e20  ntation, and in 
-0000fa40: 616c 6c0a 2020 2020 6f74 6865 7220 6361  all.    other ca
-0000fa50: 7365 7320 7765 2073 7472 6f6e 676c 7920  ses we strongly 
-0000fa60: 656e 636f 7572 6167 6520 7573 6572 7320  encourage users 
-0000fa70: 746f 2075 7365 2060 6061 7070 6c79 2829  to use ``apply()
-0000fa80: 6060 2069 6e73 7465 6164 2e0a 0a20 2020  `` instead...   
-0000fa90: 2045 7861 6d70 6c65 3a3a 0a0a 2020 2020   Example::..    
-0000faa0: 2020 3e3e 3e20 696d 706f 7274 206a 6178    >>> import jax
-0000fab0: 0a20 2020 2020 203e 3e3e 2069 6d70 6f72  .      >>> impor
-0000fac0: 7420 6a61 782e 6e75 6d70 7920 6173 206a  t jax.numpy as j
-0000fad0: 6e70 0a20 2020 2020 203e 3e3e 2069 6d70  np.      >>> imp
-0000fae0: 6f72 7420 666c 6178 2e6c 696e 656e 2061  ort flax.linen a
-0000faf0: 7320 6e6e 0a0a 2020 2020 2020 3e3e 3e20  s nn..      >>> 
-0000fb00: 636c 6173 7320 4175 746f 456e 636f 6465  class AutoEncode
-0000fb10: 7228 6e6e 2e4d 6f64 756c 6529 3a0a 2020  r(nn.Module):.  
-0000fb20: 2020 2020 2e2e 2e20 2020 6465 6620 7365      ...   def se
-0000fb30: 7475 7028 7365 6c66 293a 0a20 2020 2020  tup(self):.     
-0000fb40: 202e 2e2e 2020 2020 2073 656c 662e 656e   ...     self.en
-0000fb50: 636f 6465 7220 3d20 6e6e 2e44 656e 7365  coder = nn.Dense
-0000fb60: 2833 290a 2020 2020 2020 2e2e 2e20 2020  (3).      ...   
-0000fb70: 2020 7365 6c66 2e64 6563 6f64 6572 203d    self.decoder =
-0000fb80: 206e 6e2e 4465 6e73 6528 3529 0a20 2020   nn.Dense(5).   
-0000fb90: 2020 202e 2e2e 0a20 2020 2020 202e 2e2e     ....      ...
-0000fba0: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
-0000fbb0: 7365 6c66 2c20 7829 3a0a 2020 2020 2020  self, x):.      
-0000fbc0: 2e2e 2e20 2020 2020 7265 7475 726e 2073  ...     return s
-0000fbd0: 656c 662e 6465 636f 6465 7228 7365 6c66  elf.decoder(self
-0000fbe0: 2e65 6e63 6f64 6572 2878 2929 0a0a 2020  .encoder(x))..  
-0000fbf0: 2020 2020 3e3e 3e20 7820 3d20 6a6e 702e      >>> x = jnp.
-0000fc00: 6f6e 6573 2828 3136 2c20 3929 290a 2020  ones((16, 9)).  
-0000fc10: 2020 2020 3e3e 3e20 6165 203d 2041 7574      >>> ae = Aut
-0000fc20: 6f45 6e63 6f64 6572 2829 0a20 2020 2020  oEncoder().     
-0000fc30: 203e 3e3e 2076 6172 6961 626c 6573 203d   >>> variables =
-0000fc40: 2061 652e 696e 6974 286a 6178 2e72 616e   ae.init(jax.ran
-0000fc50: 646f 6d2e 6b65 7928 3029 2c20 7829 0a20  dom.key(0), x). 
-0000fc60: 2020 2020 203e 3e3e 206d 6f64 656c 203d       >>> model =
-0000fc70: 2061 652e 6269 6e64 2876 6172 6961 626c   ae.bind(variabl
-0000fc80: 6573 290a 2020 2020 2020 3e3e 3e20 7a20  es).      >>> z 
-0000fc90: 3d20 6d6f 6465 6c2e 656e 636f 6465 7228  = model.encoder(
-0000fca0: 7829 0a20 2020 2020 203e 3e3e 2078 5f72  x).      >>> x_r
-0000fcb0: 6563 6f6e 7374 7275 6374 6564 203d 206d  econstructed = m
-0000fcc0: 6f64 656c 2e64 6563 6f64 6572 287a 290a  odel.decoder(z).
-0000fcd0: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     
-0000fce0: 2076 6172 6961 626c 6573 3a20 4120 6469   variables: A di
-0000fcf0: 6374 696f 6e61 7279 2063 6f6e 7461 696e  ctionary contain
-0000fd00: 696e 6720 7661 7269 6162 6c65 7320 6b65  ing variables ke
-0000fd10: 7965 6420 6279 2076 6172 6961 626c 650a  yed by variable.
-0000fd20: 2020 2020 2020 2020 636f 6c6c 6563 7469          collecti
-0000fd30: 6f6e 732e 2053 6565 203a 6d6f 643a 6066  ons. See :mod:`f
-0000fd40: 6c61 782e 636f 7265 2e76 6172 6961 626c  lax.core.variabl
-0000fd50: 6573 6020 666f 7220 6d6f 7265 2064 6574  es` for more det
-0000fd60: 6169 6c73 2061 626f 7574 0a20 2020 2020  ails about.     
-0000fd70: 2020 2076 6172 6961 626c 6573 2e0a 2020     variables..  
-0000fd80: 2020 2020 2a61 7267 733a 204e 616d 6564      *args: Named
-0000fd90: 2061 7267 756d 656e 7473 2028 6e6f 7420   arguments (not 
-0000fda0: 7573 6564 292e 0a20 2020 2020 2072 6e67  used)..      rng
-0000fdb0: 733a 2061 2064 6963 7420 6f66 2050 524e  s: a dict of PRN
-0000fdc0: 474b 6579 7320 746f 2069 6e69 7469 616c  GKeys to initial
-0000fdd0: 697a 6520 7468 6520 5052 4e47 2073 6571  ize the PRNG seq
-0000fde0: 7565 6e63 6573 2e0a 2020 2020 2020 6d75  uences..      mu
-0000fdf0: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
-0000fe00: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
-0000fe10: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
-0000fe20: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
-0000fe30: 6f75 6c64 2062 650a 2020 2020 2020 2020  ould be.        
-0000fe40: 7472 6561 7465 6420 6173 206d 7574 6162  treated as mutab
-0000fe50: 6c65 3a20 6060 626f 6f6c 6060 3a20 616c  le: ``bool``: al
-0000fe60: 6c2f 6e6f 2063 6f6c 6c65 6374 696f 6e73  l/no collections
-0000fe70: 2061 7265 206d 7574 6162 6c65 2e20 6060   are mutable. ``
-0000fe80: 7374 7260 603a 0a20 2020 2020 2020 2054  str``:.        T
-0000fe90: 6865 206e 616d 6520 6f66 2061 2073 696e  he name of a sin
-0000fea0: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
-0000feb0: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
-0000fec0: 3a20 4120 6c69 7374 206f 6620 6e61 6d65  : A list of name
-0000fed0: 7320 6f66 0a20 2020 2020 2020 206d 7574  s of.        mut
-0000fee0: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
-0000fef0: 2e0a 0a20 2020 2052 6574 7572 6e73 3a0a  ...    Returns:.
-0000ff00: 2020 2020 2020 4120 636f 7079 206f 6620        A copy of 
-0000ff10: 7468 6973 2069 6e73 7461 6e63 6520 7769  this instance wi
-0000ff20: 7468 2062 6f75 6e64 2076 6172 6961 626c  th bound variabl
-0000ff30: 6573 2061 6e64 2052 4e47 732e 0a20 2020  es and RNGs..   
-0000ff40: 2022 2222 0a20 2020 204d 6f64 756c 652e   """.    Module.
-0000ff50: 5f6d 6f64 756c 655f 6368 6563 6b73 2873  _module_checks(s
-0000ff60: 656c 6629 0a0a 2020 2020 6465 6c20 6172  elf)..    del ar
-0000ff70: 6773 0a20 2020 2073 636f 7065 203d 2063  gs.    scope = c
-0000ff80: 6f72 652e 6269 6e64 2876 6172 6961 626c  ore.bind(variabl
-0000ff90: 6573 2c20 726e 6773 3d72 6e67 732c 206d  es, rngs=rngs, m
-0000ffa0: 7574 6162 6c65 3d6d 7574 6162 6c65 290a  utable=mutable).
-0000ffb0: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-0000ffc0: 636c 6f6e 6528 7061 7265 6e74 3d73 636f  clone(parent=sco
-0000ffd0: 7065 2c20 5f64 6565 705f 636c 6f6e 653d  pe, _deep_clone=
-0000ffe0: 5472 7565 290a 0a20 2064 6566 2075 6e62  True)..  def unb
-0000fff0: 696e 6428 7365 6c66 3a20 4d29 202d 3e20  ind(self: M) -> 
-00010000: 5475 706c 655b 4d2c 2056 6172 6961 626c  Tuple[M, Variabl
-00010010: 6544 6963 745d 3a0a 2020 2020 2222 2252  eDict]:.    """R
-00010020: 6574 7572 6e73 2061 6e20 756e 626f 756e  eturns an unboun
-00010030: 6420 636f 7079 206f 6620 6120 4d6f 6475  d copy of a Modu
-00010040: 6c65 2061 6e64 2069 7473 2076 6172 6961  le and its varia
-00010050: 626c 6573 2e0a 0a20 2020 2060 6075 6e62  bles...    ``unb
-00010060: 696e 6460 6020 6865 6c70 7320 6372 6561  ind`` helps crea
-00010070: 7465 2061 2073 7461 7465 6c65 7373 2076  te a stateless v
-00010080: 6572 7369 6f6e 206f 6620 6120 626f 756e  ersion of a boun
-00010090: 6420 4d6f 6475 6c65 2e0a 0a20 2020 2041  d Module...    A
-000100a0: 6e20 6578 616d 706c 6520 6f66 2061 2063  n example of a c
-000100b0: 6f6d 6d6f 6e20 7573 6520 6361 7365 3a20  ommon use case: 
-000100c0: 746f 2065 7874 7261 6374 2061 2073 7562  to extract a sub
-000100d0: 2d4d 6f64 756c 6520 6465 6669 6e65 6420  -Module defined 
-000100e0: 696e 7369 6465 0a20 2020 2060 6073 6574  inside.    ``set
-000100f0: 7570 2829 6060 2061 6e64 2069 7473 2063  up()`` and its c
-00010100: 6f72 7265 7370 6f6e 6469 6e67 2076 6172  orresponding var
-00010110: 6961 626c 6573 3a20 3129 2074 656d 706f  iables: 1) tempo
-00010120: 7261 7269 6c79 2060 6062 696e 6460 6020  rarily ``bind`` 
-00010130: 7468 650a 2020 2020 7061 7265 6e74 204d  the.    parent M
-00010140: 6f64 756c 653b 2061 6e64 2074 6865 6e20  odule; and then 
-00010150: 3229 2060 6075 6e62 696e 6460 6020 7468  2) ``unbind`` th
-00010160: 6520 6465 7369 7265 6420 7375 622d 4d6f  e desired sub-Mo
-00010170: 6475 6c65 2e20 2852 6563 616c 6c20 7468  dule. (Recall th
-00010180: 6174 0a20 2020 2060 6073 6574 7570 2829  at.    ``setup()
-00010190: 6060 2069 7320 6f6e 6c79 2063 616c 6c65  `` is only calle
-000101a0: 6420 7768 656e 2074 6865 204d 6f64 756c  d when the Modul
-000101b0: 6520 6973 2062 6f75 6e64 2e29 3a3a 0a0a  e is bound.)::..
-000101c0: 2020 2020 2020 3e3e 3e20 636c 6173 7320        >>> class 
-000101d0: 456e 636f 6465 7228 6e6e 2e4d 6f64 756c  Encoder(nn.Modul
-000101e0: 6529 3a0a 2020 2020 2020 2e2e 2e20 2020  e):.      ...   
-000101f0: 406e 6e2e 636f 6d70 6163 740a 2020 2020  @nn.compact.    
-00010200: 2020 2e2e 2e20 2020 6465 6620 5f5f 6361    ...   def __ca
-00010210: 6c6c 5f5f 2873 656c 662c 2078 293a 0a20  ll__(self, x):. 
-00010220: 2020 2020 202e 2e2e 2020 2020 202e 2e2e       ...     ...
-00010230: 0a20 2020 2020 202e 2e2e 2020 2020 2072  .      ...     r
-00010240: 6574 7572 6e20 6e6e 2e44 656e 7365 2832  eturn nn.Dense(2
-00010250: 3536 2928 7829 0a0a 2020 2020 2020 3e3e  56)(x)..      >>
-00010260: 3e20 636c 6173 7320 4465 636f 6465 7228  > class Decoder(
-00010270: 6e6e 2e4d 6f64 756c 6529 3a0a 2020 2020  nn.Module):.    
-00010280: 2020 2e2e 2e20 2020 406e 6e2e 636f 6d70    ...   @nn.comp
-00010290: 6163 740a 2020 2020 2020 2e2e 2e20 2020  act.      ...   
-000102a0: 6465 6620 5f5f 6361 6c6c 5f5f 2873 656c  def __call__(sel
-000102b0: 662c 2078 293a 0a20 2020 2020 202e 2e2e  f, x):.      ...
-000102c0: 2020 2020 202e 2e2e 0a20 2020 2020 202e       ....      .
-000102d0: 2e2e 2020 2020 2072 6574 7572 6e20 6e6e  ..     return nn
-000102e0: 2e44 656e 7365 2837 3834 2928 7829 0a0a  .Dense(784)(x)..
-000102f0: 2020 2020 2020 3e3e 3e20 636c 6173 7320        >>> class 
-00010300: 4175 746f 456e 636f 6465 7228 6e6e 2e4d  AutoEncoder(nn.M
-00010310: 6f64 756c 6529 3a0a 2020 2020 2020 2e2e  odule):.      ..
-00010320: 2e20 2020 6465 6620 7365 7475 7028 7365  .   def setup(se
-00010330: 6c66 293a 0a20 2020 2020 202e 2e2e 2020  lf):.      ...  
-00010340: 2020 2073 656c 662e 656e 636f 6465 7220     self.encoder 
-00010350: 3d20 456e 636f 6465 7228 290a 2020 2020  = Encoder().    
-00010360: 2020 2e2e 2e20 2020 2020 7365 6c66 2e64    ...     self.d
-00010370: 6563 6f64 6572 203d 2044 6563 6f64 6572  ecoder = Decoder
-00010380: 2829 0a20 2020 2020 202e 2e2e 0a20 2020  ().      ....   
-00010390: 2020 202e 2e2e 2020 2064 6566 205f 5f63     ...   def __c
-000103a0: 616c 6c5f 5f28 7365 6c66 2c20 7829 3a0a  all__(self, x):.
-000103b0: 2020 2020 2020 2e2e 2e20 2020 2020 7265        ...     re
-000103c0: 7475 726e 2073 656c 662e 6465 636f 6465  turn self.decode
-000103d0: 7228 7365 6c66 2e65 6e63 6f64 6572 2878  r(self.encoder(x
-000103e0: 2929 0a0a 2020 2020 2020 3e3e 3e20 6d6f  ))..      >>> mo
-000103f0: 6475 6c65 203d 2041 7574 6f45 6e63 6f64  dule = AutoEncod
-00010400: 6572 2829 0a20 2020 2020 203e 3e3e 2076  er().      >>> v
-00010410: 6172 6961 626c 6573 203d 206d 6f64 756c  ariables = modul
-00010420: 652e 696e 6974 286a 6178 2e72 616e 646f  e.init(jax.rando
-00010430: 6d2e 6b65 7928 3029 2c20 6a6e 702e 6f6e  m.key(0), jnp.on
-00010440: 6573 2828 312c 2037 3834 2929 290a 0a20  es((1, 784))).. 
-00010450: 2020 2020 203e 3e3e 2023 2045 7874 7261       >>> # Extra
-00010460: 6374 2074 6865 2045 6e63 6f64 6572 2073  ct the Encoder s
-00010470: 7562 2d4d 6f64 756c 6520 616e 6420 6974  ub-Module and it
-00010480: 7320 7661 7269 6162 6c65 730a 2020 2020  s variables.    
-00010490: 2020 3e3e 3e20 656e 636f 6465 722c 2065    >>> encoder, e
-000104a0: 6e63 6f64 6572 5f76 6172 7320 3d20 6d6f  ncoder_vars = mo
-000104b0: 6475 6c65 2e62 696e 6428 7661 7269 6162  dule.bind(variab
-000104c0: 6c65 7329 2e65 6e63 6f64 6572 2e75 6e62  les).encoder.unb
-000104d0: 696e 6428 290a 0a20 2020 2052 6574 7572  ind()..    Retur
-000104e0: 6e73 3a0a 2020 2020 2020 4120 7475 706c  ns:.      A tupl
-000104f0: 6520 7769 7468 2061 6e20 756e 626f 756e  e with an unboun
-00010500: 6420 636f 7079 206f 6620 7468 6973 204d  d copy of this M
-00010510: 6f64 756c 6520 616e 6420 6974 7320 7661  odule and its va
-00010520: 7269 6162 6c65 732e 0a20 2020 2022 2222  riables..    """
-00010530: 0a20 2020 204d 6f64 756c 652e 5f6d 6f64  .    Module._mod
-00010540: 756c 655f 6368 6563 6b73 2873 656c 6629  ule_checks(self)
-00010550: 0a0a 2020 2020 6966 2073 656c 662e 7363  ..    if self.sc
-00010560: 6f70 6520 6973 204e 6f6e 653a 0a20 2020  ope is None:.   
-00010570: 2020 2072 6169 7365 2065 7272 6f72 732e     raise errors.
-00010580: 4361 6c6c 556e 6269 6e64 4f6e 556e 626f  CallUnbindOnUnbo
-00010590: 756e 644d 6f64 756c 6545 7272 6f72 2829  undModuleError()
-000105a0: 0a0a 2020 2020 7661 7269 6162 6c65 7320  ..    variables 
-000105b0: 3d20 7365 6c66 2e76 6172 6961 626c 6573  = self.variables
-000105c0: 0a20 2020 206d 6f64 756c 6520 3d20 7365  .    module = se
-000105d0: 6c66 2e63 6c6f 6e65 285f 6465 6570 5f63  lf.clone(_deep_c
-000105e0: 6c6f 6e65 3d54 7275 652c 205f 7265 7365  lone=True, _rese
-000105f0: 745f 6e61 6d65 733d 5472 7565 2c20 6e61  t_names=True, na
-00010600: 6d65 3d4e 6f6e 6529 0a20 2020 2072 6574  me=None).    ret
-00010610: 7572 6e20 6d6f 6475 6c65 2c20 7661 7269  urn module, vari
-00010620: 6162 6c65 730a 0a20 2040 7472 6163 6562  ables..  @traceb
-00010630: 6163 6b5f 7574 696c 2e61 7069 5f62 6f75  ack_util.api_bou
-00010640: 6e64 6172 790a 2020 6465 6620 6170 706c  ndary.  def appl
-00010650: 7928 0a20 2020 2073 656c 662c 0a20 2020  y(.    self,.   
-00010660: 2076 6172 6961 626c 6573 3a20 5661 7269   variables: Vari
-00010670: 6162 6c65 4469 6374 2c0a 2020 2020 2a61  ableDict,.    *a
-00010680: 7267 732c 0a20 2020 2072 6e67 733a 204f  rgs,.    rngs: O
-00010690: 7074 696f 6e61 6c5b 556e 696f 6e5b 5052  ptional[Union[PR
-000106a0: 4e47 4b65 792c 2052 4e47 5365 7175 656e  NGKey, RNGSequen
-000106b0: 6365 735d 5d20 3d20 4e6f 6e65 2c0a 2020  ces]] = None,.  
-000106c0: 2020 6d65 7468 6f64 3a20 556e 696f 6e5b    method: Union[
-000106d0: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
-000106e0: 795d 2c20 7374 722c 204e 6f6e 655d 203d  y], str, None] =
-000106f0: 204e 6f6e 652c 0a20 2020 206d 7574 6162   None,.    mutab
-00010700: 6c65 3a20 436f 6c6c 6563 7469 6f6e 4669  le: CollectionFi
-00010710: 6c74 6572 203d 2046 616c 7365 2c0a 2020  lter = False,.  
-00010720: 2020 6361 7074 7572 655f 696e 7465 726d    capture_interm
-00010730: 6564 6961 7465 733a 2055 6e69 6f6e 5b62  ediates: Union[b
-00010740: 6f6f 6c2c 2043 616c 6c61 626c 655b 5b27  ool, Callable[['
-00010750: 4d6f 6475 6c65 272c 2073 7472 5d2c 2062  Module', str], b
-00010760: 6f6f 6c5d 5d20 3d20 4661 6c73 652c 0a20  ool]] = False,. 
-00010770: 2020 202a 2a6b 7761 7267 732c 0a20 2029     **kwargs,.  )
-00010780: 202d 3e20 556e 696f 6e5b 416e 792c 2054   -> Union[Any, T
-00010790: 7570 6c65 5b41 6e79 2c20 556e 696f 6e5b  uple[Any, Union[
-000107a0: 4672 6f7a 656e 5661 7269 6162 6c65 4469  FrozenVariableDi
-000107b0: 6374 2c20 4469 6374 5b73 7472 2c20 416e  ct, Dict[str, An
-000107c0: 795d 5d5d 5d3a 0a20 2020 2022 2222 4170  y]]]]:.    """Ap
-000107d0: 706c 6965 7320 6120 6d6f 6475 6c65 206d  plies a module m
-000107e0: 6574 686f 6420 746f 2076 6172 6961 626c  ethod to variabl
-000107f0: 6573 2061 6e64 2072 6574 7572 6e73 206f  es and returns o
-00010800: 7574 7075 7420 616e 6420 6d6f 6469 6669  utput and modifi
-00010810: 6564 2076 6172 6961 626c 6573 2e0a 0a20  ed variables... 
-00010820: 2020 204e 6f74 6520 7468 6174 2060 606d     Note that ``m
-00010830: 6574 686f 6460 6020 7368 6f75 6c64 2062  ethod`` should b
-00010840: 6520 7365 7420 6966 206f 6e65 2077 6f75  e set if one wou
-00010850: 6c64 206c 696b 6520 746f 2063 616c 6c20  ld like to call 
-00010860: 6060 6170 706c 7960 6020 6f6e 2061 0a20  ``apply`` on a. 
-00010870: 2020 2064 6966 6665 7265 6e74 2063 6c61     different cla
-00010880: 7373 206d 6574 686f 6420 7468 616e 2060  ss method than `
-00010890: 605f 5f63 616c 6c5f 5f60 602e 2046 6f72  `__call__``. For
-000108a0: 2069 6e73 7461 6e63 652c 2073 7570 706f   instance, suppo
-000108b0: 7365 2061 0a20 2020 2054 7261 6e73 666f  se a.    Transfo
-000108c0: 726d 6572 206d 6f64 756c 6573 2068 6173  rmer modules has
-000108d0: 2061 206d 6574 686f 6420 6361 6c6c 6564   a method called
-000108e0: 2060 6065 6e63 6f64 6560 602c 2074 6865   ``encode``, the
-000108f0: 6e20 7468 6520 666f 6c6c 6f77 696e 6720  n the following 
-00010900: 6361 6c6c 730a 2020 2020 6060 6170 706c  calls.    ``appl
-00010910: 7960 6020 6f6e 2074 6861 7420 6d65 7468  y`` on that meth
-00010920: 6f64 3a3a 0a0a 2020 2020 2020 3e3e 3e20  od::..      >>> 
-00010930: 636c 6173 7320 5472 616e 7366 6f72 6d65  class Transforme
-00010940: 7228 6e6e 2e4d 6f64 756c 6529 3a0a 2020  r(nn.Module):.  
-00010950: 2020 2020 2e2e 2e20 2020 6465 6620 656e      ...   def en
-00010960: 636f 6465 2873 656c 662c 2078 293a 0a20  code(self, x):. 
-00010970: 2020 2020 202e 2e2e 2020 2020 202e 2e2e       ...     ...
-00010980: 0a0a 2020 2020 2020 3e3e 3e20 7820 3d20  ..      >>> x = 
-00010990: 6a6e 702e 6f6e 6573 2828 3136 2c20 3929  jnp.ones((16, 9)
-000109a0: 290a 2020 2020 2020 3e3e 3e20 6d6f 6465  ).      >>> mode
-000109b0: 6c20 3d20 5472 616e 7366 6f72 6d65 7228  l = Transformer(
-000109c0: 290a 2020 2020 2020 3e3e 3e20 7661 7269  ).      >>> vari
-000109d0: 6162 6c65 7320 3d20 6d6f 6465 6c2e 696e  ables = model.in
-000109e0: 6974 286a 6178 2e72 616e 646f 6d2e 6b65  it(jax.random.ke
-000109f0: 7928 3029 2c20 782c 206d 6574 686f 643d  y(0), x, method=
-00010a00: 5472 616e 7366 6f72 6d65 722e 656e 636f  Transformer.enco
-00010a10: 6465 290a 0a20 2020 2020 203e 3e3e 2065  de)..      >>> e
-00010a20: 6e63 6f64 6564 203d 206d 6f64 656c 2e61  ncoded = model.a
-00010a30: 7070 6c79 2876 6172 6961 626c 6573 2c20  pply(variables, 
-00010a40: 782c 206d 6574 686f 643d 5472 616e 7366  x, method=Transf
-00010a50: 6f72 6d65 722e 656e 636f 6465 290a 0a20  ormer.encode).. 
-00010a60: 2020 2049 6620 6120 6675 6e63 7469 6f6e     If a function
-00010a70: 2069 6e73 7461 6e63 6520 6973 2070 726f   instance is pro
-00010a80: 7669 6465 642c 2074 6865 2075 6e62 6f75  vided, the unbou
-00010a90: 6e64 2066 756e 6374 696f 6e20 6973 2075  nd function is u
-00010aa0: 7365 642e 2046 6f72 0a20 2020 2069 6e73  sed. For.    ins
-00010ab0: 7461 6e63 652c 2074 6865 2065 7861 6d70  tance, the examp
-00010ac0: 6c65 2062 656c 6f77 2069 7320 6571 7569  le below is equi
-00010ad0: 7661 6c65 6e74 2074 6f20 7468 6520 6f6e  valent to the on
-00010ae0: 6520 6162 6f76 653a 3a0a 0a20 2020 2020  e above::..     
-00010af0: 203e 3e3e 2065 6e63 6f64 6564 203d 206d   >>> encoded = m
-00010b00: 6f64 656c 2e61 7070 6c79 2876 6172 6961  odel.apply(varia
-00010b10: 626c 6573 2c20 782c 206d 6574 686f 643d  bles, x, method=
-00010b20: 6d6f 6465 6c2e 656e 636f 6465 290a 0a20  model.encode).. 
-00010b30: 2020 2059 6f75 2063 616e 2061 6c73 6f20     You can also 
-00010b40: 7061 7373 2061 2073 7472 696e 6720 746f  pass a string to
-00010b50: 2061 2063 616c 6c61 626c 6520 6174 7472   a callable attr
-00010b60: 6962 7574 6520 6f66 2074 6865 206d 6f64  ibute of the mod
-00010b70: 756c 652e 2046 6f72 0a20 2020 2065 7861  ule. For.    exa
-00010b80: 6d70 6c65 2c20 7468 6520 7072 6576 696f  mple, the previo
-00010b90: 7573 2063 616e 2062 6520 7772 6974 7465  us can be writte
-00010ba0: 6e20 6173 3a3a 0a0a 2020 2020 2020 3e3e  n as::..      >>
-00010bb0: 3e20 656e 636f 6465 6420 3d20 6d6f 6465  > encoded = mode
-00010bc0: 6c2e 6170 706c 7928 7661 7269 6162 6c65  l.apply(variable
-00010bd0: 732c 2078 2c20 6d65 7468 6f64 3d27 656e  s, x, method='en
-00010be0: 636f 6465 2729 0a0a 2020 2020 4e6f 7465  code')..    Note
-00010bf0: 2060 606d 6574 686f 6460 6020 6361 6e20   ``method`` can 
-00010c00: 616c 736f 2062 6520 6120 6675 6e63 7469  also be a functi
-00010c10: 6f6e 2074 6861 7420 6973 206e 6f74 2064  on that is not d
-00010c20: 6566 696e 6564 2069 6e0a 2020 2020 6060  efined in.    ``
-00010c30: 5472 616e 7366 6f72 6d65 7260 602e 2049  Transformer``. I
-00010c40: 6e20 7468 6174 2063 6173 652c 2074 6865  n that case, the
-00010c50: 2066 756e 6374 696f 6e20 7368 6f75 6c64   function should
-00010c60: 2068 6176 6520 6174 206c 6561 7374 206f   have at least o
-00010c70: 6e65 0a20 2020 2061 7267 756d 656e 7420  ne.    argument 
-00010c80: 7265 7072 6573 656e 7469 6e67 2061 6e20  representing an 
-00010c90: 696e 7374 616e 6365 206f 6620 7468 6520  instance of the 
-00010ca0: 4d6f 6475 6c65 2063 6c61 7373 3a3a 0a0a  Module class::..
-00010cb0: 2020 2020 2020 3e3e 3e20 6465 6620 6f74        >>> def ot
-00010cc0: 6865 725f 666e 2869 6e73 7461 6e63 652c  her_fn(instance,
-00010cd0: 2078 293a 0a20 2020 2020 202e 2e2e 2020   x):.      ...  
-00010ce0: 2023 2069 6e73 7461 6e63 652e 736f 6d65   # instance.some
-00010cf0: 5f6d 6f64 756c 655f 6174 7472 282e 2e2e  _module_attr(...
-00010d00: 290a 2020 2020 2020 2e2e 2e20 2020 696e  ).      ...   in
-00010d10: 7374 616e 6365 2e65 6e63 6f64 650a 2020  stance.encode.  
-00010d20: 2020 2020 2e2e 2e20 2020 2e2e 2e0a 0a20      ...   ..... 
-00010d30: 2020 2020 203e 3e3e 206d 6f64 656c 2e61       >>> model.a
-00010d40: 7070 6c79 2876 6172 6961 626c 6573 2c20  pply(variables, 
-00010d50: 782c 206d 6574 686f 643d 6f74 6865 725f  x, method=other_
-00010d60: 666e 290a 0a20 2020 2041 7267 733a 0a20  fn)..    Args:. 
-00010d70: 2020 2020 2076 6172 6961 626c 6573 3a20       variables: 
-00010d80: 4120 6469 6374 696f 6e61 7279 2063 6f6e  A dictionary con
-00010d90: 7461 696e 696e 6720 7661 7269 6162 6c65  taining variable
-00010da0: 7320 6b65 7965 6420 6279 2076 6172 6961  s keyed by varia
-00010db0: 626c 650a 2020 2020 2020 2020 636f 6c6c  ble.        coll
-00010dc0: 6563 7469 6f6e 732e 2053 6565 203a 6d6f  ections. See :mo
-00010dd0: 643a 6066 6c61 782e 636f 7265 2e76 6172  d:`flax.core.var
-00010de0: 6961 626c 6573 6020 666f 7220 6d6f 7265  iables` for more
-00010df0: 2064 6574 6169 6c73 2061 626f 7574 0a20   details about. 
-00010e00: 2020 2020 2020 2076 6172 6961 626c 6573         variables
-00010e10: 2e0a 2020 2020 2020 2a61 7267 733a 204e  ..      *args: N
-00010e20: 616d 6564 2061 7267 756d 656e 7473 2070  amed arguments p
-00010e30: 6173 7365 6420 746f 2074 6865 2073 7065  assed to the spe
-00010e40: 6369 6669 6564 2061 7070 6c79 206d 6574  cified apply met
-00010e50: 686f 642e 0a20 2020 2020 2072 6e67 733a  hod..      rngs:
-00010e60: 2061 2064 6963 7420 6f66 2050 524e 474b   a dict of PRNGK
-00010e70: 6579 7320 746f 2069 6e69 7469 616c 697a  eys to initializ
-00010e80: 6520 7468 6520 5052 4e47 2073 6571 7565  e the PRNG seque
-00010e90: 6e63 6573 2e20 5468 6520 2270 6172 616d  nces. The "param
-00010ea0: 7322 0a20 2020 2020 2020 2050 524e 4720  s".        PRNG 
-00010eb0: 7365 7175 656e 6365 2069 7320 7573 6564  sequence is used
-00010ec0: 2074 6f20 696e 6974 6961 6c69 7a65 2070   to initialize p
-00010ed0: 6172 616d 6574 6572 732e 0a20 2020 2020  arameters..     
-00010ee0: 206d 6574 686f 643a 2041 2066 756e 6374   method: A funct
-00010ef0: 696f 6e20 746f 2063 616c 6c20 6170 706c  ion to call appl
-00010f00: 7920 6f6e 2e20 5468 6973 2069 7320 6765  y on. This is ge
-00010f10: 6e65 7261 6c6c 7920 6120 6675 6e63 7469  nerally a functi
-00010f20: 6f6e 2069 6e20 7468 650a 2020 2020 2020  on in the.      
-00010f30: 2020 6d6f 6475 6c65 2e20 4966 2070 726f    module. If pro
-00010f40: 7669 6465 642c 2061 7070 6c69 6573 2074  vided, applies t
-00010f50: 6869 7320 6d65 7468 6f64 2e20 4966 206e  his method. If n
-00010f60: 6f74 2070 726f 7669 6465 642c 2061 7070  ot provided, app
-00010f70: 6c69 6573 2074 6865 0a20 2020 2020 2020  lies the.       
-00010f80: 2060 605f 5f63 616c 6c5f 5f60 6020 6d65   ``__call__`` me
-00010f90: 7468 6f64 206f 6620 7468 6520 6d6f 6475  thod of the modu
-00010fa0: 6c65 2e20 4120 7374 7269 6e67 2063 616e  le. A string can
-00010fb0: 2061 6c73 6f20 6265 2070 726f 7669 6465   also be provide
-00010fc0: 6420 746f 0a20 2020 2020 2020 2073 7065  d to.        spe
-00010fd0: 6369 6679 2061 206d 6574 686f 6420 6279  cify a method by
-00010fe0: 206e 616d 652e 0a20 2020 2020 206d 7574   name..      mut
-00010ff0: 6162 6c65 3a20 4361 6e20 6265 2062 6f6f  able: Can be boo
-00011000: 6c2c 2073 7472 2c20 6f72 206c 6973 742e  l, str, or list.
-00011010: 2053 7065 6369 6669 6573 2077 6869 6368   Specifies which
-00011020: 2063 6f6c 6c65 6374 696f 6e73 2073 686f   collections sho
-00011030: 756c 6420 6265 0a20 2020 2020 2020 2074  uld be.        t
-00011040: 7265 6174 6564 2061 7320 6d75 7461 626c  reated as mutabl
-00011050: 653a 2060 6062 6f6f 6c60 603a 2061 6c6c  e: ``bool``: all
-00011060: 2f6e 6f20 636f 6c6c 6563 7469 6f6e 7320  /no collections 
-00011070: 6172 6520 6d75 7461 626c 652e 2060 6073  are mutable. ``s
-00011080: 7472 6060 3a0a 2020 2020 2020 2020 5468  tr``:.        Th
-00011090: 6520 6e61 6d65 206f 6620 6120 7369 6e67  e name of a sing
-000110a0: 6c65 206d 7574 6162 6c65 2063 6f6c 6c65  le mutable colle
-000110b0: 6374 696f 6e2e 2060 606c 6973 7460 603a  ction. ``list``:
-000110c0: 2041 206c 6973 7420 6f66 206e 616d 6573   A list of names
-000110d0: 206f 660a 2020 2020 2020 2020 6d75 7461   of.        muta
-000110e0: 626c 6520 636f 6c6c 6563 7469 6f6e 732e  ble collections.
-000110f0: 0a20 2020 2020 2063 6170 7475 7265 5f69  .      capture_i
-00011100: 6e74 6572 6d65 6469 6174 6573 3a20 4966  ntermediates: If
-00011110: 2060 6054 7275 6560 602c 2063 6170 7475   ``True``, captu
-00011120: 7265 7320 696e 7465 726d 6564 6961 7465  res intermediate
-00011130: 2072 6574 7572 6e20 7661 6c75 6573 206f   return values o
-00011140: 660a 2020 2020 2020 2020 616c 6c20 4d6f  f.        all Mo
-00011150: 6475 6c65 7320 696e 7369 6465 2074 6865  dules inside the
-00011160: 2022 696e 7465 726d 6564 6961 7465 7322   "intermediates"
-00011170: 2063 6f6c 6c65 6374 696f 6e2e 2042 7920   collection. By 
-00011180: 6465 6661 756c 742c 206f 6e6c 7920 7468  default, only th
-00011190: 650a 2020 2020 2020 2020 7265 7475 726e  e.        return
-000111a0: 2076 616c 7565 7320 6f66 2061 6c6c 2060   values of all `
-000111b0: 605f 5f63 616c 6c5f 5f60 6020 6d65 7468  `__call__`` meth
-000111c0: 6f64 7320 6172 6520 7374 6f72 6564 2e20  ods are stored. 
-000111d0: 4120 6675 6e63 7469 6f6e 2063 616e 2062  A function can b
-000111e0: 650a 2020 2020 2020 2020 7061 7373 6564  e.        passed
-000111f0: 2074 6f20 6368 616e 6765 2074 6865 2066   to change the f
-00011200: 696c 7465 7220 6265 6861 7669 6f72 2e20  ilter behavior. 
-00011210: 5468 6520 6669 6c74 6572 2066 756e 6374  The filter funct
-00011220: 696f 6e20 7461 6b65 7320 7468 650a 2020  ion takes the.  
-00011230: 2020 2020 2020 4d6f 6475 6c65 2069 6e73        Module ins
-00011240: 7461 6e63 6520 616e 6420 6d65 7468 6f64  tance and method
-00011250: 206e 616d 6520 616e 6420 7265 7475 726e   name and return
-00011260: 7320 6120 626f 6f6c 2069 6e64 6963 6174  s a bool indicat
-00011270: 696e 6720 7768 6574 6865 720a 2020 2020  ing whether.    
-00011280: 2020 2020 7468 6520 6f75 7470 7574 206f      the output o
-00011290: 6620 7468 6174 206d 6574 686f 6420 696e  f that method in
-000112a0: 766f 6361 7469 6f6e 2073 686f 756c 6420  vocation should 
-000112b0: 6265 2073 746f 7265 642e 0a20 2020 2020  be stored..     
-000112c0: 202a 2a6b 7761 7267 733a 204b 6579 776f   **kwargs: Keywo
-000112d0: 7264 2061 7267 756d 656e 7473 2070 6173  rd arguments pas
-000112e0: 7365 6420 746f 2074 6865 2073 7065 6369  sed to the speci
-000112f0: 6669 6564 2061 7070 6c79 206d 6574 686f  fied apply metho
-00011300: 642e 0a0a 2020 2020 5265 7475 726e 733a  d...    Returns:
-00011310: 0a20 2020 2020 2049 6620 6060 6d75 7461  .      If ``muta
-00011320: 626c 6560 6020 6973 2046 616c 7365 2c20  ble`` is False, 
-00011330: 7265 7475 726e 7320 6f75 7470 7574 2e20  returns output. 
-00011340: 4966 2061 6e79 2063 6f6c 6c65 6374 696f  If any collectio
-00011350: 6e73 2061 7265 0a20 2020 2020 206d 7574  ns are.      mut
-00011360: 6162 6c65 2c20 7265 7475 726e 7320 6060  able, returns ``
-00011370: 286f 7574 7075 742c 2076 6172 7329 6060  (output, vars)``
-00011380: 2c20 7768 6572 6520 6060 7661 7273 6060  , where ``vars``
-00011390: 2061 7265 2069 7320 6120 6469 6374 0a20   are is a dict. 
-000113a0: 2020 2020 206f 6620 7468 6520 6d6f 6469       of the modi
-000113b0: 6669 6564 2063 6f6c 6c65 6374 696f 6e73  fied collections
-000113c0: 2e0a 2020 2020 2222 220a 2020 2020 4d6f  ..    """.    Mo
-000113d0: 6475 6c65 2e5f 6d6f 6475 6c65 5f63 6865  dule._module_che
-000113e0: 636b 7328 7365 6c66 290a 0a20 2020 2069  cks(self)..    i
-000113f0: 6620 726e 6773 2069 7320 6e6f 7420 4e6f  f rngs is not No
-00011400: 6e65 2061 6e64 206e 6f74 2069 7369 6e73  ne and not isins
-00011410: 7461 6e63 6528 726e 6773 2c20 6469 6374  tance(rngs, dict
-00011420: 293a 0a20 2020 2020 2069 6620 6e6f 7420  ):.      if not 
-00011430: 636f 7265 2e73 636f 7065 2e5f 6973 5f76  core.scope._is_v
-00011440: 616c 6964 5f72 6e67 2872 6e67 7329 3a0a  alid_rng(rngs):.
-00011450: 2020 2020 2020 2020 7261 6973 6520 6572          raise er
-00011460: 726f 7273 2e49 6e76 616c 6964 526e 6745  rors.InvalidRngE
-00011470: 7272 6f72 280a 2020 2020 2020 2020 2020  rror(.          
-00011480: 2752 4e47 7320 7368 6f75 6c64 2062 6520  'RNGs should be 
-00011490: 6f66 2073 6861 7065 2028 322c 2920 6f72  of shape (2,) or
-000114a0: 2050 524e 474b 6579 2069 6e20 4d6f 6475   PRNGKey in Modu
-000114b0: 6c65 2027 0a20 2020 2020 2020 2020 2066  le '.          f
-000114c0: 277b 7365 6c66 2e5f 5f63 6c61 7373 5f5f  '{self.__class__
-000114d0: 2e5f 5f6e 616d 655f 5f7d 2c20 6275 7420  .__name__}, but 
-000114e0: 726e 6773 2061 7265 3a20 7b72 6e67 737d  rngs are: {rngs}
-000114f0: 270a 2020 2020 2020 2020 290a 2020 2020  '.        ).    
-00011500: 2020 726e 6773 203d 207b 2770 6172 616d    rngs = {'param
-00011510: 7327 3a20 726e 6773 7d0a 0a20 2020 2069  s': rngs}..    i
-00011520: 6620 6973 696e 7374 616e 6365 286d 6574  f isinstance(met
-00011530: 686f 642c 2073 7472 293a 0a20 2020 2020  hod, str):.     
-00011540: 2061 7474 7269 6275 7465 5f6e 616d 6520   attribute_name 
-00011550: 3d20 6d65 7468 6f64 0a20 2020 2020 206d  = method.      m
-00011560: 6574 686f 6420 3d20 6765 7461 7474 7228  ethod = getattr(
-00011570: 7365 6c66 2c20 6174 7472 6962 7574 655f  self, attribute_
-00011580: 6e61 6d65 290a 2020 2020 2020 6966 206e  name).      if n
-00011590: 6f74 2063 616c 6c61 626c 6528 6d65 7468  ot callable(meth
-000115a0: 6f64 293a 0a20 2020 2020 2020 2063 6c61  od):.        cla
-000115b0: 7373 5f6e 616d 6520 3d20 7479 7065 2873  ss_name = type(s
-000115c0: 656c 6629 2e5f 5f6e 616d 655f 5f0a 2020  elf).__name__.  
-000115d0: 2020 2020 2020 7261 6973 6520 5479 7065        raise Type
-000115e0: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         
-000115f0: 2066 2227 7b63 6c61 7373 5f6e 616d 657d   f"'{class_name}
-00011600: 2e7b 6174 7472 6962 7574 655f 6e61 6d65  .{attribute_name
-00011610: 7d27 206d 7573 7420 6265 2061 2063 616c  }' must be a cal
-00011620: 6c61 626c 652c 2067 6f74 220a 2020 2020  lable, got".    
-00011630: 2020 2020 2020 6627 207b 7479 7065 286d        f' {type(m
-00011640: 6574 686f 6429 7d2e 270a 2020 2020 2020  ethod)}.'.      
-00011650: 2020 290a 2020 2020 2020 2320 6966 2074    ).      # if t
-00011660: 6865 2060 6d65 7468 6f64 6020 7374 7269  he `method` stri
-00011670: 6e67 2069 7320 6120 7375 626d 6f64 756c  ng is a submodul
-00011680: 652c 2077 6520 6372 6561 7465 2061 206c  e, we create a l
-00011690: 616d 6264 6120 6675 6e63 7469 6f6e 0a20  ambda function. 
-000116a0: 2020 2020 2023 2074 6861 7420 6361 6c6c       # that call
-000116b0: 7320 7468 6520 7375 626d 6f64 756c 652c  s the submodule,
-000116c0: 2066 6f72 7761 7264 696e 6720 616c 6c20   forwarding all 
-000116d0: 6172 6775 6d65 6e74 732e 0a20 2020 2020  arguments..     
-000116e0: 2069 6620 6973 696e 7374 616e 6365 286d   if isinstance(m
-000116f0: 6574 686f 642c 204d 6f64 756c 6529 3a0a  ethod, Module):.
-00011700: 2020 2020 2020 2020 6d65 7468 6f64 203d          method =
-00011710: 206c 616d 6264 6120 7365 6c66 2c20 2a61   lambda self, *a
-00011720: 7267 732c 202a 2a6b 7761 7267 733a 2067  rgs, **kwargs: g
-00011730: 6574 6174 7472 2873 656c 662c 2061 7474  etattr(self, att
-00011740: 7269 6275 7465 5f6e 616d 6529 280a 2020  ribute_name)(.  
-00011750: 2020 2020 2020 2020 2a61 7267 732c 202a          *args, *
-00011760: 2a6b 7761 7267 730a 2020 2020 2020 2020  *kwargs.        
-00011770: 290a 2020 2020 656c 6966 206d 6574 686f  ).    elif metho
-00011780: 6420 6973 204e 6f6e 653a 0a20 2020 2020  d is None:.     
-00011790: 206d 6574 686f 6420 3d20 7365 6c66 2e5f   method = self._
-000117a0: 5f63 616c 6c5f 5f0a 2020 2020 6d65 7468  _call__.    meth
-000117b0: 6f64 203d 205f 6765 745f 756e 626f 756e  od = _get_unboun
-000117c0: 645f 666e 286d 6574 686f 6429 0a20 2020  d_fn(method).   
-000117d0: 2072 6574 7572 6e20 6170 706c 7928 0a20   return apply(. 
-000117e0: 2020 2020 206d 6574 686f 642c 0a20 2020       method,.   
-000117f0: 2020 2073 656c 662c 0a20 2020 2020 206d     self,.      m
-00011800: 7574 6162 6c65 3d6d 7574 6162 6c65 2c0a  utable=mutable,.
-00011810: 2020 2020 2020 6361 7074 7572 655f 696e        capture_in
-00011820: 7465 726d 6564 6961 7465 733d 6361 7074  termediates=capt
-00011830: 7572 655f 696e 7465 726d 6564 6961 7465  ure_intermediate
-00011840: 732c 0a20 2020 2029 2876 6172 6961 626c  s,.    )(variabl
-00011850: 6573 2c20 2a61 7267 732c 202a 2a6b 7761  es, *args, **kwa
-00011860: 7267 732c 2072 6e67 733d 726e 6773 290a  rgs, rngs=rngs).
-00011870: 0a20 2040 7472 6163 6562 6163 6b5f 7574  .  @traceback_ut
-00011880: 696c 2e61 7069 5f62 6f75 6e64 6172 790a  il.api_boundary.
-00011890: 2020 6465 6620 696e 6974 5f77 6974 685f    def init_with_
-000118a0: 6f75 7470 7574 280a 2020 2020 7365 6c66  output(.    self
-000118b0: 2c0a 2020 2020 726e 6773 3a20 556e 696f  ,.    rngs: Unio
-000118c0: 6e5b 5052 4e47 4b65 792c 2052 4e47 5365  n[PRNGKey, RNGSe
-000118d0: 7175 656e 6365 735d 2c0a 2020 2020 2a61  quences],.    *a
-000118e0: 7267 732c 0a20 2020 206d 6574 686f 643a  rgs,.    method:
-000118f0: 2055 6e69 6f6e 5b43 616c 6c61 626c 655b   Union[Callable[
-00011900: 2e2e 2e2c 2041 6e79 5d2c 2073 7472 2c20  ..., Any], str, 
-00011910: 4e6f 6e65 5d20 3d20 4e6f 6e65 2c0a 2020  None] = None,.  
-00011920: 2020 6d75 7461 626c 653a 2043 6f6c 6c65    mutable: Colle
-00011930: 6374 696f 6e46 696c 7465 7220 3d20 4465  ctionFilter = De
-00011940: 6e79 4c69 7374 2827 696e 7465 726d 6564  nyList('intermed
-00011950: 6961 7465 7327 292c 0a20 2020 2063 6170  iates'),.    cap
-00011960: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
-00011970: 6573 3a20 556e 696f 6e5b 626f 6f6c 2c20  es: Union[bool, 
-00011980: 4361 6c6c 6162 6c65 5b5b 274d 6f64 756c  Callable[['Modul
-00011990: 6527 2c20 7374 725d 2c20 626f 6f6c 5d5d  e', str], bool]]
-000119a0: 203d 2046 616c 7365 2c0a 2020 2020 2a2a   = False,.    **
-000119b0: 6b77 6172 6773 2c0a 2020 2920 2d3e 2054  kwargs,.  ) -> T
-000119c0: 7570 6c65 5b41 6e79 2c20 556e 696f 6e5b  uple[Any, Union[
-000119d0: 4672 6f7a 656e 5661 7269 6162 6c65 4469  FrozenVariableDi
-000119e0: 6374 2c20 4469 6374 5b73 7472 2c20 416e  ct, Dict[str, An
-000119f0: 795d 5d5d 3a0a 2020 2020 2222 2249 6e69  y]]]:.    """Ini
-00011a00: 7469 616c 697a 6573 2061 206d 6f64 756c  tializes a modul
-00011a10: 6520 6d65 7468 6f64 2077 6974 6820 7661  e method with va
-00011a20: 7269 6162 6c65 7320 616e 6420 7265 7475  riables and retu
-00011a30: 726e 7320 6f75 7470 7574 2061 6e64 206d  rns output and m
-00011a40: 6f64 6966 6965 6420 7661 7269 6162 6c65  odified variable
-00011a50: 732e 0a0a 2020 2020 4172 6773 3a0a 2020  s...    Args:.  
-00011a60: 2020 2020 726e 6773 3a20 5468 6520 726e      rngs: The rn
-00011a70: 6773 2066 6f72 2074 6865 2076 6172 6961  gs for the varia
-00011a80: 626c 6520 636f 6c6c 6563 7469 6f6e 732e  ble collections.
-00011a90: 0a20 2020 2020 202a 6172 6773 3a20 4e61  .      *args: Na
-00011aa0: 6d65 6420 6172 6775 6d65 6e74 7320 7061  med arguments pa
-00011ab0: 7373 6564 2074 6f20 7468 6520 696e 6974  ssed to the init
-00011ac0: 2066 756e 6374 696f 6e2e 0a20 2020 2020   function..     
-00011ad0: 206d 6574 686f 643a 2041 6e20 6f70 7469   method: An opti
-00011ae0: 6f6e 616c 206d 6574 686f 642e 2049 6620  onal method. If 
-00011af0: 7072 6f76 6964 6564 2c20 6170 706c 6965  provided, applie
-00011b00: 7320 7468 6973 206d 6574 686f 642e 2049  s this method. I
-00011b10: 6620 6e6f 740a 2020 2020 2020 2020 7072  f not.        pr
-00011b20: 6f76 6964 6564 2c20 6170 706c 6965 7320  ovided, applies 
-00011b30: 7468 6520 6060 5f5f 6361 6c6c 5f5f 6060  the ``__call__``
-00011b40: 206d 6574 686f 642e 2041 2073 7472 696e   method. A strin
-00011b50: 6720 6361 6e20 616c 736f 2062 650a 2020  g can also be.  
-00011b60: 2020 2020 2020 7072 6f76 6964 6564 2074        provided t
-00011b70: 6f20 7370 6563 6966 7920 6120 6d65 7468  o specify a meth
-00011b80: 6f64 2062 7920 6e61 6d65 2e0a 2020 2020  od by name..    
-00011b90: 2020 6d75 7461 626c 653a 2043 616e 2062    mutable: Can b
-00011ba0: 6520 626f 6f6c 2c20 7374 722c 206f 7220  e bool, str, or 
-00011bb0: 6c69 7374 2e20 5370 6563 6966 6965 7320  list. Specifies 
-00011bc0: 7768 6963 6820 636f 6c6c 6563 7469 6f6e  which collection
-00011bd0: 7320 7368 6f75 6c64 2062 650a 2020 2020  s should be.    
-00011be0: 2020 2020 7472 6561 7465 6420 6173 206d      treated as m
-00011bf0: 7574 6162 6c65 3a20 6060 626f 6f6c 6060  utable: ``bool``
-00011c00: 3a20 616c 6c2f 6e6f 2063 6f6c 6c65 6374  : all/no collect
-00011c10: 696f 6e73 2061 7265 206d 7574 6162 6c65  ions are mutable
-00011c20: 2e20 6060 7374 7260 603a 0a20 2020 2020  . ``str``:.     
-00011c30: 2020 2054 6865 206e 616d 6520 6f66 2061     The name of a
-00011c40: 2073 696e 676c 6520 6d75 7461 626c 6520   single mutable 
-00011c50: 636f 6c6c 6563 7469 6f6e 2e20 6060 6c69  collection. ``li
-00011c60: 7374 6060 3a20 4120 6c69 7374 206f 6620  st``: A list of 
-00011c70: 6e61 6d65 7320 6f66 0a20 2020 2020 2020  names of.       
-00011c80: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
-00011c90: 696f 6e73 2e20 4279 2064 6566 6175 6c74  ions. By default
-00011ca0: 2c20 616c 6c20 636f 6c6c 6563 7469 6f6e  , all collection
-00011cb0: 7320 6578 6365 7074 2022 696e 7465 726d  s except "interm
-00011cc0: 6564 6961 7465 7322 0a20 2020 2020 2020  ediates".       
-00011cd0: 2061 7265 206d 7574 6162 6c65 2e0a 2020   are mutable..  
-00011ce0: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
-00011cf0: 726d 6564 6961 7465 733a 2049 6620 6060  rmediates: If ``
-00011d00: 5472 7565 6060 2c20 6361 7074 7572 6573  True``, captures
-00011d10: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
-00011d20: 7475 726e 2076 616c 7565 7320 6f66 0a20  turn values of. 
-00011d30: 2020 2020 2020 2061 6c6c 204d 6f64 756c         all Modul
-00011d40: 6573 2069 6e73 6964 6520 7468 6520 2269  es inside the "i
-00011d50: 6e74 6572 6d65 6469 6174 6573 2220 636f  ntermediates" co
-00011d60: 6c6c 6563 7469 6f6e 2e20 4279 2064 6566  llection. By def
-00011d70: 6175 6c74 206f 6e6c 7920 7468 650a 2020  ault only the.  
-00011d80: 2020 2020 2020 7265 7475 726e 2076 616c        return val
-00011d90: 7565 7320 6f66 2061 6c6c 2060 605f 5f63  ues of all ``__c
-00011da0: 616c 6c5f 5f60 6020 6d65 7468 6f64 7320  all__`` methods 
-00011db0: 6172 6520 7374 6f72 6564 2e20 4120 6675  are stored. A fu
-00011dc0: 6e63 7469 6f6e 2063 616e 2062 650a 2020  nction can be.  
-00011dd0: 2020 2020 2020 7061 7373 6564 2074 6f20        passed to 
-00011de0: 6368 616e 6765 2074 6865 2066 696c 7465  change the filte
-00011df0: 7220 6265 6861 7669 6f72 2e20 5468 6520  r behavior. The 
-00011e00: 6669 6c74 6572 2066 756e 6374 696f 6e20  filter function 
-00011e10: 7461 6b65 7320 7468 650a 2020 2020 2020  takes the.      
-00011e20: 2020 4d6f 6475 6c65 2069 6e73 7461 6e63    Module instanc
-00011e30: 6520 616e 6420 6d65 7468 6f64 206e 616d  e and method nam
-00011e40: 6520 616e 6420 7265 7475 726e 7320 6120  e and returns a 
-00011e50: 626f 6f6c 2069 6e64 6963 6174 696e 6720  bool indicating 
-00011e60: 7768 6574 6865 720a 2020 2020 2020 2020  whether.        
-00011e70: 7468 6520 6f75 7470 7574 206f 6620 7468  the output of th
-00011e80: 6174 206d 6574 686f 6420 696e 766f 6361  at method invoca
-00011e90: 7469 6f6e 2073 686f 756c 6420 6265 2073  tion should be s
-00011ea0: 746f 7265 642e 0a20 2020 2020 202a 2a6b  tored..      **k
-00011eb0: 7761 7267 733a 204b 6579 776f 7264 2061  wargs: Keyword a
-00011ec0: 7267 756d 656e 7473 2070 6173 7365 6420  rguments passed 
-00011ed0: 746f 2074 6865 2069 6e69 7420 6675 6e63  to the init func
-00011ee0: 7469 6f6e 2e0a 0a20 2020 2052 6574 7572  tion...    Retur
-00011ef0: 6e73 3a0a 2020 2020 2020 6060 286f 7574  ns:.      ``(out
-00011f00: 7075 742c 2076 6172 7329 6060 2c20 7768  put, vars)``, wh
-00011f10: 6572 6520 6060 7661 7273 6060 2061 7265  ere ``vars`` are
-00011f20: 2069 7320 6120 6469 6374 206f 6620 7468   is a dict of th
-00011f30: 6520 6d6f 6469 6669 6564 0a20 2020 2020  e modified.     
-00011f40: 2063 6f6c 6c65 6374 696f 6e73 2e0a 2020   collections..  
-00011f50: 2020 2222 220a 2020 2020 4d6f 6475 6c65    """.    Module
-00011f60: 2e5f 6d6f 6475 6c65 5f63 6865 636b 7328  ._module_checks(
-00011f70: 7365 6c66 290a 0a20 2020 2069 6620 6e6f  self)..    if no
-00011f80: 7420 6973 696e 7374 616e 6365 2872 6e67  t isinstance(rng
-00011f90: 732c 2064 6963 7429 3a0a 2020 2020 2020  s, dict):.      
-00011fa0: 6966 206e 6f74 2063 6f72 652e 7363 6f70  if not core.scop
-00011fb0: 652e 5f69 735f 7661 6c69 645f 726e 6728  e._is_valid_rng(
-00011fc0: 726e 6773 293a 0a20 2020 2020 2020 2072  rngs):.        r
-00011fd0: 6169 7365 2065 7272 6f72 732e 496e 7661  aise errors.Inva
-00011fe0: 6c69 6452 6e67 4572 726f 7228 0a20 2020  lidRngError(.   
-00011ff0: 2020 2020 2020 2027 524e 4773 2073 686f         'RNGs sho
-00012000: 756c 6420 6265 206f 6620 7368 6170 6520  uld be of shape 
-00012010: 2832 2c29 206f 7220 5052 4e47 4b65 7920  (2,) or PRNGKey 
-00012020: 696e 204d 6f64 756c 6520 270a 2020 2020  in Module '.    
-00012030: 2020 2020 2020 6627 7b73 656c 662e 5f5f        f'{self.__
-00012040: 636c 6173 735f 5f2e 5f5f 6e61 6d65 5f5f  class__.__name__
-00012050: 7d2c 2062 7574 2072 6e67 7320 6172 653a  }, but rngs are:
-00012060: 207b 726e 6773 7d27 0a20 2020 2020 2020   {rngs}'.       
-00012070: 2029 0a20 2020 2020 2072 6e67 7320 3d20   ).      rngs = 
-00012080: 7b27 7061 7261 6d73 273a 2072 6e67 737d  {'params': rngs}
-00012090: 0a0a 2020 2020 6966 2069 7369 6e73 7461  ..    if isinsta
-000120a0: 6e63 6528 6d65 7468 6f64 2c20 7374 7229  nce(method, str)
-000120b0: 3a0a 2020 2020 2020 6174 7472 6962 7574  :.      attribut
-000120c0: 655f 6e61 6d65 203d 206d 6574 686f 640a  e_name = method.
-000120d0: 2020 2020 2020 6d65 7468 6f64 203d 2067        method = g
-000120e0: 6574 6174 7472 2873 656c 662c 2061 7474  etattr(self, att
-000120f0: 7269 6275 7465 5f6e 616d 6529 0a20 2020  ribute_name).   
-00012100: 2020 2069 6620 6e6f 7420 6361 6c6c 6162     if not callab
-00012110: 6c65 286d 6574 686f 6429 3a0a 2020 2020  le(method):.    
-00012120: 2020 2020 636c 6173 735f 6e61 6d65 203d      class_name =
-00012130: 2074 7970 6528 7365 6c66 292e 5f5f 6e61   type(self).__na
-00012140: 6d65 5f5f 0a20 2020 2020 2020 2072 6169  me__.        rai
-00012150: 7365 2054 7970 6545 7272 6f72 280a 2020  se TypeError(.  
-00012160: 2020 2020 2020 2020 6622 277b 636c 6173          f"'{clas
-00012170: 735f 6e61 6d65 7d2e 7b61 7474 7269 6275  s_name}.{attribu
-00012180: 7465 5f6e 616d 657d 2720 6d75 7374 2062  te_name}' must b
-00012190: 6520 6120 6361 6c6c 6162 6c65 2c20 676f  e a callable, go
-000121a0: 7422 0a20 2020 2020 2020 2020 2066 2720  t".          f' 
-000121b0: 7b74 7970 6528 6d65 7468 6f64 297d 2e27  {type(method)}.'
-000121c0: 0a20 2020 2020 2020 2029 0a20 2020 2065  .        ).    e
-000121d0: 6c69 6620 6d65 7468 6f64 2069 7320 4e6f  lif method is No
-000121e0: 6e65 3a0a 2020 2020 2020 6d65 7468 6f64  ne:.      method
-000121f0: 203d 2073 656c 662e 5f5f 6361 6c6c 5f5f   = self.__call__
-00012200: 0a20 2020 206d 6574 686f 6420 3d20 5f67  .    method = _g
-00012210: 6574 5f75 6e62 6f75 6e64 5f66 6e28 6d65  et_unbound_fn(me
-00012220: 7468 6f64 290a 2020 2020 7265 7475 726e  thod).    return
-00012230: 2069 6e69 745f 7769 7468 5f6f 7574 7075   init_with_outpu
-00012240: 7428 0a20 2020 2020 206d 6574 686f 642c  t(.      method,
-00012250: 0a20 2020 2020 2073 656c 662c 0a20 2020  .      self,.   
-00012260: 2020 206d 7574 6162 6c65 3d6d 7574 6162     mutable=mutab
-00012270: 6c65 2c0a 2020 2020 2020 6361 7074 7572  le,.      captur
-00012280: 655f 696e 7465 726d 6564 6961 7465 733d  e_intermediates=
-00012290: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
-000122a0: 6961 7465 732c 0a20 2020 2029 2872 6e67  iates,.    )(rng
-000122b0: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
-000122c0: 6773 290a 0a20 2040 7472 6163 6562 6163  gs)..  @tracebac
-000122d0: 6b5f 7574 696c 2e61 7069 5f62 6f75 6e64  k_util.api_bound
-000122e0: 6172 790a 2020 6465 6620 696e 6974 280a  ary.  def init(.
-000122f0: 2020 2020 7365 6c66 2c0a 2020 2020 726e      self,.    rn
-00012300: 6773 3a20 556e 696f 6e5b 5052 4e47 4b65  gs: Union[PRNGKe
-00012310: 792c 2052 4e47 5365 7175 656e 6365 735d  y, RNGSequences]
-00012320: 2c0a 2020 2020 2a61 7267 732c 0a20 2020  ,.    *args,.   
-00012330: 206d 6574 686f 643a 2055 6e69 6f6e 5b43   method: Union[C
-00012340: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
-00012350: 5d2c 2073 7472 2c20 4e6f 6e65 5d20 3d20  ], str, None] = 
-00012360: 4e6f 6e65 2c0a 2020 2020 6d75 7461 626c  None,.    mutabl
-00012370: 653a 2043 6f6c 6c65 6374 696f 6e46 696c  e: CollectionFil
-00012380: 7465 7220 3d20 4465 6e79 4c69 7374 2827  ter = DenyList('
-00012390: 696e 7465 726d 6564 6961 7465 7327 292c  intermediates'),
-000123a0: 0a20 2020 2063 6170 7475 7265 5f69 6e74  .    capture_int
-000123b0: 6572 6d65 6469 6174 6573 3a20 556e 696f  ermediates: Unio
-000123c0: 6e5b 626f 6f6c 2c20 4361 6c6c 6162 6c65  n[bool, Callable
-000123d0: 5b5b 274d 6f64 756c 6527 2c20 7374 725d  [['Module', str]
-000123e0: 2c20 626f 6f6c 5d5d 203d 2046 616c 7365  , bool]] = False
-000123f0: 2c0a 2020 2020 2a2a 6b77 6172 6773 2c0a  ,.    **kwargs,.
-00012400: 2020 2920 2d3e 2055 6e69 6f6e 5b46 726f    ) -> Union[Fro
-00012410: 7a65 6e56 6172 6961 626c 6544 6963 742c  zenVariableDict,
-00012420: 2044 6963 745b 7374 722c 2041 6e79 5d5d   Dict[str, Any]]
-00012430: 3a0a 2020 2020 2222 2249 6e69 7469 616c  :.    """Initial
-00012440: 697a 6573 2061 206d 6f64 756c 6520 6d65  izes a module me
-00012450: 7468 6f64 2077 6974 6820 7661 7269 6162  thod with variab
-00012460: 6c65 7320 616e 6420 7265 7475 726e 7320  les and returns 
-00012470: 6d6f 6469 6669 6564 2076 6172 6961 626c  modified variabl
-00012480: 6573 2e0a 0a20 2020 2060 6069 6e69 7460  es...    ``init`
-00012490: 6020 7461 6b65 7320 6173 2066 6972 7374  ` takes as first
-000124a0: 2061 7267 756d 656e 7420 6569 7468 6572   argument either
-000124b0: 2061 2073 696e 676c 6520 6060 5052 4e47   a single ``PRNG
-000124c0: 4b65 7960 602c 206f 7220 610a 2020 2020  Key``, or a.    
-000124d0: 6469 6374 696f 6e61 7279 206d 6170 7069  dictionary mappi
-000124e0: 6e67 2076 6172 6961 626c 6520 636f 6c6c  ng variable coll
-000124f0: 6563 7469 6f6e 7320 6e61 6d65 7320 746f  ections names to
-00012500: 2074 6865 6972 2060 6050 524e 474b 6579   their ``PRNGKey
-00012510: 7360 602c 2061 6e64 0a20 2020 2077 696c  s``, and.    wil
-00012520: 6c20 6361 6c6c 2060 606d 6574 686f 6460  l call ``method`
-00012530: 6020 2877 6869 6368 2069 7320 7468 6520  ` (which is the 
-00012540: 6d6f 6475 6c65 2773 2060 605f 5f63 616c  module's ``__cal
-00012550: 6c5f 5f60 6020 6675 6e63 7469 6f6e 2062  l__`` function b
-00012560: 790a 2020 2020 6465 6661 756c 7429 2070  y.    default) p
-00012570: 6173 7369 6e67 2060 602a 6172 6773 6060  assing ``*args``
-00012580: 2061 6e64 2060 602a 2a6b 7761 7267 7360   and ``**kwargs`
-00012590: 602c 2061 6e64 2072 6574 7572 6e73 0a20  `, and returns. 
-000125a0: 2020 2061 2064 6963 7469 6f6e 6172 7920     a dictionary 
-000125b0: 6f66 2069 6e69 7469 616c 697a 6564 2076  of initialized v
-000125c0: 6172 6961 626c 6573 2e0a 0a20 2020 2045  ariables...    E
-000125d0: 7861 6d70 6c65 3a3a 0a0a 2020 2020 2020  xample::..      
-000125e0: 3e3e 3e20 696d 706f 7274 2066 6c61 782e  >>> import flax.
-000125f0: 6c69 6e65 6e20 6173 206e 6e0a 2020 2020  linen as nn.    
-00012600: 2020 3e3e 3e20 696d 706f 7274 206a 6178    >>> import jax
-00012610: 2e6e 756d 7079 2061 7320 6a6e 700a 2020  .numpy as jnp.  
-00012620: 2020 2020 3e3e 3e20 696d 706f 7274 206a      >>> import j
-00012630: 6178 0a0a 2020 2020 2020 3e3e 3e20 636c  ax..      >>> cl
-00012640: 6173 7320 466f 6f28 6e6e 2e4d 6f64 756c  ass Foo(nn.Modul
-00012650: 6529 3a0a 2020 2020 2020 2e2e 2e20 2020  e):.      ...   
-00012660: 406e 6e2e 636f 6d70 6163 740a 2020 2020  @nn.compact.    
-00012670: 2020 2e2e 2e20 2020 6465 6620 5f5f 6361    ...   def __ca
-00012680: 6c6c 5f5f 2873 656c 662c 2078 2c20 7472  ll__(self, x, tr
-00012690: 6169 6e29 3a0a 2020 2020 2020 2e2e 2e20  ain):.      ... 
-000126a0: 2020 2020 7820 3d20 6e6e 2e44 656e 7365      x = nn.Dense
-000126b0: 2831 3629 2878 290a 2020 2020 2020 2e2e  (16)(x).      ..
-000126c0: 2e20 2020 2020 7820 3d20 6e6e 2e42 6174  .     x = nn.Bat
-000126d0: 6368 4e6f 726d 2875 7365 5f72 756e 6e69  chNorm(use_runni
-000126e0: 6e67 5f61 7665 7261 6765 3d6e 6f74 2074  ng_average=not t
-000126f0: 7261 696e 2928 7829 0a20 2020 2020 202e  rain)(x).      .
-00012700: 2e2e 2020 2020 2078 203d 206e 6e2e 7265  ..     x = nn.re
-00012710: 6c75 2878 290a 2020 2020 2020 2e2e 2e20  lu(x).      ... 
-00012720: 2020 2020 7265 7475 726e 206e 6e2e 4465      return nn.De
-00012730: 6e73 6528 3129 2878 290a 0a20 2020 2020  nse(1)(x)..     
-00012740: 203e 3e3e 206d 6f64 756c 6520 3d20 466f   >>> module = Fo
-00012750: 6f28 290a 2020 2020 2020 3e3e 3e20 6b65  o().      >>> ke
-00012760: 7920 3d20 6a61 782e 7261 6e64 6f6d 2e6b  y = jax.random.k
-00012770: 6579 2830 290a 2020 2020 2020 3e3e 3e20  ey(0).      >>> 
-00012780: 7661 7269 6162 6c65 7320 3d20 6d6f 6475  variables = modu
-00012790: 6c65 2e69 6e69 7428 6b65 792c 206a 6e70  le.init(key, jnp
-000127a0: 2e65 6d70 7479 2828 312c 2037 2929 2c20  .empty((1, 7)), 
-000127b0: 7472 6169 6e3d 4661 6c73 6529 0a0a 2020  train=False)..  
-000127c0: 2020 4966 2079 6f75 2070 6173 7320 6120    If you pass a 
-000127d0: 7369 6e67 6c65 2060 6050 524e 474b 6579  single ``PRNGKey
-000127e0: 6060 2c20 466c 6178 2077 696c 6c20 7573  ``, Flax will us
-000127f0: 6520 6974 2074 6f20 6665 6564 2074 6865  e it to feed the
-00012800: 2060 6027 7061 7261 6d73 2760 600a 2020   ``'params'``.  
-00012810: 2020 524e 4720 7374 7265 616d 2e20 2049    RNG stream.  I
-00012820: 6620 796f 7520 7761 6e74 2074 6f20 7573  f you want to us
-00012830: 6520 6120 6469 6666 6572 656e 7420 524e  e a different RN
-00012840: 4720 7374 7265 616d 206f 7220 6e65 6564  G stream or need
-00012850: 2074 6f20 7573 650a 2020 2020 6d75 6c74   to use.    mult
-00012860: 6970 6c65 2073 7472 6561 6d73 2c20 796f  iple streams, yo
-00012870: 7520 6d75 7374 2070 6173 7320 6120 6469  u must pass a di
-00012880: 6374 696f 6e61 7279 206d 6170 7069 6e67  ctionary mapping
-00012890: 2065 6163 6820 524e 4720 7374 7265 616d   each RNG stream
-000128a0: 206e 616d 650a 2020 2020 746f 2069 7473   name.    to its
-000128b0: 2063 6f72 7265 7370 6f6e 6469 6e67 2060   corresponding `
-000128c0: 6050 524e 474b 6579 6060 2074 6f20 6060  `PRNGKey`` to ``
-000128d0: 696e 6974 6060 2e0a 0a20 2020 2045 7861  init``...    Exa
-000128e0: 6d70 6c65 3a3a 0a0a 2020 2020 2020 3e3e  mple::..      >>
-000128f0: 3e20 636c 6173 7320 466f 6f28 6e6e 2e4d  > class Foo(nn.M
-00012900: 6f64 756c 6529 3a0a 2020 2020 2020 2e2e  odule):.      ..
-00012910: 2e20 2020 406e 6e2e 636f 6d70 6163 740a  .   @nn.compact.
-00012920: 2020 2020 2020 2e2e 2e20 2020 6465 6620        ...   def 
-00012930: 5f5f 6361 6c6c 5f5f 2873 656c 662c 2078  __call__(self, x
-00012940: 2c20 7472 6169 6e29 3a0a 2020 2020 2020  , train):.      
-00012950: 2e2e 2e20 2020 2020 7820 3d20 6e6e 2e44  ...     x = nn.D
-00012960: 656e 7365 2831 3629 2878 290a 2020 2020  ense(16)(x).    
-00012970: 2020 2e2e 2e20 2020 2020 7820 3d20 6e6e    ...     x = nn
-00012980: 2e42 6174 6368 4e6f 726d 2875 7365 5f72  .BatchNorm(use_r
-00012990: 756e 6e69 6e67 5f61 7665 7261 6765 3d6e  unning_average=n
-000129a0: 6f74 2074 7261 696e 2928 7829 0a20 2020  ot train)(x).   
-000129b0: 2020 202e 2e2e 2020 2020 2078 203d 206e     ...     x = n
-000129c0: 6e2e 7265 6c75 2878 290a 2020 2020 2020  n.relu(x).      
-000129d0: 2e2e 2e0a 2020 2020 2020 2e2e 2e20 2020  ....      ...   
-000129e0: 2020 2320 4164 6420 6761 7573 7369 616e    # Add gaussian
-000129f0: 206e 6f69 7365 0a20 2020 2020 202e 2e2e   noise.      ...
-00012a00: 2020 2020 206e 6f69 7365 5f6b 6579 203d       noise_key =
-00012a10: 2073 656c 662e 6d61 6b65 5f72 6e67 2827   self.make_rng('
-00012a20: 6e6f 6973 6527 290a 2020 2020 2020 2e2e  noise').      ..
-00012a30: 2e20 2020 2020 7820 3d20 7820 2b20 6a61  .     x = x + ja
-00012a40: 782e 7261 6e64 6f6d 2e6e 6f72 6d61 6c28  x.random.normal(
-00012a50: 6e6f 6973 655f 6b65 792c 2078 2e73 6861  noise_key, x.sha
-00012a60: 7065 290a 2020 2020 2020 2e2e 2e0a 2020  pe).      ....  
-00012a70: 2020 2020 2e2e 2e20 2020 2020 7265 7475      ...     retu
-00012a80: 726e 206e 6e2e 4465 6e73 6528 3129 2878  rn nn.Dense(1)(x
-00012a90: 290a 0a20 2020 2020 203e 3e3e 206d 6f64  )..      >>> mod
-00012aa0: 756c 6520 3d20 466f 6f28 290a 2020 2020  ule = Foo().    
-00012ab0: 2020 3e3e 3e20 726e 6773 203d 207b 2770    >>> rngs = {'p
-00012ac0: 6172 616d 7327 3a20 6a61 782e 7261 6e64  arams': jax.rand
-00012ad0: 6f6d 2e6b 6579 2830 292c 0a20 2020 2020  om.key(0),.     
-00012ae0: 202e 2e2e 2020 2020 2020 2020 2027 6e6f   ...         'no
-00012af0: 6973 6527 3a20 6a61 782e 7261 6e64 6f6d  ise': jax.random
-00012b00: 2e6b 6579 2831 297d 0a20 2020 2020 203e  .key(1)}.      >
-00012b10: 3e3e 2076 6172 6961 626c 6573 203d 206d  >> variables = m
-00012b20: 6f64 756c 652e 696e 6974 2872 6e67 732c  odule.init(rngs,
-00012b30: 206a 6e70 2e65 6d70 7479 2828 312c 2037   jnp.empty((1, 7
-00012b40: 2929 2c20 7472 6169 6e3d 4661 6c73 6529  )), train=False)
-00012b50: 0a0a 2020 2020 4a69 7474 696e 6720 6060  ..    Jitting ``
-00012b60: 696e 6974 6060 2069 6e69 7469 616c 697a  init`` initializ
-00012b70: 6573 2061 206d 6f64 656c 206c 617a 696c  es a model lazil
-00012b80: 7920 7573 696e 6720 6f6e 6c79 2074 6865  y using only the
-00012b90: 2073 6861 7065 7320 6f66 2074 6865 0a20   shapes of the. 
-00012ba0: 2020 2070 726f 7669 6465 6420 6172 6775     provided argu
-00012bb0: 6d65 6e74 732c 2061 6e64 2061 766f 6964  ments, and avoid
-00012bc0: 7320 636f 6d70 7574 696e 6720 7468 6520  s computing the 
-00012bd0: 666f 7277 6172 6420 7061 7373 2077 6974  forward pass wit
-00012be0: 6820 6163 7475 616c 0a20 2020 2076 616c  h actual.    val
-00012bf0: 7565 732e 2045 7861 6d70 6c65 3a3a 0a0a  ues. Example::..
-00012c00: 2020 2020 2020 3e3e 3e20 6d6f 6475 6c65        >>> module
-00012c10: 203d 206e 6e2e 4465 6e73 6528 3129 0a20   = nn.Dense(1). 
-00012c20: 2020 2020 203e 3e3e 2069 6e69 745f 6a69       >>> init_ji
-00012c30: 7420 3d20 6a61 782e 6a69 7428 6d6f 6475  t = jax.jit(modu
-00012c40: 6c65 2e69 6e69 7429 0a20 2020 2020 203e  le.init).      >
-00012c50: 3e3e 2076 6172 6961 626c 6573 203d 2069  >> variables = i
-00012c60: 6e69 745f 6a69 7428 6a61 782e 7261 6e64  nit_jit(jax.rand
-00012c70: 6f6d 2e6b 6579 2830 292c 206a 6e70 2e65  om.key(0), jnp.e
-00012c80: 6d70 7479 2828 312c 2037 2929 290a 0a20  mpty((1, 7))).. 
-00012c90: 2020 2060 6069 6e69 7460 6020 6973 2061     ``init`` is a
-00012ca0: 206c 6967 6874 2077 7261 7070 6572 206f   light wrapper o
-00012cb0: 7665 7220 6060 6170 706c 7960 602c 2073  ver ``apply``, s
-00012cc0: 6f20 6f74 6865 7220 6060 6170 706c 7960  o other ``apply`
-00012cd0: 6020 6172 6775 6d65 6e74 730a 2020 2020  ` arguments.    
-00012ce0: 6c69 6b65 2060 606d 6574 686f 6460 602c  like ``method``,
-00012cf0: 2060 606d 7574 6162 6c65 6060 2c20 616e   ``mutable``, an
-00012d00: 6420 6060 6361 7074 7572 655f 696e 7465  d ``capture_inte
-00012d10: 726d 6564 6961 7465 7360 6020 6172 6520  rmediates`` are 
-00012d20: 616c 736f 0a20 2020 2061 7661 696c 6162  also.    availab
-00012d30: 6c65 2e0a 0a20 2020 2041 7267 733a 0a20  le...    Args:. 
-00012d40: 2020 2020 2072 6e67 733a 2054 6865 2072       rngs: The r
-00012d50: 6e67 7320 666f 7220 7468 6520 7661 7269  ngs for the vari
-00012d60: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
-00012d70: 2e0a 2020 2020 2020 2a61 7267 733a 204e  ..      *args: N
-00012d80: 616d 6564 2061 7267 756d 656e 7473 2070  amed arguments p
-00012d90: 6173 7365 6420 746f 2074 6865 2069 6e69  assed to the ini
-00012da0: 7420 6675 6e63 7469 6f6e 2e0a 2020 2020  t function..    
-00012db0: 2020 6d65 7468 6f64 3a20 416e 206f 7074    method: An opt
-00012dc0: 696f 6e61 6c20 6d65 7468 6f64 2e20 4966  ional method. If
-00012dd0: 2070 726f 7669 6465 642c 2061 7070 6c69   provided, appli
-00012de0: 6573 2074 6869 7320 6d65 7468 6f64 2e20  es this method. 
-00012df0: 4966 206e 6f74 0a20 2020 2020 2020 2070  If not.        p
-00012e00: 726f 7669 6465 642c 2061 7070 6c69 6573  rovided, applies
-00012e10: 2074 6865 2060 605f 5f63 616c 6c5f 5f60   the ``__call__`
-00012e20: 6020 6d65 7468 6f64 2e20 4120 7374 7269  ` method. A stri
-00012e30: 6e67 2063 616e 2061 6c73 6f20 6265 2070  ng can also be p
-00012e40: 726f 7669 6465 640a 2020 2020 2020 2020  rovided.        
-00012e50: 746f 2073 7065 6369 6679 2061 206d 6574  to specify a met
-00012e60: 686f 6420 6279 206e 616d 652e 0a20 2020  hod by name..   
-00012e70: 2020 206d 7574 6162 6c65 3a20 4361 6e20     mutable: Can 
-00012e80: 6265 2062 6f6f 6c2c 2073 7472 2c20 6f72  be bool, str, or
-00012e90: 206c 6973 742e 2053 7065 6369 6669 6573   list. Specifies
-00012ea0: 2077 6869 6368 2063 6f6c 6c65 6374 696f   which collectio
-00012eb0: 6e73 2073 686f 756c 6420 6265 0a20 2020  ns should be.   
-00012ec0: 2020 2020 2074 7265 6174 6564 2061 7320       treated as 
-00012ed0: 6d75 7461 626c 653a 2060 6062 6f6f 6c60  mutable: ``bool`
-00012ee0: 603a 2061 6c6c 2f6e 6f20 636f 6c6c 6563  `: all/no collec
-00012ef0: 7469 6f6e 7320 6172 6520 6d75 7461 626c  tions are mutabl
-00012f00: 652e 2060 6073 7472 6060 3a0a 2020 2020  e. ``str``:.    
-00012f10: 2020 2020 5468 6520 6e61 6d65 206f 6620      The name of 
-00012f20: 6120 7369 6e67 6c65 206d 7574 6162 6c65  a single mutable
-00012f30: 2063 6f6c 6c65 6374 696f 6e2e 2060 606c   collection. ``l
-00012f40: 6973 7460 603a 2041 206c 6973 7420 6f66  ist``: A list of
-00012f50: 206e 616d 6573 206f 660a 2020 2020 2020   names of.      
-00012f60: 2020 6d75 7461 626c 6520 636f 6c6c 6563    mutable collec
-00012f70: 7469 6f6e 732e 2042 7920 6465 6661 756c  tions. By defaul
-00012f80: 7420 616c 6c20 636f 6c6c 6563 7469 6f6e  t all collection
-00012f90: 7320 6578 6365 7074 2022 696e 7465 726d  s except "interm
-00012fa0: 6564 6961 7465 7322 0a20 2020 2020 2020  ediates".       
-00012fb0: 2061 7265 206d 7574 6162 6c65 2e0a 2020   are mutable..  
-00012fc0: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
-00012fd0: 726d 6564 6961 7465 733a 2049 6620 6060  rmediates: If ``
-00012fe0: 5472 7565 6060 2c20 6361 7074 7572 6573  True``, captures
-00012ff0: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
-00013000: 7475 726e 2076 616c 7565 7320 6f66 0a20  turn values of. 
-00013010: 2020 2020 2020 2061 6c6c 204d 6f64 756c         all Modul
-00013020: 6573 2069 6e73 6964 6520 7468 6520 2269  es inside the "i
-00013030: 6e74 6572 6d65 6469 6174 6573 2220 636f  ntermediates" co
-00013040: 6c6c 6563 7469 6f6e 2e20 4279 2064 6566  llection. By def
-00013050: 6175 6c74 206f 6e6c 7920 7468 650a 2020  ault only the.  
-00013060: 2020 2020 2020 7265 7475 726e 2076 616c        return val
-00013070: 7565 7320 6f66 2061 6c6c 2060 605f 5f63  ues of all ``__c
-00013080: 616c 6c5f 5f60 6020 6d65 7468 6f64 7320  all__`` methods 
-00013090: 6172 6520 7374 6f72 6564 2e20 4120 6675  are stored. A fu
-000130a0: 6e63 7469 6f6e 2063 616e 2062 650a 2020  nction can be.  
-000130b0: 2020 2020 2020 7061 7373 6564 2074 6f20        passed to 
-000130c0: 6368 616e 6765 2074 6865 2066 696c 7465  change the filte
-000130d0: 7220 6265 6861 7669 6f72 2e20 5468 6520  r behavior. The 
-000130e0: 6669 6c74 6572 2066 756e 6374 696f 6e20  filter function 
-000130f0: 7461 6b65 7320 7468 650a 2020 2020 2020  takes the.      
-00013100: 2020 4d6f 6475 6c65 2069 6e73 7461 6e63    Module instanc
-00013110: 6520 616e 6420 6d65 7468 6f64 206e 616d  e and method nam
-00013120: 6520 616e 6420 7265 7475 726e 7320 6120  e and returns a 
-00013130: 626f 6f6c 2069 6e64 6963 6174 696e 6720  bool indicating 
-00013140: 7768 6574 6865 720a 2020 2020 2020 2020  whether.        
-00013150: 7468 6520 6f75 7470 7574 206f 6620 7468  the output of th
-00013160: 6174 206d 6574 686f 6420 696e 766f 6361  at method invoca
-00013170: 7469 6f6e 2073 686f 756c 6420 6265 2073  tion should be s
-00013180: 746f 7265 642e 0a20 2020 2020 202a 2a6b  tored..      **k
-00013190: 7761 7267 733a 204b 6579 776f 7264 2061  wargs: Keyword a
-000131a0: 7267 756d 656e 7473 2070 6173 7365 6420  rguments passed 
-000131b0: 746f 2074 6865 2069 6e69 7420 6675 6e63  to the init func
-000131c0: 7469 6f6e 2e0a 0a20 2020 2052 6574 7572  tion...    Retur
-000131d0: 6e73 3a0a 2020 2020 2020 5468 6520 696e  ns:.      The in
-000131e0: 6974 6961 6c69 7a65 6420 7661 7269 6162  itialized variab
-000131f0: 6c65 2064 6963 742e 0a20 2020 2022 2222  le dict..    """
-00013200: 0a20 2020 204d 6f64 756c 652e 5f6d 6f64  .    Module._mod
-00013210: 756c 655f 6368 6563 6b73 2873 656c 6629  ule_checks(self)
-00013220: 0a0a 2020 2020 5f2c 2076 5f6f 7574 203d  ..    _, v_out =
-00013230: 2073 656c 662e 696e 6974 5f77 6974 685f   self.init_with_
-00013240: 6f75 7470 7574 280a 2020 2020 2020 726e  output(.      rn
-00013250: 6773 2c0a 2020 2020 2020 2a61 7267 732c  gs,.      *args,
-00013260: 0a20 2020 2020 206d 6574 686f 643d 6d65  .      method=me
-00013270: 7468 6f64 2c0a 2020 2020 2020 6d75 7461  thod,.      muta
-00013280: 626c 653d 6d75 7461 626c 652c 0a20 2020  ble=mutable,.   
-00013290: 2020 2063 6170 7475 7265 5f69 6e74 6572     capture_inter
-000132a0: 6d65 6469 6174 6573 3d63 6170 7475 7265  mediates=capture
-000132b0: 5f69 6e74 6572 6d65 6469 6174 6573 2c0a  _intermediates,.
-000132c0: 2020 2020 2020 2a2a 6b77 6172 6773 2c0a        **kwargs,.
-000132d0: 2020 2020 290a 2020 2020 7265 7475 726e      ).    return
-000132e0: 2076 5f6f 7574 0a0a 2020 4074 7261 6365   v_out..  @trace
-000132f0: 6261 636b 5f75 7469 6c2e 6170 695f 626f  back_util.api_bo
-00013300: 756e 6461 7279 0a20 2064 6566 206c 617a  undary.  def laz
-00013310: 795f 696e 6974 280a 2020 2020 7365 6c66  y_init(.    self
-00013320: 2c0a 2020 2020 726e 6773 3a20 556e 696f  ,.    rngs: Unio
-00013330: 6e5b 5052 4e47 4b65 792c 2052 4e47 5365  n[PRNGKey, RNGSe
-00013340: 7175 656e 6365 735d 2c0a 2020 2020 2a61  quences],.    *a
-00013350: 7267 732c 0a20 2020 206d 6574 686f 643a  rgs,.    method:
-00013360: 204f 7074 696f 6e61 6c5b 4361 6c6c 6162   Optional[Callab
-00013370: 6c65 5b2e 2e2e 2c20 416e 795d 5d20 3d20  le[..., Any]] = 
-00013380: 4e6f 6e65 2c0a 2020 2020 6d75 7461 626c  None,.    mutabl
-00013390: 653a 2043 6f6c 6c65 6374 696f 6e46 696c  e: CollectionFil
-000133a0: 7465 7220 3d20 4465 6e79 4c69 7374 2827  ter = DenyList('
-000133b0: 696e 7465 726d 6564 6961 7465 7327 292c  intermediates'),
-000133c0: 0a20 2020 202a 2a6b 7761 7267 732c 0a20  .    **kwargs,. 
-000133d0: 2029 202d 3e20 4672 6f7a 656e 5661 7269   ) -> FrozenVari
-000133e0: 6162 6c65 4469 6374 3a0a 2020 2020 2222  ableDict:.    ""
-000133f0: 2249 6e69 7469 616c 697a 6573 2061 206d  "Initializes a m
-00013400: 6f64 756c 6520 7769 7468 6f75 7420 636f  odule without co
-00013410: 6d70 7574 696e 6720 6f6e 2061 6e20 6163  mputing on an ac
-00013420: 7475 616c 2069 6e70 7574 2e0a 0a20 2020  tual input...   
-00013430: 206c 617a 795f 696e 6974 2077 696c 6c20   lazy_init will 
-00013440: 696e 6974 6961 6c69 7a65 2074 6865 2076  initialize the v
-00013450: 6172 6961 626c 6573 2077 6974 686f 7574  ariables without
-00013460: 2064 6f69 6e67 2075 6e6e 6563 6573 7361   doing unnecessa
-00013470: 7279 2063 6f6d 7075 7465 2e0a 2020 2020  ry compute..    
-00013480: 5468 6520 696e 7075 7420 6461 7461 2073  The input data s
-00013490: 686f 756c 6420 6265 2070 6173 7365 6420  hould be passed 
-000134a0: 6173 2061 2060 606a 6178 2e53 6861 7065  as a ``jax.Shape
-000134b0: 4474 7970 6553 7472 7563 7460 6020 7768  DtypeStruct`` wh
-000134c0: 6963 680a 2020 2020 7370 6563 6966 6965  ich.    specifie
-000134d0: 7320 7468 6520 7368 6170 6520 616e 6420  s the shape and 
-000134e0: 6474 7970 6520 6f66 2074 6865 2069 6e70  dtype of the inp
-000134f0: 7574 2062 7574 206e 6f20 636f 6e63 7265  ut but no concre
-00013500: 7465 2064 6174 612e 0a0a 2020 2020 4578  te data...    Ex
-00013510: 616d 706c 653a 3a0a 0a20 2020 2020 203e  ample::..      >
-00013520: 3e3e 206d 6f64 656c 203d 206e 6e2e 4465  >> model = nn.De
-00013530: 6e73 6528 6665 6174 7572 6573 3d32 3536  nse(features=256
-00013540: 290a 2020 2020 2020 3e3e 3e20 7661 7269  ).      >>> vari
-00013550: 6162 6c65 7320 3d20 6d6f 6465 6c2e 6c61  ables = model.la
-00013560: 7a79 5f69 6e69 7428 0a20 2020 2020 202e  zy_init(.      .
-00013570: 2e2e 2020 2020 206a 6178 2e72 616e 646f  ..     jax.rando
-00013580: 6d2e 6b65 7928 3029 2c20 6a61 782e 5368  m.key(0), jax.Sh
-00013590: 6170 6544 7479 7065 5374 7275 6374 2828  apeDtypeStruct((
-000135a0: 312c 2031 3238 292c 206a 6e70 2e66 6c6f  1, 128), jnp.flo
-000135b0: 6174 3332 2929 0a0a 2020 2020 5468 6520  at32))..    The 
-000135c0: 6172 6773 2061 6e64 206b 7761 7267 7320  args and kwargs 
-000135d0: 6172 6773 2070 6173 7365 6420 746f 2060  args passed to `
-000135e0: 606c 617a 795f 696e 6974 6060 2063 616e  `lazy_init`` can
-000135f0: 2062 6520 6120 6d69 7820 6f66 0a20 2020   be a mix of.   
-00013600: 2063 6f6e 6372 6574 6520 286a 6178 2061   concrete (jax a
-00013610: 7272 6179 732c 2073 6361 6c61 7273 2c20  rrays, scalars, 
-00013620: 626f 6f6c 7329 2061 6e64 2061 6273 7472  bools) and abstr
-00013630: 6163 7420 2853 6861 7065 4474 7970 6553  act (ShapeDtypeS
-00013640: 7472 7563 7429 0a20 2020 2076 616c 7565  truct).    value
-00013650: 732e 2043 6f6e 6372 6574 6520 7661 6c75  s. Concrete valu
-00013660: 6573 2061 7265 206f 6e6c 7920 6e65 6365  es are only nece
-00013670: 7373 6172 7920 666f 7220 6172 6775 6d65  ssary for argume
-00013680: 6e74 7320 7468 6174 2061 6666 6563 740a  nts that affect.
-00013690: 2020 2020 7468 6520 696e 6974 6961 6c69      the initiali
-000136a0: 7a61 7469 6f6e 206f 6620 7661 7269 6162  zation of variab
-000136b0: 6c65 732e 2046 6f72 2065 7861 6d70 6c65  les. For example
-000136c0: 2c20 7468 6520 6d6f 6465 6c20 6d69 6768  , the model migh
-000136d0: 7420 6578 7065 6374 0a20 2020 2061 206b  t expect.    a k
-000136e0: 6579 776f 7264 2061 7267 2074 6861 7420  eyword arg that 
-000136f0: 656e 6162 6c65 732f 6469 7361 626c 6573  enables/disables
-00013700: 2061 2073 7562 7061 7274 206f 6620 7468   a subpart of th
-00013710: 6520 6d6f 6465 6c2e 0a20 2020 2049 6e20  e model..    In 
-00013720: 7468 6973 2063 6173 652c 2061 6e20 6578  this case, an ex
-00013730: 706c 6963 6974 2076 616c 7565 2028 5472  plicit value (Tr
-00013740: 7565 2f46 6c61 7365 2920 7368 6f75 6c64  ue/Flase) should
-00013750: 2062 6520 7061 7373 6564 206f 7468 6572   be passed other
-00013760: 7769 7365 0a20 2020 2060 606c 617a 795f  wise.    ``lazy_
-00013770: 696e 6974 6060 2063 616e 6e6f 7420 696e  init`` cannot in
-00013780: 6665 7220 7768 6963 6820 7661 7269 6162  fer which variab
-00013790: 6c65 7320 7368 6f75 6c64 2062 6520 696e  les should be in
-000137a0: 6974 6961 6c69 7a65 642e 0a0a 2020 2020  itialized...    
-000137b0: 4172 6773 3a0a 2020 2020 2020 726e 6773  Args:.      rngs
-000137c0: 3a20 5468 6520 726e 6773 2066 6f72 2074  : The rngs for t
-000137d0: 6865 2076 6172 6961 626c 6520 636f 6c6c  he variable coll
-000137e0: 6563 7469 6f6e 732e 0a20 2020 2020 202a  ections..      *
-000137f0: 6172 6773 3a20 6172 6775 6d65 6e74 7320  args: arguments 
-00013800: 7061 7373 6564 2074 6f20 7468 6520 696e  passed to the in
-00013810: 6974 2066 756e 6374 696f 6e2e 0a20 2020  it function..   
-00013820: 2020 206d 6574 686f 643a 2041 6e20 6f70     method: An op
-00013830: 7469 6f6e 616c 206d 6574 686f 642e 2049  tional method. I
-00013840: 6620 7072 6f76 6964 6564 2c20 6170 706c  f provided, appl
-00013850: 6965 7320 7468 6973 206d 6574 686f 642e  ies this method.
-00013860: 2049 6620 6e6f 740a 2020 2020 2020 2020   If not.        
-00013870: 7072 6f76 6964 6564 2c20 6170 706c 6965  provided, applie
-00013880: 7320 7468 6520 6060 5f5f 6361 6c6c 5f5f  s the ``__call__
-00013890: 6060 206d 6574 686f 642e 0a20 2020 2020  `` method..     
-000138a0: 206d 7574 6162 6c65 3a20 4361 6e20 6265   mutable: Can be
-000138b0: 2062 6f6f 6c2c 2073 7472 2c20 6f72 206c   bool, str, or l
-000138c0: 6973 742e 2053 7065 6369 6669 6573 2077  ist. Specifies w
-000138d0: 6869 6368 2063 6f6c 6c65 6374 696f 6e73  hich collections
-000138e0: 2073 686f 756c 6420 6265 0a20 2020 2020   should be.     
-000138f0: 2020 2074 7265 6174 6564 2061 7320 6d75     treated as mu
-00013900: 7461 626c 653a 2060 6062 6f6f 6c60 603a  table: ``bool``:
-00013910: 2061 6c6c 2f6e 6f20 636f 6c6c 6563 7469   all/no collecti
-00013920: 6f6e 7320 6172 6520 6d75 7461 626c 652e  ons are mutable.
-00013930: 2060 6073 7472 6060 3a0a 2020 2020 2020   ``str``:.      
-00013940: 2020 5468 6520 6e61 6d65 206f 6620 6120    The name of a 
-00013950: 7369 6e67 6c65 206d 7574 6162 6c65 2063  single mutable c
-00013960: 6f6c 6c65 6374 696f 6e2e 2060 606c 6973  ollection. ``lis
-00013970: 7460 603a 2041 206c 6973 7420 6f66 206e  t``: A list of n
-00013980: 616d 6573 206f 660a 2020 2020 2020 2020  ames of.        
-00013990: 6d75 7461 626c 6520 636f 6c6c 6563 7469  mutable collecti
-000139a0: 6f6e 732e 2042 7920 6465 6661 756c 7420  ons. By default 
-000139b0: 616c 6c20 636f 6c6c 6563 7469 6f6e 7320  all collections 
-000139c0: 6578 6365 7074 2022 696e 7465 726d 6564  except "intermed
-000139d0: 6961 7465 7322 0a20 2020 2020 2020 2061  iates".        a
-000139e0: 7265 206d 7574 6162 6c65 2e0a 2020 2020  re mutable..    
-000139f0: 2020 2a2a 6b77 6172 6773 3a20 4b65 7977    **kwargs: Keyw
-00013a00: 6f72 6420 6172 6775 6d65 6e74 7320 7061  ord arguments pa
-00013a10: 7373 6564 2074 6f20 7468 6520 696e 6974  ssed to the init
-00013a20: 2066 756e 6374 696f 6e2e 0a0a 2020 2020   function...    
-00013a30: 5265 7475 726e 733a 0a20 2020 2020 2054  Returns:.      T
-00013a40: 6865 2069 6e69 7469 616c 697a 6564 2076  he initialized v
-00013a50: 6172 6961 626c 6520 6469 6374 2e0a 2020  ariable dict..  
-00013a60: 2020 2222 220a 2020 2020 4d6f 6475 6c65    """.    Module
-00013a70: 2e5f 6d6f 6475 6c65 5f63 6865 636b 7328  ._module_checks(
-00013a80: 7365 6c66 290a 0a20 2020 2064 6566 206c  self)..    def l
-00013a90: 617a 795f 7772 6170 7065 7228 726e 6773  azy_wrapper(rngs
-00013aa0: 2c20 2a61 7267 732c 202a 2a6b 7761 7267  , *args, **kwarg
-00013ab0: 7329 3a0a 2020 2020 2020 7265 7475 726e  s):.      return
-00013ac0: 2073 656c 662e 696e 6974 2872 6e67 732c   self.init(rngs,
-00013ad0: 202a 6172 6773 2c20 6d65 7468 6f64 3d6d   *args, method=m
-00013ae0: 6574 686f 642c 206d 7574 6162 6c65 3d6d  ethod, mutable=m
-00013af0: 7574 6162 6c65 2c20 2a2a 6b77 6172 6773  utable, **kwargs
-00013b00: 290a 0a20 2020 2072 6574 7572 6e20 7061  )..    return pa
-00013b10: 7274 6961 6c5f 6576 616c 2e6c 617a 795f  rtial_eval.lazy_
-00013b20: 696e 6974 286c 617a 795f 7772 6170 7065  init(lazy_wrappe
-00013b30: 7229 2872 6e67 732c 202a 6172 6773 2c20  r)(rngs, *args, 
-00013b40: 2a2a 6b77 6172 6773 290a 0a20 2040 7072  **kwargs)..  @pr
-00013b50: 6f70 6572 7479 0a20 2064 6566 2076 6172  operty.  def var
-00013b60: 6961 626c 6573 2873 656c 6629 202d 3e20  iables(self) -> 
-00013b70: 5661 7269 6162 6c65 4469 6374 3a0a 2020  VariableDict:.  
-00013b80: 2020 2222 2252 6574 7572 6e73 2074 6865    """Returns the
-00013b90: 2076 6172 6961 626c 6573 2069 6e20 7468   variables in th
-00013ba0: 6973 206d 6f64 756c 652e 2222 220a 2020  is module.""".  
-00013bb0: 2020 6966 2073 656c 662e 7363 6f70 6520    if self.scope 
-00013bc0: 6973 204e 6f6e 653a 0a20 2020 2020 2072  is None:.      r
-00013bd0: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
-00013be0: 2243 616e 2774 2061 6363 6573 7320 7661  "Can't access va
-00013bf0: 7269 6162 6c65 7320 6f6e 2075 6e62 6f75  riables on unbou
-00013c00: 6e64 206d 6f64 756c 6573 2229 0a20 2020  nd modules").   
-00013c10: 2072 6574 7572 6e20 7365 6c66 2e73 636f   return self.sco
-00013c20: 7065 2e76 6172 6961 626c 6573 2829 0a0a  pe.variables()..
-00013c30: 2020 6465 6620 6765 745f 7661 7269 6162    def get_variab
-00013c40: 6c65 2873 656c 662c 2063 6f6c 3a20 7374  le(self, col: st
-00013c50: 722c 206e 616d 653a 2073 7472 2c20 6465  r, name: str, de
-00013c60: 6661 756c 743a 204f 7074 696f 6e61 6c5b  fault: Optional[
-00013c70: 545d 203d 204e 6f6e 6529 202d 3e20 543a  T] = None) -> T:
-00013c80: 0a20 2020 2022 2222 5265 7472 6965 7665  .    """Retrieve
-00013c90: 7320 7468 6520 7661 6c75 6520 6f66 2061  s the value of a
-00013ca0: 2056 6172 6961 626c 652e 0a0a 2020 2020   Variable...    
-00013cb0: 4172 6773 3a0a 2020 2020 2020 636f 6c3a  Args:.      col:
-00013cc0: 2074 6865 2076 6172 6961 626c 6520 636f   the variable co
-00013cd0: 6c6c 6563 7469 6f6e 2e0a 2020 2020 2020  llection..      
-00013ce0: 6e61 6d65 3a20 7468 6520 6e61 6d65 206f  name: the name o
-00013cf0: 6620 7468 6520 7661 7269 6162 6c65 2e0a  f the variable..
-00013d00: 2020 2020 2020 6465 6661 756c 743a 2074        default: t
-00013d10: 6865 2064 6566 6175 6c74 2076 616c 7565  he default value
-00013d20: 2074 6f20 7265 7475 726e 2069 6620 7468   to return if th
-00013d30: 6520 7661 7269 6162 6c65 2064 6f65 7320  e variable does 
-00013d40: 6e6f 7420 6578 6973 7420 696e 0a20 2020  not exist in.   
-00013d50: 2020 2020 2074 6869 7320 7363 6f70 652e       this scope.
-00013d60: 0a0a 2020 2020 5265 7475 726e 733a 0a20  ..    Returns:. 
-00013d70: 2020 2020 2054 6865 2076 616c 7565 206f       The value o
-00013d80: 6620 7468 6520 696e 7075 7420 7661 7269  f the input vari
-00013d90: 6162 6c65 2c20 6f66 2074 6865 2064 6566  able, of the def
-00013da0: 6175 6c74 2076 616c 7565 2069 6620 7468  ault value if th
-00013db0: 6520 7661 7269 6162 6c65 0a20 2020 2020  e variable.     
-00013dc0: 2064 6f65 736e 2774 2065 7869 7374 2069   doesn't exist i
-00013dd0: 6e20 7468 6973 2073 636f 7065 2e0a 2020  n this scope..  
-00013de0: 2020 2222 220a 2020 2020 6966 2073 656c    """.    if sel
-00013df0: 662e 7363 6f70 6520 6973 204e 6f6e 653a  f.scope is None:
-00013e00: 0a20 2020 2020 2072 6169 7365 2056 616c  .      raise Val
-00013e10: 7565 4572 726f 7228 2243 616e 2774 2061  ueError("Can't a
-00013e20: 6363 6573 7320 7661 7269 6162 6c65 7320  ccess variables 
-00013e30: 6f6e 2075 6e62 6f75 6e64 206d 6f64 756c  on unbound modul
-00013e40: 6573 2229 0a20 2020 2072 6574 7572 6e20  es").    return 
-00013e50: 7365 6c66 2e73 636f 7065 2e67 6574 5f76  self.scope.get_v
-00013e60: 6172 6961 626c 6528 636f 6c2c 206e 616d  ariable(col, nam
-00013e70: 652c 2064 6566 6175 6c74 290a 0a20 2064  e, default)..  d
-00013e80: 6566 2070 7574 5f76 6172 6961 626c 6528  ef put_variable(
-00013e90: 7365 6c66 2c20 636f 6c3a 2073 7472 2c20  self, col: str, 
-00013ea0: 6e61 6d65 3a20 7374 722c 2076 616c 7565  name: str, value
-00013eb0: 3a20 416e 7929 3a0a 2020 2020 2222 2255  : Any):.    """U
-00013ec0: 7064 6174 6573 2074 6865 2076 616c 7565  pdates the value
-00013ed0: 206f 6620 7468 6520 6769 7665 6e20 7661   of the given va
-00013ee0: 7269 6162 6c65 2069 6620 6974 2069 7320  riable if it is 
-00013ef0: 6d75 7461 626c 652c 206f 7220 616e 2065  mutable, or an e
-00013f00: 7272 6f72 206f 7468 6572 7769 7365 2e0a  rror otherwise..
-00013f10: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     
-00013f20: 2063 6f6c 3a20 7468 6520 7661 7269 6162   col: the variab
-00013f30: 6c65 2063 6f6c 6c65 6374 696f 6e2e 0a20  le collection.. 
-00013f40: 2020 2020 206e 616d 653a 2074 6865 206e       name: the n
-00013f50: 616d 6520 6f66 2074 6865 2076 6172 6961  ame of the varia
-00013f60: 626c 652e 0a20 2020 2020 2076 616c 7565  ble..      value
-00013f70: 3a20 7468 6520 6e65 7720 7661 6c75 6520  : the new value 
-00013f80: 6f66 2074 6865 2076 6172 6961 626c 652e  of the variable.
-00013f90: 0a20 2020 2022 2222 0a20 2020 2069 6620  .    """.    if 
-00013fa0: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
-00013fb0: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
-00013fc0: 5661 6c75 6545 7272 6f72 2822 4361 6e27  ValueError("Can'
-00013fd0: 7420 6163 6365 7373 2076 6172 6961 626c  t access variabl
-00013fe0: 6573 206f 6e20 756e 626f 756e 6420 6d6f  es on unbound mo
-00013ff0: 6475 6c65 7322 290a 2020 2020 7365 6c66  dules").    self
-00014000: 2e73 636f 7065 2e70 7574 5f76 6172 6961  .scope.put_varia
-00014010: 626c 6528 636f 6c2c 206e 616d 652c 2076  ble(col, name, v
-00014020: 616c 7565 290a 0a20 2040 6f76 6572 6c6f  alue)..  @overlo
-00014030: 6164 0a20 2064 6566 2073 6f77 2873 656c  ad.  def sow(sel
-00014040: 662c 2063 6f6c 3a20 7374 722c 206e 616d  f, col: str, nam
-00014050: 653a 2073 7472 2c20 7661 6c75 653a 2041  e: str, value: A
-00014060: 6e79 2920 2d3e 2062 6f6f 6c3a 0a20 2020  ny) -> bool:.   
-00014070: 202e 2e2e 0a0a 2020 406f 7665 726c 6f61   .....  @overloa
-00014080: 640a 2020 6465 6620 736f 7728 0a20 2020  d.  def sow(.   
-00014090: 2073 656c 662c 0a20 2020 2063 6f6c 3a20   self,.    col: 
-000140a0: 7374 722c 0a20 2020 206e 616d 653a 2073  str,.    name: s
-000140b0: 7472 2c0a 2020 2020 7661 6c75 653a 2054  tr,.    value: T
-000140c0: 2c0a 2020 2020 7265 6475 6365 5f66 6e3a  ,.    reduce_fn:
-000140d0: 2043 616c 6c61 626c 655b 5b4b 2c20 545d   Callable[[K, T]
-000140e0: 2c20 4b5d 203d 2074 7570 6c65 5f72 6564  , K] = tuple_red
-000140f0: 7563 652c 0a20 2020 2069 6e69 745f 666e  uce,.    init_fn
-00014100: 3a20 4361 6c6c 6162 6c65 5b5b 5d2c 204b  : Callable[[], K
-00014110: 5d20 3d20 7475 706c 655f 696e 6974 2c20  ] = tuple_init, 
-00014120: 2023 2074 7970 653a 2069 676e 6f72 650a   # type: ignore.
-00014130: 2020 2920 2d3e 2062 6f6f 6c3a 0a20 2020    ) -> bool:.   
-00014140: 202e 2e2e 0a0a 2020 6465 6620 736f 7728   .....  def sow(
-00014150: 0a20 2020 2073 656c 662c 0a20 2020 2063  .    self,.    c
-00014160: 6f6c 3a20 7374 722c 0a20 2020 206e 616d  ol: str,.    nam
-00014170: 653a 2073 7472 2c0a 2020 2020 7661 6c75  e: str,.    valu
-00014180: 653a 2054 2c0a 2020 2020 7265 6475 6365  e: T,.    reduce
-00014190: 5f66 6e3a 2043 616c 6c61 626c 655b 5b4b  _fn: Callable[[K
-000141a0: 2c20 545d 2c20 4b5d 203d 2074 7570 6c65  , T], K] = tuple
-000141b0: 5f72 6564 7563 652c 0a20 2020 2069 6e69  _reduce,.    ini
-000141c0: 745f 666e 3a20 4361 6c6c 6162 6c65 5b5b  t_fn: Callable[[
-000141d0: 5d2c 204b 5d20 3d20 7475 706c 655f 696e  ], K] = tuple_in
-000141e0: 6974 2c20 2023 2074 7970 653a 2069 676e  it,  # type: ign
-000141f0: 6f72 650a 2020 2920 2d3e 2062 6f6f 6c3a  ore.  ) -> bool:
-00014200: 0a20 2020 2022 2222 5374 6f72 6573 2061  .    """Stores a
-00014210: 2076 616c 7565 2069 6e20 6120 636f 6c6c   value in a coll
-00014220: 6563 7469 6f6e 2e0a 0a20 2020 2043 6f6c  ection...    Col
-00014230: 6c65 6374 696f 6e73 2063 616e 2062 6520  lections can be 
-00014240: 7573 6564 2074 6f20 636f 6c6c 6563 7420  used to collect 
-00014250: 696e 7465 726d 6564 6961 7465 2076 616c  intermediate val
-00014260: 7565 7320 7769 7468 6f75 740a 2020 2020  ues without.    
-00014270: 7468 6520 6f76 6572 6865 6164 206f 6620  the overhead of 
-00014280: 6578 706c 6963 6974 6c79 2070 6173 7369  explicitly passi
-00014290: 6e67 2061 2063 6f6e 7461 696e 6572 2074  ng a container t
-000142a0: 6872 6f75 6768 2065 6163 6820 4d6f 6475  hrough each Modu
-000142b0: 6c65 2063 616c 6c2e 0a0a 2020 2020 4966  le call...    If
-000142c0: 2074 6865 2074 6172 6765 7420 636f 6c6c   the target coll
-000142d0: 6563 7469 6f6e 2069 7320 6e6f 7420 6d75  ection is not mu
-000142e0: 7461 626c 6520 6060 736f 7760 6020 6265  table ``sow`` be
-000142f0: 6861 7665 7320 6c69 6b65 2061 206e 6f2d  haves like a no-
-00014300: 6f70 0a20 2020 2061 6e64 2072 6574 7572  op.    and retur
-00014310: 6e73 2060 6046 616c 7365 6060 2e0a 0a20  ns ``False``... 
-00014320: 2020 2045 7861 6d70 6c65 3a3a 0a0a 2020     Example::..  
-00014330: 2020 2020 3e3e 3e20 696d 706f 7274 206a      >>> import j
-00014340: 6178 0a20 2020 2020 203e 3e3e 2069 6d70  ax.      >>> imp
-00014350: 6f72 7420 6a61 782e 6e75 6d70 7920 6173  ort jax.numpy as
-00014360: 206a 6e70 0a20 2020 2020 203e 3e3e 2069   jnp.      >>> i
-00014370: 6d70 6f72 7420 666c 6178 2e6c 696e 656e  mport flax.linen
-00014380: 2061 7320 6e6e 0a0a 2020 2020 2020 3e3e   as nn..      >>
-00014390: 3e20 636c 6173 7320 466f 6f28 6e6e 2e4d  > class Foo(nn.M
-000143a0: 6f64 756c 6529 3a0a 2020 2020 2020 2e2e  odule):.      ..
-000143b0: 2e20 2020 406e 6e2e 636f 6d70 6163 740a  .   @nn.compact.
-000143c0: 2020 2020 2020 2e2e 2e20 2020 6465 6620        ...   def 
-000143d0: 5f5f 6361 6c6c 5f5f 2873 656c 662c 2078  __call__(self, x
-000143e0: 293a 0a20 2020 2020 202e 2e2e 2020 2020  ):.      ...    
-000143f0: 2068 203d 206e 6e2e 4465 6e73 6528 3429   h = nn.Dense(4)
-00014400: 2878 290a 2020 2020 2020 2e2e 2e20 2020  (x).      ...   
-00014410: 2020 7365 6c66 2e73 6f77 2827 696e 7465    self.sow('inte
-00014420: 726d 6564 6961 7465 7327 2c20 2768 272c  rmediates', 'h',
-00014430: 2068 290a 2020 2020 2020 2e2e 2e20 2020   h).      ...   
-00014440: 2020 7265 7475 726e 206e 6e2e 4465 6e73    return nn.Dens
-00014450: 6528 3229 2868 290a 0a20 2020 2020 203e  e(2)(h)..      >
-00014460: 3e3e 2078 203d 206a 6e70 2e6f 6e65 7328  >> x = jnp.ones(
-00014470: 2831 362c 2039 2929 0a20 2020 2020 203e  (16, 9)).      >
-00014480: 3e3e 206d 6f64 656c 203d 2046 6f6f 2829  >> model = Foo()
-00014490: 0a20 2020 2020 203e 3e3e 2076 6172 6961  .      >>> varia
-000144a0: 626c 6573 203d 206d 6f64 656c 2e69 6e69  bles = model.ini
-000144b0: 7428 6a61 782e 7261 6e64 6f6d 2e6b 6579  t(jax.random.key
-000144c0: 2830 292c 2078 290a 2020 2020 2020 3e3e  (0), x).      >>
-000144d0: 3e20 792c 2073 7461 7465 203d 206d 6f64  > y, state = mod
-000144e0: 656c 2e61 7070 6c79 2876 6172 6961 626c  el.apply(variabl
-000144f0: 6573 2c20 782c 206d 7574 6162 6c65 3d5b  es, x, mutable=[
-00014500: 2769 6e74 6572 6d65 6469 6174 6573 275d  'intermediates']
-00014510: 290a 2020 2020 2020 3e3e 3e20 7072 696e  ).      >>> prin
-00014520: 7428 7374 6174 655b 2769 6e74 6572 6d65  t(state['interme
-00014530: 6469 6174 6573 275d 290a 2020 2020 2020  diates']).      
-00014540: 7b27 6827 3a20 2841 7272 6179 285b 5b2d  {'h': (Array([[-
-00014550: 312e 3530 3331 3731 2020 2c20 2030 2e37  1.503171  ,  0.7
-00014560: 3337 3737 3034 202c 202d 302e 3539 3338  377704 , -0.5938
-00014570: 3832 3134 2c20 2d31 2e30 3037 3930 3139  8214, -1.0079019
-00014580: 205d 2c0a 2020 2020 2020 2020 2020 2020   ],.            
-00014590: 205b 2d31 2e35 3033 3137 3120 202c 2020   [-1.503171  ,  
-000145a0: 302e 3733 3737 3730 3420 2c20 2d30 2e35  0.7377704 , -0.5
-000145b0: 3933 3838 3231 342c 202d 312e 3030 3739  9388214, -1.0079
-000145c0: 3031 3920 5d2c 0a20 2020 2020 2020 2020  019 ],.         
-000145d0: 2020 2020 5b2d 312e 3530 3331 3731 2020      [-1.503171  
-000145e0: 2c20 2030 2e37 3337 3737 3034 202c 202d  ,  0.7377704 , -
-000145f0: 302e 3539 3338 3832 3134 2c20 2d31 2e30  0.59388214, -1.0
-00014600: 3037 3930 3139 205d 2c0a 2020 2020 2020  079019 ],.      
-00014610: 2020 2020 2020 205b 2d31 2e35 3033 3137         [-1.50317
-00014620: 3120 202c 2020 302e 3733 3737 3730 3420  1  ,  0.7377704 
-00014630: 2c20 2d30 2e35 3933 3838 3231 342c 202d  , -0.59388214, -
-00014640: 312e 3030 3739 3031 3920 5d2c 0a20 2020  1.0079019 ],.   
-00014650: 2020 2020 2020 2020 2020 5b2d 312e 3530            [-1.50
-00014660: 3331 3731 2020 2c20 2030 2e37 3337 3737  3171  ,  0.73777
-00014670: 3034 202c 202d 302e 3539 3338 3832 3134  04 , -0.59388214
-00014680: 2c20 2d31 2e30 3037 3930 3139 205d 2c0a  , -1.0079019 ],.
-00014690: 2020 2020 2020 2020 2020 2020 205b 2d31               [-1
-000146a0: 2e35 3033 3137 3120 202c 2020 302e 3733  .503171  ,  0.73
-000146b0: 3737 3730 3420 2c20 2d30 2e35 3933 3838  77704 , -0.59388
-000146c0: 3231 342c 202d 312e 3030 3739 3031 3920  214, -1.0079019 
-000146d0: 5d2c 0a20 2020 2020 2020 2020 2020 2020  ],.             
-000146e0: 5b2d 312e 3530 3331 3731 2020 2c20 2030  [-1.503171  ,  0
-000146f0: 2e37 3337 3737 3034 202c 202d 302e 3539  .7377704 , -0.59
-00014700: 3338 3832 3134 2c20 2d31 2e30 3037 3930  388214, -1.00790
-00014710: 3139 205d 2c0a 2020 2020 2020 2020 2020  19 ],.          
-00014720: 2020 205b 2d31 2e35 3033 3137 3120 202c     [-1.503171  ,
-00014730: 2020 302e 3733 3737 3730 3420 2c20 2d30    0.7377704 , -0
-00014740: 2e35 3933 3838 3231 342c 202d 312e 3030  .59388214, -1.00
-00014750: 3739 3031 3920 5d2c 0a20 2020 2020 2020  79019 ],.       
-00014760: 2020 2020 2020 5b2d 312e 3530 3331 3731        [-1.503171
-00014770: 2020 2c20 2030 2e37 3337 3737 3034 202c    ,  0.7377704 ,
-00014780: 202d 302e 3539 3338 3832 3134 2c20 2d31   -0.59388214, -1
-00014790: 2e30 3037 3930 3139 205d 2c0a 2020 2020  .0079019 ],.    
-000147a0: 2020 2020 2020 2020 205b 2d31 2e35 3033           [-1.503
-000147b0: 3137 3120 202c 2020 302e 3733 3737 3730  171  ,  0.737770
-000147c0: 3420 2c20 2d30 2e35 3933 3838 3231 342c  4 , -0.59388214,
-000147d0: 202d 312e 3030 3739 3031 3920 5d2c 0a20   -1.0079019 ],. 
-000147e0: 2020 2020 2020 2020 2020 2020 5b2d 312e              [-1.
-000147f0: 3530 3331 3731 2020 2c20 2030 2e37 3337  503171  ,  0.737
-00014800: 3737 3034 202c 202d 302e 3539 3338 3832  7704 , -0.593882
-00014810: 3134 2c20 2d31 2e30 3037 3930 3139 205d  14, -1.0079019 ]
-00014820: 2c0a 2020 2020 2020 2020 2020 2020 205b  ,.             [
-00014830: 2d31 2e35 3033 3137 3120 202c 2020 302e  -1.503171  ,  0.
-00014840: 3733 3737 3730 3420 2c20 2d30 2e35 3933  7377704 , -0.593
-00014850: 3838 3231 342c 202d 312e 3030 3739 3031  88214, -1.007901
-00014860: 3920 5d2c 0a20 2020 2020 2020 2020 2020  9 ],.           
-00014870: 2020 5b2d 312e 3530 3331 3731 2020 2c20    [-1.503171  , 
-00014880: 2030 2e37 3337 3737 3034 202c 202d 302e   0.7377704 , -0.
-00014890: 3539 3338 3832 3134 2c20 2d31 2e30 3037  59388214, -1.007
-000148a0: 3930 3139 205d 2c0a 2020 2020 2020 2020  9019 ],.        
-000148b0: 2020 2020 205b 2d31 2e35 3033 3137 3120       [-1.503171 
-000148c0: 202c 2020 302e 3733 3737 3730 3420 2c20   ,  0.7377704 , 
-000148d0: 2d30 2e35 3933 3838 3231 342c 202d 312e  -0.59388214, -1.
-000148e0: 3030 3739 3031 3920 5d2c 0a20 2020 2020  0079019 ],.     
-000148f0: 2020 2020 2020 2020 5b2d 312e 3530 3331          [-1.5031
-00014900: 3731 2020 2c20 2030 2e37 3337 3737 3034  71  ,  0.7377704
-00014910: 202c 202d 302e 3539 3338 3832 3134 2c20   , -0.59388214, 
-00014920: 2d31 2e30 3037 3930 3139 205d 2c0a 2020  -1.0079019 ],.  
-00014930: 2020 2020 2020 2020 2020 205b 2d31 2e35             [-1.5
-00014940: 3033 3137 3120 202c 2020 302e 3733 3737  03171  ,  0.7377
-00014950: 3730 3420 2c20 2d30 2e35 3933 3838 3231  704 , -0.5938821
-00014960: 342c 202d 312e 3030 3739 3031 3920 5d5d  4, -1.0079019 ]]
-00014970: 2c20 2020 2020 2064 7479 7065 3d66 6c6f  ,      dtype=flo
-00014980: 6174 3332 292c 297d 0a0a 2020 2020 4279  at32),)}..    By
-00014990: 2064 6566 6175 6c74 2074 6865 2076 616c   default the val
-000149a0: 7565 7320 6172 6520 7374 6f72 6564 2069  ues are stored i
-000149b0: 6e20 6120 7475 706c 6520 616e 6420 6561  n a tuple and ea
-000149c0: 6368 2073 746f 7265 6420 7661 6c75 650a  ch stored value.
-000149d0: 2020 2020 6973 2061 7070 656e 6465 6420      is appended 
-000149e0: 6174 2074 6865 2065 6e64 2e20 5468 6973  at the end. This
-000149f0: 2077 6179 2061 6c6c 2069 6e74 6572 6d65   way all interme
-00014a00: 6469 6174 6573 2063 616e 2062 6520 7472  diates can be tr
-00014a10: 6163 6b65 6420 7768 656e 0a20 2020 2074  acked when.    t
-00014a20: 6865 2073 616d 6520 6d6f 6475 6c65 2069  he same module i
-00014a30: 7320 6361 6c6c 6564 206d 756c 7469 706c  s called multipl
-00014a40: 6520 7469 6d65 732e 2041 6c74 6572 6e61  e times. Alterna
-00014a50: 7469 7665 6c79 2c20 6120 6375 7374 6f6d  tively, a custom
-00014a60: 0a20 2020 2069 6e69 742f 7265 6475 6365  .    init/reduce
-00014a70: 2066 756e 6374 696f 6e20 6361 6e20 6265   function can be
-00014a80: 2070 6173 7365 643a 3a0a 0a20 2020 2020   passed::..     
-00014a90: 203e 3e3e 2063 6c61 7373 2046 6f6f 3228   >>> class Foo2(
-00014aa0: 6e6e 2e4d 6f64 756c 6529 3a0a 2020 2020  nn.Module):.    
-00014ab0: 2020 2e2e 2e20 2020 406e 6e2e 636f 6d70    ...   @nn.comp
-00014ac0: 6163 740a 2020 2020 2020 2e2e 2e20 2020  act.      ...   
-00014ad0: 6465 6620 5f5f 6361 6c6c 5f5f 2873 656c  def __call__(sel
-00014ae0: 662c 2078 293a 0a20 2020 2020 202e 2e2e  f, x):.      ...
-00014af0: 2020 2020 2069 6e69 745f 666e 203d 206c       init_fn = l
-00014b00: 616d 6264 613a 2030 0a20 2020 2020 202e  ambda: 0.      .
-00014b10: 2e2e 2020 2020 2072 6564 7563 655f 666e  ..     reduce_fn
-00014b20: 203d 206c 616d 6264 6120 612c 2062 3a20   = lambda a, b: 
-00014b30: 6120 2b20 620a 2020 2020 2020 2e2e 2e20  a + b.      ... 
-00014b40: 2020 2020 7365 6c66 2e73 6f77 2827 696e      self.sow('in
-00014b50: 7465 726d 6564 6961 7465 7327 2c20 2768  termediates', 'h
-00014b60: 272c 2078 2c0a 2020 2020 2020 2e2e 2e20  ', x,.      ... 
-00014b70: 2020 2020 2020 2020 2020 2020 2020 696e                in
-00014b80: 6974 5f66 6e3d 696e 6974 5f66 6e2c 2072  it_fn=init_fn, r
-00014b90: 6564 7563 655f 666e 3d72 6564 7563 655f  educe_fn=reduce_
-00014ba0: 666e 290a 2020 2020 2020 2e2e 2e20 2020  fn).      ...   
-00014bb0: 2020 7365 6c66 2e73 6f77 2827 696e 7465    self.sow('inte
-00014bc0: 726d 6564 6961 7465 7327 2c20 2768 272c  rmediates', 'h',
-00014bd0: 2078 202a 2032 2c0a 2020 2020 2020 2e2e   x * 2,.      ..
-00014be0: 2e20 2020 2020 2020 2020 2020 2020 2020  .               
-00014bf0: 696e 6974 5f66 6e3d 696e 6974 5f66 6e2c  init_fn=init_fn,
-00014c00: 2072 6564 7563 655f 666e 3d72 6564 7563   reduce_fn=reduc
-00014c10: 655f 666e 290a 2020 2020 2020 2e2e 2e20  e_fn).      ... 
-00014c20: 2020 2020 7265 7475 726e 2078 0a0a 2020      return x..  
-00014c30: 2020 2020 3e3e 3e20 7820 3d20 6a6e 702e      >>> x = jnp.
-00014c40: 6f6e 6573 2828 312c 2031 2929 0a20 2020  ones((1, 1)).   
-00014c50: 2020 203e 3e3e 206d 6f64 656c 203d 2046     >>> model = F
-00014c60: 6f6f 3228 290a 2020 2020 2020 3e3e 3e20  oo2().      >>> 
-00014c70: 7661 7269 6162 6c65 7320 3d20 6d6f 6465  variables = mode
-00014c80: 6c2e 696e 6974 286a 6178 2e72 616e 646f  l.init(jax.rando
-00014c90: 6d2e 6b65 7928 3029 2c20 7829 0a20 2020  m.key(0), x).   
-00014ca0: 2020 203e 3e3e 2079 2c20 7374 6174 6520     >>> y, state 
-00014cb0: 3d20 6d6f 6465 6c2e 6170 706c 7928 0a20  = model.apply(. 
-00014cc0: 2020 2020 202e 2e2e 2020 2020 2076 6172       ...     var
-00014cd0: 6961 626c 6573 2c20 782c 206d 7574 6162  iables, x, mutab
-00014ce0: 6c65 3d5b 2769 6e74 6572 6d65 6469 6174  le=['intermediat
-00014cf0: 6573 275d 290a 2020 2020 2020 3e3e 3e20  es']).      >>> 
-00014d00: 7072 696e 7428 7374 6174 655b 2769 6e74  print(state['int
-00014d10: 6572 6d65 6469 6174 6573 275d 290a 2020  ermediates']).  
-00014d20: 2020 2020 7b27 6827 3a20 4172 7261 7928      {'h': Array(
-00014d30: 5b5b 332e 5d5d 2c20 6474 7970 653d 666c  [[3.]], dtype=fl
-00014d40: 6f61 7433 3229 7d0a 0a20 2020 2041 7267  oat32)}..    Arg
-00014d50: 733a 0a20 2020 2020 2063 6f6c 3a20 5468  s:.      col: Th
-00014d60: 6520 6e61 6d65 206f 6620 7468 6520 7661  e name of the va
-00014d70: 7269 6162 6c65 2063 6f6c 6c65 6374 696f  riable collectio
-00014d80: 6e2e 0a20 2020 2020 206e 616d 653a 2054  n..      name: T
-00014d90: 6865 206e 616d 6520 6f66 2074 6865 2076  he name of the v
-00014da0: 6172 6961 626c 652e 0a20 2020 2020 2076  ariable..      v
-00014db0: 616c 7565 3a20 5468 6520 7661 6c75 6520  alue: The value 
-00014dc0: 6f66 2074 6865 2076 6172 6961 626c 652e  of the variable.
-00014dd0: 0a20 2020 2020 2072 6564 7563 655f 666e  .      reduce_fn
-00014de0: 3a20 5468 6520 6675 6e63 7469 6f6e 2075  : The function u
-00014df0: 7365 6420 746f 2063 6f6d 6269 6e65 2074  sed to combine t
-00014e00: 6865 2065 7869 7374 696e 6720 7661 6c75  he existing valu
-00014e10: 6520 7769 7468 2074 6865 206e 6577 0a20  e with the new. 
-00014e20: 2020 2020 2020 2076 616c 7565 2e20 5468         value. Th
-00014e30: 6520 6465 6661 756c 7420 6973 2074 6f20  e default is to 
-00014e40: 6170 7065 6e64 2074 6865 2076 616c 7565  append the value
-00014e50: 2074 6f20 6120 7475 706c 652e 0a20 2020   to a tuple..   
-00014e60: 2020 2069 6e69 745f 666e 3a20 466f 7220     init_fn: For 
-00014e70: 7468 6520 6669 7273 7420 7661 6c75 6520  the first value 
-00014e80: 7374 6f72 6564 2c20 6060 7265 6475 6365  stored, ``reduce
-00014e90: 5f66 6e60 6020 7769 6c6c 2062 6520 7061  _fn`` will be pa
-00014ea0: 7373 6564 2074 6865 2072 6573 756c 740a  ssed the result.
-00014eb0: 2020 2020 2020 2020 6f66 2060 6069 6e69          of ``ini
-00014ec0: 745f 666e 6060 2074 6f67 6574 6865 7220  t_fn`` together 
-00014ed0: 7769 7468 2074 6865 2076 616c 7565 2074  with the value t
-00014ee0: 6f20 6265 2073 746f 7265 642e 2054 6865  o be stored. The
-00014ef0: 2064 6566 6175 6c74 2069 7320 616e 0a20   default is an. 
-00014f00: 2020 2020 2020 2065 6d70 7479 2074 7570         empty tup
-00014f10: 6c65 2e0a 0a20 2020 2052 6574 7572 6e73  le...    Returns
-00014f20: 3a0a 2020 2020 2020 6060 5472 7565 6060  :.      ``True``
-00014f30: 2069 6620 7468 6520 7661 6c75 6520 6861   if the value ha
-00014f40: 7320 6265 656e 2073 746f 7265 6420 7375  s been stored su
-00014f50: 6363 6573 7366 756c 6c79 2c20 6060 4661  ccessfully, ``Fa
-00014f60: 6c73 6560 6020 6f74 6865 7277 6973 652e  lse`` otherwise.
-00014f70: 0a20 2020 2022 2222 0a20 2020 2069 6620  .    """.    if 
-00014f80: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
-00014f90: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
-00014fa0: 5661 6c75 6545 7272 6f72 2822 4361 6e27  ValueError("Can'
-00014fb0: 7420 7374 6f72 6520 7661 7269 6162 6c65  t store variable
-00014fc0: 7320 6f6e 2075 6e62 6f75 6e64 206d 6f64  s on unbound mod
-00014fd0: 756c 6573 2229 0a20 2020 2069 6620 6e6f  ules").    if no
-00014fe0: 7420 7365 6c66 2e73 636f 7065 2e69 735f  t self.scope.is_
-00014ff0: 6d75 7461 626c 655f 636f 6c6c 6563 7469  mutable_collecti
-00015000: 6f6e 2863 6f6c 293a 0a20 2020 2020 2072  on(col):.      r
-00015010: 6574 7572 6e20 4661 6c73 650a 2020 2020  eturn False.    
-00015020: 6966 2073 656c 662e 7363 6f70 652e 6861  if self.scope.ha
-00015030: 735f 7661 7269 6162 6c65 2863 6f6c 2c20  s_variable(col, 
-00015040: 6e61 6d65 293a 0a20 2020 2020 2078 7320  name):.      xs 
-00015050: 3d20 7365 6c66 2e73 636f 7065 2e67 6574  = self.scope.get
-00015060: 5f76 6172 6961 626c 6528 636f 6c2c 206e  _variable(col, n
-00015070: 616d 6529 0a20 2020 2065 6c73 653a 0a20  ame).    else:. 
-00015080: 2020 2020 2073 656c 662e 7363 6f70 652e       self.scope.
-00015090: 7265 7365 7276 6528 6e61 6d65 2c20 636f  reserve(name, co
-000150a0: 6c29 0a20 2020 2020 2073 656c 662e 5f73  l).      self._s
-000150b0: 7461 7465 2e63 6869 6c64 7265 6e5b 6e61  tate.children[na
-000150c0: 6d65 5d20 3d20 636f 6c0a 2020 2020 2020  me] = col.      
-000150d0: 7873 203d 2069 6e69 745f 666e 2829 0a20  xs = init_fn(). 
-000150e0: 2020 2078 7320 3d20 7265 6475 6365 5f66     xs = reduce_f
-000150f0: 6e28 7873 2c20 7661 6c75 6529 0a20 2020  n(xs, value).   
-00015100: 2073 656c 662e 7363 6f70 652e 7075 745f   self.scope.put_
-00015110: 7661 7269 6162 6c65 2863 6f6c 2c20 6e61  variable(col, na
-00015120: 6d65 2c20 7873 290a 2020 2020 7265 7475  me, xs).    retu
-00015130: 726e 2054 7275 650a 0a20 2064 6566 2070  rn True..  def p
-00015140: 6572 7475 7262 280a 2020 2020 7365 6c66  erturb(.    self
-00015150: 2c20 6e61 6d65 3a20 7374 722c 2076 616c  , name: str, val
-00015160: 7565 3a20 542c 2063 6f6c 6c65 6374 696f  ue: T, collectio
-00015170: 6e3a 2073 7472 203d 2027 7065 7274 7572  n: str = 'pertur
-00015180: 6261 7469 6f6e 7327 0a20 2029 202d 3e20  bations'.  ) -> 
-00015190: 543a 0a20 2020 2022 2222 4164 6420 616e  T:.    """Add an
-000151a0: 207a 6572 6f2d 7661 6c75 6520 7661 7269   zero-value vari
-000151b0: 6162 6c65 2028 2770 6572 7475 7262 6174  able ('perturbat
-000151c0: 696f 6e27 2920 746f 2074 6865 2069 6e74  ion') to the int
-000151d0: 6572 6d65 6469 6174 6520 7661 6c75 652e  ermediate value.
-000151e0: 0a0a 2020 2020 5468 6520 6772 6164 6965  ..    The gradie
-000151f0: 6e74 206f 6620 6060 7661 6c75 6560 6020  nt of ``value`` 
-00015200: 776f 756c 6420 6265 2074 6865 2073 616d  would be the sam
-00015210: 6520 6173 2074 6865 2067 7261 6469 656e  e as the gradien
-00015220: 7420 6f66 2074 6869 730a 2020 2020 7065  t of this.    pe
-00015230: 7274 7572 6261 7469 6f6e 2076 6172 6961  rturbation varia
-00015240: 626c 652e 2054 6865 7265 666f 7265 2c20  ble. Therefore, 
-00015250: 6966 2079 6f75 2064 6566 696e 6520 796f  if you define yo
-00015260: 7572 206c 6f73 7320 6675 6e63 7469 6f6e  ur loss function
-00015270: 2077 6974 680a 2020 2020 626f 7468 2070   with.    both p
-00015280: 6172 616d 7320 616e 6420 7065 7274 7572  arams and pertur
-00015290: 6261 7469 6f6e 7320 6173 2073 7461 6e64  bations as stand
-000152a0: 616c 6f6e 6520 6172 6775 6d65 6e74 732c  alone arguments,
-000152b0: 2079 6f75 2063 616e 2067 6574 2074 6865   you can get the
-000152c0: 0a20 2020 2069 6e74 6572 6d65 6469 6174  .    intermediat
-000152d0: 6520 6772 6164 6965 6e74 7320 6f66 2060  e gradients of `
-000152e0: 6076 616c 7565 6060 2062 7920 7275 6e6e  `value`` by runn
-000152f0: 696e 6720 6060 6a61 782e 6772 6164 6060  ing ``jax.grad``
-00015300: 206f 6e20 7468 6520 7065 7274 7572 6261   on the perturba
-00015310: 7469 6f6e 0a20 2020 2061 7267 756d 656e  tion.    argumen
-00015320: 742e 0a0a 2020 2020 4e6f 7465 3a20 7468  t...    Note: th
-00015330: 6973 2069 7320 616e 2065 7870 6572 696d  is is an experim
-00015340: 656e 7461 6c20 4150 4920 616e 6420 6d61  ental API and ma
-00015350: 7920 6265 2074 7765 616b 6564 206c 6174  y be tweaked lat
-00015360: 6572 2066 6f72 2062 6574 7465 720a 2020  er for better.  
-00015370: 2020 7065 7266 6f72 6d61 6e63 6520 616e    performance an
-00015380: 6420 7573 6162 696c 6974 792e 0a20 2020  d usability..   
-00015390: 2041 7420 6974 7320 6375 7272 656e 7420   At its current 
-000153a0: 7374 6167 652c 2069 7420 6372 6561 7465  stage, it create
-000153b0: 7320 6578 7472 6120 6475 6d6d 7920 7661  s extra dummy va
-000153c0: 7269 6162 6c65 7320 7468 6174 206f 6363  riables that occ
-000153d0: 7570 6965 7320 6578 7472 610a 2020 2020  upies extra.    
-000153e0: 6d65 6d6f 7279 2073 7061 6365 2e20 5573  memory space. Us
-000153f0: 6520 6974 206f 6e6c 7920 746f 2064 6562  e it only to deb
-00015400: 7567 2067 7261 6469 656e 7473 2069 6e20  ug gradients in 
-00015410: 7472 6169 6e69 6e67 2e0a 0a20 2020 2045  training...    E
-00015420: 7861 6d70 6c65 3a3a 0a0a 2020 2020 2020  xample::..      
-00015430: 3e3e 3e20 636c 6173 7320 466f 6f28 6e6e  >>> class Foo(nn
-00015440: 2e4d 6f64 756c 6529 3a0a 2020 2020 2020  .Module):.      
-00015450: 2e2e 2e20 2020 406e 6e2e 636f 6d70 6163  ...   @nn.compac
-00015460: 740a 2020 2020 2020 2e2e 2e20 2020 6465  t.      ...   de
-00015470: 6620 5f5f 6361 6c6c 5f5f 2873 656c 662c  f __call__(self,
-00015480: 2078 293a 0a20 2020 2020 202e 2e2e 2020   x):.      ...  
-00015490: 2020 2078 203d 206e 6e2e 4465 6e73 6528     x = nn.Dense(
-000154a0: 3329 2878 290a 2020 2020 2020 2e2e 2e20  3)(x).      ... 
-000154b0: 2020 2020 7820 3d20 7365 6c66 2e70 6572      x = self.per
-000154c0: 7475 7262 2827 6465 6e73 6533 272c 2078  turb('dense3', x
-000154d0: 290a 2020 2020 2020 2e2e 2e20 2020 2020  ).      ...     
-000154e0: 7265 7475 726e 206e 6e2e 4465 6e73 6528  return nn.Dense(
-000154f0: 3229 2878 290a 0a20 2020 2020 203e 3e3e  2)(x)..      >>>
-00015500: 2064 6566 206c 6f73 7328 7661 7269 6162   def loss(variab
-00015510: 6c65 732c 2069 6e70 7574 732c 2074 6172  les, inputs, tar
-00015520: 6765 7473 293a 0a20 2020 2020 202e 2e2e  gets):.      ...
-00015530: 2020 2070 7265 6473 203d 206d 6f64 656c     preds = model
-00015540: 2e61 7070 6c79 2876 6172 6961 626c 6573  .apply(variables
-00015550: 2c20 696e 7075 7473 290a 2020 2020 2020  , inputs).      
-00015560: 2e2e 2e20 2020 7265 7475 726e 206a 6e70  ...   return jnp
-00015570: 2e73 7175 6172 6528 7072 6564 7320 2d20  .square(preds - 
-00015580: 7461 7267 6574 7329 2e6d 6561 6e28 290a  targets).mean().
-00015590: 0a20 2020 2020 203e 3e3e 2078 203d 206a  .      >>> x = j
-000155a0: 6e70 2e6f 6e65 7328 2832 2c20 3929 290a  np.ones((2, 9)).
-000155b0: 2020 2020 2020 3e3e 3e20 7920 3d20 6a6e        >>> y = jn
-000155c0: 702e 6f6e 6573 2828 322c 2032 2929 0a20  p.ones((2, 2)). 
-000155d0: 2020 2020 203e 3e3e 206d 6f64 656c 203d       >>> model =
-000155e0: 2046 6f6f 2829 0a20 2020 2020 203e 3e3e   Foo().      >>>
-000155f0: 2076 6172 6961 626c 6573 203d 206d 6f64   variables = mod
-00015600: 656c 2e69 6e69 7428 6a61 782e 7261 6e64  el.init(jax.rand
-00015610: 6f6d 2e6b 6579 2830 292c 2078 290a 2020  om.key(0), x).  
-00015620: 2020 2020 3e3e 3e20 696e 746d 5f67 7261      >>> intm_gra
-00015630: 6473 203d 206a 6178 2e67 7261 6428 6c6f  ds = jax.grad(lo
-00015640: 7373 2c20 6172 676e 756d 733d 3029 2876  ss, argnums=0)(v
-00015650: 6172 6961 626c 6573 2c20 782c 2079 290a  ariables, x, y).
-00015660: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
-00015670: 696e 746d 5f67 7261 6473 5b27 7065 7274  intm_grads['pert
-00015680: 7572 6261 7469 6f6e 7327 5d5b 2764 656e  urbations']['den
-00015690: 7365 3327 5d29 0a20 2020 2020 205b 5b2d  se3']).      [[-
-000156a0: 312e 3435 3639 3234 2020 202d 302e 3434  1.456924   -0.44
-000156b0: 3333 3235 3337 2020 302e 3032 3432 3238  332537  0.024228
-000156c0: 3437 5d0a 2020 2020 2020 205b 2d31 2e34  47].       [-1.4
-000156d0: 3536 3932 3420 2020 2d30 2e34 3433 3332  56924   -0.44332
-000156e0: 3533 3720 2030 2e30 3234 3232 3834 375d  537  0.02422847]
-000156f0: 5d0a 0a20 2020 2049 6620 7065 7274 7572  ]..    If pertur
-00015700: 6261 7469 6f6e 7320 6172 6520 6e6f 7420  bations are not 
-00015710: 7061 7373 6564 2074 6f20 6060 6170 706c  passed to ``appl
-00015720: 7960 602c 2060 6070 6572 7475 7262 6060  y``, ``perturb``
-00015730: 2062 6568 6176 6573 206c 696b 6520 6120   behaves like a 
-00015740: 6e6f 2d6f 700a 2020 2020 736f 2079 6f75  no-op.    so you
-00015750: 2063 616e 2065 6173 696c 7920 6469 7361   can easily disa
-00015760: 626c 6520 7468 6520 6265 6861 7669 6f72  ble the behavior
-00015770: 2077 6865 6e20 6e6f 7420 6e65 6564 6564   when not needed
-00015780: 3a3a 0a0a 2020 2020 2020 3e3e 3e20 6d6f  ::..      >>> mo
-00015790: 6465 6c2e 6170 706c 7928 7661 7269 6162  del.apply(variab
-000157a0: 6c65 732c 2078 2920 2320 776f 726b 7320  les, x) # works 
-000157b0: 6173 2065 7870 6563 7465 640a 2020 2020  as expected.    
-000157c0: 2020 4172 7261 7928 5b5b 2d31 2e30 3938    Array([[-1.098
-000157d0: 3031 3238 202c 202d 302e 3637 3936 3137  0128 , -0.679617
-000157e0: 3335 5d2c 0a20 2020 2020 2020 2020 2020  35],.           
-000157f0: 2020 5b2d 312e 3039 3830 3132 3820 2c20    [-1.0980128 , 
-00015800: 2d30 2e36 3739 3631 3733 355d 5d2c 2064  -0.67961735]], d
-00015810: 7479 7065 3d66 6c6f 6174 3332 290a 2020  type=float32).  
-00015820: 2020 2020 3e3e 3e20 6d6f 6465 6c2e 6170      >>> model.ap
-00015830: 706c 7928 7b27 7061 7261 6d73 273a 2076  ply({'params': v
-00015840: 6172 6961 626c 6573 5b27 7061 7261 6d73  ariables['params
-00015850: 275d 7d2c 2078 2920 2320 6265 6861 7665  ']}, x) # behave
-00015860: 7320 6c69 6b65 2061 206e 6f2d 6f70 0a20  s like a no-op. 
-00015870: 2020 2020 2041 7272 6179 285b 5b2d 312e       Array([[-1.
-00015880: 3039 3830 3132 3820 2c20 2d30 2e36 3739  0980128 , -0.679
-00015890: 3631 3733 355d 2c0a 2020 2020 2020 2020  61735],.        
-000158a0: 2020 2020 205b 2d31 2e30 3938 3031 3238       [-1.0980128
-000158b0: 202c 202d 302e 3637 3936 3137 3335 5d5d   , -0.67961735]]
-000158c0: 2c20 6474 7970 653d 666c 6f61 7433 3229  , dtype=float32)
-000158d0: 0a20 2020 2020 203e 3e3e 2069 6e74 6d5f  .      >>> intm_
-000158e0: 6772 6164 7320 3d20 6a61 782e 6772 6164  grads = jax.grad
-000158f0: 286c 6f73 732c 2061 7267 6e75 6d73 3d30  (loss, argnums=0
-00015900: 2928 7b27 7061 7261 6d73 273a 2076 6172  )({'params': var
-00015910: 6961 626c 6573 5b27 7061 7261 6d73 275d  iables['params']
-00015920: 7d2c 2078 2c20 7929 0a20 2020 2020 203e  }, x, y).      >
-00015930: 3e3e 2027 7065 7274 7572 6261 7469 6f6e  >> 'perturbation
-00015940: 7327 206e 6f74 2069 6e20 696e 746d 5f67  s' not in intm_g
-00015950: 7261 6473 0a20 2020 2020 2054 7275 650a  rads.      True.
-00015960: 2020 2020 2222 220a 2020 2020 6966 2073      """.    if s
-00015970: 656c 662e 7363 6f70 6520 6973 204e 6f6e  elf.scope is Non
-00015980: 653a 0a20 2020 2020 2072 6169 7365 2056  e:.      raise V
-00015990: 616c 7565 4572 726f 7228 2243 616e 2774  alueError("Can't
-000159a0: 2073 746f 7265 2076 6172 6961 626c 6573   store variables
-000159b0: 206f 6e20 756e 626f 756e 6420 6d6f 6475   on unbound modu
-000159c0: 6c65 7322 290a 0a20 2020 2069 6620 7365  les")..    if se
-000159d0: 6c66 2e69 735f 6d75 7461 626c 655f 636f  lf.is_mutable_co
-000159e0: 6c6c 6563 7469 6f6e 2863 6f6c 6c65 6374  llection(collect
-000159f0: 696f 6e29 3a0a 2020 2020 2020 6966 206e  ion):.      if n
-00015a00: 6f74 2073 656c 662e 7363 6f70 652e 6861  ot self.scope.ha
-00015a10: 735f 7661 7269 6162 6c65 2863 6f6c 6c65  s_variable(colle
-00015a20: 6374 696f 6e2c 206e 616d 6529 3a0a 2020  ction, name):.  
-00015a30: 2020 2020 2020 7365 6c66 2e73 636f 7065        self.scope
-00015a40: 2e72 6573 6572 7665 286e 616d 652c 2063  .reserve(name, c
-00015a50: 6f6c 6c65 6374 696f 6e29 0a20 2020 2020  ollection).     
-00015a60: 2020 2073 656c 662e 5f73 7461 7465 2e63     self._state.c
-00015a70: 6869 6c64 7265 6e5b 6e61 6d65 5d20 3d20  hildren[name] = 
-00015a80: 636f 6c6c 6563 7469 6f6e 0a20 2020 2020  collection.     
-00015a90: 2020 2073 656c 662e 7363 6f70 652e 7075     self.scope.pu
-00015aa0: 745f 7661 7269 6162 6c65 2863 6f6c 6c65  t_variable(colle
-00015ab0: 6374 696f 6e2c 206e 616d 652c 206a 6e70  ction, name, jnp
-00015ac0: 2e7a 6572 6f73 5f6c 696b 6528 7661 6c75  .zeros_like(valu
-00015ad0: 6529 2920 2023 2074 7970 653a 2069 676e  e))  # type: ign
-00015ae0: 6f72 650a 0a20 2020 2069 6620 636f 6c6c  ore..    if coll
-00015af0: 6563 7469 6f6e 2069 6e20 7365 6c66 2e73  ection in self.s
-00015b00: 636f 7065 2e72 6f6f 742e 5f76 6172 6961  cope.root._varia
-00015b10: 626c 6573 3a0a 2020 2020 2020 6966 2073  bles:.      if s
-00015b20: 656c 662e 7363 6f70 652e 6861 735f 7661  elf.scope.has_va
-00015b30: 7269 6162 6c65 2863 6f6c 6c65 6374 696f  riable(collectio
-00015b40: 6e2c 206e 616d 6529 3a0a 2020 2020 2020  n, name):.      
-00015b50: 2020 7661 6c75 6520 2b3d 2073 656c 662e    value += self.
-00015b60: 7363 6f70 652e 6765 745f 7661 7269 6162  scope.get_variab
-00015b70: 6c65 2863 6f6c 6c65 6374 696f 6e2c 206e  le(collection, n
-00015b80: 616d 6529 2020 2320 7479 7065 3a20 6967  ame)  # type: ig
-00015b90: 6e6f 7265 0a20 2020 2020 2065 6c73 653a  nore.      else:
-00015ba0: 0a20 2020 2020 2020 2072 6169 7365 2056  .        raise V
-00015bb0: 616c 7565 4572 726f 7228 6622 5065 7274  alueError(f"Pert
-00015bc0: 7572 6261 7469 6f6e 2063 6f6c 6c65 6374  urbation collect
-00015bd0: 696f 6e20 7b63 6f6c 6c65 6374 696f 6e7d  ion {collection}
-00015be0: 2070 7265 7365 6e74 2c20 6275 7420 220a   present, but ".
-00015bf0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00015c00: 2020 2020 2020 2020 2066 226d 6973 7369           f"missi
-00015c10: 6e67 2070 6572 7475 7262 6174 696f 6e20  ng perturbation 
-00015c20: 7661 7269 6162 6c65 207b 6e61 6d65 7d22  variable {name}"
-00015c30: 290a 0a20 2020 2072 6574 7572 6e20 7661  )..    return va
-00015c40: 6c75 650a 0a20 2064 6566 2074 6162 756c  lue..  def tabul
-00015c50: 6174 6528 0a20 2020 2073 656c 662c 0a20  ate(.    self,. 
-00015c60: 2020 2072 6e67 733a 2055 6e69 6f6e 5b50     rngs: Union[P
-00015c70: 524e 474b 6579 2c20 524e 4753 6571 7565  RNGKey, RNGSeque
-00015c80: 6e63 6573 5d2c 0a20 2020 202a 6172 6773  nces],.    *args
-00015c90: 2c0a 2020 2020 6465 7074 683a 204f 7074  ,.    depth: Opt
-00015ca0: 696f 6e61 6c5b 696e 745d 203d 204e 6f6e  ional[int] = Non
-00015cb0: 652c 0a20 2020 2073 686f 775f 7265 7065  e,.    show_repe
-00015cc0: 6174 6564 3a20 626f 6f6c 203d 2046 616c  ated: bool = Fal
-00015cd0: 7365 2c0a 2020 2020 6d75 7461 626c 653a  se,.    mutable:
-00015ce0: 2043 6f6c 6c65 6374 696f 6e46 696c 7465   CollectionFilte
-00015cf0: 7220 3d20 4465 6e79 4c69 7374 2827 696e  r = DenyList('in
-00015d00: 7465 726d 6564 6961 7465 7327 292c 0a20  termediates'),. 
-00015d10: 2020 2063 6f6e 736f 6c65 5f6b 7761 7267     console_kwarg
-00015d20: 733a 204f 7074 696f 6e61 6c5b 4d61 7070  s: Optional[Mapp
-00015d30: 696e 675b 7374 722c 2041 6e79 5d5d 203d  ing[str, Any]] =
-00015d40: 204e 6f6e 652c 0a20 2020 2074 6162 6c65   None,.    table
-00015d50: 5f6b 7761 7267 733a 204d 6170 7069 6e67  _kwargs: Mapping
-00015d60: 5b73 7472 2c20 416e 795d 203d 204d 6170  [str, Any] = Map
-00015d70: 7069 6e67 5072 6f78 7954 7970 6528 7b7d  pingProxyType({}
-00015d80: 292c 0a20 2020 2063 6f6c 756d 6e5f 6b77  ),.    column_kw
-00015d90: 6172 6773 3a20 4d61 7070 696e 675b 7374  args: Mapping[st
-00015da0: 722c 2041 6e79 5d20 3d20 4d61 7070 696e  r, Any] = Mappin
-00015db0: 6750 726f 7879 5479 7065 287b 7d29 2c0a  gProxyType({}),.
-00015dc0: 2020 2020 636f 6d70 7574 655f 666c 6f70      compute_flop
-00015dd0: 733a 2062 6f6f 6c20 3d20 4661 6c73 652c  s: bool = False,
-00015de0: 0a20 2020 2063 6f6d 7075 7465 5f76 6a70  .    compute_vjp
-00015df0: 5f66 6c6f 7073 3a20 626f 6f6c 203d 2046  _flops: bool = F
-00015e00: 616c 7365 2c0a 2020 2020 2a2a 6b77 6172  alse,.    **kwar
-00015e10: 6773 2c0a 2020 2920 2d3e 2073 7472 3a0a  gs,.  ) -> str:.
-00015e20: 2020 2020 2222 2243 7265 6174 6573 2061      """Creates a
-00015e30: 2073 756d 6d61 7279 206f 6620 7468 6520   summary of the 
-00015e40: 4d6f 6475 6c65 2072 6570 7265 7365 6e74  Module represent
-00015e50: 6564 2061 7320 6120 7461 626c 652e 0a0a  ed as a table...
-00015e60: 2020 2020 5468 6973 206d 6574 686f 6420      This method 
-00015e70: 6861 7320 7468 6520 7361 6d65 2073 6967  has the same sig
-00015e80: 6e61 7475 7265 2061 6e64 2069 6e74 6572  nature and inter
-00015e90: 6e61 6c6c 7920 6361 6c6c 7320 6060 4d6f  nally calls ``Mo
-00015ea0: 6475 6c65 2e69 6e69 7460 602c 0a20 2020  dule.init``,.   
-00015eb0: 2062 7574 2069 6e73 7465 6164 206f 6620   but instead of 
-00015ec0: 7265 7475 726e 696e 6720 7468 6520 7661  returning the va
-00015ed0: 7269 6162 6c65 732c 2069 7420 7265 7475  riables, it retu
-00015ee0: 726e 7320 7468 6520 7374 7269 6e67 2073  rns the string s
-00015ef0: 756d 6d61 7269 7a69 6e67 0a20 2020 2074  ummarizing.    t
-00015f00: 6865 204d 6f64 756c 6520 696e 2061 2074  he Module in a t
-00015f10: 6162 6c65 2e20 6060 7461 6275 6c61 7465  able. ``tabulate
-00015f20: 6060 2075 7365 7320 6060 6a61 782e 6576  `` uses ``jax.ev
-00015f30: 616c 5f73 6861 7065 6060 2074 6f20 7275  al_shape`` to ru
-00015f40: 6e20 7468 6520 666f 7277 6172 640a 2020  n the forward.  
-00015f50: 2020 636f 6d70 7574 6174 696f 6e20 7769    computation wi
-00015f60: 7468 6f75 7420 636f 6e73 756d 696e 6720  thout consuming 
-00015f70: 616e 7920 464c 4f50 7320 6f72 2061 6c6c  any FLOPs or all
-00015f80: 6f63 6174 696e 6720 6d65 6d6f 7279 2e0a  ocating memory..
-00015f90: 0a20 2020 2041 6464 6974 696f 6e61 6c20  .    Additional 
-00015fa0: 6172 6775 6d65 6e74 7320 6361 6e20 6265  arguments can be
-00015fb0: 2070 6173 7365 6420 696e 746f 2074 6865   passed into the
-00015fc0: 2060 6063 6f6e 736f 6c65 5f6b 7761 7267   ``console_kwarg
-00015fd0: 7360 6020 6172 6775 6d65 6e74 2c20 666f  s`` argument, fo
-00015fe0: 720a 2020 2020 6578 616d 706c 652c 2060  r.    example, `
-00015ff0: 607b 2777 6964 7468 273a 2031 3230 7d60  `{'width': 120}`
-00016000: 602e 2046 6f72 2061 2066 756c 6c20 6c69  `. For a full li
-00016010: 7374 206f 6620 6060 636f 6e73 6f6c 655f  st of ``console_
-00016020: 6b77 6172 6773 6060 2061 7267 756d 656e  kwargs`` argumen
-00016030: 7473 2c0a 2020 2020 7365 653a 0a20 2020  ts,.    see:.   
-00016040: 2068 7474 7073 3a2f 2f72 6963 682e 7265   https://rich.re
-00016050: 6164 7468 6564 6f63 732e 696f 2f65 6e2f  adthedocs.io/en/
-00016060: 7374 6162 6c65 2f72 6566 6572 656e 6365  stable/reference
-00016070: 2f63 6f6e 736f 6c65 2e68 746d 6c23 7269  /console.html#ri
-00016080: 6368 2e63 6f6e 736f 6c65 2e43 6f6e 736f  ch.console.Conso
-00016090: 6c65 0a0a 2020 2020 4578 616d 706c 653a  le..    Example:
-000160a0: 3a0a 0a20 2020 2020 203e 3e3e 2069 6d70  :..      >>> imp
-000160b0: 6f72 7420 666c 6178 2e6c 696e 656e 2061  ort flax.linen a
-000160c0: 7320 6e6e 0a20 2020 2020 203e 3e3e 2069  s nn.      >>> i
-000160d0: 6d70 6f72 7420 6a61 782c 206a 6178 2e6e  mport jax, jax.n
-000160e0: 756d 7079 2061 7320 6a6e 700a 0a20 2020  umpy as jnp..   
-000160f0: 2020 203e 3e3e 2063 6c61 7373 2046 6f6f     >>> class Foo
-00016100: 286e 6e2e 4d6f 6475 6c65 293a 0a20 2020  (nn.Module):.   
-00016110: 2020 202e 2e2e 2020 2040 6e6e 2e63 6f6d     ...   @nn.com
-00016120: 7061 6374 0a20 2020 2020 202e 2e2e 2020  pact.      ...  
-00016130: 2064 6566 205f 5f63 616c 6c5f 5f28 7365   def __call__(se
-00016140: 6c66 2c20 7829 3a0a 2020 2020 2020 2e2e  lf, x):.      ..
-00016150: 2e20 2020 2020 6820 3d20 6e6e 2e44 656e  .     h = nn.Den
-00016160: 7365 2834 2928 7829 0a20 2020 2020 202e  se(4)(x).      .
-00016170: 2e2e 2020 2020 2072 6574 7572 6e20 6e6e  ..     return nn
-00016180: 2e44 656e 7365 2832 2928 6829 0a0a 2020  .Dense(2)(h)..  
-00016190: 2020 2020 3e3e 3e20 7820 3d20 6a6e 702e      >>> x = jnp.
-000161a0: 6f6e 6573 2828 3136 2c20 3929 290a 0a20  ones((16, 9)).. 
-000161b0: 2020 2020 203e 3e3e 2023 2070 7269 6e74       >>> # print
-000161c0: 2846 6f6f 2829 2e74 6162 756c 6174 6528  (Foo().tabulate(
-000161d0: 0a20 2020 2020 203e 3e3e 2023 2020 2020  .      >>> #    
-000161e0: 206a 6178 2e72 616e 646f 6d2e 6b65 7928   jax.random.key(
-000161f0: 3029 2c20 782c 2063 6f6d 7075 7465 5f66  0), x, compute_f
-00016200: 6c6f 7073 3d54 7275 652c 2063 6f6d 7075  lops=True, compu
-00016210: 7465 5f76 6a70 5f66 6c6f 7073 3d54 7275  te_vjp_flops=Tru
-00016220: 6529 290a 0a20 2020 2054 6869 7320 6769  e))..    This gi
-00016230: 7665 7320 7468 6520 666f 6c6c 6f77 696e  ves the followin
-00016240: 6720 6f75 7470 7574 3a3a 0a0a 2020 2020  g output::..    
-00016250: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016260: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016270: 2020 2020 2020 2020 466f 6f20 5375 6d6d          Foo Summ
-00016280: 6172 790a 2020 2020 2020 e294 8fe2 9481  ary.      ......
-00016290: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-000162a0: 9481 e294 81e2 9481 e294 b3e2 9481 e294  ................
-000162b0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-000162c0: e294 81e2 94b3 e294 81e2 9481 e294 81e2  ................
-000162d0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-000162e0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-000162f0: e294 81e2 94b3 e294 81e2 9481 e294 81e2  ................
-00016300: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00016310: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00016320: e294 81e2 94b3 e294 81e2 9481 e294 81e2  ................
-00016330: 9481 e294 81e2 9481 e294 81e2 94b3 e294  ................
-00016340: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00016350: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00016360: 94b3 e294 81e2 9481 e294 81e2 9481 e294  ................
-00016370: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00016380: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00016390: 9481 e294 81e2 9493 0a20 2020 2020 20e2  .........      .
-000163a0: 9483 2070 6174 6820 2020 20e2 9483 206d  .. path    ... m
-000163b0: 6f64 756c 6520 e294 8320 696e 7075 7473  odule ... inputs
-000163c0: 2020 2020 2020 2020 e294 8320 6f75 7470          ... outp
-000163d0: 7574 7320 2020 2020 2020 e294 8320 666c  uts       ... fl
-000163e0: 6f70 7320 e294 8320 766a 705f 666c 6f70  ops ... vjp_flop
-000163f0: 7320 e294 8320 7061 7261 6d73 2020 2020  s ... params    
-00016400: 2020 2020 2020 e294 830a 2020 2020 2020        ....      
-00016410: e294 a1e2 9481 e294 81e2 9481 e294 81e2  ................
-00016420: 9481 e294 81e2 9481 e294 81e2 9481 e295  ................
-00016430: 87e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00016440: e294 81e2 9481 e294 81e2 9587 e294 81e2  ................
-00016450: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00016460: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00016470: e294 81e2 9481 e294 81e2 9587 e294 81e2  ................
-00016480: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00016490: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-000164a0: e294 81e2 9481 e294 81e2 9587 e294 81e2  ................
-000164b0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-000164c0: 81e2 9587 e294 81e2 9481 e294 81e2 9481  ................
-000164d0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-000164e0: 9481 e294 81e2 9587 e294 81e2 9481 e294  ................
-000164f0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00016500: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00016510: 9481 e294 81e2 9481 e294 81e2 94a9 0a20  ............... 
-00016520: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
-00016530: 20e2 9482 2046 6f6f 2020 2020 e294 8220   ... Foo    ... 
-00016540: 666c 6f61 7433 325b 3136 2c39 5d20 e294  float32[16,9] ..
-00016550: 8220 666c 6f61 7433 325b 3136 2c32 5d20  . float32[16,2] 
-00016560: e294 8220 3135 3034 2020 e294 8220 3434  ... 1504  ... 44
-00016570: 3630 2020 2020 2020 e294 8220 2020 2020  60      ...     
-00016580: 2020 2020 2020 2020 2020 2020 e294 820a              ....
-00016590: 2020 2020 2020 e294 9ce2 9480 e294 80e2        ..........
-000165a0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000165b0: 80e2 9480 e294 bce2 9480 e294 80e2 9480  ................
-000165c0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000165d0: 94bc e294 80e2 9480 e294 80e2 9480 e294  ................
-000165e0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-000165f0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016600: 94bc e294 80e2 9480 e294 80e2 9480 e294  ................
-00016610: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00016620: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016630: 94bc e294 80e2 9480 e294 80e2 9480 e294  ................
-00016640: 80e2 9480 e294 80e2 94bc e294 80e2 9480  ................
-00016650: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016660: 9480 e294 80e2 9480 e294 80e2 94bc e294  ................
-00016670: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00016680: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016690: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000166a0: 80e2 94a4 0a20 2020 2020 20e2 9482 2044  .....      ... D
-000166b0: 656e 7365 5f30 20e2 9482 2044 656e 7365  ense_0 ... Dense
-000166c0: 2020 e294 8220 666c 6f61 7433 325b 3136    ... float32[16
-000166d0: 2c39 5d20 e294 8220 666c 6f61 7433 325b  ,9] ... float32[
-000166e0: 3136 2c34 5d20 e294 8220 3132 3136 2020  16,4] ... 1216  
-000166f0: e294 8220 3336 3230 2020 2020 2020 e294  ... 3620      ..
-00016700: 8220 6269 6173 3a20 2020 2020 2020 2020  . bias:         
-00016710: 2020 e294 820a 2020 2020 2020 e294 8220    ....      ... 
-00016720: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
-00016730: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
-00016740: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
-00016750: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
-00016760: 20e2 9482 2020 2020 2020 2020 2020 20e2   ...           .
-00016770: 9482 2066 6c6f 6174 3332 5b34 5d20 2020  .. float32[4]   
-00016780: 2020 20e2 9482 0a20 2020 2020 20e2 9482     ....      ...
-00016790: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
-000167a0: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
-000167b0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-000167c0: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
-000167d0: 2020 e294 8220 2020 2020 2020 2020 2020    ...           
-000167e0: e294 8220 6b65 726e 656c 3a20 2020 2020  ... kernel:     
-000167f0: 2020 2020 e294 820a 2020 2020 2020 e294      ....      ..
-00016800: 8220 2020 2020 2020 2020 e294 8220 2020  .         ...   
-00016810: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
-00016820: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
-00016830: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
-00016840: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
-00016850: 20e2 9482 2066 6c6f 6174 3332 5b39 2c34   ... float32[9,4
-00016860: 5d20 2020 20e2 9482 0a20 2020 2020 20e2  ]    ....      .
-00016870: 9482 2020 2020 2020 2020 20e2 9482 2020  ..         ...  
-00016880: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-00016890: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
-000168a0: 2020 2020 2020 2020 2020 e294 8220 2020            ...   
-000168b0: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
-000168c0: 2020 e294 8220 2020 2020 2020 2020 2020    ...           
-000168d0: 2020 2020 2020 e294 820a 2020 2020 2020        ....      
-000168e0: e294 8220 2020 2020 2020 2020 e294 8220  ...         ... 
-000168f0: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
-00016900: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
-00016910: 2020 2020 2020 2020 2020 20e2 9482 2020             ...  
-00016920: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
-00016930: 2020 20e2 9482 2034 3020 2831 3630 2042     ... 40 (160 B
-00016940: 2920 2020 2020 20e2 9482 0a20 2020 2020  )      ....     
-00016950: 20e2 949c e294 80e2 9480 e294 80e2 9480   ...............
-00016960: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016970: 94bc e294 80e2 9480 e294 80e2 9480 e294  ................
-00016980: 80e2 9480 e294 80e2 9480 e294 bce2 9480  ................
-00016990: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000169a0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000169b0: 80e2 9480 e294 80e2 9480 e294 bce2 9480  ................
-000169c0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000169d0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000169e0: 80e2 9480 e294 80e2 9480 e294 bce2 9480  ................
-000169f0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016a00: 9480 e294 bce2 9480 e294 80e2 9480 e294  ................
-00016a10: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00016a20: e294 80e2 9480 e294 bce2 9480 e294 80e2  ................
-00016a30: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00016a40: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00016a50: e294 80e2 9480 e294 80e2 9480 e294 a40a  ................
-00016a60: 2020 2020 2020 e294 8220 4465 6e73 655f        ... Dense_
-00016a70: 3120 e294 8220 4465 6e73 6520 20e2 9482  1 ... Dense  ...
-00016a80: 2066 6c6f 6174 3332 5b31 362c 345d 20e2   float32[16,4] .
-00016a90: 9482 2066 6c6f 6174 3332 5b31 362c 325d  .. float32[16,2]
-00016aa0: 20e2 9482 2032 3838 2020 20e2 9482 2038   ... 288   ... 8
-00016ab0: 3430 2020 2020 2020 20e2 9482 2062 6961  40       ... bia
-00016ac0: 733a 2020 2020 2020 2020 2020 20e2 9482  s:           ...
-00016ad0: 0a20 2020 2020 20e2 9482 2020 2020 2020  .      ...      
-00016ae0: 2020 20e2 9482 2020 2020 2020 2020 e294     ...        ..
-00016af0: 8220 2020 2020 2020 2020 2020 2020 2020  .               
-00016b00: e294 8220 2020 2020 2020 2020 2020 2020  ...             
-00016b10: 2020 e294 8220 2020 2020 2020 e294 8220    ...       ... 
-00016b20: 2020 2020 2020 2020 2020 e294 8220 666c            ... fl
-00016b30: 6f61 7433 325b 325d 2020 2020 2020 e294  oat32[2]      ..
-00016b40: 820a 2020 2020 2020 e294 8220 2020 2020  ..      ...     
-00016b50: 2020 2020 e294 8220 2020 2020 2020 20e2      ...        .
-00016b60: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
-00016b70: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
-00016b80: 2020 20e2 9482 2020 2020 2020 20e2 9482     ...       ...
-00016b90: 2020 2020 2020 2020 2020 20e2 9482 206b             ... k
-00016ba0: 6572 6e65 6c3a 2020 2020 2020 2020 20e2  ernel:         .
-00016bb0: 9482 0a20 2020 2020 20e2 9482 2020 2020  ...      ...    
-00016bc0: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
-00016bd0: e294 8220 2020 2020 2020 2020 2020 2020  ...             
-00016be0: 2020 e294 8220 2020 2020 2020 2020 2020    ...           
-00016bf0: 2020 2020 e294 8220 2020 2020 2020 e294      ...       ..
-00016c00: 8220 2020 2020 2020 2020 2020 e294 8220  .           ... 
-00016c10: 666c 6f61 7433 325b 342c 325d 2020 2020  float32[4,2]    
-00016c20: e294 820a 2020 2020 2020 e294 8220 2020  ....      ...   
-00016c30: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-00016c40: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
-00016c50: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
-00016c60: 2020 2020 20e2 9482 2020 2020 2020 20e2       ...       .
-00016c70: 9482 2020 2020 2020 2020 2020 20e2 9482  ..           ...
-00016c80: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016c90: 20e2 9482 0a20 2020 2020 20e2 9482 2020   ....      ...  
-00016ca0: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
-00016cb0: 2020 e294 8220 2020 2020 2020 2020 2020    ...           
-00016cc0: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
-00016cd0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-00016ce0: e294 8220 2020 2020 2020 2020 2020 e294  ...           ..
-00016cf0: 8220 3130 2028 3430 2042 2920 2020 2020  . 10 (40 B)     
-00016d00: 2020 e294 820a 2020 2020 2020 e294 9ce2    ....      ....
-00016d10: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00016d20: 80e2 9480 e294 80e2 9480 e294 bce2 9480  ................
-00016d30: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016d40: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
-00016d50: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00016d60: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016d70: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
-00016d80: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00016d90: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016da0: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
-00016db0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-00016dc0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016dd0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00016de0: 80e2 94bc e294 80e2 9480 e294 80e2 9480  ................
-00016df0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016e00: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00016e10: 80e2 9480 e294 80e2 94a4 0a20 2020 2020  ...........     
-00016e20: 20e2 9482 2020 2020 2020 2020 20e2 9482   ...         ...
-00016e30: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
-00016e40: 2020 2020 2020 2020 2020 e294 8220 2020            ...   
-00016e50: 2020 2020 2020 2020 2020 2020 e294 8220              ... 
-00016e60: 2020 2020 2020 e294 8220 2020 2020 546f        ...     To
-00016e70: 7461 6c20 e294 8220 3530 2028 3230 3020  tal ... 50 (200 
-00016e80: 4229 2020 2020 2020 e294 820a 2020 2020  B)      ....    
-00016e90: 2020 e294 94e2 9480 e294 80e2 9480 e294    ..............
-00016ea0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00016eb0: e294 b4e2 9480 e294 80e2 9480 e294 80e2  ................
-00016ec0: 9480 e294 80e2 9480 e294 80e2 94b4 e294  ................
-00016ed0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00016ee0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016ef0: 9480 e294 80e2 9480 e294 80e2 94b4 e294  ................
-00016f00: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00016f10: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016f20: 9480 e294 80e2 9480 e294 80e2 94b4 e294  ................
-00016f30: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00016f40: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
-00016f50: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00016f60: 80e2 9480 e294 80e2 94b4 e294 80e2 9480  ................
-00016f70: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00016f80: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00016f90: 80e2 9480 e294 80e2 9480 e294 80e2 9498  ................
-00016fa0: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00016fb0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00016fc0: 2020 2020 2020 546f 7461 6c20 5061 7261        Total Para
-00016fd0: 6d65 7465 7273 3a20 3530 2028 3230 3020  meters: 50 (200 
-00016fe0: 4229 0a0a 2020 2020 2a2a 4e6f 7465 2a2a  B)..    **Note**
-00016ff0: 3a20 726f 7773 206f 7264 6572 2069 6e20  : rows order in 
-00017000: 7468 6520 7461 626c 6520 646f 6573 206e  the table does n
-00017010: 6f74 2072 6570 7265 7365 6e74 2065 7865  ot represent exe
-00017020: 6375 7469 6f6e 206f 7264 6572 2c0a 2020  cution order,.  
-00017030: 2020 696e 7374 6561 6420 6974 2061 6c69    instead it ali
-00017040: 676e 7320 7769 7468 2074 6865 206f 7264  gns with the ord
-00017050: 6572 206f 6620 6b65 7973 2069 6e20 6060  er of keys in ``
-00017060: 7661 7269 6162 6c65 7360 6020 7768 6963  variables`` whic
-00017070: 6820 6172 6520 736f 7274 6564 0a20 2020  h are sorted.   
-00017080: 2061 6c70 6861 6265 7469 6361 6c6c 792e   alphabetically.
-00017090: 0a0a 2020 2020 2a2a 4e6f 7465 2a2a 3a20  ..    **Note**: 
-000170a0: 6060 766a 705f 666c 6f70 7360 6020 7265  ``vjp_flops`` re
-000170b0: 7475 726e 7320 6060 3060 6020 6966 2074  turns ``0`` if t
-000170c0: 6865 206d 6f64 756c 6520 6973 206e 6f74  he module is not
-000170d0: 2064 6966 6665 7265 6e74 6961 626c 652e   differentiable.
-000170e0: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    
-000170f0: 2020 726e 6773 3a20 5468 6520 726e 6773    rngs: The rngs
-00017100: 2066 6f72 2074 6865 2076 6172 6961 626c   for the variabl
-00017110: 6520 636f 6c6c 6563 7469 6f6e 7320 6173  e collections as
-00017120: 2070 6173 7365 6420 746f 2060 604d 6f64   passed to ``Mod
-00017130: 756c 652e 696e 6974 6060 2e0a 2020 2020  ule.init``..    
-00017140: 2020 2a61 7267 733a 2054 6865 2061 7267    *args: The arg
-00017150: 756d 656e 7473 2074 6f20 7468 6520 666f  uments to the fo
-00017160: 7277 6172 6420 636f 6d70 7574 6174 696f  rward computatio
-00017170: 6e2e 0a20 2020 2020 2064 6570 7468 3a20  n..      depth: 
-00017180: 636f 6e74 726f 6c73 2068 6f77 206d 616e  controls how man
-00017190: 7920 7375 626d 6f64 756c 6520 6465 6570  y submodule deep
-000171a0: 2074 6865 2073 756d 6d61 7279 2063 616e   the summary can
-000171b0: 2067 6f2e 2042 7920 6465 6661 756c 742c   go. By default,
-000171c0: 0a20 2020 2020 2020 2069 7473 2060 604e  .        its ``N
-000171d0: 6f6e 6560 6020 7768 6963 6820 6d65 616e  one`` which mean
-000171e0: 7320 6e6f 206c 696d 6974 2e20 4966 2061  s no limit. If a
-000171f0: 2073 7562 6d6f 6475 6c65 2069 7320 6e6f   submodule is no
-00017200: 7420 7368 6f77 6e20 6265 6361 7573 6520  t shown because 
-00017210: 6f66 0a20 2020 2020 2020 2074 6865 2064  of.        the d
-00017220: 6570 7468 206c 696d 6974 2c20 6974 7320  epth limit, its 
-00017230: 7061 7261 6d65 7465 7220 636f 756e 7420  parameter count 
-00017240: 616e 6420 6279 7465 7320 7769 6c6c 2062  and bytes will b
-00017250: 6520 6164 6465 6420 746f 2074 6865 2072  e added to the r
-00017260: 6f77 0a20 2020 2020 2020 206f 6620 6974  ow.        of it
-00017270: 7320 6669 7273 7420 7368 6f77 6e20 616e  s first shown an
-00017280: 6365 7374 6f72 2073 7563 6820 7468 6174  cestor such that
-00017290: 2074 6865 2073 756d 206f 6620 616c 6c20   the sum of all 
-000172a0: 726f 7773 2061 6c77 6179 7320 6164 6473  rows always adds
-000172b0: 0a20 2020 2020 2020 2075 7020 746f 2074  .        up to t
-000172c0: 6865 2074 6f74 616c 206e 756d 6265 7220  he total number 
-000172d0: 6f66 2070 6172 616d 6574 6572 7320 6f66  of parameters of
-000172e0: 2074 6865 204d 6f64 756c 652e 0a20 2020   the Module..   
-000172f0: 2020 2073 686f 775f 7265 7065 6174 6564     show_repeated
-00017300: 3a20 4966 2060 6054 7275 6560 602c 2072  : If ``True``, r
-00017310: 6570 6561 7465 6420 6361 6c6c 7320 746f  epeated calls to
-00017320: 2074 6865 2073 616d 6520 6d6f 6475 6c65   the same module
-00017330: 2077 696c 6c20 6265 2073 686f 776e 0a20   will be shown. 
-00017340: 2020 2020 2020 2069 6e20 7468 6520 7461         in the ta
-00017350: 626c 652c 206f 7468 6572 7769 7365 206f  ble, otherwise o
-00017360: 6e6c 7920 7468 6520 6669 7273 7420 6361  nly the first ca
-00017370: 6c6c 2077 696c 6c20 6265 2073 686f 776e  ll will be shown
-00017380: 2e20 4465 6661 756c 7420 6973 0a20 2020  . Default is.   
-00017390: 2020 2020 2060 6046 616c 7365 6060 2e0a       ``False``..
-000173a0: 2020 2020 2020 6d75 7461 626c 653a 2043        mutable: C
-000173b0: 616e 2062 6520 626f 6f6c 2c20 7374 722c  an be bool, str,
-000173c0: 206f 7220 6c69 7374 2e20 5370 6563 6966   or list. Specif
-000173d0: 6965 7320 7768 6963 6820 636f 6c6c 6563  ies which collec
-000173e0: 7469 6f6e 7320 7368 6f75 6c64 2062 650a  tions should be.
-000173f0: 2020 2020 2020 2020 7472 6561 7465 6420          treated 
-00017400: 6173 206d 7574 6162 6c65 3a20 6060 626f  as mutable: ``bo
-00017410: 6f6c 6060 3a20 616c 6c2f 6e6f 2063 6f6c  ol``: all/no col
-00017420: 6c65 6374 696f 6e73 2061 7265 206d 7574  lections are mut
-00017430: 6162 6c65 2e20 6060 7374 7260 603a 0a20  able. ``str``:. 
-00017440: 2020 2020 2020 2054 6865 206e 616d 6520         The name 
-00017450: 6f66 2061 2073 696e 676c 6520 6d75 7461  of a single muta
-00017460: 626c 6520 636f 6c6c 6563 7469 6f6e 2e20  ble collection. 
-00017470: 6060 6c69 7374 6060 3a20 4120 6c69 7374  ``list``: A list
-00017480: 206f 6620 6e61 6d65 7320 6f66 0a20 2020   of names of.   
-00017490: 2020 2020 206d 7574 6162 6c65 2063 6f6c       mutable col
-000174a0: 6c65 6374 696f 6e73 2e20 4279 2064 6566  lections. By def
-000174b0: 6175 6c74 2c20 616c 6c20 636f 6c6c 6563  ault, all collec
-000174c0: 7469 6f6e 7320 6578 6365 7074 2027 696e  tions except 'in
-000174d0: 7465 726d 6564 6961 7465 7327 0a20 2020  termediates'.   
-000174e0: 2020 2020 2061 7265 206d 7574 6162 6c65       are mutable
-000174f0: 2e0a 2020 2020 2020 636f 6e73 6f6c 655f  ..      console_
-00017500: 6b77 6172 6773 3a20 416e 206f 7074 696f  kwargs: An optio
-00017510: 6e61 6c20 6469 6374 696f 6e61 7279 2077  nal dictionary w
-00017520: 6974 6820 6164 6469 7469 6f6e 616c 206b  ith additional k
-00017530: 6579 776f 7264 2061 7267 756d 656e 7473  eyword arguments
-00017540: 0a20 2020 2020 2020 2074 6861 7420 6172  .        that ar
-00017550: 6520 7061 7373 6564 2074 6f20 6060 7269  e passed to ``ri
-00017560: 6368 2e63 6f6e 736f 6c65 2e43 6f6e 736f  ch.console.Conso
-00017570: 6c65 6060 2077 6865 6e20 7265 6e64 6572  le`` when render
-00017580: 696e 6720 7468 6520 7461 626c 652e 0a20  ing the table.. 
-00017590: 2020 2020 2020 2044 6566 6175 6c74 2061         Default a
-000175a0: 7267 756d 656e 7473 2061 7265 2060 607b  rguments are ``{
-000175b0: 2766 6f72 6365 5f74 6572 6d69 6e61 6c27  'force_terminal'
-000175c0: 3a20 5472 7565 2c20 2766 6f72 6365 5f6a  : True, 'force_j
-000175d0: 7570 7974 6572 273a 0a20 2020 2020 2020  upyter':.       
-000175e0: 2046 616c 7365 7d60 602e 0a20 2020 2020   False}``..     
-000175f0: 2074 6162 6c65 5f6b 7761 7267 733a 2041   table_kwargs: A
-00017600: 6e20 6f70 7469 6f6e 616c 2064 6963 7469  n optional dicti
-00017610: 6f6e 6172 7920 7769 7468 2061 6464 6974  onary with addit
-00017620: 696f 6e61 6c20 6b65 7977 6f72 6420 6172  ional keyword ar
-00017630: 6775 6d65 6e74 730a 2020 2020 2020 2020  guments.        
-00017640: 7468 6174 2061 7265 2070 6173 7365 6420  that are passed 
-00017650: 746f 2060 6072 6963 682e 7461 626c 652e  to ``rich.table.
-00017660: 5461 626c 6560 6020 636f 6e73 7472 7563  Table`` construc
-00017670: 746f 722e 0a20 2020 2020 2063 6f6c 756d  tor..      colum
-00017680: 6e5f 6b77 6172 6773 3a20 416e 206f 7074  n_kwargs: An opt
-00017690: 696f 6e61 6c20 6469 6374 696f 6e61 7279  ional dictionary
-000176a0: 2077 6974 6820 6164 6469 7469 6f6e 616c   with additional
-000176b0: 206b 6579 776f 7264 2061 7267 756d 656e   keyword argumen
-000176c0: 7473 0a20 2020 2020 2020 2074 6861 7420  ts.        that 
-000176d0: 6172 6520 7061 7373 6564 2074 6f20 6060  are passed to ``
-000176e0: 7269 6368 2e74 6162 6c65 2e54 6162 6c65  rich.table.Table
-000176f0: 2e61 6464 5f63 6f6c 756d 6e60 6020 7768  .add_column`` wh
-00017700: 656e 2061 6464 696e 6720 636f 6c75 6d6e  en adding column
-00017710: 7320 746f 0a20 2020 2020 2020 2074 6865  s to.        the
-00017720: 2074 6162 6c65 2e0a 2020 2020 2020 636f   table..      co
-00017730: 6d70 7574 655f 666c 6f70 733a 2077 6865  mpute_flops: whe
-00017740: 7468 6572 2074 6f20 696e 636c 7564 6520  ther to include 
-00017750: 6120 6060 666c 6f70 7360 6020 636f 6c75  a ``flops`` colu
-00017760: 6d6e 2069 6e20 7468 6520 7461 626c 6520  mn in the table 
-00017770: 6c69 7374 696e 670a 2020 2020 2020 2020  listing.        
-00017780: 7468 6520 6573 7469 6d61 7465 6420 464c  the estimated FL
-00017790: 4f50 7320 636f 7374 206f 6620 6561 6368  OPs cost of each
-000177a0: 206d 6f64 756c 6520 666f 7277 6172 6420   module forward 
-000177b0: 7061 7373 2e20 446f 6573 2069 6e63 7572  pass. Does incur
-000177c0: 2061 6374 7561 6c0a 2020 2020 2020 2020   actual.        
-000177d0: 6f6e 2d64 6576 6963 6520 636f 6d70 7574  on-device comput
-000177e0: 6174 696f 6e20 2f20 636f 6d70 696c 6174  ation / compilat
-000177f0: 696f 6e20 2f20 6d65 6d6f 7279 2061 6c6c  ion / memory all
-00017800: 6f63 6174 696f 6e2c 2062 7574 2073 7469  ocation, but sti
-00017810: 6c6c 0a20 2020 2020 2020 2069 6e74 726f  ll.        intro
-00017820: 6475 6365 7320 6f76 6572 6865 6164 2066  duces overhead f
-00017830: 6f72 206c 6172 6765 206d 6f64 756c 6573  or large modules
-00017840: 2028 652e 672e 2065 7874 7261 2032 3020   (e.g. extra 20 
-00017850: 7365 636f 6e64 7320 666f 7220 610a 2020  seconds for a.  
-00017860: 2020 2020 2020 5374 6162 6c65 2044 6966        Stable Dif
-00017870: 6675 7369 6f6e 2773 2055 4e65 742c 2077  fusion's UNet, w
-00017880: 6865 7265 6173 206f 7468 6572 7769 7365  hereas otherwise
-00017890: 2074 6162 756c 6174 696f 6e20 776f 756c   tabulation woul
-000178a0: 6420 6669 6e69 7368 2069 6e20 350a 2020  d finish in 5.  
-000178b0: 2020 2020 2020 7365 636f 6e64 7329 2e0a        seconds)..
-000178c0: 2020 2020 2020 636f 6d70 7574 655f 766a        compute_vj
-000178d0: 705f 666c 6f70 733a 2077 6865 7468 6572  p_flops: whether
-000178e0: 2074 6f20 696e 636c 7564 6520 6120 6060   to include a ``
-000178f0: 766a 705f 666c 6f70 7360 6020 636f 6c75  vjp_flops`` colu
-00017900: 6d6e 2069 6e20 7468 6520 7461 626c 650a  mn in the table.
-00017910: 2020 2020 2020 2020 6c69 7374 696e 6720          listing 
-00017920: 7468 6520 6573 7469 6d61 7465 6420 464c  the estimated FL
-00017930: 4f50 7320 636f 7374 206f 6620 6561 6368  OPs cost of each
-00017940: 206d 6f64 756c 6520 6261 636b 7761 7264   module backward
-00017950: 2070 6173 732e 0a20 2020 2020 2020 2049   pass..        I
-00017960: 6e74 726f 6475 6365 7320 6120 636f 6d70  ntroduces a comp
-00017970: 7574 6520 6f76 6572 6865 6164 206f 6620  ute overhead of 
-00017980: 6162 6f75 7420 322d 3358 206f 6620 6060  about 2-3X of ``
-00017990: 636f 6d70 7574 655f 666c 6f70 7360 602e  compute_flops``.
-000179a0: 0a20 2020 2020 202a 2a6b 7761 7267 733a  .      **kwargs:
-000179b0: 206b 6579 776f 7264 2061 7267 756d 656e   keyword argumen
-000179c0: 7473 2074 6f20 7061 7373 2074 6f20 7468  ts to pass to th
-000179d0: 6520 666f 7277 6172 6420 636f 6d70 7574  e forward comput
-000179e0: 6174 696f 6e2e 0a0a 2020 2020 5265 7475  ation...    Retu
-000179f0: 726e 733a 0a20 2020 2020 2041 2073 7472  rns:.      A str
-00017a00: 696e 6720 7375 6d6d 6172 697a 696e 6720  ing summarizing 
-00017a10: 7468 6520 4d6f 6475 6c65 2e0a 2020 2020  the Module..    
-00017a20: 2222 220a 2020 2020 6672 6f6d 2066 6c61  """.    from fla
-00017a30: 782e 6c69 6e65 6e20 696d 706f 7274 2073  x.linen import s
-00017a40: 756d 6d61 7279 0a0a 2020 2020 7461 6275  ummary..    tabu
-00017a50: 6c61 7465 5f66 6e20 3d20 7375 6d6d 6172  late_fn = summar
-00017a60: 792e 7461 6275 6c61 7465 280a 2020 2020  y.tabulate(.    
-00017a70: 2020 7365 6c66 2c0a 2020 2020 2020 726e    self,.      rn
-00017a80: 6773 2c0a 2020 2020 2020 6465 7074 683d  gs,.      depth=
-00017a90: 6465 7074 682c 0a20 2020 2020 2073 686f  depth,.      sho
-00017aa0: 775f 7265 7065 6174 6564 3d73 686f 775f  w_repeated=show_
-00017ab0: 7265 7065 6174 6564 2c0a 2020 2020 2020  repeated,.      
-00017ac0: 6d75 7461 626c 653d 6d75 7461 626c 652c  mutable=mutable,
-00017ad0: 0a20 2020 2020 2063 6f6e 736f 6c65 5f6b  .      console_k
-00017ae0: 7761 7267 733d 636f 6e73 6f6c 655f 6b77  wargs=console_kw
-00017af0: 6172 6773 2c0a 2020 2020 2020 7461 626c  args,.      tabl
-00017b00: 655f 6b77 6172 6773 3d74 6162 6c65 5f6b  e_kwargs=table_k
-00017b10: 7761 7267 732c 0a20 2020 2020 2063 6f6c  wargs,.      col
-00017b20: 756d 6e5f 6b77 6172 6773 3d63 6f6c 756d  umn_kwargs=colum
-00017b30: 6e5f 6b77 6172 6773 2c0a 2020 2020 2020  n_kwargs,.      
-00017b40: 636f 6d70 7574 655f 666c 6f70 733d 636f  compute_flops=co
-00017b50: 6d70 7574 655f 666c 6f70 732c 0a20 2020  mpute_flops,.   
-00017b60: 2020 2063 6f6d 7075 7465 5f76 6a70 5f66     compute_vjp_f
-00017b70: 6c6f 7073 3d63 6f6d 7075 7465 5f76 6a70  lops=compute_vjp
-00017b80: 5f66 6c6f 7073 2c0a 2020 2020 290a 2020  _flops,.    ).  
-00017b90: 2020 7265 7475 726e 2074 6162 756c 6174    return tabulat
-00017ba0: 655f 666e 282a 6172 6773 2c20 2a2a 6b77  e_fn(*args, **kw
-00017bb0: 6172 6773 290a 0a20 2064 6566 206d 6f64  args)..  def mod
-00017bc0: 756c 655f 7061 7468 7328 0a20 2020 2073  ule_paths(.    s
-00017bd0: 656c 662c 0a20 2020 2072 6e67 733a 2055  elf,.    rngs: U
-00017be0: 6e69 6f6e 5b50 524e 474b 6579 2c20 524e  nion[PRNGKey, RN
-00017bf0: 4753 6571 7565 6e63 6573 5d2c 0a20 2020  GSequences],.   
-00017c00: 202a 6172 6773 2c0a 2020 2020 7368 6f77   *args,.    show
-00017c10: 5f72 6570 6561 7465 643a 2062 6f6f 6c20  _repeated: bool 
-00017c20: 3d20 4661 6c73 652c 0a20 2020 206d 7574  = False,.    mut
-00017c30: 6162 6c65 3a20 436f 6c6c 6563 7469 6f6e  able: Collection
-00017c40: 4669 6c74 6572 203d 2044 656e 794c 6973  Filter = DenyLis
-00017c50: 7428 2769 6e74 6572 6d65 6469 6174 6573  t('intermediates
-00017c60: 2729 2c0a 2020 2020 2a2a 6b77 6172 6773  '),.    **kwargs
-00017c70: 2c0a 2020 2920 2d3e 2064 6963 745b 7374  ,.  ) -> dict[st
-00017c80: 722c 2027 4d6f 6475 6c65 275d 3a0a 2020  r, 'Module']:.  
-00017c90: 2020 2222 2252 6574 7572 6e73 2061 2064    """Returns a d
-00017ca0: 6963 7469 6f6e 6172 7920 6d61 7070 696e  ictionary mappin
-00017cb0: 6720 6d6f 6475 6c65 2070 6174 6873 2074  g module paths t
-00017cc0: 6f20 6d6f 6475 6c65 2069 6e73 7461 6e63  o module instanc
-00017cd0: 6573 2e0a 0a20 2020 2054 6869 7320 6d65  es...    This me
-00017ce0: 7468 6f64 2068 6173 2074 6865 2073 616d  thod has the sam
-00017cf0: 6520 7369 676e 6174 7572 6520 616e 6420  e signature and 
-00017d00: 696e 7465 726e 616c 6c79 2063 616c 6c73  internally calls
-00017d10: 2060 604d 6f64 756c 652e 696e 6974 6060   ``Module.init``
-00017d20: 2c0a 2020 2020 6275 7420 696e 7374 6561  ,.    but instea
-00017d30: 6420 6f66 2072 6574 7572 6e69 6e67 2074  d of returning t
-00017d40: 6865 2076 6172 6961 626c 6573 2c20 6974  he variables, it
-00017d50: 2072 6574 7572 6e73 2061 2064 6963 7469   returns a dicti
-00017d60: 6f6e 6172 7920 6d61 7070 696e 670a 2020  onary mapping.  
-00017d70: 2020 6d6f 6475 6c65 2070 6174 6873 2074    module paths t
-00017d80: 6f20 756e 626f 756e 6465 6420 636f 7069  o unbounded copi
-00017d90: 6573 206f 6620 6d6f 6475 6c65 2069 6e73  es of module ins
-00017da0: 7461 6e63 6573 2074 6861 7420 7765 7265  tances that were
-00017db0: 2075 7365 640a 2020 2020 6174 2072 756e   used.    at run
-00017dc0: 7469 6d65 2e20 6060 6d6f 6475 6c65 5f70  time. ``module_p
-00017dd0: 6174 6873 6060 2075 7365 7320 6060 6a61  aths`` uses ``ja
-00017de0: 782e 6576 616c 5f73 6861 7065 6060 2074  x.eval_shape`` t
-00017df0: 6f20 7275 6e20 7468 6520 666f 7277 6172  o run the forwar
-00017e00: 640a 2020 2020 636f 6d70 7574 6174 696f  d.    computatio
-00017e10: 6e20 7769 7468 6f75 7420 636f 6e73 756d  n without consum
-00017e20: 696e 6720 616e 7920 464c 4f50 7320 6f72  ing any FLOPs or
-00017e30: 2061 6c6c 6f63 6174 696e 6720 6d65 6d6f   allocating memo
-00017e40: 7279 2e0a 0a20 2020 2045 7861 6d70 6c65  ry...    Example
-00017e50: 3a3a 0a0a 2020 2020 2020 3e3e 3e20 696d  ::..      >>> im
-00017e60: 706f 7274 2066 6c61 782e 6c69 6e65 6e20  port flax.linen 
-00017e70: 6173 206e 6e0a 2020 2020 2020 3e3e 3e20  as nn.      >>> 
-00017e80: 696d 706f 7274 206a 6178 2c20 6a61 782e  import jax, jax.
-00017e90: 6e75 6d70 7920 6173 206a 6e70 0a0a 2020  numpy as jnp..  
-00017ea0: 2020 2020 3e3e 3e20 636c 6173 7320 466f      >>> class Fo
-00017eb0: 6f28 6e6e 2e4d 6f64 756c 6529 3a0a 2020  o(nn.Module):.  
-00017ec0: 2020 2020 2e2e 2e20 2020 406e 6e2e 636f      ...   @nn.co
-00017ed0: 6d70 6163 740a 2020 2020 2020 2e2e 2e20  mpact.      ... 
-00017ee0: 2020 6465 6620 5f5f 6361 6c6c 5f5f 2873    def __call__(s
-00017ef0: 656c 662c 2078 293a 0a20 2020 2020 202e  elf, x):.      .
-00017f00: 2e2e 2020 2020 2068 203d 206e 6e2e 4465  ..     h = nn.De
-00017f10: 6e73 6528 3429 2878 290a 2020 2020 2020  nse(4)(x).      
-00017f20: 2e2e 2e20 2020 2020 7265 7475 726e 206e  ...     return n
-00017f30: 6e2e 4465 6e73 6528 3229 2868 290a 0a20  n.Dense(2)(h).. 
-00017f40: 2020 2020 203e 3e3e 2078 203d 206a 6e70       >>> x = jnp
-00017f50: 2e6f 6e65 7328 2831 362c 2039 2929 0a20  .ones((16, 9)). 
-00017f60: 2020 2020 203e 3e3e 206d 6f64 756c 6573       >>> modules
-00017f70: 203d 2046 6f6f 2829 2e6d 6f64 756c 655f   = Foo().module_
-00017f80: 7061 7468 7328 6a61 782e 7261 6e64 6f6d  paths(jax.random
-00017f90: 2e6b 6579 2830 292c 2078 290a 2020 2020  .key(0), x).    
-00017fa0: 2020 3e3e 3e20 7072 696e 7428 7b0a 2020    >>> print({.  
-00017fb0: 2020 2020 2e2e 2e20 2020 2020 703a 2074      ...     p: t
-00017fc0: 7970 6528 6d29 2e5f 5f6e 616d 655f 5f20  ype(m).__name__ 
-00017fd0: 666f 7220 702c 206d 2069 6e20 6d6f 6475  for p, m in modu
-00017fe0: 6c65 732e 6974 656d 7328 290a 2020 2020  les.items().    
-00017ff0: 2020 2e2e 2e20 7d29 0a20 2020 2020 207b    ... }).      {
-00018000: 2727 3a20 2746 6f6f 272c 2027 4465 6e73  '': 'Foo', 'Dens
-00018010: 655f 3027 3a20 2744 656e 7365 272c 2027  e_0': 'Dense', '
-00018020: 4465 6e73 655f 3127 3a20 2744 656e 7365  Dense_1': 'Dense
-00018030: 277d 0a0a 2020 2020 4172 6773 3a0a 2020  '}..    Args:.  
-00018040: 2020 2020 726e 6773 3a20 5468 6520 726e      rngs: The rn
-00018050: 6773 2066 6f72 2074 6865 2076 6172 6961  gs for the varia
-00018060: 626c 6520 636f 6c6c 6563 7469 6f6e 7320  ble collections 
-00018070: 6173 2070 6173 7365 6420 746f 2060 604d  as passed to ``M
-00018080: 6f64 756c 652e 696e 6974 6060 2e0a 2020  odule.init``..  
-00018090: 2020 2020 2a61 7267 733a 2054 6865 2061      *args: The a
-000180a0: 7267 756d 656e 7473 2074 6f20 7468 6520  rguments to the 
-000180b0: 666f 7277 6172 6420 636f 6d70 7574 6174  forward computat
-000180c0: 696f 6e2e 0a20 2020 2020 2073 686f 775f  ion..      show_
-000180d0: 7265 7065 6174 6564 3a20 4966 2060 6054  repeated: If ``T
-000180e0: 7275 6560 602c 2072 6570 6561 7465 6420  rue``, repeated 
-000180f0: 6361 6c6c 7320 746f 2074 6865 2073 616d  calls to the sam
-00018100: 6520 6d6f 6475 6c65 2077 696c 6c20 6265  e module will be
-00018110: 0a20 2020 2020 2020 2073 686f 776e 2069  .        shown i
-00018120: 6e20 7468 6520 7461 626c 652c 206f 7468  n the table, oth
-00018130: 6572 7769 7365 206f 6e6c 7920 7468 6520  erwise only the 
-00018140: 6669 7273 7420 6361 6c6c 2077 696c 6c20  first call will 
-00018150: 6265 2073 686f 776e 2e0a 2020 2020 2020  be shown..      
-00018160: 2020 4465 6661 756c 7420 6973 2060 6046    Default is ``F
-00018170: 616c 7365 6060 2e0a 2020 2020 2020 6d75  alse``..      mu
-00018180: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
-00018190: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
-000181a0: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
-000181b0: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
-000181c0: 6f75 6c64 0a20 2020 2020 2020 2062 6520  ould.        be 
-000181d0: 7472 6561 7465 6420 6173 206d 7574 6162  treated as mutab
-000181e0: 6c65 3a20 6060 626f 6f6c 6060 3a20 616c  le: ``bool``: al
-000181f0: 6c2f 6e6f 2063 6f6c 6c65 6374 696f 6e73  l/no collections
-00018200: 2061 7265 206d 7574 6162 6c65 2e0a 2020   are mutable..  
-00018210: 2020 2020 2020 6060 7374 7260 603a 2054        ``str``: T
-00018220: 6865 206e 616d 6520 6f66 2061 2073 696e  he name of a sin
-00018230: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
-00018240: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
-00018250: 3a20 4120 6c69 7374 206f 660a 2020 2020  : A list of.    
-00018260: 2020 2020 6e61 6d65 7320 6f66 206d 7574      names of mut
-00018270: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
-00018280: 2e20 4279 2064 6566 6175 6c74 2c20 616c  . By default, al
-00018290: 6c20 636f 6c6c 6563 7469 6f6e 7320 6578  l collections ex
-000182a0: 6365 7074 0a20 2020 2020 2020 2027 696e  cept.        'in
-000182b0: 7465 726d 6564 6961 7465 7327 2061 7265  termediates' are
-000182c0: 206d 7574 6162 6c65 2e0a 2020 2020 2020   mutable..      
-000182d0: 2a2a 6b77 6172 6773 3a20 6b65 7977 6f72  **kwargs: keywor
-000182e0: 6420 6172 6775 6d65 6e74 7320 746f 2070  d arguments to p
-000182f0: 6173 7320 746f 2074 6865 2066 6f72 7761  ass to the forwa
-00018300: 7264 2063 6f6d 7075 7461 7469 6f6e 2e0a  rd computation..
-00018310: 0a20 2020 2052 6574 7572 6e73 3a0a 2020  .    Returns:.  
-00018320: 2020 2020 4120 6469 6374 6069 6f6e 6172      A dict`ionar
-00018330: 7920 6d61 7070 696e 6720 6d6f 6475 6c65  y mapping module
-00018340: 2070 6174 6873 2074 6f20 6d6f 6475 6c65   paths to module
-00018350: 2069 6e73 7461 6e63 6573 2e0a 2020 2020   instances..    
-00018360: 2222 220a 2020 2020 6672 6f6d 2066 6c61  """.    from fla
-00018370: 782e 6c69 6e65 6e20 696d 706f 7274 2073  x.linen import s
-00018380: 756d 6d61 7279 0a0a 2020 2020 7461 626c  ummary..    tabl
-00018390: 6520 3d20 7375 6d6d 6172 792e 5f67 6574  e = summary._get
-000183a0: 5f6d 6f64 756c 655f 7461 626c 6528 0a20  _module_table(. 
-000183b0: 2020 2020 206d 6f64 756c 653d 7365 6c66       module=self
-000183c0: 2c0a 2020 2020 2020 6465 7074 683d 4e6f  ,.      depth=No
-000183d0: 6e65 2c0a 2020 2020 2020 7368 6f77 5f72  ne,.      show_r
-000183e0: 6570 6561 7465 643d 7368 6f77 5f72 6570  epeated=show_rep
-000183f0: 6561 7465 642c 0a20 2020 2020 2063 6f6d  eated,.      com
-00018400: 7075 7465 5f66 6c6f 7073 3d46 616c 7365  pute_flops=False
-00018410: 2c0a 2020 2020 2020 636f 6d70 7574 655f  ,.      compute_
-00018420: 766a 705f 666c 6f70 733d 4661 6c73 652c  vjp_flops=False,
-00018430: 0a20 2020 2029 2872 6e67 732c 202a 6172  .    )(rngs, *ar
-00018440: 6773 2c20 2a2a 6b77 6172 6773 2c20 6d75  gs, **kwargs, mu
-00018450: 7461 626c 653d 6d75 7461 626c 6529 0a0a  table=mutable)..
-00018460: 2020 2020 7265 7475 726e 207b 272f 272e      return {'/'.
-00018470: 6a6f 696e 2872 6f77 2e70 6174 6829 3a20  join(row.path): 
-00018480: 726f 772e 6d6f 6475 6c65 5f63 6f70 7920  row.module_copy 
-00018490: 666f 7220 726f 7720 696e 2074 6162 6c65  for row in table
-000184a0: 7d0a 0a0a 5f50 6172 656e 7454 7970 6520  }..._ParentType 
-000184b0: 3d20 556e 696f 6e5b 5479 7065 5b4d 6f64  = Union[Type[Mod
-000184c0: 756c 655d 2c20 5363 6f70 652c 2054 7970  ule], Scope, Typ
-000184d0: 655b 5f53 656e 7469 6e65 6c5d 2c20 4e6f  e[_Sentinel], No
-000184e0: 6e65 5d0a 0a0a 6465 6620 6d65 7267 655f  ne]...def merge_
-000184f0: 7061 7261 6d28 6e61 6d65 3a20 7374 722c  param(name: str,
-00018500: 2061 3a20 4f70 7469 6f6e 616c 5b54 5d2c   a: Optional[T],
-00018510: 2062 3a20 4f70 7469 6f6e 616c 5b54 5d29   b: Optional[T])
-00018520: 202d 3e20 543a 0a20 2022 2222 4d65 7267   -> T:.  """Merg
-00018530: 6573 2063 6f6e 7374 7275 6374 696f 6e2d  es construction-
-00018540: 2061 6e64 2063 616c 6c2d 7469 6d65 2061   and call-time a
-00018550: 7267 756d 656e 742e 0a0a 2020 5468 6973  rgument...  This
-00018560: 2069 7320 6120 7574 696c 6974 7920 666f   is a utility fo
-00018570: 7220 7375 7070 6f72 7469 6e67 2061 2070  r supporting a p
-00018580: 6174 7465 726e 2077 6865 7265 2061 204d  attern where a M
-00018590: 6f64 756c 6520 6879 7065 7270 6172 616d  odule hyperparam
-000185a0: 6574 6572 0a20 2063 616e 2062 6520 7061  eter.  can be pa
-000185b0: 7373 6564 2065 6974 6865 7220 746f 2060  ssed either to `
-000185c0: 605f 5f69 6e69 745f 5f60 6020 6f72 2060  `__init__`` or `
-000185d0: 605f 5f63 616c 6c5f 5f60 602c 2061 6e64  `__call__``, and
-000185e0: 2074 6865 2076 616c 7565 2074 6861 7420   the value that 
-000185f0: 6973 0a20 206e 6f74 2060 604e 6f6e 6560  is.  not ``None`
-00018600: 6020 7769 6c6c 2062 6520 7573 6564 2e0a  ` will be used..
-00018610: 0a20 2045 7861 6d70 6c65 3a3a 0a0a 2020  .  Example::..  
-00018620: 2020 3e3e 3e20 696d 706f 7274 2066 6c61    >>> import fla
-00018630: 782e 6c69 6e65 6e20 6173 206e 6e0a 2020  x.linen as nn.  
-00018640: 2020 3e3e 3e20 6672 6f6d 2074 7970 696e    >>> from typin
-00018650: 6720 696d 706f 7274 204f 7074 696f 6e61  g import Optiona
-00018660: 6c0a 0a20 2020 203e 3e3e 2063 6c61 7373  l..    >>> class
-00018670: 2046 6f6f 286e 6e2e 4d6f 6475 6c65 293a   Foo(nn.Module):
-00018680: 0a20 2020 202e 2e2e 2020 2074 7261 696e  .    ...   train
-00018690: 3a20 4f70 7469 6f6e 616c 5b62 6f6f 6c5d  : Optional[bool]
-000186a0: 203d 204e 6f6e 650a 0a20 2020 202e 2e2e   = None..    ...
-000186b0: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
-000186c0: 7365 6c66 2c20 7472 6169 6e3a 204f 7074  self, train: Opt
-000186d0: 696f 6e61 6c5b 626f 6f6c 5d20 3d20 4e6f  ional[bool] = No
-000186e0: 6e65 293a 0a20 2020 202e 2e2e 2020 2020  ne):.    ...    
-000186f0: 2074 7261 696e 203d 206e 6e2e 6d65 7267   train = nn.merg
-00018700: 655f 7061 7261 6d28 2774 7261 696e 272c  e_param('train',
-00018710: 2073 656c 662e 7472 6169 6e2c 2074 7261   self.train, tra
-00018720: 696e 290a 0a20 2041 6e20 6572 726f 7220  in)..  An error 
-00018730: 6973 2074 6872 6f77 6e20 7768 656e 2062  is thrown when b
-00018740: 6f74 6820 6172 6775 6d65 6e74 7320 6172  oth arguments ar
-00018750: 6520 6060 4e6f 6e65 6060 206f 7220 626f  e ``None`` or bo
-00018760: 7468 2076 616c 7565 7320 6172 6520 6e6f  th values are no
-00018770: 740a 2020 6060 4e6f 6e65 6060 2e0a 0a20  t.  ``None``... 
-00018780: 2041 7267 733a 0a20 2020 206e 616d 653a   Args:.    name:
-00018790: 2074 6865 206e 616d 6520 6f66 2074 6865   the name of the
-000187a0: 2070 6172 616d 6574 6572 2e20 5573 6564   parameter. Used
-000187b0: 2066 6f72 2065 7272 6f72 206d 6573 7361   for error messa
-000187c0: 6765 732e 0a20 2020 2061 3a20 6f70 7469  ges..    a: opti
-000187d0: 6f6e 2061 0a20 2020 2062 3a20 6f70 7469  on a.    b: opti
-000187e0: 6f6e 2062 0a0a 2020 5265 7475 726e 733a  on b..  Returns:
-000187f0: 0a20 2020 2061 206f 7220 6220 7768 6963  .    a or b whic
-00018800: 6865 7665 7220 6973 206e 6f74 2060 604e  hever is not ``N
-00018810: 6f6e 6560 602e 0a20 2022 2222 0a20 2069  one``..  """.  i
-00018820: 6620 6120 6973 204e 6f6e 6520 616e 6420  f a is None and 
-00018830: 6220 6973 204e 6f6e 653a 0a20 2020 2072  b is None:.    r
-00018840: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
-00018850: 0a20 2020 2020 2066 2750 6172 616d 6574  .      f'Paramet
-00018860: 6572 2022 7b6e 616d 657d 2220 6d75 7374  er "{name}" must
-00018870: 2062 6520 7061 7373 6564 2074 6f20 7468   be passed to th
-00018880: 6520 636f 6e73 7472 7563 746f 7220 6f72  e constructor or
-00018890: 2061 7420 6361 6c6c 2074 696d 652e 270a   at call time.'.
-000188a0: 2020 2020 290a 2020 6966 2061 2069 7320      ).  if a is 
-000188b0: 6e6f 7420 4e6f 6e65 2061 6e64 2062 2069  not None and b i
-000188c0: 7320 6e6f 7420 4e6f 6e65 3a0a 2020 2020  s not None:.    
-000188d0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-000188e0: 280a 2020 2020 2020 6627 5061 7261 6d65  (.      f'Parame
-000188f0: 7465 7220 227b 6e61 6d65 7d22 2077 6173  ter "{name}" was
-00018900: 2070 6173 7365 6420 746f 2074 6865 2063   passed to the c
-00018910: 6f6e 7374 7275 6374 6f72 2061 6e64 2061  onstructor and a
-00018920: 7420 6361 6c6c 2074 696d 652e 270a 2020  t call time.'.  
-00018930: 2020 2020 2720 5368 6f75 6c64 2062 6520      ' Should be 
-00018940: 7061 7373 6564 206a 7573 7420 6f6e 6365  passed just once
-00018950: 2e27 0a20 2020 2029 0a20 2069 6620 6120  .'.    ).  if a 
-00018960: 6973 204e 6f6e 653a 0a20 2020 2061 7373  is None:.    ass
-00018970: 6572 7420 6220 6973 206e 6f74 204e 6f6e  ert b is not Non
-00018980: 650a 2020 2020 7265 7475 726e 2062 0a20  e.    return b. 
-00018990: 2072 6574 7572 6e20 610a 0a0a 4074 7261   return a...@tra
-000189a0: 6365 6261 636b 5f75 7469 6c2e 6170 695f  ceback_util.api_
-000189b0: 626f 756e 6461 7279 0a64 6566 2061 7070  boundary.def app
-000189c0: 6c79 280a 2020 666e 3a20 4361 6c6c 6162  ly(.  fn: Callab
-000189d0: 6c65 5b2e 2e2e 2c20 416e 795d 2c0a 2020  le[..., Any],.  
-000189e0: 6d6f 6475 6c65 3a20 4d6f 6475 6c65 2c0a  module: Module,.
-000189f0: 2020 6d75 7461 626c 653a 2043 6f6c 6c65    mutable: Colle
-00018a00: 6374 696f 6e46 696c 7465 7220 3d20 4661  ctionFilter = Fa
-00018a10: 6c73 652c 0a20 2063 6170 7475 7265 5f69  lse,.  capture_i
-00018a20: 6e74 6572 6d65 6469 6174 6573 3a20 556e  ntermediates: Un
-00018a30: 696f 6e5b 626f 6f6c 2c20 4361 6c6c 6162  ion[bool, Callab
-00018a40: 6c65 5b5b 4d6f 6475 6c65 2c20 7374 725d  le[[Module, str]
-00018a50: 2c20 626f 6f6c 5d5d 203d 2046 616c 7365  , bool]] = False
-00018a60: 2c0a 2920 2d3e 2043 616c 6c61 626c 655b  ,.) -> Callable[
-00018a70: 2e2e 2e2c 2041 6e79 5d3a 0a20 2022 2222  ..., Any]:.  """
-00018a80: 4372 6561 7465 7320 616e 2061 7070 6c79  Creates an apply
-00018a90: 2066 756e 6374 696f 6e20 746f 2063 616c   function to cal
-00018aa0: 6c20 6060 666e 6060 2077 6974 6820 6120  l ``fn`` with a 
-00018ab0: 626f 756e 6420 6d6f 6475 6c65 2e0a 0a20  bound module... 
-00018ac0: 2055 6e6c 696b 6520 6060 4d6f 6475 6c65   Unlike ``Module
-00018ad0: 2e61 7070 6c79 6060 2074 6869 7320 6675  .apply`` this fu
-00018ae0: 6e63 7469 6f6e 2072 6574 7572 6e73 2061  nction returns a
-00018af0: 206e 6577 2066 756e 6374 696f 6e20 7769   new function wi
-00018b00: 7468 2074 6865 0a20 2073 6967 6e61 7475  th the.  signatu
-00018b10: 7265 2060 6028 7661 7269 6162 6c65 732c  re ``(variables,
-00018b20: 202a 6172 6773 2c20 726e 6773 3d4e 6f6e   *args, rngs=Non
-00018b30: 652c 202a 2a6b 7761 7267 7329 202d 3e20  e, **kwargs) -> 
-00018b40: 5460 6020 7768 6572 6520 6060 5460 6020  T`` where ``T`` 
-00018b50: 6973 2074 6865 0a20 2072 6574 7572 6e20  is the.  return 
-00018b60: 7479 7065 206f 6620 6060 666e 6060 2e20  type of ``fn``. 
-00018b70: 4966 2060 606d 7574 6162 6c65 6060 2069  If ``mutable`` i
-00018b80: 7320 6e6f 7420 6060 4661 6c73 6560 6020  s not ``False`` 
-00018b90: 7468 6520 7265 7475 726e 2074 7970 6520  the return type 
-00018ba0: 6973 2061 0a20 2074 7570 6c65 2077 6865  is a.  tuple whe
-00018bb0: 7265 2074 6865 2073 6563 6f6e 6420 6974  re the second it
-00018bc0: 656d 2069 7320 6120 6060 4672 6f7a 656e  em is a ``Frozen
-00018bd0: 4469 6374 6060 2077 6974 6820 7468 6520  Dict`` with the 
-00018be0: 6d75 7461 7465 6420 7661 7269 6162 6c65  mutated variable
-00018bf0: 732e 0a0a 2020 5468 6520 6170 706c 7920  s...  The apply 
-00018c00: 6675 6e63 7469 6f6e 2074 6861 7420 6973  function that is
-00018c10: 2072 6574 7572 6e65 6420 6361 6e20 6265   returned can be
-00018c20: 2064 6972 6563 746c 7920 636f 6d70 6f73   directly compos
-00018c30: 6564 2077 6974 680a 2020 4a41 5820 7472  ed with.  JAX tr
-00018c40: 616e 7366 6f72 6d61 7469 6f6e 7320 6c69  ansformations li
-00018c50: 6b65 2060 606a 6178 2e6a 6974 6060 3a3a  ke ``jax.jit``::
-00018c60: 0a0a 2020 2020 3e3e 3e20 636c 6173 7320  ..    >>> class 
-00018c70: 466f 6f28 6e6e 2e4d 6f64 756c 6529 3a0a  Foo(nn.Module):.
-00018c80: 2020 2020 2e2e 2e20 2020 6465 6620 656e      ...   def en
-00018c90: 636f 6465 2873 656c 662c 2078 293a 0a20  code(self, x):. 
-00018ca0: 2020 202e 2e2e 2020 2020 202e 2e2e 0a20     ...     .... 
-00018cb0: 2020 202e 2e2e 2020 2064 6566 2064 6563     ...   def dec
-00018cc0: 6f64 6528 7365 6c66 2c20 7829 3a0a 2020  ode(self, x):.  
-00018cd0: 2020 2e2e 2e20 2020 2020 2e2e 2e0a 0a20    ...     ..... 
-00018ce0: 2020 203e 3e3e 2064 6566 2066 2866 6f6f     >>> def f(foo
-00018cf0: 2c20 7829 3a0a 2020 2020 2e2e 2e20 2020  , x):.    ...   
-00018d00: 7a20 3d20 666f 6f2e 656e 636f 6465 2878  z = foo.encode(x
-00018d10: 290a 2020 2020 2e2e 2e20 2020 7920 3d20  ).    ...   y = 
-00018d20: 666f 6f2e 6465 636f 6465 287a 290a 2020  foo.decode(z).  
-00018d30: 2020 2e2e 2e20 2020 2320 2e2e 2e0a 2020    ...   # ....  
-00018d40: 2020 2e2e 2e20 2020 7265 7475 726e 2079    ...   return y
-00018d50: 0a0a 2020 2020 3e3e 3e20 7661 7269 6162  ..    >>> variab
-00018d60: 6c65 7320 3d20 7b7d 0a20 2020 203e 3e3e  les = {}.    >>>
-00018d70: 2066 6f6f 203d 2046 6f6f 2829 0a20 2020   foo = Foo().   
-00018d80: 203e 3e3e 2066 5f6a 6974 7465 6420 3d20   >>> f_jitted = 
-00018d90: 6a61 782e 6a69 7428 6e6e 2e61 7070 6c79  jax.jit(nn.apply
-00018da0: 2866 2c20 666f 6f29 290a 2020 2020 3e3e  (f, foo)).    >>
-00018db0: 3e20 665f 6a69 7474 6564 2876 6172 6961  > f_jitted(varia
-00018dc0: 626c 6573 2c20 6a6e 702e 6f6e 6573 2828  bles, jnp.ones((
-00018dd0: 312c 2033 2929 290a 0a20 2041 7267 733a  1, 3)))..  Args:
-00018de0: 0a20 2020 2066 6e3a 2054 6865 2066 756e  .    fn: The fun
-00018df0: 6374 696f 6e20 7468 6174 2073 686f 756c  ction that shoul
-00018e00: 6420 6265 2061 7070 6c69 6564 2e20 5468  d be applied. Th
-00018e10: 6520 6669 7273 7420 6172 6775 6d65 6e74  e first argument
-00018e20: 2070 6173 7365 6420 7769 6c6c 2062 650a   passed will be.
-00018e30: 2020 2020 2020 6120 6d6f 6475 6c65 2069        a module i
-00018e40: 6e73 7461 6e63 6520 6f66 2074 6865 2060  nstance of the `
-00018e50: 606d 6f64 756c 6560 6020 7769 7468 2076  `module`` with v
-00018e60: 6172 6961 626c 6573 2061 6e64 2052 4e47  ariables and RNG
-00018e70: 7320 626f 756e 6420 746f 2069 742e 0a20  s bound to it.. 
-00018e80: 2020 206d 6f64 756c 653a 2054 6865 2060     module: The `
-00018e90: 604d 6f64 756c 6560 6020 7468 6174 2077  `Module`` that w
-00018ea0: 696c 6c20 6265 2075 7365 6420 746f 2062  ill be used to b
-00018eb0: 696e 6420 7661 7269 6162 6c65 7320 616e  ind variables an
-00018ec0: 6420 524e 4773 2074 6f2e 2054 6865 0a20  d RNGs to. The. 
-00018ed0: 2020 2020 2060 604d 6f64 756c 6560 6020       ``Module`` 
-00018ee0: 7061 7373 6564 2061 7320 7468 6520 6669  passed as the fi
-00018ef0: 7273 7420 6172 6775 6d65 6e74 2074 6f20  rst argument to 
-00018f00: 6060 666e 6060 2077 696c 6c20 6265 2061  ``fn`` will be a
-00018f10: 2063 6c6f 6e65 206f 660a 2020 2020 2020   clone of.      
-00018f20: 6d6f 6475 6c65 2e0a 2020 2020 6d75 7461  module..    muta
-00018f30: 626c 653a 2043 616e 2062 6520 626f 6f6c  ble: Can be bool
-00018f40: 2c20 7374 722c 206f 7220 6c69 7374 2e20  , str, or list. 
-00018f50: 5370 6563 6966 6965 7320 7768 6963 6820  Specifies which 
-00018f60: 636f 6c6c 6563 7469 6f6e 7320 7368 6f75  collections shou
-00018f70: 6c64 2062 650a 2020 2020 2020 7472 6561  ld be.      trea
-00018f80: 7465 6420 6173 206d 7574 6162 6c65 3a20  ted as mutable: 
-00018f90: 6060 626f 6f6c 6060 3a20 616c 6c2f 6e6f  ``bool``: all/no
-00018fa0: 2063 6f6c 6c65 6374 696f 6e73 2061 7265   collections are
-00018fb0: 206d 7574 6162 6c65 2e20 6060 7374 7260   mutable. ``str`
-00018fc0: 603a 2054 6865 0a20 2020 2020 206e 616d  `: The.      nam
-00018fd0: 6520 6f66 2061 2073 696e 676c 6520 6d75  e of a single mu
-00018fe0: 7461 626c 6520 636f 6c6c 6563 7469 6f6e  table collection
-00018ff0: 2e20 6060 6c69 7374 6060 3a20 4120 6c69  . ``list``: A li
-00019000: 7374 206f 6620 6e61 6d65 7320 6f66 206d  st of names of m
-00019010: 7574 6162 6c65 0a20 2020 2020 2063 6f6c  utable.      col
-00019020: 6c65 6374 696f 6e73 2e0a 2020 2020 6361  lections..    ca
-00019030: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
-00019040: 7465 733a 2049 6620 6060 5472 7565 6060  tes: If ``True``
-00019050: 2c20 6361 7074 7572 6573 2069 6e74 6572  , captures inter
-00019060: 6d65 6469 6174 6520 7265 7475 726e 2076  mediate return v
-00019070: 616c 7565 7320 6f66 2061 6c6c 0a20 2020  alues of all.   
-00019080: 2020 204d 6f64 756c 6573 2069 6e73 6964     Modules insid
-00019090: 6520 7468 6520 2269 6e74 6572 6d65 6469  e the "intermedi
-000190a0: 6174 6573 2220 636f 6c6c 6563 7469 6f6e  ates" collection
-000190b0: 2e20 4279 2064 6566 6175 6c74 2c20 6f6e  . By default, on
-000190c0: 6c79 2074 6865 2072 6574 7572 6e0a 2020  ly the return.  
-000190d0: 2020 2020 7661 6c75 6573 206f 6620 616c      values of al
-000190e0: 6c20 605f 5f63 616c 6c5f 5f60 206d 6574  l `__call__` met
-000190f0: 686f 6473 2061 7265 2073 746f 7265 642e  hods are stored.
-00019100: 2041 2066 756e 6374 696f 6e20 6361 6e20   A function can 
-00019110: 6265 2070 6173 7365 6420 746f 0a20 2020  be passed to.   
-00019120: 2020 2063 6861 6e67 6520 7468 6520 6669     change the fi
-00019130: 6c74 6572 2062 6568 6176 696f 722e 2054  lter behavior. T
-00019140: 6865 2066 696c 7465 7220 6675 6e63 7469  he filter functi
-00019150: 6f6e 2074 616b 6573 2074 6865 204d 6f64  on takes the Mod
-00019160: 756c 6520 696e 7374 616e 6365 0a20 2020  ule instance.   
-00019170: 2020 2061 6e64 206d 6574 686f 6420 6e61     and method na
-00019180: 6d65 2061 6e64 2072 6574 7572 6e73 2061  me and returns a
-00019190: 2062 6f6f 6c20 696e 6469 6361 7469 6e67   bool indicating
-000191a0: 2077 6865 7468 6572 2074 6865 206f 7574   whether the out
-000191b0: 7075 7420 6f66 2074 6861 740a 2020 2020  put of that.    
-000191c0: 2020 6d65 7468 6f64 2069 6e76 6f63 6174    method invocat
-000191d0: 696f 6e20 7368 6f75 6c64 2062 6520 7374  ion should be st
-000191e0: 6f72 6564 2e0a 0a20 2052 6574 7572 6e73  ored...  Returns
-000191f0: 3a0a 2020 2020 5468 6520 6170 706c 7920  :.    The apply 
-00019200: 6675 6e63 7469 6f6e 2077 7261 7070 696e  function wrappin
-00019210: 6720 6060 666e 6060 2e0a 2020 2222 220a  g ``fn``..  """.
-00019220: 0a20 2040 6675 6e63 746f 6f6c 732e 7772  .  @functools.wr
-00019230: 6170 7328 666e 290a 2020 6465 6620 7363  aps(fn).  def sc
-00019240: 6f70 655f 666e 2873 636f 7065 2c20 2a61  ope_fn(scope, *a
-00019250: 7267 732c 202a 2a6b 7761 7267 7329 3a0a  rgs, **kwargs):.
-00019260: 2020 2020 5f63 6f6e 7465 7874 2e63 6170      _context.cap
-00019270: 7475 7265 5f73 7461 636b 2e61 7070 656e  ture_stack.appen
-00019280: 6428 6361 7074 7572 655f 696e 7465 726d  d(capture_interm
-00019290: 6564 6961 7465 7329 0a20 2020 2074 7279  ediates).    try
-000192a0: 3a0a 2020 2020 2020 7265 7475 726e 2066  :.      return f
-000192b0: 6e28 6d6f 6475 6c65 2e63 6c6f 6e65 2870  n(module.clone(p
-000192c0: 6172 656e 743d 7363 6f70 652c 205f 6465  arent=scope, _de
-000192d0: 6570 5f63 6c6f 6e65 3d54 7275 6529 2c20  ep_clone=True), 
-000192e0: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
-000192f0: 0a20 2020 2066 696e 616c 6c79 3a0a 2020  .    finally:.  
-00019300: 2020 2020 5f63 6f6e 7465 7874 2e63 6170      _context.cap
-00019310: 7475 7265 5f73 7461 636b 2e70 6f70 2829  ture_stack.pop()
-00019320: 0a0a 2020 6966 2063 6170 7475 7265 5f69  ..  if capture_i
-00019330: 6e74 6572 6d65 6469 6174 6573 2069 7320  ntermediates is 
-00019340: 5472 7565 3a20 2023 2070 796c 696e 743a  True:  # pylint:
-00019350: 2064 6973 6162 6c65 3d67 2d62 6f6f 6c2d   disable=g-bool-
-00019360: 6964 2d63 6f6d 7061 7269 736f 6e0a 2020  id-comparison.  
-00019370: 2020 6361 7074 7572 655f 696e 7465 726d    capture_interm
-00019380: 6564 6961 7465 7320 3d20 6361 7074 7572  ediates = captur
-00019390: 655f 6361 6c6c 5f69 6e74 6572 6d65 6469  e_call_intermedi
-000193a0: 6174 6573 0a20 2069 6620 6361 7074 7572  ates.  if captur
-000193b0: 655f 696e 7465 726d 6564 6961 7465 733a  e_intermediates:
-000193c0: 0a20 2020 206d 7574 6162 6c65 203d 2075  .    mutable = u
-000193d0: 6e69 6f6e 5f66 696c 7465 7273 286d 7574  nion_filters(mut
-000193e0: 6162 6c65 2c20 2769 6e74 6572 6d65 6469  able, 'intermedi
-000193f0: 6174 6573 2729 0a20 2072 6574 7572 6e20  ates').  return 
-00019400: 636f 7265 2e61 7070 6c79 2873 636f 7065  core.apply(scope
-00019410: 5f66 6e2c 206d 7574 6162 6c65 3d6d 7574  _fn, mutable=mut
-00019420: 6162 6c65 290a 0a0a 4074 7261 6365 6261  able)...@traceba
-00019430: 636b 5f75 7469 6c2e 6170 695f 626f 756e  ck_util.api_boun
-00019440: 6461 7279 0a64 6566 2069 6e69 745f 7769  dary.def init_wi
-00019450: 7468 5f6f 7574 7075 7428 0a20 2066 6e3a  th_output(.  fn:
-00019460: 2043 616c 6c61 626c 655b 2e2e 2e2c 2041   Callable[..., A
-00019470: 6e79 5d2c 0a20 206d 6f64 756c 653a 204d  ny],.  module: M
-00019480: 6f64 756c 652c 0a20 206d 7574 6162 6c65  odule,.  mutable
-00019490: 3a20 436f 6c6c 6563 7469 6f6e 4669 6c74  : CollectionFilt
-000194a0: 6572 203d 2044 656e 794c 6973 7428 2769  er = DenyList('i
-000194b0: 6e74 6572 6d65 6469 6174 6573 2729 2c0a  ntermediates'),.
-000194c0: 2020 6361 7074 7572 655f 696e 7465 726d    capture_interm
-000194d0: 6564 6961 7465 733a 2055 6e69 6f6e 5b62  ediates: Union[b
-000194e0: 6f6f 6c2c 2043 616c 6c61 626c 655b 5b4d  ool, Callable[[M
-000194f0: 6f64 756c 652c 2073 7472 5d2c 2062 6f6f  odule, str], boo
-00019500: 6c5d 5d20 3d20 4661 6c73 652c 0a29 202d  l]] = False,.) -
-00019510: 3e20 4361 6c6c 6162 6c65 5b2e 2e2e 2c20  > Callable[..., 
-00019520: 5475 706c 655b 416e 792c 2055 6e69 6f6e  Tuple[Any, Union
-00019530: 5b46 726f 7a65 6e56 6172 6961 626c 6544  [FrozenVariableD
-00019540: 6963 742c 2044 6963 745b 7374 722c 2041  ict, Dict[str, A
-00019550: 6e79 5d5d 5d5d 3a0a 2020 2222 2243 7265  ny]]]]:.  """Cre
-00019560: 6174 6573 2061 6e20 696e 6974 2066 756e  ates an init fun
-00019570: 6374 696f 6e20 746f 2063 616c 6c20 6060  ction to call ``
-00019580: 666e 6060 2077 6974 6820 6120 626f 756e  fn`` with a boun
-00019590: 6420 6d6f 6475 6c65 2074 6861 7420 616c  d module that al
-000195a0: 736f 2072 6574 7572 6e73 2074 6865 2066  so returns the f
-000195b0: 756e 6374 696f 6e20 6f75 7470 7574 732e  unction outputs.
-000195c0: 0a0a 2020 556e 6c69 6b65 2060 604d 6f64  ..  Unlike ``Mod
-000195d0: 756c 652e 696e 6974 5f77 6974 685f 6f75  ule.init_with_ou
-000195e0: 7470 7574 6060 2074 6869 7320 6675 6e63  tput`` this func
-000195f0: 7469 6f6e 2072 6574 7572 6e73 2061 206e  tion returns a n
-00019600: 6577 2066 756e 6374 696f 6e20 7769 7468  ew function with
-00019610: 0a20 2074 6865 2073 6967 6e61 7475 7265  .  the signature
-00019620: 2060 6028 726e 6773 2c20 2a61 7267 732c   ``(rngs, *args,
-00019630: 202a 2a6b 7761 7267 7329 202d 3e20 2854   **kwargs) -> (T
-00019640: 2c20 7661 7269 6162 6c65 7329 6060 2077  , variables)`` w
-00019650: 6865 7265 2060 6054 6060 2069 7320 7468  here ``T`` is th
-00019660: 650a 2020 7265 7475 726e 2074 7970 6520  e.  return type 
-00019670: 6f66 2060 6066 6e60 602e 2054 6865 2072  of ``fn``. The r
-00019680: 6e67 7320 6361 6e20 6265 2061 2064 6963  ngs can be a dic
-00019690: 7420 6f66 2050 524e 474b 6579 7320 6f72  t of PRNGKeys or
-000196a0: 2061 2073 696e 676c 650a 2020 6060 6050   a single.  ```P
-000196b0: 524e 474b 6579 6060 2077 6869 6368 2069  RNGKey`` which i
-000196c0: 7320 6571 7569 7661 6c65 6e74 2074 6f20  s equivalent to 
-000196d0: 7061 7373 696e 6720 6120 6469 6374 2077  passing a dict w
-000196e0: 6974 6820 6f6e 6520 5052 4e47 4b65 7920  ith one PRNGKey 
-000196f0: 7769 7468 2074 6865 0a20 206e 616d 6520  with the.  name 
-00019700: 2270 6172 616d 7322 2e0a 0a20 2054 6865  "params"...  The
-00019710: 2069 6e69 7420 6675 6e63 7469 6f6e 2074   init function t
-00019720: 6861 7420 6973 2072 6574 7572 6e65 6420  hat is returned 
-00019730: 6361 6e20 6265 2064 6972 6563 746c 7920  can be directly 
-00019740: 636f 6d70 6f73 6564 2077 6974 680a 2020  composed with.  
-00019750: 4a41 5820 7472 616e 7366 6f72 6d61 7469  JAX transformati
-00019760: 6f6e 7320 6c69 6b65 2060 606a 6178 2e6a  ons like ``jax.j
-00019770: 6974 6060 3a3a 0a0a 2020 2020 3e3e 3e20  it``::..    >>> 
-00019780: 636c 6173 7320 466f 6f28 6e6e 2e4d 6f64  class Foo(nn.Mod
-00019790: 756c 6529 3a0a 2020 2020 2e2e 2e20 2020  ule):.    ...   
-000197a0: 6465 6620 656e 636f 6465 2873 656c 662c  def encode(self,
-000197b0: 2078 293a 0a20 2020 202e 2e2e 2020 2020   x):.    ...    
-000197c0: 202e 2e2e 0a20 2020 202e 2e2e 2020 2064   ....    ...   d
-000197d0: 6566 2064 6563 6f64 6528 7365 6c66 2c20  ef decode(self, 
-000197e0: 7829 3a0a 2020 2020 2e2e 2e20 2020 2020  x):.    ...     
-000197f0: 2e2e 2e0a 0a20 2020 203e 3e3e 2064 6566  .....    >>> def
-00019800: 2066 2866 6f6f 2c20 7829 3a0a 2020 2020   f(foo, x):.    
-00019810: 2e2e 2e20 2020 7a20 3d20 666f 6f2e 656e  ...   z = foo.en
-00019820: 636f 6465 2878 290a 2020 2020 2e2e 2e20  code(x).    ... 
-00019830: 2020 7920 3d20 666f 6f2e 6465 636f 6465    y = foo.decode
-00019840: 287a 290a 2020 2020 2e2e 2e20 2020 2320  (z).    ...   # 
-00019850: 2e2e 2e0a 2020 2020 2e2e 2e20 2020 7265  ....    ...   re
-00019860: 7475 726e 2079 0a0a 2020 2020 3e3e 3e20  turn y..    >>> 
-00019870: 666f 6f20 3d20 466f 6f28 290a 2020 2020  foo = Foo().    
-00019880: 3e3e 3e20 665f 6a69 7474 6564 203d 206a  >>> f_jitted = j
-00019890: 6178 2e6a 6974 286e 6e2e 696e 6974 5f77  ax.jit(nn.init_w
-000198a0: 6974 685f 6f75 7470 7574 2866 2c20 666f  ith_output(f, fo
-000198b0: 6f29 290a 2020 2020 3e3e 3e20 792c 2076  o)).    >>> y, v
-000198c0: 6172 6961 626c 6573 203d 2066 5f6a 6974  ariables = f_jit
-000198d0: 7465 6428 6a61 782e 7261 6e64 6f6d 2e6b  ted(jax.random.k
-000198e0: 6579 2830 292c 206a 6e70 2e6f 6e65 7328  ey(0), jnp.ones(
-000198f0: 2831 2c20 3329 2929 0a0a 2020 4172 6773  (1, 3)))..  Args
-00019900: 3a0a 2020 2020 666e 3a20 5468 6520 6675  :.    fn: The fu
-00019910: 6e63 7469 6f6e 2074 6861 7420 7368 6f75  nction that shou
-00019920: 6c64 2062 6520 6170 706c 6965 642e 2054  ld be applied. T
-00019930: 6865 2066 6972 7374 2061 7267 756d 656e  he first argumen
-00019940: 7420 7061 7373 6564 2077 696c 6c20 6265  t passed will be
-00019950: 0a20 2020 2020 2061 206d 6f64 756c 6520  .      a module 
-00019960: 696e 7374 616e 6365 206f 6620 7468 6520  instance of the 
-00019970: 6060 6d6f 6475 6c65 6060 2077 6974 6820  ``module`` with 
-00019980: 7661 7269 6162 6c65 7320 616e 6420 524e  variables and RN
-00019990: 4773 2062 6f75 6e64 2074 6f20 6974 2e0a  Gs bound to it..
-000199a0: 2020 2020 6d6f 6475 6c65 3a20 5468 6520      module: The 
-000199b0: 6060 4d6f 6475 6c65 6060 2074 6861 7420  ``Module`` that 
-000199c0: 7769 6c6c 2062 6520 7573 6564 2074 6f20  will be used to 
-000199d0: 6269 6e64 2076 6172 6961 626c 6573 2061  bind variables a
-000199e0: 6e64 2052 4e47 7320 746f 2e20 5468 650a  nd RNGs to. The.
-000199f0: 2020 2020 2020 6060 4d6f 6475 6c65 6060        ``Module``
-00019a00: 2070 6173 7365 6420 6173 2074 6865 2066   passed as the f
-00019a10: 6972 7374 2061 7267 756d 656e 7420 746f  irst argument to
-00019a20: 2060 6066 6e60 6020 7769 6c6c 2062 6520   ``fn`` will be 
-00019a30: 6120 636c 6f6e 6520 6f66 0a20 2020 2020  a clone of.     
-00019a40: 206d 6f64 756c 652e 0a20 2020 206d 7574   module..    mut
-00019a50: 6162 6c65 3a20 4361 6e20 6265 2062 6f6f  able: Can be boo
-00019a60: 6c2c 2073 7472 2c20 6f72 206c 6973 742e  l, str, or list.
-00019a70: 2053 7065 6369 6669 6573 2077 6869 6368   Specifies which
-00019a80: 2063 6f6c 6c65 6374 696f 6e73 2073 686f   collections sho
-00019a90: 756c 6420 6265 0a20 2020 2020 2074 7265  uld be.      tre
-00019aa0: 6174 6564 2061 7320 6d75 7461 626c 653a  ated as mutable:
-00019ab0: 2060 6062 6f6f 6c60 603a 2061 6c6c 2f6e   ``bool``: all/n
-00019ac0: 6f20 636f 6c6c 6563 7469 6f6e 7320 6172  o collections ar
-00019ad0: 6520 6d75 7461 626c 652e 2060 6073 7472  e mutable. ``str
-00019ae0: 6060 3a20 5468 650a 2020 2020 2020 6e61  ``: The.      na
-00019af0: 6d65 206f 6620 6120 7369 6e67 6c65 206d  me of a single m
-00019b00: 7574 6162 6c65 2063 6f6c 6c65 6374 696f  utable collectio
-00019b10: 6e2e 2060 606c 6973 7460 603a 2041 206c  n. ``list``: A l
-00019b20: 6973 7420 6f66 206e 616d 6573 206f 6620  ist of names of 
-00019b30: 6d75 7461 626c 650a 2020 2020 2020 636f  mutable.      co
-00019b40: 6c6c 6563 7469 6f6e 732e 2042 7920 6465  llections. By de
-00019b50: 6661 756c 742c 2061 6c6c 2063 6f6c 6c65  fault, all colle
-00019b60: 6374 696f 6e73 2065 7863 6570 7420 2269  ctions except "i
-00019b70: 6e74 6572 6d65 6469 6174 6573 2220 6172  ntermediates" ar
-00019b80: 650a 2020 2020 2020 6d75 7461 626c 652e  e.      mutable.
-00019b90: 0a20 2020 2063 6170 7475 7265 5f69 6e74  .    capture_int
-00019ba0: 6572 6d65 6469 6174 6573 3a20 4966 2060  ermediates: If `
-00019bb0: 6054 7275 6560 602c 2063 6170 7475 7265  `True``, capture
-00019bc0: 7320 696e 7465 726d 6564 6961 7465 2072  s intermediate r
-00019bd0: 6574 7572 6e20 7661 6c75 6573 206f 6620  eturn values of 
-00019be0: 616c 6c0a 2020 2020 2020 4d6f 6475 6c65  all.      Module
-00019bf0: 7320 696e 7369 6465 2074 6865 2022 696e  s inside the "in
-00019c00: 7465 726d 6564 6961 7465 7322 2063 6f6c  termediates" col
-00019c10: 6c65 6374 696f 6e2e 2042 7920 6465 6661  lection. By defa
-00019c20: 756c 742c 206f 6e6c 7920 7468 6520 7265  ult, only the re
-00019c30: 7475 726e 0a20 2020 2020 2076 616c 7565  turn.      value
-00019c40: 7320 6f66 2061 6c6c 2060 5f5f 6361 6c6c  s of all `__call
-00019c50: 5f5f 6020 6d65 7468 6f64 7320 6172 6520  __` methods are 
-00019c60: 7374 6f72 6564 2e20 4120 6675 6e63 7469  stored. A functi
-00019c70: 6f6e 2063 616e 2062 6520 7061 7373 6564  on can be passed
-00019c80: 2074 6f0a 2020 2020 2020 6368 616e 6765   to.      change
-00019c90: 2074 6865 2066 696c 7465 7220 6265 6861   the filter beha
-00019ca0: 7669 6f72 2e20 5468 6520 6669 6c74 6572  vior. The filter
-00019cb0: 2066 756e 6374 696f 6e20 7461 6b65 7320   function takes 
-00019cc0: 7468 6520 4d6f 6475 6c65 2069 6e73 7461  the Module insta
-00019cd0: 6e63 650a 2020 2020 2020 616e 6420 6d65  nce.      and me
-00019ce0: 7468 6f64 206e 616d 6520 616e 6420 7265  thod name and re
-00019cf0: 7475 726e 7320 6120 626f 6f6c 2069 6e64  turns a bool ind
-00019d00: 6963 6174 696e 6720 7768 6574 6865 7220  icating whether 
-00019d10: 7468 6520 6f75 7470 7574 206f 6620 7468  the output of th
-00019d20: 6174 0a20 2020 2020 206d 6574 686f 6420  at.      method 
-00019d30: 696e 766f 6361 7469 6f6e 2073 686f 756c  invocation shoul
-00019d40: 6420 6265 2073 746f 7265 642e 0a0a 2020  d be stored...  
-00019d50: 5265 7475 726e 733a 0a20 2020 2054 6865  Returns:.    The
-00019d60: 2069 6e69 7420 6675 6e63 7469 6f6e 2077   init function w
-00019d70: 7261 7070 696e 6720 6060 666e 6060 2e0a  rapping ``fn``..
-00019d80: 2020 2222 220a 0a20 2040 6675 6e63 746f    """..  @functo
-00019d90: 6f6c 732e 7772 6170 7328 666e 290a 2020  ols.wraps(fn).  
-00019da0: 6465 6620 7363 6f70 655f 666e 2873 636f  def scope_fn(sco
-00019db0: 7065 2c20 2a61 7267 732c 202a 2a6b 7761  pe, *args, **kwa
-00019dc0: 7267 7329 3a0a 2020 2020 5f63 6f6e 7465  rgs):.    _conte
-00019dd0: 7874 2e63 6170 7475 7265 5f73 7461 636b  xt.capture_stack
-00019de0: 2e61 7070 656e 6428 6361 7074 7572 655f  .append(capture_
-00019df0: 696e 7465 726d 6564 6961 7465 7329 0a20  intermediates). 
-00019e00: 2020 2074 7279 3a0a 2020 2020 2020 7265     try:.      re
-00019e10: 7475 726e 2066 6e28 6d6f 6475 6c65 2e63  turn fn(module.c
-00019e20: 6c6f 6e65 2870 6172 656e 743d 7363 6f70  lone(parent=scop
-00019e30: 652c 205f 6465 6570 5f63 6c6f 6e65 3d54  e, _deep_clone=T
-00019e40: 7275 6529 2c20 2a61 7267 732c 202a 2a6b  rue), *args, **k
-00019e50: 7761 7267 7329 0a20 2020 2066 696e 616c  wargs).    final
-00019e60: 6c79 3a0a 2020 2020 2020 5f63 6f6e 7465  ly:.      _conte
-00019e70: 7874 2e63 6170 7475 7265 5f73 7461 636b  xt.capture_stack
-00019e80: 2e70 6f70 2829 0a0a 2020 6966 2063 6170  .pop()..  if cap
-00019e90: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
-00019ea0: 6573 2069 7320 5472 7565 3a20 2023 2070  es is True:  # p
-00019eb0: 796c 696e 743a 2064 6973 6162 6c65 3d67  ylint: disable=g
-00019ec0: 2d62 6f6f 6c2d 6964 2d63 6f6d 7061 7269  -bool-id-compari
-00019ed0: 736f 6e0a 2020 2020 6361 7074 7572 655f  son.    capture_
-00019ee0: 696e 7465 726d 6564 6961 7465 7320 3d20  intermediates = 
-00019ef0: 6361 7074 7572 655f 6361 6c6c 5f69 6e74  capture_call_int
-00019f00: 6572 6d65 6469 6174 6573 0a20 2069 6620  ermediates.  if 
-00019f10: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
-00019f20: 6961 7465 733a 0a20 2020 206d 7574 6162  iates:.    mutab
-00019f30: 6c65 203d 2075 6e69 6f6e 5f66 696c 7465  le = union_filte
-00019f40: 7273 286d 7574 6162 6c65 2c20 2769 6e74  rs(mutable, 'int
-00019f50: 6572 6d65 6469 6174 6573 2729 0a20 2072  ermediates').  r
-00019f60: 6574 7572 6e20 636f 7265 2e69 6e69 7428  eturn core.init(
-00019f70: 7363 6f70 655f 666e 2c20 6d75 7461 626c  scope_fn, mutabl
-00019f80: 653d 6d75 7461 626c 6529 0a0a 0a40 7472  e=mutable)...@tr
-00019f90: 6163 6562 6163 6b5f 7574 696c 2e61 7069  aceback_util.api
-00019fa0: 5f62 6f75 6e64 6172 790a 6465 6620 696e  _boundary.def in
-00019fb0: 6974 280a 2020 666e 3a20 4361 6c6c 6162  it(.  fn: Callab
-00019fc0: 6c65 5b2e 2e2e 2c20 416e 795d 2c0a 2020  le[..., Any],.  
-00019fd0: 6d6f 6475 6c65 3a20 4d6f 6475 6c65 2c0a  module: Module,.
-00019fe0: 2020 6d75 7461 626c 653a 2043 6f6c 6c65    mutable: Colle
-00019ff0: 6374 696f 6e46 696c 7465 7220 3d20 4465  ctionFilter = De
-0001a000: 6e79 4c69 7374 2827 696e 7465 726d 6564  nyList('intermed
-0001a010: 6961 7465 7327 292c 0a20 2063 6170 7475  iates'),.  captu
-0001a020: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
-0001a030: 3a20 556e 696f 6e5b 626f 6f6c 2c20 4361  : Union[bool, Ca
-0001a040: 6c6c 6162 6c65 5b5b 4d6f 6475 6c65 2c20  llable[[Module, 
-0001a050: 7374 725d 2c20 626f 6f6c 5d5d 203d 2046  str], bool]] = F
-0001a060: 616c 7365 2c0a 2920 2d3e 2043 616c 6c61  alse,.) -> Calla
-0001a070: 626c 655b 2e2e 2e2c 2055 6e69 6f6e 5b46  ble[..., Union[F
-0001a080: 726f 7a65 6e56 6172 6961 626c 6544 6963  rozenVariableDic
-0001a090: 742c 2044 6963 745b 7374 722c 2041 6e79  t, Dict[str, Any
-0001a0a0: 5d5d 5d3a 0a20 2022 2222 4372 6561 7465  ]]]:.  """Create
-0001a0b0: 7320 616e 2069 6e69 7420 6675 6e63 7469  s an init functi
-0001a0c0: 6f6e 2074 6f20 6361 6c6c 2060 6066 6e60  on to call ``fn`
-0001a0d0: 6020 7769 7468 2061 2062 6f75 6e64 206d  ` with a bound m
-0001a0e0: 6f64 756c 652e 0a0a 2020 556e 6c69 6b65  odule...  Unlike
-0001a0f0: 2060 604d 6f64 756c 652e 696e 6974 6060   ``Module.init``
-0001a100: 2074 6869 7320 6675 6e63 7469 6f6e 2072   this function r
-0001a110: 6574 7572 6e73 2061 206e 6577 2066 756e  eturns a new fun
-0001a120: 6374 696f 6e20 7769 7468 2074 6865 2073  ction with the s
-0001a130: 6967 6e61 7475 7265 0a20 2060 6028 726e  ignature.  ``(rn
-0001a140: 6773 2c20 2a61 7267 732c 202a 2a6b 7761  gs, *args, **kwa
-0001a150: 7267 7329 202d 3e20 7661 7269 6162 6c65  rgs) -> variable
-0001a160: 7360 602e 0a20 2054 6865 2072 6e67 7320  s``..  The rngs 
-0001a170: 6361 6e20 6265 2061 2064 6963 7420 6f66  can be a dict of
-0001a180: 2050 524e 474b 6579 7320 6f72 2061 2073   PRNGKeys or a s
-0001a190: 696e 676c 6520 6060 6050 524e 474b 6579  ingle ```PRNGKey
-0001a1a0: 6060 2077 6869 6368 2069 730a 2020 6571  `` which is.  eq
-0001a1b0: 7569 7661 6c65 6e74 2074 6f20 7061 7373  uivalent to pass
-0001a1c0: 696e 6720 6120 6469 6374 2077 6974 6820  ing a dict with 
-0001a1d0: 6f6e 6520 5052 4e47 4b65 7920 7769 7468  one PRNGKey with
-0001a1e0: 2074 6865 206e 616d 6520 2270 6172 616d   the name "param
-0001a1f0: 7322 2e0a 0a20 2054 6865 2069 6e69 7420  s"...  The init 
-0001a200: 6675 6e63 7469 6f6e 2074 6861 7420 6973  function that is
-0001a210: 2072 6574 7572 6e65 6420 6361 6e20 6265   returned can be
-0001a220: 2064 6972 6563 746c 7920 636f 6d70 6f73   directly compos
-0001a230: 6564 2077 6974 680a 2020 4a41 5820 7472  ed with.  JAX tr
-0001a240: 616e 7366 6f72 6d61 7469 6f6e 7320 6c69  ansformations li
-0001a250: 6b65 2060 606a 6178 2e6a 6974 6060 3a3a  ke ``jax.jit``::
-0001a260: 0a0a 2020 2020 3e3e 3e20 636c 6173 7320  ..    >>> class 
-0001a270: 466f 6f28 6e6e 2e4d 6f64 756c 6529 3a0a  Foo(nn.Module):.
-0001a280: 2020 2020 2e2e 2e20 2020 6465 6620 656e      ...   def en
-0001a290: 636f 6465 2873 656c 662c 2078 293a 0a20  code(self, x):. 
-0001a2a0: 2020 202e 2e2e 2020 2020 202e 2e2e 0a20     ...     .... 
-0001a2b0: 2020 202e 2e2e 2020 2064 6566 2064 6563     ...   def dec
-0001a2c0: 6f64 6528 7365 6c66 2c20 7829 3a0a 2020  ode(self, x):.  
-0001a2d0: 2020 2e2e 2e20 2020 2020 2e2e 2e0a 0a20    ...     ..... 
-0001a2e0: 2020 203e 3e3e 2064 6566 2066 2866 6f6f     >>> def f(foo
-0001a2f0: 2c20 7829 3a0a 2020 2020 2e2e 2e20 2020  , x):.    ...   
-0001a300: 7a20 3d20 666f 6f2e 656e 636f 6465 2878  z = foo.encode(x
-0001a310: 290a 2020 2020 2e2e 2e20 2020 7920 3d20  ).    ...   y = 
-0001a320: 666f 6f2e 6465 636f 6465 287a 290a 2020  foo.decode(z).  
-0001a330: 2020 2e2e 2e20 2020 2320 2e2e 2e0a 2020    ...   # ....  
-0001a340: 2020 2e2e 2e20 2020 7265 7475 726e 2079    ...   return y
-0001a350: 0a0a 2020 2020 3e3e 3e20 666f 6f20 3d20  ..    >>> foo = 
-0001a360: 466f 6f28 290a 2020 2020 3e3e 3e20 665f  Foo().    >>> f_
-0001a370: 6a69 7474 6564 203d 206a 6178 2e6a 6974  jitted = jax.jit
-0001a380: 286e 6e2e 696e 6974 2866 2c20 666f 6f29  (nn.init(f, foo)
-0001a390: 290a 2020 2020 3e3e 3e20 7661 7269 6162  ).    >>> variab
-0001a3a0: 6c65 7320 3d20 665f 6a69 7474 6564 286a  les = f_jitted(j
-0001a3b0: 6178 2e72 616e 646f 6d2e 6b65 7928 3029  ax.random.key(0)
-0001a3c0: 2c20 6a6e 702e 6f6e 6573 2828 312c 2033  , jnp.ones((1, 3
-0001a3d0: 2929 290a 0a20 2041 7267 733a 0a20 2020  )))..  Args:.   
-0001a3e0: 2066 6e3a 2054 6865 2066 756e 6374 696f   fn: The functio
-0001a3f0: 6e20 7468 6174 2073 686f 756c 6420 6265  n that should be
-0001a400: 2061 7070 6c69 6564 2e20 5468 6520 6669   applied. The fi
-0001a410: 7273 7420 6172 6775 6d65 6e74 2070 6173  rst argument pas
-0001a420: 7365 6420 7769 6c6c 2062 650a 2020 2020  sed will be.    
-0001a430: 2020 6120 6d6f 6475 6c65 2069 6e73 7461    a module insta
-0001a440: 6e63 6520 6f66 2074 6865 2060 606d 6f64  nce of the ``mod
-0001a450: 756c 6560 6020 7769 7468 2076 6172 6961  ule`` with varia
-0001a460: 626c 6573 2061 6e64 2052 4e47 7320 626f  bles and RNGs bo
-0001a470: 756e 6420 746f 2069 742e 0a20 2020 206d  und to it..    m
-0001a480: 6f64 756c 653a 2054 6865 2060 604d 6f64  odule: The ``Mod
-0001a490: 756c 6560 6020 7468 6174 2077 696c 6c20  ule`` that will 
-0001a4a0: 6265 2075 7365 6420 746f 2062 696e 6420  be used to bind 
-0001a4b0: 7661 7269 6162 6c65 7320 616e 6420 524e  variables and RN
-0001a4c0: 4773 2074 6f2e 2054 6865 0a20 2020 2020  Gs to. The.     
-0001a4d0: 2060 604d 6f64 756c 6560 6020 7061 7373   ``Module`` pass
-0001a4e0: 6564 2061 7320 7468 6520 6669 7273 7420  ed as the first 
-0001a4f0: 6172 6775 6d65 6e74 2074 6f20 6060 666e  argument to ``fn
-0001a500: 6060 2077 696c 6c20 6265 2061 2063 6c6f  `` will be a clo
-0001a510: 6e65 206f 660a 2020 2020 2020 6d6f 6475  ne of.      modu
-0001a520: 6c65 2e0a 2020 2020 6d75 7461 626c 653a  le..    mutable:
-0001a530: 2043 616e 2062 6520 626f 6f6c 2c20 7374   Can be bool, st
-0001a540: 722c 206f 7220 6c69 7374 2e20 5370 6563  r, or list. Spec
-0001a550: 6966 6965 7320 7768 6963 6820 636f 6c6c  ifies which coll
-0001a560: 6563 7469 6f6e 7320 7368 6f75 6c64 2062  ections should b
-0001a570: 650a 2020 2020 2020 7472 6561 7465 6420  e.      treated 
-0001a580: 6173 206d 7574 6162 6c65 3a20 6060 626f  as mutable: ``bo
-0001a590: 6f6c 6060 3a20 616c 6c2f 6e6f 2063 6f6c  ol``: all/no col
-0001a5a0: 6c65 6374 696f 6e73 2061 7265 206d 7574  lections are mut
-0001a5b0: 6162 6c65 2e20 6060 7374 7260 603a 2054  able. ``str``: T
-0001a5c0: 6865 0a20 2020 2020 206e 616d 6520 6f66  he.      name of
-0001a5d0: 2061 2073 696e 676c 6520 6d75 7461 626c   a single mutabl
-0001a5e0: 6520 636f 6c6c 6563 7469 6f6e 2e20 6060  e collection. ``
-0001a5f0: 6c69 7374 6060 3a20 4120 6c69 7374 206f  list``: A list o
-0001a600: 6620 6e61 6d65 7320 6f66 206d 7574 6162  f names of mutab
-0001a610: 6c65 0a20 2020 2020 2063 6f6c 6c65 6374  le.      collect
-0001a620: 696f 6e73 2e20 4279 2064 6566 6175 6c74  ions. By default
-0001a630: 2c20 616c 6c20 636f 6c6c 6563 7469 6f6e  , all collection
-0001a640: 7320 6578 6365 7074 2022 696e 7465 726d  s except "interm
-0001a650: 6564 6961 7465 7322 2061 7265 0a20 2020  ediates" are.   
-0001a660: 2020 206d 7574 6162 6c65 2e0a 2020 2020     mutable..    
-0001a670: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
-0001a680: 6961 7465 733a 2049 6620 6054 7275 6560  iates: If `True`
-0001a690: 2c20 6361 7074 7572 6573 2069 6e74 6572  , captures inter
-0001a6a0: 6d65 6469 6174 6520 7265 7475 726e 2076  mediate return v
-0001a6b0: 616c 7565 7320 6f66 2061 6c6c 0a20 2020  alues of all.   
-0001a6c0: 2020 204d 6f64 756c 6573 2069 6e73 6964     Modules insid
-0001a6d0: 6520 7468 6520 2269 6e74 6572 6d65 6469  e the "intermedi
-0001a6e0: 6174 6573 2220 636f 6c6c 6563 7469 6f6e  ates" collection
-0001a6f0: 2e20 4279 2064 6566 6175 6c74 2c20 6f6e  . By default, on
-0001a700: 6c79 2074 6865 2072 6574 7572 6e0a 2020  ly the return.  
-0001a710: 2020 2020 7661 6c75 6573 206f 6620 616c      values of al
-0001a720: 6c20 605f 5f63 616c 6c5f 5f60 206d 6574  l `__call__` met
-0001a730: 686f 6473 2061 7265 2073 746f 7265 642e  hods are stored.
-0001a740: 2041 2066 756e 6374 696f 6e20 6361 6e20   A function can 
-0001a750: 6265 2070 6173 7365 6420 746f 0a20 2020  be passed to.   
-0001a760: 2020 2063 6861 6e67 6520 7468 6520 6669     change the fi
-0001a770: 6c74 6572 2062 6568 6176 696f 722e 2054  lter behavior. T
-0001a780: 6865 2066 696c 7465 7220 6675 6e63 7469  he filter functi
-0001a790: 6f6e 2074 616b 6573 2074 6865 204d 6f64  on takes the Mod
-0001a7a0: 756c 6520 696e 7374 616e 6365 0a20 2020  ule instance.   
-0001a7b0: 2020 2061 6e64 206d 6574 686f 6420 6e61     and method na
-0001a7c0: 6d65 2061 6e64 2072 6574 7572 6e73 2061  me and returns a
-0001a7d0: 2062 6f6f 6c20 696e 6469 6361 7469 6e67   bool indicating
-0001a7e0: 2077 6865 7468 6572 2074 6865 206f 7574   whether the out
-0001a7f0: 7075 7420 6f66 2074 6861 740a 2020 2020  put of that.    
-0001a800: 2020 6d65 7468 6f64 2069 6e76 6f63 6174    method invocat
-0001a810: 696f 6e20 7368 6f75 6c64 2062 6520 7374  ion should be st
-0001a820: 6f72 6564 2e0a 0a20 2052 6574 7572 6e73  ored...  Returns
-0001a830: 3a0a 2020 2020 5468 6520 696e 6974 2066  :.    The init f
-0001a840: 756e 6374 696f 6e20 7772 6170 7069 6e67  unction wrapping
-0001a850: 2060 6066 6e60 602e 0a20 2022 2222 0a20   ``fn``..  """. 
-0001a860: 2069 6e69 745f 666e 203d 2069 6e69 745f   init_fn = init_
-0001a870: 7769 7468 5f6f 7574 7075 7428 666e 2c20  with_output(fn, 
-0001a880: 6d6f 6475 6c65 2c20 6d75 7461 626c 652c  module, mutable,
-0001a890: 2063 6170 7475 7265 5f69 6e74 6572 6d65   capture_interme
-0001a8a0: 6469 6174 6573 290a 0a20 2040 6675 6e63  diates)..  @func
-0001a8b0: 746f 6f6c 732e 7772 6170 7328 696e 6974  tools.wraps(init
-0001a8c0: 5f66 6e29 0a20 2064 6566 2069 6e69 745f  _fn).  def init_
-0001a8d0: 7772 6170 7065 7228 2a61 7267 732c 202a  wrapper(*args, *
-0001a8e0: 2a6b 7761 7267 7329 3a0a 2020 2020 7265  *kwargs):.    re
-0001a8f0: 7475 726e 2069 6e69 745f 666e 282a 6172  turn init_fn(*ar
-0001a900: 6773 2c20 2a2a 6b77 6172 6773 295b 315d  gs, **kwargs)[1]
-0001a910: 0a0a 2020 7265 7475 726e 2069 6e69 745f  ..  return init_
-0001a920: 7772 6170 7065 720a 0a0a 2320 544f 444f  wrapper...# TODO
-0001a930: 2863 6761 7263 6961 6529 3a20 7765 2061  (cgarciae): we a
-0001a940: 7265 2064 6566 696e 696e 6720 436f 6d70  re defining Comp
-0001a950: 6163 744e 616d 6553 636f 7065 206a 7573  actNameScope jus
-0001a960: 7420 746f 0a23 2061 766f 6964 2061 2070  t to.# avoid a p
-0001a970: 7974 7970 6520 6275 6720 7769 7468 2074  ytype bug with t
-0001a980: 6865 2046 6c61 7820 6f76 6572 6c61 792e  he Flax overlay.
-0001a990: 2057 6520 7368 6f75 6c64 2061 696d 2074   We should aim t
-0001a9a0: 6f0a 2320 7265 6d6f 7665 2069 6e20 7468  o.# remove in th
-0001a9b0: 6520 6174 2073 6f6d 6520 706f 696e 7420  e at some point 
-0001a9c0: 6173 2069 7473 206e 6f74 2065 7267 6f6e  as its not ergon
-0001a9d0: 6f6d 6963 2e0a 6966 206e 6f74 2074 7970  omic..if not typ
-0001a9e0: 696e 672e 5459 5045 5f43 4845 434b 494e  ing.TYPE_CHECKIN
-0001a9f0: 473a 0a0a 2020 636c 6173 7320 436f 6d70  G:..  class Comp
-0001aa00: 6163 744e 616d 6553 636f 7065 284d 6f64  actNameScope(Mod
-0001aa10: 756c 6529 3a0a 2020 2020 666e 3a20 4361  ule):.    fn: Ca
-0001aa20: 6c6c 6162 6c65 0a20 2020 206d 6f64 756c  llable.    modul
-0001aa30: 655f 666e 3a20 4361 6c6c 6162 6c65 5b5b  e_fn: Callable[[
-0001aa40: 5d2c 204d 6f64 756c 655d 0a0a 2020 2020  ], Module]..    
-0001aa50: 4063 6f6d 7061 6374 0a20 2020 2064 6566  @compact.    def
-0001aa60: 205f 5f63 616c 6c5f 5f28 7365 6c66 2c20   __call__(self, 
-0001aa70: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
-0001aa80: 202d 3e20 416e 793a 0a20 2020 2020 2072   -> Any:.      r
-0001aa90: 6574 7572 6e20 7365 6c66 2e66 6e28 7365  eturn self.fn(se
-0001aaa0: 6c66 2e6d 6f64 756c 655f 666e 2829 2c20  lf.module_fn(), 
-0001aab0: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
-0001aac0: 0a65 6c73 653a 0a0a 2020 4064 6174 6163  .else:..  @datac
-0001aad0: 6c61 7373 6573 2e64 6174 6163 6c61 7373  lasses.dataclass
-0001aae0: 0a20 2063 6c61 7373 2043 6f6d 7061 6374  .  class Compact
-0001aaf0: 4e61 6d65 5363 6f70 653a 0a20 2020 2066  NameScope:.    f
-0001ab00: 6e3a 2043 616c 6c61 626c 650a 2020 2020  n: Callable.    
-0001ab10: 6d6f 6475 6c65 5f66 6e3a 2043 616c 6c61  module_fn: Calla
-0001ab20: 626c 650a 2020 2020 6e61 6d65 3a20 7374  ble.    name: st
-0001ab30: 720a 0a20 2020 2064 6566 205f 5f63 616c  r..    def __cal
-0001ab40: 6c5f 5f28 7365 6c66 2c20 2a61 7267 732c  l__(self, *args,
-0001ab50: 202a 2a6b 7761 7267 7329 202d 3e20 416e   **kwargs) -> An
-0001ab60: 793a 0a20 2020 2020 202e 2e2e 0a         y:.      ....
+000013a0: 7469 6e65 6c2e 0a0a 2020 6465 6620 5f5f  tinel...  def __
+000013b0: 7265 6475 6365 5f5f 2873 656c 6629 3a0a  reduce__(self):.
+000013c0: 2020 2020 7265 7475 726e 205f 6765 745f      return _get_
+000013d0: 756e 7370 6563 6966 6965 645f 7061 7265  unspecified_pare
+000013e0: 6e74 2c20 2829 0a0a 0a64 6566 205f 6765  nt, ()...def _ge
+000013f0: 745f 756e 7370 6563 6966 6965 645f 7061  t_unspecified_pa
+00001400: 7265 6e74 2829 3a0a 2020 7265 7475 726e  rent():.  return
+00001410: 205f 756e 7370 6563 6966 6965 645f 7061   _unspecified_pa
+00001420: 7265 6e74 0a0a 0a5f 756e 7370 6563 6966  rent..._unspecif
+00001430: 6965 645f 7061 7265 6e74 203d 205f 5365  ied_parent = _Se
+00001440: 6e74 696e 656c 2829 0a0a 0a23 2045 6e61  ntinel()...# Ena
+00001450: 626c 6520 6175 746f 6d61 7469 6320 6e61  ble automatic na
+00001460: 6d65 645f 6361 6c6c 2077 7261 7070 696e  med_call wrappin
+00001470: 6720 666f 7220 6c61 6265 6c6c 696e 6720  g for labelling 
+00001480: 7072 6f66 696c 6520 7472 6163 6573 2e0a  profile traces..
+00001490: 2320 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  # --------------
+000014a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000014d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a  ---------------.
+000014e0: 5f75 7365 5f6e 616d 6564 5f63 616c 6c20  _use_named_call 
+000014f0: 3d20 636f 6e66 6967 2e66 6c61 785f 7072  = config.flax_pr
+00001500: 6f66 696c 650a 0a0a 6465 6620 5f64 6572  ofile...def _der
+00001510: 6976 655f 7072 6f66 696c 696e 675f 6e61  ive_profiling_na
+00001520: 6d65 286d 6f64 756c 652c 2066 6e29 3a0a  me(module, fn):.
+00001530: 2020 666e 5f6e 616d 6520 3d20 5f67 6574    fn_name = _get
+00001540: 5f66 6e5f 6e61 6d65 2866 6e29 0a20 206d  _fn_name(fn).  m
+00001550: 6574 686f 645f 7375 6666 6978 203d 2066  ethod_suffix = f
+00001560: 272e 7b66 6e5f 6e61 6d65 7d27 2069 6620  '.{fn_name}' if 
+00001570: 666e 5f6e 616d 6520 213d 2027 5f5f 6361  fn_name != '__ca
+00001580: 6c6c 5f5f 2720 656c 7365 2027 270a 2020  ll__' else ''.  
+00001590: 6d6f 6475 6c65 5f6e 616d 6520 3d20 6d6f  module_name = mo
+000015a0: 6475 6c65 2e6e 616d 6520 6f72 206d 6f64  dule.name or mod
+000015b0: 756c 652e 5f5f 636c 6173 735f 5f2e 5f5f  ule.__class__.__
+000015c0: 6e61 6d65 5f5f 0a20 2072 6574 7572 6e20  name__.  return 
+000015d0: 6627 7b6d 6f64 756c 655f 6e61 6d65 7d7b  f'{module_name}{
+000015e0: 6d65 7468 6f64 5f73 7566 6669 787d 270a  method_suffix}'.
+000015f0: 0a0a 6465 6620 656e 6162 6c65 5f6e 616d  ..def enable_nam
+00001600: 6564 5f63 616c 6c28 293a 0a20 2022 2222  ed_call():.  """
+00001610: 456e 6162 6c65 7320 6e61 6d65 6420 6361  Enables named ca
+00001620: 6c6c 2077 7261 7070 696e 6720 666f 7220  ll wrapping for 
+00001630: 6c61 6265 6c6c 696e 6720 7072 6f66 696c  labelling profil
+00001640: 6520 7472 6163 6573 2e0a 0a20 2057 6865  e traces...  Whe
+00001650: 6e20 6e61 6d65 6420 6361 6c6c 2077 7261  n named call wra
+00001660: 7070 696e 6720 6973 2065 6e61 626c 6564  pping is enabled
+00001670: 2061 6c6c 204a 4158 206f 7073 2065 7865   all JAX ops exe
+00001680: 6375 7465 6420 696e 2061 204d 6f64 756c  cuted in a Modul
+00001690: 650a 2020 7769 6c6c 2062 6520 7275 6e20  e.  will be run 
+000016a0: 756e 6465 7220 6060 6a61 782e 6e61 6d65  under ``jax.name
+000016b0: 645f 7363 6f70 6560 602e 2054 6865 2060  d_scope``. The `
+000016c0: 604d 6f64 756c 6560 6020 636c 6173 7320  `Module`` class 
+000016d0: 6e61 6d65 2077 696c 6c0a 2020 7368 6f77  name will.  show
+000016e0: 2075 7020 6172 6f75 6e64 2074 6865 206f   up around the o
+000016f0: 7065 7261 7469 6f6e 7320 6265 6c6f 6e67  perations belong
+00001700: 696e 6720 746f 2074 6861 7420 4d6f 6475  ing to that Modu
+00001710: 6c65 2069 6e20 7468 650a 2020 5465 6e73  le in the.  Tens
+00001720: 6f72 626f 6172 6420 7072 6f66 696c 696e  orboard profilin
+00001730: 6720 5549 2c20 7369 6d70 6c69 6679 696e  g UI, simplifyin
+00001740: 6720 7468 6520 7072 6f66 696c 696e 6720  g the profiling 
+00001750: 7072 6f63 6573 732e 0a0a 2020 4e6f 7465  process...  Note
+00001760: 2074 6861 7420 6060 6a61 782e 6e61 6d65   that ``jax.name
+00001770: 645f 7363 6f70 6560 6020 6f6e 6c79 2077  d_scope`` only w
+00001780: 6f72 6b73 2066 6f72 0a20 2063 6f6d 7069  orks for.  compi
+00001790: 6c65 6420 6675 6e63 7469 6f6e 7320 2865  led functions (e
+000017a0: 2e67 2e3a 2075 7369 6e67 206a 6178 2e6a  .g.: using jax.j
+000017b0: 6974 206f 7220 6a61 782e 706d 6170 292e  it or jax.pmap).
+000017c0: 0a20 2022 2222 0a20 2067 6c6f 6261 6c20  .  """.  global 
+000017d0: 5f75 7365 5f6e 616d 6564 5f63 616c 6c0a  _use_named_call.
+000017e0: 2020 5f75 7365 5f6e 616d 6564 5f63 616c    _use_named_cal
+000017f0: 6c20 3d20 5472 7565 0a0a 0a64 6566 2064  l = True...def d
+00001800: 6973 6162 6c65 5f6e 616d 6564 5f63 616c  isable_named_cal
+00001810: 6c28 293a 0a20 2022 2222 4469 7361 626c  l():.  """Disabl
+00001820: 6573 206e 616d 6564 2063 616c 6c20 7772  es named call wr
+00001830: 6170 7069 6e67 2e0a 0a20 2053 6565 2060  apping...  See `
+00001840: 6065 6e61 626c 655f 6e61 6d65 645f 6361  `enable_named_ca
+00001850: 6c6c 6060 0a20 2022 2222 0a20 2067 6c6f  ll``.  """.  glo
+00001860: 6261 6c20 5f75 7365 5f6e 616d 6564 5f63  bal _use_named_c
+00001870: 616c 6c0a 2020 5f75 7365 5f6e 616d 6564  all.  _use_named
+00001880: 5f63 616c 6c20 3d20 4661 6c73 650a 0a0a  _call = False...
+00001890: 4063 6f6e 7465 7874 6c69 622e 636f 6e74  @contextlib.cont
+000018a0: 6578 746d 616e 6167 6572 0a64 6566 206f  extmanager.def o
+000018b0: 7665 7272 6964 655f 6e61 6d65 645f 6361  verride_named_ca
+000018c0: 6c6c 2865 6e61 626c 653a 2062 6f6f 6c20  ll(enable: bool 
+000018d0: 3d20 5472 7565 293a 0a20 2023 2070 796c  = True):.  # pyl
+000018e0: 696e 743a 2064 6973 6162 6c65 3d67 2d64  int: disable=g-d
+000018f0: 6f63 2d72 6574 7572 6e2d 6f72 2d79 6965  oc-return-or-yie
+00001900: 6c64 0a20 2022 2222 5265 7475 726e 7320  ld.  """Returns 
+00001910: 6120 636f 6e74 6578 7420 6d61 6e61 6765  a context manage
+00001920: 7220 7468 6174 2065 6e61 626c 6573 2f64  r that enables/d
+00001930: 6973 6162 6c65 7320 6e61 6d65 6420 6361  isables named ca
+00001940: 6c6c 2077 7261 7070 696e 672e 0a0a 2020  ll wrapping...  
+00001950: 4172 6773 3a0a 2020 2020 656e 6162 6c65  Args:.    enable
+00001960: 3a20 4966 2074 7275 652c 2065 6e61 626c  : If true, enabl
+00001970: 6573 206e 616d 6564 2063 616c 6c20 7772  es named call wr
+00001980: 6170 7069 6e67 2066 6f72 206c 6162 656c  apping for label
+00001990: 6c69 6e67 2070 726f 6669 6c65 2074 7261  ling profile tra
+000019a0: 6365 732e 0a20 2020 2020 2028 7365 6520  ces..      (see 
+000019b0: 6060 656e 6162 6c65 645f 6e61 6d65 645f  ``enabled_named_
+000019c0: 6361 6c6c 6060 292e 0a20 2022 2222 0a20  call``)..  """. 
+000019d0: 2023 2070 796c 696e 743a 2065 6e61 626c   # pylint: enabl
+000019e0: 653d 672d 646f 632d 7265 7475 726e 2d6f  e=g-doc-return-o
+000019f0: 722d 7969 656c 640a 2020 676c 6f62 616c  r-yield.  global
+00001a00: 205f 7573 655f 6e61 6d65 645f 6361 6c6c   _use_named_call
+00001a10: 0a20 2075 7365 5f6e 616d 6564 5f63 616c  .  use_named_cal
+00001a20: 6c5f 7072 6576 203d 205f 7573 655f 6e61  l_prev = _use_na
+00001a30: 6d65 645f 6361 6c6c 0a20 205f 7573 655f  med_call.  _use_
+00001a40: 6e61 6d65 645f 6361 6c6c 203d 2065 6e61  named_call = ena
+00001a50: 626c 650a 2020 7472 793a 0a20 2020 2079  ble.  try:.    y
+00001a60: 6965 6c64 0a20 2066 696e 616c 6c79 3a0a  ield.  finally:.
+00001a70: 2020 2020 5f75 7365 5f6e 616d 6564 5f63      _use_named_c
+00001a80: 616c 6c20 3d20 7573 655f 6e61 6d65 645f  all = use_named_
+00001a90: 6361 6c6c 5f70 7265 760a 0a0a 2320 496e  call_prev...# In
+00001aa0: 7465 7263 6570 7420 6d6f 6475 6c65 206d  tercept module m
+00001ab0: 6574 686f 6473 2e0a 2320 2d2d 2d2d 2d2d  ethods..# ------
+00001ac0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001ad0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001ae0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001af0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00001b00: 2d2d 2d2d 2d2d 2d0a 4064 6174 6163 6c61  -------.@datacla
+00001b10: 7373 6573 2e64 6174 6163 6c61 7373 2866  sses.dataclass(f
+00001b20: 726f 7a65 6e3d 5472 7565 290a 636c 6173  rozen=True).clas
+00001b30: 7320 496e 7465 7263 6570 746f 7243 6f6e  s InterceptorCon
+00001b40: 7465 7874 3a0a 2020 2222 2252 6561 6420  text:.  """Read 
+00001b50: 6f6e 6c79 2073 7461 7465 2073 686f 7769  only state showi
+00001b60: 6e67 2074 6865 2063 616c 6c69 6e67 2063  ng the calling c
+00001b70: 6f6e 7465 7874 2066 6f72 206d 6574 686f  ontext for metho
+00001b80: 6420 696e 7465 7263 6570 746f 7273 2e0a  d interceptors..
+00001b90: 0a20 2041 7474 7269 6275 7465 733a 0a20  .  Attributes:. 
+00001ba0: 2020 206d 6f64 756c 653a 2054 6865 204d     module: The M
+00001bb0: 6f64 756c 6520 696e 7374 616e 6365 2077  odule instance w
+00001bc0: 686f 7365 206d 6574 686f 6420 6973 2062  hose method is b
+00001bd0: 6569 6e67 2063 616c 6c65 642e 0a20 2020  eing called..   
+00001be0: 206d 6574 686f 645f 6e61 6d65 3a20 5468   method_name: Th
+00001bf0: 6520 6e61 6d65 206f 6620 7468 6520 6d65  e name of the me
+00001c00: 7468 6f64 2062 6569 6e67 2063 616c 6c65  thod being calle
+00001c10: 6420 6f6e 2074 6865 206d 6f64 756c 652e  d on the module.
+00001c20: 0a20 2020 206f 7269 675f 6d65 7468 6f64  .    orig_method
+00001c30: 3a20 5468 6520 6f72 6967 696e 616c 206d  : The original m
+00001c40: 6574 686f 6420 6465 6669 6e65 6420 6f6e  ethod defined on
+00001c50: 2074 6865 206d 6f64 756c 652e 2043 616c   the module. Cal
+00001c60: 6c69 6e67 2069 7420 7769 6c6c 0a20 2020  ling it will.   
+00001c70: 2020 2073 686f 7274 2063 6972 6375 6974     short circuit
+00001c80: 2061 6c6c 206f 7468 6572 2069 6e74 6572   all other inter
+00001c90: 6365 7074 6f72 732e 0a20 2022 2222 0a0a  ceptors..  """..
+00001ca0: 2020 6d6f 6475 6c65 3a20 274d 6f64 756c    module: 'Modul
+00001cb0: 6527 0a20 206d 6574 686f 645f 6e61 6d65  e'.  method_name
+00001cc0: 3a20 7374 720a 2020 6f72 6967 5f6d 6574  : str.  orig_met
+00001cd0: 686f 643a 2043 616c 6c61 626c 655b 2e2e  hod: Callable[..
+00001ce0: 2e2c 2041 6e79 5d0a 0a0a 636c 6173 7320  ., Any]...class 
+00001cf0: 5468 7265 6164 4c6f 6361 6c53 7461 636b  ThreadLocalStack
+00001d00: 2874 6872 6561 6469 6e67 2e6c 6f63 616c  (threading.local
+00001d10: 293a 0a20 2022 2222 5468 7265 6164 2d6c  ):.  """Thread-l
+00001d20: 6f63 616c 2073 7461 636b 2e22 2222 0a0a  ocal stack."""..
+00001d30: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s
+00001d40: 656c 6629 3a0a 2020 2020 7365 6c66 2e5f  elf):.    self._
+00001d50: 7374 6f72 6167 6520 3d20 5b5d 0a0a 2020  storage = []..  
+00001d60: 6465 6620 7075 7368 2873 656c 662c 2065  def push(self, e
+00001d70: 6c65 6d3a 2041 6e79 2920 2d3e 204e 6f6e  lem: Any) -> Non
+00001d80: 653a 0a20 2020 2073 656c 662e 5f73 746f  e:.    self._sto
+00001d90: 7261 6765 2e61 7070 656e 6428 656c 656d  rage.append(elem
+00001da0: 290a 0a20 2064 6566 2070 6f70 2873 656c  )..  def pop(sel
+00001db0: 6629 202d 3e20 416e 793a 0a20 2020 2072  f) -> Any:.    r
+00001dc0: 6574 7572 6e20 7365 6c66 2e5f 7374 6f72  eturn self._stor
+00001dd0: 6167 652e 706f 7028 290a 0a20 2064 6566  age.pop()..  def
+00001de0: 205f 5f69 7465 725f 5f28 7365 6c66 2920   __iter__(self) 
+00001df0: 2d3e 2049 7465 7261 746f 725b 416e 795d  -> Iterator[Any]
+00001e00: 3a0a 2020 2020 7265 7475 726e 2069 7465  :.    return ite
+00001e10: 7228 7265 7665 7273 6564 2873 656c 662e  r(reversed(self.
+00001e20: 5f73 746f 7261 6765 2929 0a0a 2020 6465  _storage))..  de
+00001e30: 6620 5f5f 6c65 6e5f 5f28 7365 6c66 2920  f __len__(self) 
+00001e40: 2d3e 2069 6e74 3a0a 2020 2020 7265 7475  -> int:.    retu
+00001e50: 726e 206c 656e 2873 656c 662e 5f73 746f  rn len(self._sto
+00001e60: 7261 6765 290a 0a20 2064 6566 205f 5f72  rage)..  def __r
+00001e70: 6570 725f 5f28 7365 6c66 2920 2d3e 2073  epr__(self) -> s
+00001e80: 7472 3a0a 2020 2020 7265 7475 726e 2066  tr:.    return f
+00001e90: 277b 7365 6c66 2e5f 5f63 6c61 7373 5f5f  '{self.__class__
+00001ea0: 2e5f 5f6e 616d 655f 5f7d 287b 7365 6c66  .__name__}({self
+00001eb0: 2e5f 7374 6f72 6167 657d 2927 0a0a 0a41  ._storage})'...A
+00001ec0: 7267 7320 3d20 5475 706c 655b 416e 795d  rgs = Tuple[Any]
+00001ed0: 0a4b 7761 7267 7320 3d20 4469 6374 5b73  .Kwargs = Dict[s
+00001ee0: 7472 2c20 416e 795d 0a4e 6578 7447 6574  tr, Any].NextGet
+00001ef0: 7465 7220 3d20 4361 6c6c 6162 6c65 5b2e  ter = Callable[.
+00001f00: 2e2e 2c20 416e 795d 0a49 6e74 6572 6365  .., Any].Interce
+00001f10: 7074 6f72 203d 2043 616c 6c61 626c 655b  ptor = Callable[
+00001f20: 5b4e 6578 7447 6574 7465 722c 2041 7267  [NextGetter, Arg
+00001f30: 732c 204b 7761 7267 732c 2049 6e74 6572  s, Kwargs, Inter
+00001f40: 6365 7074 6f72 436f 6e74 6578 745d 2c20  ceptorContext], 
+00001f50: 416e 795d 0a5f 676c 6f62 616c 5f69 6e74  Any]._global_int
+00001f60: 6572 6365 7074 6f72 5f73 7461 636b 203d  erceptor_stack =
+00001f70: 2054 6872 6561 644c 6f63 616c 5374 6163   ThreadLocalStac
+00001f80: 6b28 290a 0a0a 4063 6f6e 7465 7874 6c69  k()...@contextli
+00001f90: 622e 636f 6e74 6578 746d 616e 6167 6572  b.contextmanager
+00001fa0: 0a64 6566 2069 6e74 6572 6365 7074 5f6d  .def intercept_m
+00001fb0: 6574 686f 6473 2869 6e74 6572 6365 7074  ethods(intercept
+00001fc0: 6f72 3a20 496e 7465 7263 6570 746f 7229  or: Interceptor)
+00001fd0: 3a0a 2020 2320 7079 6c69 6e74 3a20 6469  :.  # pylint: di
+00001fe0: 7361 626c 653d 672d 646f 632d 7265 7475  sable=g-doc-retu
+00001ff0: 726e 2d6f 722d 7969 656c 640a 2020 7222  rn-or-yield.  r"
+00002000: 2222 5265 6769 7374 6572 7320 6120 6e65  ""Registers a ne
+00002010: 7720 6d65 7468 6f64 2069 6e74 6572 6365  w method interce
+00002020: 7074 6f72 2e0a 0a20 204d 6574 686f 6420  ptor...  Method 
+00002030: 696e 7465 7263 6570 746f 7273 2061 6c6c  interceptors all
+00002040: 6f77 2079 6f75 2074 6f20 2861 7420 6120  ow you to (at a 
+00002050: 6469 7374 616e 6365 2920 696e 7465 7263  distance) interc
+00002060: 6570 7420 6d65 7468 6f64 2063 616c 6c73  ept method calls
+00002070: 2074 6f0a 2020 6d6f 6475 6c65 732e 2049   to.  modules. I
+00002080: 7420 776f 726b 7320 7369 6d69 6c61 726c  t works similarl
+00002090: 7920 746f 2064 6563 6f72 6174 6f72 732e  y to decorators.
+000020a0: 2059 6f75 2063 6f75 6c64 206d 6f64 6966   You could modif
+000020b0: 7920 6172 6773 2f6b 7761 7267 7320 6265  y args/kwargs be
+000020c0: 666f 7265 0a20 2063 616c 6c69 6e67 2074  fore.  calling t
+000020d0: 6865 2075 6e64 6572 6c79 696e 6720 6d65  he underlying me
+000020e0: 7468 6f64 2061 6e64 2f6f 7220 6d6f 6469  thod and/or modi
+000020f0: 6679 2074 6865 2072 6573 756c 7420 7265  fy the result re
+00002100: 7475 726e 696e 6720 6672 6f6d 2063 616c  turning from cal
+00002110: 6c69 6e67 0a20 2074 6865 2075 6e64 6572  ling.  the under
+00002120: 6c79 696e 6720 6d65 7468 6f64 2e20 4f72  lying method. Or
+00002130: 2079 6f75 2063 6f75 6c64 2063 6f6d 706c   you could compl
+00002140: 6574 656c 7920 736b 6970 2063 616c 6c69  etely skip calli
+00002150: 6e67 2074 6865 2075 6e64 6572 6c79 696e  ng the underlyin
+00002160: 670a 2020 6d65 7468 6f64 2061 6e64 2064  g.  method and d
+00002170: 6563 6964 6520 746f 2064 6f20 736f 6d65  ecide to do some
+00002180: 7468 696e 6720 6469 6666 6572 656e 746c  thing differentl
+00002190: 792e 2020 466f 7220 6578 616d 706c 653a  y.  For example:
+000021a0: 3a0a 0a20 2020 203e 3e3e 2069 6d70 6f72  :..    >>> impor
+000021b0: 7420 666c 6178 2e6c 696e 656e 2061 7320  t flax.linen as 
+000021c0: 6e6e 0a20 2020 203e 3e3e 2069 6d70 6f72  nn.    >>> impor
+000021d0: 7420 6a61 782e 6e75 6d70 7920 6173 206a  t jax.numpy as j
+000021e0: 6e70 0a20 2020 202e 2e2e 0a20 2020 203e  np.    ....    >
+000021f0: 3e3e 2063 6c61 7373 2046 6f6f 286e 6e2e  >> class Foo(nn.
+00002200: 4d6f 6475 6c65 293a 0a20 2020 202e 2e2e  Module):.    ...
+00002210: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
+00002220: 7365 6c66 2c20 7829 3a0a 2020 2020 2e2e  self, x):.    ..
+00002230: 2e20 2020 2020 7265 7475 726e 2078 0a20  .     return x. 
+00002240: 2020 202e 2e2e 0a20 2020 203e 3e3e 2064     ....    >>> d
+00002250: 6566 206d 795f 696e 7465 7263 6570 746f  ef my_intercepto
+00002260: 7231 286e 6578 745f 6675 6e2c 2061 7267  r1(next_fun, arg
+00002270: 732c 206b 7761 7267 732c 2063 6f6e 7465  s, kwargs, conte
+00002280: 7874 293a 0a20 2020 202e 2e2e 2020 2070  xt):.    ...   p
+00002290: 7269 6e74 2827 6361 6c6c 696e 6720 6d79  rint('calling my
+000022a0: 5f69 6e74 6572 6365 7074 6f72 3127 290a  _interceptor1').
+000022b0: 2020 2020 2e2e 2e20 2020 7265 7475 726e      ...   return
+000022c0: 206e 6578 745f 6675 6e28 2a61 7267 732c   next_fun(*args,
+000022d0: 202a 2a6b 7761 7267 7329 0a20 2020 202e   **kwargs).    .
+000022e0: 2e2e 0a20 2020 203e 3e3e 2066 6f6f 203d  ...    >>> foo =
+000022f0: 2046 6f6f 2829 0a20 2020 203e 3e3e 2077   Foo().    >>> w
+00002300: 6974 6820 6e6e 2e69 6e74 6572 6365 7074  ith nn.intercept
+00002310: 5f6d 6574 686f 6473 286d 795f 696e 7465  _methods(my_inte
+00002320: 7263 6570 746f 7231 293a 0a20 2020 202e  rceptor1):.    .
+00002330: 2e2e 2020 205f 203d 2066 6f6f 286a 6e70  ..   _ = foo(jnp
+00002340: 2e6f 6e65 7328 5b31 5d29 290a 2020 2020  .ones([1])).    
+00002350: 6361 6c6c 696e 6720 6d79 5f69 6e74 6572  calling my_inter
+00002360: 6365 7074 6f72 310a 0a20 2059 6f75 2063  ceptor1..  You c
+00002370: 6f75 6c64 2061 6c73 6f20 7265 6769 7374  ould also regist
+00002380: 6572 206d 756c 7469 706c 6520 696e 7465  er multiple inte
+00002390: 7263 6570 746f 7273 206f 6e20 7468 6520  rceptors on the 
+000023a0: 7361 6d65 206d 6574 686f 642e 2049 6e74  same method. Int
+000023b0: 6572 6365 7074 6f72 730a 2020 7769 6c6c  erceptors.  will
+000023c0: 2072 756e 2069 6e20 6f72 6465 722e 2046   run in order. F
+000023d0: 6f72 2065 7861 6d70 6c65 3a3a 0a0a 2020  or example::..  
+000023e0: 2020 3e3e 3e20 6465 6620 6d79 5f69 6e74    >>> def my_int
+000023f0: 6572 6365 7074 6f72 3228 6e65 7874 5f66  erceptor2(next_f
+00002400: 756e 2c20 6172 6773 2c20 6b77 6172 6773  un, args, kwargs
+00002410: 2c20 636f 6e74 6578 7429 3a0a 2020 2020  , context):.    
+00002420: 2e2e 2e20 2020 7072 696e 7428 2763 616c  ...   print('cal
+00002430: 6c69 6e67 206d 795f 696e 7465 7263 6570  ling my_intercep
+00002440: 746f 7232 2729 0a20 2020 202e 2e2e 2020  tor2').    ...  
+00002450: 2072 6574 7572 6e20 6e65 7874 5f66 756e   return next_fun
+00002460: 282a 6172 6773 2c20 2a2a 6b77 6172 6773  (*args, **kwargs
+00002470: 290a 2020 2020 2e2e 2e0a 2020 2020 3e3e  ).    ....    >>
+00002480: 3e20 7769 7468 206e 6e2e 696e 7465 7263  > with nn.interc
+00002490: 6570 745f 6d65 7468 6f64 7328 6d79 5f69  ept_methods(my_i
+000024a0: 6e74 6572 6365 7074 6f72 3129 2c20 5c0a  nterceptor1), \.
+000024b0: 2020 2020 2e2e 2e20 2020 2020 206e 6e2e      ...      nn.
+000024c0: 696e 7465 7263 6570 745f 6d65 7468 6f64  intercept_method
+000024d0: 7328 6d79 5f69 6e74 6572 6365 7074 6f72  s(my_interceptor
+000024e0: 3229 3a0a 2020 2020 2e2e 2e20 2020 5f20  2):.    ...   _ 
+000024f0: 3d20 666f 6f28 6a6e 702e 6f6e 6573 285b  = foo(jnp.ones([
+00002500: 315d 2929 0a20 2020 2063 616c 6c69 6e67  1])).    calling
+00002510: 206d 795f 696e 7465 7263 6570 746f 7231   my_interceptor1
+00002520: 0a20 2020 2063 616c 6c69 6e67 206d 795f  .    calling my_
+00002530: 696e 7465 7263 6570 746f 7232 0a0a 2020  interceptor2..  
+00002540: 596f 7520 636f 756c 6420 736b 6970 206f  You could skip o
+00002550: 7468 6572 2069 6e74 6572 6365 7074 6f72  ther interceptor
+00002560: 7320 6279 2064 6972 6563 746c 7920 6361  s by directly ca
+00002570: 6c6c 696e 6720 7468 650a 2020 6060 636f  lling the.  ``co
+00002580: 6e74 6578 742e 6f72 6967 5f6d 6574 686f  ntext.orig_metho
+00002590: 6460 602e 2046 6f72 2065 7861 6d70 6c65  d``. For example
+000025a0: 3a3a 0a0a 2020 2020 3e3e 3e20 6465 6620  ::..    >>> def 
+000025b0: 6d79 5f69 6e74 6572 6365 7074 6f72 3328  my_interceptor3(
+000025c0: 6e65 7874 5f66 756e 2c20 6172 6773 2c20  next_fun, args, 
+000025d0: 6b77 6172 6773 2c20 636f 6e74 6578 7429  kwargs, context)
+000025e0: 3a0a 2020 2020 2e2e 2e20 2020 7072 696e  :.    ...   prin
+000025f0: 7428 2763 616c 6c69 6e67 206d 795f 696e  t('calling my_in
+00002600: 7465 7263 6570 746f 7233 2729 0a20 2020  terceptor3').   
+00002610: 202e 2e2e 2020 2072 6574 7572 6e20 636f   ...   return co
+00002620: 6e74 6578 742e 6f72 6967 5f6d 6574 686f  ntext.orig_metho
+00002630: 6428 2a61 7267 732c 202a 2a6b 7761 7267  d(*args, **kwarg
+00002640: 7329 0a20 2020 203e 3e3e 2077 6974 6820  s).    >>> with 
+00002650: 6e6e 2e69 6e74 6572 6365 7074 5f6d 6574  nn.intercept_met
+00002660: 686f 6473 286d 795f 696e 7465 7263 6570  hods(my_intercep
+00002670: 746f 7233 292c 205c 0a20 2020 202e 2e2e  tor3), \.    ...
+00002680: 2020 2020 2020 6e6e 2e69 6e74 6572 6365        nn.interce
+00002690: 7074 5f6d 6574 686f 6473 286d 795f 696e  pt_methods(my_in
+000026a0: 7465 7263 6570 746f 7231 292c 205c 0a20  terceptor1), \. 
+000026b0: 2020 202e 2e2e 2020 2020 2020 6e6e 2e69     ...      nn.i
+000026c0: 6e74 6572 6365 7074 5f6d 6574 686f 6473  ntercept_methods
+000026d0: 286d 795f 696e 7465 7263 6570 746f 7232  (my_interceptor2
+000026e0: 293a 0a20 2020 202e 2e2e 2020 205f 203d  ):.    ...   _ =
+000026f0: 2066 6f6f 286a 6e70 2e6f 6e65 7328 5b31   foo(jnp.ones([1
+00002700: 5d29 290a 2020 2020 6361 6c6c 696e 6720  ])).    calling 
+00002710: 6d79 5f69 6e74 6572 6365 7074 6f72 330a  my_interceptor3.
+00002720: 0a20 2054 6865 2066 6f6c 6c6f 7769 6e67  .  The following
+00002730: 206d 6574 686f 6473 2063 6f75 6c64 6e27   methods couldn'
+00002740: 7420 6265 2069 6e74 6572 6365 7074 6564  t be intercepted
+00002750: 3a0a 0a20 2031 2e20 4d65 7468 6f64 7320  :..  1. Methods 
+00002760: 6465 636f 7261 746f 7265 6420 7769 7468  decoratored with
+00002770: 2060 606e 6e2e 6e6f 7772 6170 6060 2e0a   ``nn.nowrap``..
+00002780: 2020 322e 2044 756e 6465 7220 6d65 7468    2. Dunder meth
+00002790: 6f64 7320 696e 636c 7564 696e 6720 6060  ods including ``
+000027a0: 5f5f 6571 5f5f 6060 2c20 6060 5f5f 7265  __eq__``, ``__re
+000027b0: 7072 5f5f 6060 2c20 6060 5f5f 696e 6974  pr__``, ``__init
+000027c0: 5f5f 6060 2c20 6060 5f5f 6861 7368 5f5f  __``, ``__hash__
+000027d0: 6060 2c20 616e 6420 6060 5f5f 706f 7374  ``, and ``__post
+000027e0: 5f69 6e69 745f 5f60 602e 0a20 2033 2e20  _init__``..  3. 
+000027f0: 4d6f 6475 6c65 2064 6174 6163 6c61 7373  Module dataclass
+00002800: 2066 6965 6c64 732e 0a20 2034 2e20 4d6f   fields..  4. Mo
+00002810: 6475 6c65 2064 6573 6372 6970 746f 7273  dule descriptors
+00002820: 2e0a 0a20 2041 7267 733a 0a20 2020 2069  ...  Args:.    i
+00002830: 6e74 6572 6365 7074 6f72 3a20 4120 6d65  nterceptor: A me
+00002840: 7468 6f64 2069 6e74 6572 6365 7074 6f72  thod interceptor
+00002850: 2e0a 2020 2222 220a 2020 5f67 6c6f 6261  ..  """.  _globa
+00002860: 6c5f 696e 7465 7263 6570 746f 725f 7374  l_interceptor_st
+00002870: 6163 6b2e 7075 7368 2869 6e74 6572 6365  ack.push(interce
+00002880: 7074 6f72 290a 2020 7472 793a 0a20 2020  ptor).  try:.   
+00002890: 2079 6965 6c64 0a20 2066 696e 616c 6c79   yield.  finally
+000028a0: 3a0a 2020 2020 6173 7365 7274 205f 676c  :.    assert _gl
+000028b0: 6f62 616c 5f69 6e74 6572 6365 7074 6f72  obal_interceptor
+000028c0: 5f73 7461 636b 2e70 6f70 2829 2069 7320  _stack.pop() is 
+000028d0: 696e 7465 7263 6570 746f 720a 0a0a 6465  interceptor...de
+000028e0: 6620 7275 6e5f 696e 7465 7263 6570 746f  f run_intercepto
+000028f0: 7273 280a 2020 6f72 6967 5f6d 6574 686f  rs(.  orig_metho
+00002900: 643a 2043 616c 6c61 626c 655b 2e2e 2e2c  d: Callable[...,
+00002910: 2041 6e79 5d2c 0a20 206d 6f64 756c 653a   Any],.  module:
+00002920: 2027 4d6f 6475 6c65 272c 0a20 202a 6172   'Module',.  *ar
+00002930: 6773 2c0a 2020 2a2a 6b77 6172 6773 2c0a  gs,.  **kwargs,.
+00002940: 2920 2d3e 2041 6e79 3a0a 2020 2222 2252  ) -> Any:.  """R
+00002950: 756e 7320 6d65 7468 6f64 2069 6e74 6572  uns method inter
+00002960: 6365 7074 6f72 732e 2222 220a 2020 6d65  ceptors.""".  me
+00002970: 7468 6f64 5f6e 616d 6520 3d20 5f67 6574  thod_name = _get
+00002980: 5f66 6e5f 6e61 6d65 286f 7269 675f 6d65  _fn_name(orig_me
+00002990: 7468 6f64 290a 2020 6675 6e20 3d20 6675  thod).  fun = fu
+000029a0: 6e63 746f 6f6c 732e 7061 7274 6961 6c28  nctools.partial(
+000029b0: 6f72 6967 5f6d 6574 686f 642c 206d 6f64  orig_method, mod
+000029c0: 756c 6529 0a20 2063 6f6e 7465 7874 203d  ule).  context =
+000029d0: 2049 6e74 6572 6365 7074 6f72 436f 6e74   InterceptorCont
+000029e0: 6578 7428 6d6f 6475 6c65 2c20 6d65 7468  ext(module, meth
+000029f0: 6f64 5f6e 616d 652c 2066 756e 290a 0a20  od_name, fun).. 
+00002a00: 2064 6566 2077 7261 705f 696e 7465 7263   def wrap_interc
+00002a10: 6570 746f 7228 696e 7465 7263 6570 746f  eptor(intercepto
+00002a20: 722c 2066 756e 293a 0a20 2020 2022 2222  r, fun):.    """
+00002a30: 5772 6170 7320 6066 756e 6020 7769 7468  Wraps `fun` with
+00002a40: 2060 696e 7465 7263 6570 746f 7260 2e22   `interceptor`."
+00002a50: 2222 0a0a 2020 2020 4066 756e 6374 6f6f  ""..    @functoo
+00002a60: 6c73 2e77 7261 7073 2866 756e 290a 2020  ls.wraps(fun).  
+00002a70: 2020 6465 6620 7772 6170 7065 6428 2a61    def wrapped(*a
+00002a80: 7267 732c 202a 2a6b 7761 7267 7329 3a0a  rgs, **kwargs):.
+00002a90: 2020 2020 2020 7265 7475 726e 2069 6e74        return int
+00002aa0: 6572 6365 7074 6f72 2866 756e 2c20 6172  erceptor(fun, ar
+00002ab0: 6773 2c20 6b77 6172 6773 2c20 636f 6e74  gs, kwargs, cont
+00002ac0: 6578 7429 0a0a 2020 2020 7265 7475 726e  ext)..    return
+00002ad0: 2077 7261 7070 6564 0a0a 2020 2320 5772   wrapped..  # Wr
+00002ae0: 6170 7320 696e 7465 7263 6570 746f 7273  aps interceptors
+00002af0: 2061 726f 756e 6420 7468 6520 6f72 6967   around the orig
+00002b00: 696e 616c 206d 6574 686f 642e 2054 6865  inal method. The
+00002b10: 2069 6e6e 6572 6d6f 7374 2069 6e74 6572   innermost inter
+00002b20: 6365 7074 6f72 2069 730a 2020 2320 7468  ceptor is.  # th
+00002b30: 6520 6c61 7374 206f 6e65 2061 6464 6564  e last one added
+00002b40: 2061 6e64 2064 6972 6563 746c 7920 7772   and directly wr
+00002b50: 6170 7065 6420 6172 6f75 6e64 2074 6865  apped around the
+00002b60: 206f 7269 6769 6e61 6c20 626f 756e 6420   original bound 
+00002b70: 6d65 7468 6f64 2e0a 2020 666f 7220 696e  method..  for in
+00002b80: 7465 7263 6570 746f 7220 696e 205f 676c  terceptor in _gl
+00002b90: 6f62 616c 5f69 6e74 6572 6365 7074 6f72  obal_interceptor
+00002ba0: 5f73 7461 636b 3a0a 2020 2020 6675 6e20  _stack:.    fun 
+00002bb0: 3d20 7772 6170 5f69 6e74 6572 6365 7074  = wrap_intercept
+00002bc0: 6f72 2869 6e74 6572 6365 7074 6f72 2c20  or(interceptor, 
+00002bd0: 6675 6e29 0a20 2072 6574 7572 6e20 6675  fun).  return fu
+00002be0: 6e28 2a61 7267 732c 202a 2a6b 7761 7267  n(*args, **kwarg
+00002bf0: 7329 0a0a 0a23 2055 7469 6c69 7469 6573  s)...# Utilities
+00002c00: 2066 6f72 2070 7974 7265 6573 206f 6620   for pytrees of 
+00002c10: 4d6f 6475 6c65 7320 6465 6669 6e65 6420  Modules defined 
+00002c20: 696e 7369 6465 2073 6574 7570 2829 0a23  inside setup().#
+00002c30: 202d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d   ---------------
+00002c40: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00002c50: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00002c60: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00002c70: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 0a0a  --------------..
+00002c80: 0a64 6566 205f 736f 7274 6564 5f69 7465  .def _sorted_ite
+00002c90: 6d73 2878 293a 0a20 2022 2222 5265 7475  ms(x):.  """Retu
+00002ca0: 726e 7320 6974 656d 7320 6f66 2061 2064  rns items of a d
+00002cb0: 6963 7420 6f72 6465 7265 6420 6279 206b  ict ordered by k
+00002cc0: 6579 732e 2222 220a 2020 7265 7475 726e  eys.""".  return
+00002cd0: 2073 6f72 7465 6428 782e 6974 656d 7328   sorted(x.items(
+00002ce0: 292c 206b 6579 3d6c 616d 6264 6120 783a  ), key=lambda x:
+00002cf0: 2078 5b30 5d29 0a0a 0a64 6566 205f 6765   x[0])...def _ge
+00002d00: 745f 7375 6666 6978 5f76 616c 7565 5f70  t_suffix_value_p
+00002d10: 6169 7273 280a 2020 7472 6565 5f6f 725f  airs(.  tree_or_
+00002d20: 6c65 6166 3a20 416e 792c 0a29 202d 3e20  leaf: Any,.) -> 
+00002d30: 4c69 7374 5b54 7570 6c65 5b73 7472 2c20  List[Tuple[str, 
+00002d40: 5479 7065 5b27 4d6f 6475 6c65 275d 5d5d  Type['Module']]]
+00002d50: 3a0a 2020 2222 2248 656c 7065 7220 666f  :.  """Helper fo
+00002d60: 7220 6e61 6d69 6e67 2070 7974 7265 6573  r naming pytrees
+00002d70: 206f 6620 7375 626d 6f64 756c 6573 2e22   of submodules."
+00002d80: 2222 0a20 2064 6963 745f 6f72 5f6c 6561  "".  dict_or_lea
+00002d90: 6620 3d20 7365 7269 616c 697a 6174 696f  f = serializatio
+00002da0: 6e2e 746f 5f73 7461 7465 5f64 6963 7428  n.to_state_dict(
+00002db0: 7472 6565 5f6f 725f 6c65 6166 290a 2020  tree_or_leaf).  
+00002dc0: 6966 206e 6f74 2069 7369 6e73 7461 6e63  if not isinstanc
+00002dd0: 6528 6469 6374 5f6f 725f 6c65 6166 2c20  e(dict_or_leaf, 
+00002de0: 6469 6374 2920 6f72 206e 6f74 2064 6963  dict) or not dic
+00002df0: 745f 6f72 5f6c 6561 663a 0a20 2020 2072  t_or_leaf:.    r
+00002e00: 6574 7572 6e20 5b28 2727 2c20 7472 6565  eturn [('', tree
+00002e10: 5f6f 725f 6c65 6166 295d 0a20 2065 6c73  _or_leaf)].  els
+00002e20: 653a 0a20 2020 2066 6c61 745f 6469 6374  e:.    flat_dict
+00002e30: 203d 2074 7261 7665 7273 655f 7574 696c   = traverse_util
+00002e40: 2e66 6c61 7474 656e 5f64 6963 7428 6469  .flatten_dict(di
+00002e50: 6374 5f6f 725f 6c65 6166 290a 2020 2020  ct_or_leaf).    
+00002e60: 7265 7475 726e 205b 2827 5f27 202b 2027  return [('_' + '
+00002e70: 5f27 2e6a 6f69 6e28 6b29 2c20 7629 2066  _'.join(k), v) f
+00002e80: 6f72 206b 2c20 7620 696e 205f 736f 7274  or k, v in _sort
+00002e90: 6564 5f69 7465 6d73 2866 6c61 745f 6469  ed_items(flat_di
+00002ea0: 6374 295d 0a0a 0a64 6566 205f 6d61 705f  ct)]...def _map_
+00002eb0: 6f76 6572 5f6d 6f64 756c 6573 5f69 6e5f  over_modules_in_
+00002ec0: 7472 6565 2866 6e2c 2074 7265 655f 6f72  tree(fn, tree_or
+00002ed0: 5f6c 6561 6629 3a0a 2020 2222 2248 656c  _leaf):.  """Hel
+00002ee0: 7065 7220 666f 7220 6d61 7070 696e 6720  per for mapping 
+00002ef0: 6675 6e63 7469 6f6e 206f 7665 7220 7375  function over su
+00002f00: 626d 6f64 756c 6573 2e22 2222 0a20 2064  bmodules.""".  d
+00002f10: 6963 745f 6f72 5f6c 6561 6620 3d20 7365  ict_or_leaf = se
+00002f20: 7269 616c 697a 6174 696f 6e2e 746f 5f73  rialization.to_s
+00002f30: 7461 7465 5f64 6963 7428 7472 6565 5f6f  tate_dict(tree_o
+00002f40: 725f 6c65 6166 290a 2020 6966 206e 6f74  r_leaf).  if not
+00002f50: 2069 7369 6e73 7461 6e63 6528 6469 6374   isinstance(dict
+00002f60: 5f6f 725f 6c65 6166 2c20 6469 6374 2920  _or_leaf, dict) 
+00002f70: 6f72 206e 6f74 2064 6963 745f 6f72 5f6c  or not dict_or_l
+00002f80: 6561 663a 0a20 2020 2072 6574 7572 6e20  eaf:.    return 
+00002f90: 666e 2827 272c 2074 7265 655f 6f72 5f6c  fn('', tree_or_l
+00002fa0: 6561 6629 0a20 2065 6c73 653a 0a20 2020  eaf).  else:.   
+00002fb0: 2066 6c61 745f 6469 6374 203d 2074 7261   flat_dict = tra
+00002fc0: 7665 7273 655f 7574 696c 2e66 6c61 7474  verse_util.flatt
+00002fd0: 656e 5f64 6963 7428 6469 6374 5f6f 725f  en_dict(dict_or_
+00002fe0: 6c65 6166 2c20 6b65 6570 5f65 6d70 7479  leaf, keep_empty
+00002ff0: 5f6e 6f64 6573 3d54 7275 6529 0a20 2020  _nodes=True).   
+00003000: 206d 6170 7065 645f 666c 6174 5f64 6963   mapped_flat_dic
+00003010: 7420 3d20 7b0a 2020 2020 2020 6b3a 2066  t = {.      k: f
+00003020: 6e28 275f 2720 2b20 275f 272e 6a6f 696e  n('_' + '_'.join
+00003030: 286b 292c 2076 2920 666f 7220 6b2c 2076  (k), v) for k, v
+00003040: 2069 6e20 5f73 6f72 7465 645f 6974 656d   in _sorted_item
+00003050: 7328 666c 6174 5f64 6963 7429 0a20 2020  s(flat_dict).   
+00003060: 207d 0a20 2020 2072 6574 7572 6e20 7365   }.    return se
+00003070: 7269 616c 697a 6174 696f 6e2e 6672 6f6d  rialization.from
+00003080: 5f73 7461 7465 5f64 6963 7428 0a20 2020  _state_dict(.   
+00003090: 2020 2074 7265 655f 6f72 5f6c 6561 662c     tree_or_leaf,
+000030a0: 2074 7261 7665 7273 655f 7574 696c 2e75   traverse_util.u
+000030b0: 6e66 6c61 7474 656e 5f64 6963 7428 6d61  nflatten_dict(ma
+000030c0: 7070 6564 5f66 6c61 745f 6469 6374 290a  pped_flat_dict).
+000030d0: 2020 2020 290a 0a0a 6465 6620 5f66 7265      )...def _fre
+000030e0: 657a 655f 6174 7472 2876 616c 3a20 416e  eze_attr(val: An
+000030f0: 7929 202d 3e20 416e 793a 0a20 2022 2222  y) -> Any:.  """
+00003100: 5265 6375 7273 6976 656c 7920 7772 6170  Recursively wrap
+00003110: 2074 6865 2067 6976 656e 2061 7474 7269   the given attri
+00003120: 6275 7465 2060 7661 7260 2069 6e20 6060  bute `var` in ``
+00003130: 4672 6f7a 656e 4469 6374 6060 2e22 2222  FrozenDict``."""
+00003140: 0a20 2069 6620 6973 696e 7374 616e 6365  .  if isinstance
+00003150: 2876 616c 2c20 2864 6963 742c 2046 726f  (val, (dict, Fro
+00003160: 7a65 6e44 6963 7429 293a 0a20 2020 2072  zenDict)):.    r
+00003170: 6574 7572 6e20 4672 6f7a 656e 4469 6374  eturn FrozenDict
+00003180: 287b 6b3a 205f 6672 6565 7a65 5f61 7474  ({k: _freeze_att
+00003190: 7228 7629 2066 6f72 206b 2c20 7620 696e  r(v) for k, v in
+000031a0: 2076 616c 2e69 7465 6d73 2829 7d29 0a20   val.items()}). 
+000031b0: 2065 6c69 6620 6973 696e 7374 616e 6365   elif isinstance
+000031c0: 2876 616c 2c20 7475 706c 6529 3a0a 2020  (val, tuple):.  
+000031d0: 2020 2320 5370 6563 6961 6c20 6361 7365    # Special case
+000031e0: 206e 616d 6564 7475 706c 6573 2061 6e64   namedtuples and
+000031f0: 2073 7065 6369 616c 204a 4158 2074 7570   special JAX tup
+00003200: 6c65 2073 7472 7563 7475 7265 7320 6f74  le structures ot
+00003210: 6865 7277 6973 6520 7468 6579 0a20 2020  herwise they.   
+00003220: 2023 2077 6f75 6c64 2062 6520 646f 776e   # would be down
+00003230: 6772 6164 6564 2074 6f20 6e6f 726d 616c  graded to normal
+00003240: 2074 7570 6c65 732e 0a20 2020 2069 6620   tuples..    if 
+00003250: 6861 7361 7474 7228 7661 6c2c 2027 5f66  hasattr(val, '_f
+00003260: 6965 6c64 7327 2920 6f72 2074 7970 6528  ields') or type(
+00003270: 7661 6c29 2e5f 5f6e 616d 655f 5f20 3d3d  val).__name__ ==
+00003280: 2027 5061 7274 6974 696f 6e53 7065 6327   'PartitionSpec'
+00003290: 3a0a 2020 2020 2020 7265 7475 726e 2074  :.      return t
+000032a0: 7970 6528 7661 6c29 282a 5b5f 6672 6565  ype(val)(*[_free
+000032b0: 7a65 5f61 7474 7228 7629 2066 6f72 2076  ze_attr(v) for v
+000032c0: 2069 6e20 7661 6c5d 290a 2020 2020 656c   in val]).    el
+000032d0: 7365 3a0a 2020 2020 2020 7265 7475 726e  se:.      return
+000032e0: 2074 7570 6c65 285f 6672 6565 7a65 5f61   tuple(_freeze_a
+000032f0: 7474 7228 7629 2066 6f72 2076 2069 6e20  ttr(v) for v in 
+00003300: 7661 6c29 0a20 2065 6c69 6620 6973 696e  val).  elif isin
+00003310: 7374 616e 6365 2876 616c 2c20 6c69 7374  stance(val, list
+00003320: 293a 0a20 2020 2072 6574 7572 6e20 7475  ):.    return tu
+00003330: 706c 6528 5f66 7265 657a 655f 6174 7472  ple(_freeze_attr
+00003340: 2876 2920 666f 7220 7620 696e 2076 616c  (v) for v in val
+00003350: 290a 2020 656c 7365 3a0a 2020 2020 7265  ).  else:.    re
+00003360: 7475 726e 2076 616c 0a0a 0a23 204d 6574  turn val...# Met
+00003370: 686f 6420 7772 6170 7069 6e67 206f 6620  hod wrapping of 
+00003380: 2263 6f6d 7061 6374 206d 6574 686f 6473  "compact methods
+00003390: 2220 616e 6420 7365 7475 7028 290a 2320  " and setup().# 
+000033a0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000033b0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000033c0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000033d0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+000033e0: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d0a 6465  -------------.de
+000033f0: 6620 636f 6d70 6163 7428 6675 6e3a 205f  f compact(fun: _
+00003400: 4361 6c6c 6162 6c65 5429 202d 3e20 5f43  CallableT) -> _C
+00003410: 616c 6c61 626c 6554 3a0a 2020 2222 224d  allableT:.  """M
+00003420: 6172 6b73 2074 6865 2067 6976 656e 206d  arks the given m
+00003430: 6f64 756c 6520 6d65 7468 6f64 2061 6c6c  odule method all
+00003440: 6f77 696e 6720 696e 6c69 6e65 6420 7375  owing inlined su
+00003450: 626d 6f64 756c 6573 2e0a 0a20 204d 6574  bmodules...  Met
+00003460: 686f 6473 2077 7261 7070 6564 2069 6e20  hods wrapped in 
+00003470: 4063 6f6d 7061 6374 2063 616e 2064 6566  @compact can def
+00003480: 696e 6520 7375 626d 6f64 756c 6573 2064  ine submodules d
+00003490: 6972 6563 746c 7920 7769 7468 696e 2074  irectly within t
+000034a0: 6865 206d 6574 686f 642e 0a0a 2020 466f  he method...  Fo
+000034b0: 7220 696e 7374 616e 6365 3a3a 0a0a 2020  r instance::..  
+000034c0: 2020 3e3e 3e20 696d 706f 7274 2066 6c61    >>> import fla
+000034d0: 782e 6c69 6e65 6e20 6173 206e 6e0a 0a20  x.linen as nn.. 
+000034e0: 2020 203e 3e3e 2063 6c61 7373 2046 6f6f     >>> class Foo
+000034f0: 286e 6e2e 4d6f 6475 6c65 293a 0a20 2020  (nn.Module):.   
+00003500: 202e 2e2e 2020 2040 6e6e 2e63 6f6d 7061   ...   @nn.compa
+00003510: 6374 0a20 2020 202e 2e2e 2020 2064 6566  ct.    ...   def
+00003520: 205f 5f63 616c 6c5f 5f28 7365 6c66 2c20   __call__(self, 
+00003530: 782c 2066 6561 7475 7265 7329 3a0a 2020  x, features):.  
+00003540: 2020 2e2e 2e20 2020 2020 7820 3d20 6e6e    ...     x = nn
+00003550: 2e44 656e 7365 2866 6561 7475 7265 7329  .Dense(features)
+00003560: 2878 290a 2020 2020 2e2e 2e20 2020 2020  (x).    ...     
+00003570: 2e2e 2e0a 2020 2020 2e2e 2e20 2020 2020  ....    ...     
+00003580: 7265 7475 726e 2078 0a0a 2020 4174 206d  return x..  At m
+00003590: 6f73 7420 6f6e 6520 6d65 7468 6f64 2069  ost one method i
+000035a0: 6e20 6561 6368 204d 6f64 756c 6520 6d61  n each Module ma
+000035b0: 7920 6265 2077 7261 7070 6564 2077 6974  y be wrapped wit
+000035c0: 6820 4063 6f6d 7061 6374 2e0a 0a20 2041  h @compact...  A
+000035d0: 7267 733a 0a20 2020 2066 756e 3a20 5468  rgs:.    fun: Th
+000035e0: 6520 4d6f 6475 6c65 206d 6574 686f 6420  e Module method 
+000035f0: 746f 206d 6172 6b20 6173 2063 6f6d 7061  to mark as compa
+00003600: 6374 2e0a 0a20 2052 6574 7572 6e73 3a0a  ct...  Returns:.
+00003610: 2020 2020 5468 6520 6769 7665 6e20 6675      The given fu
+00003620: 6e63 7469 6f6e 2060 6066 756e 6060 206d  nction ``fun`` m
+00003630: 6172 6b65 6420 6173 2063 6f6d 7061 6374  arked as compact
+00003640: 2e0a 2020 2222 220a 2020 6675 6e2e 636f  ..  """.  fun.co
+00003650: 6d70 6163 7420 3d20 5472 7565 2020 2320  mpact = True  # 
+00003660: 7479 7065 3a20 6967 6e6f 7265 5b61 7474  type: ignore[att
+00003670: 722d 6465 6669 6e65 645d 0a20 2072 6574  r-defined].  ret
+00003680: 7572 6e20 6675 6e0a 0a0a 6465 6620 6e6f  urn fun...def no
+00003690: 7772 6170 2866 756e 3a20 5f43 616c 6c61  wrap(fun: _Calla
+000036a0: 626c 6554 2920 2d3e 205f 4361 6c6c 6162  bleT) -> _Callab
+000036b0: 6c65 543a 0a20 2022 2222 4d61 726b 7320  leT:.  """Marks 
+000036c0: 7468 6520 6769 7665 6e20 6d6f 6475 6c65  the given module
+000036d0: 206d 6574 686f 6420 6173 2061 2068 656c   method as a hel
+000036e0: 7065 7220 6d65 7468 6f64 2074 6861 7420  per method that 
+000036f0: 6e65 6564 6e27 7420 6265 2077 7261 7070  needn't be wrapp
+00003700: 6564 2e0a 0a20 204d 6574 686f 6473 2077  ed...  Methods w
+00003710: 7261 7070 6564 2069 6e20 6060 406e 6f77  rapped in ``@now
+00003720: 7261 7060 6020 6172 6520 7072 6976 6174  rap`` are privat
+00003730: 6520 6865 6c70 6572 206d 6574 686f 6473  e helper methods
+00003740: 2074 6861 7420 6e65 6564 6e27 7420 6265   that needn't be
+00003750: 2077 7261 7070 6564 0a20 2077 6974 6820   wrapped.  with 
+00003760: 7468 6520 7374 6174 6520 6861 6e64 6c65  the state handle
+00003770: 7220 6f72 2061 2073 6570 6172 6174 6520  r or a separate 
+00003780: 6e61 6d65 645f 6361 6c6c 2074 7261 6e73  named_call trans
+00003790: 666f 726d 2e0a 0a20 2054 6869 7320 6973  form...  This is
+000037a0: 206e 6565 6465 6420 696e 2073 6576 6572   needed in sever
+000037b0: 616c 2063 6f6e 6372 6574 6520 696e 7374  al concrete inst
+000037c0: 616e 6365 733a 0a20 2020 2d20 6966 2079  ances:.   - if y
+000037d0: 6f75 2772 6520 7375 6263 6c61 7373 696e  ou're subclassin
+000037e0: 6720 6120 6d65 7468 6f64 206c 696b 6520  g a method like 
+000037f0: 4d6f 6475 6c65 2e70 6172 616d 2061 6e64  Module.param and
+00003800: 2064 6f6e 2774 2077 616e 7420 7468 6973   don't want this
+00003810: 0a20 2020 2020 6f76 6572 7269 6465 6e20  .     overriden 
+00003820: 636f 7265 2066 756e 6374 696f 6e20 6465  core function de
+00003830: 636f 7261 7465 6420 7769 7468 2074 6865  corated with the
+00003840: 2073 7461 7465 206d 616e 6167 656d 656e   state managemen
+00003850: 7420 7772 6170 7065 722e 0a20 2020 2d20  t wrapper..   - 
+00003860: 4966 2079 6f75 2077 616e 7420 6120 6d65  If you want a me
+00003870: 7468 6f64 2074 6f20 6265 2063 616c 6c61  thod to be calla
+00003880: 626c 6520 6672 6f6d 2061 6e20 756e 626f  ble from an unbo
+00003890: 756e 6420 4d6f 6475 6c65 2028 652e 672e  und Module (e.g.
+000038a0: 3a20 610a 2020 2020 2066 756e 6374 696f  : a.     functio
+000038b0: 6e20 6f66 2063 6f6e 7374 7275 6374 696f  n of constructio
+000038c0: 6e20 6f66 2061 7267 756d 656e 7473 2074  n of arguments t
+000038d0: 6861 7420 646f 6573 6e27 7420 6465 7065  hat doesn't depe
+000038e0: 6e64 206f 6e20 7061 7261 6d73 2f52 4e47  nd on params/RNG
+000038f0: 7329 2e0a 2020 2020 2049 6620 796f 7520  s)..     If you 
+00003900: 7761 6e74 2074 6f20 6c65 6172 6e20 6d6f  want to learn mo
+00003910: 7265 2061 626f 7574 2068 6f77 2046 6c61  re about how Fla
+00003920: 7820 4d6f 6475 6c65 7320 6d61 6e61 6765  x Modules manage
+00003930: 2074 6865 6972 2073 7461 7465 2072 6561   their state rea
+00003940: 6420 7468 650a 2020 2020 205b 5468 6520  d the.     [The 
+00003950: 466c 6178 204d 6f64 756c 6520 6c69 6665  Flax Module life
+00003960: 6379 636c 655d 2868 7474 7073 3a2f 2f66  cycle](https://f
+00003970: 6c61 782e 7265 6164 7468 6564 6f63 732e  lax.readthedocs.
+00003980: 696f 2f65 6e2f 6c61 7465 7374 2f64 6576  io/en/latest/dev
+00003990: 656c 6f70 6572 5f6e 6f74 6573 2f6d 6f64  eloper_notes/mod
+000039a0: 756c 655f 6c69 6665 6379 636c 652e 6874  ule_lifecycle.ht
+000039b0: 6d6c 290a 2020 2020 2067 7569 6465 2e0a  ml).     guide..
+000039c0: 0a20 2046 6f72 2069 6e73 7461 6e63 653a  .  For instance:
+000039d0: 3a0a 0a20 2020 203e 3e3e 2069 6d70 6f72  :..    >>> impor
+000039e0: 7420 666c 6178 2e6c 696e 656e 2061 7320  t flax.linen as 
+000039f0: 6e6e 0a20 2020 203e 3e3e 2069 6d70 6f72  nn.    >>> impor
+00003a00: 7420 6a61 782c 206a 6178 2e6e 756d 7079  t jax, jax.numpy
+00003a10: 2061 7320 6a6e 700a 0a20 2020 203e 3e3e   as jnp..    >>>
+00003a20: 2063 6c61 7373 2046 6f6f 286e 6e2e 4d6f   class Foo(nn.Mo
+00003a30: 6475 6c65 293a 0a20 2020 202e 2e2e 2020  dule):.    ...  
+00003a40: 206e 756d 5f66 6561 7475 7265 733a 2069   num_features: i
+00003a50: 6e74 0a0a 2020 2020 2e2e 2e20 2020 406e  nt..    ...   @n
+00003a60: 6e2e 6e6f 7772 6170 0a20 2020 202e 2e2e  n.nowrap.    ...
+00003a70: 2020 2064 6566 205f 6d61 6b65 5f64 656e     def _make_den
+00003a80: 7365 2873 656c 662c 206e 756d 5f66 6561  se(self, num_fea
+00003a90: 7475 7265 7329 3a0a 2020 2020 2e2e 2e20  tures):.    ... 
+00003aa0: 2020 2020 7265 7475 726e 206e 6e2e 4465      return nn.De
+00003ab0: 6e73 6528 6e75 6d5f 6665 6174 7572 6573  nse(num_features
+00003ac0: 290a 0a20 2020 202e 2e2e 2020 2040 6e6e  )..    ...   @nn
+00003ad0: 2e63 6f6d 7061 6374 0a20 2020 202e 2e2e  .compact.    ...
+00003ae0: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
+00003af0: 7365 6c66 2c20 7829 3a0a 2020 2020 2e2e  self, x):.    ..
+00003b00: 2e20 2020 2020 2320 6e6f 7720 7361 6665  .     # now safe
+00003b10: 2074 6f20 7573 6520 636f 6e73 7472 7563   to use construc
+00003b20: 746f 7220 6865 6c70 6572 2065 7665 6e20  tor helper even 
+00003b30: 6966 2075 7369 6e67 206e 616d 6564 5f63  if using named_c
+00003b40: 616c 6c0a 2020 2020 2e2e 2e20 2020 2020  all.    ...     
+00003b50: 6465 6e73 6520 3d20 7365 6c66 2e5f 6d61  dense = self._ma
+00003b60: 6b65 5f64 656e 7365 2873 656c 662e 6e75  ke_dense(self.nu
+00003b70: 6d5f 6665 6174 7572 6573 290a 2020 2020  m_features).    
+00003b80: 2e2e 2e20 2020 2020 7265 7475 726e 2064  ...     return d
+00003b90: 656e 7365 2878 290a 0a20 2041 7267 733a  ense(x)..  Args:
+00003ba0: 0a20 2020 2066 756e 3a20 5468 6520 4d6f  .    fun: The Mo
+00003bb0: 6475 6c65 206d 6574 686f 6420 746f 206d  dule method to m
+00003bc0: 6172 6b20 6173 206e 6f77 7261 702e 0a0a  ark as nowrap...
+00003bd0: 2020 5265 7475 726e 733a 0a20 2020 2054    Returns:.    T
+00003be0: 6865 2067 6976 656e 2066 756e 6374 696f  he given functio
+00003bf0: 6e20 6060 6675 6e60 6020 6d61 726b 6564  n ``fun`` marked
+00003c00: 2061 7320 6e6f 7772 6170 2e0a 2020 2222   as nowrap..  ""
+00003c10: 220a 2020 6675 6e2e 6e6f 7772 6170 203d  ".  fun.nowrap =
+00003c20: 2054 7275 6520 2023 2074 7970 653a 2069   True  # type: i
+00003c30: 676e 6f72 655b 6174 7472 2d64 6566 696e  gnore[attr-defin
+00003c40: 6564 5d0a 2020 7265 7475 726e 2066 756e  ed].  return fun
+00003c50: 0a0a 0a64 6566 2063 6f6d 7061 6374 5f6e  ...def compact_n
+00003c60: 616d 655f 7363 6f70 6528 6675 6e3a 205f  ame_scope(fun: _
+00003c70: 4361 6c6c 6162 6c65 5429 202d 3e20 5f43  CallableT) -> _C
+00003c80: 616c 6c61 626c 6554 3a0a 2020 2222 2243  allableT:.  """C
+00003c90: 7265 6174 6573 2063 6f6d 7061 6374 2073  reates compact s
+00003ca0: 7562 6d6f 6475 6c65 7320 6672 6f6d 2061  ubmodules from a
+00003cb0: 206d 6574 686f 642e 0a0a 2020 5468 6973   method...  This
+00003cc0: 2069 7320 6120 6465 636f 7261 746f 7220   is a decorator 
+00003cd0: 7468 6174 2061 6c6c 6f77 7320 796f 7520  that allows you 
+00003ce0: 746f 2064 6566 696e 6520 636f 6d70 6163  to define compac
+00003cf0: 7420 7375 626d 6f64 756c 6573 2066 726f  t submodules fro
+00003d00: 6d20 610a 2020 6d65 7468 6f64 2e20 4974  m a.  method. It
+00003d10: 2773 2069 6e74 656e 7469 6f6e 2069 7320  's intention is 
+00003d20: 746f 206d 616b 6520 6974 2065 6173 6965  to make it easie
+00003d30: 7220 746f 2070 6f72 7420 636f 6465 2048  r to port code H
+00003d40: 6169 6b75 2063 6f64 6520 746f 2046 6c61  aiku code to Fla
+00003d50: 780a 2020 6279 2070 726f 7669 6469 6e67  x.  by providing
+00003d60: 2074 6865 2073 616d 6520 6675 6e63 7469   the same functi
+00003d70: 6f6e 616c 6974 792e 0a0a 2020 4578 616d  onality...  Exam
+00003d80: 706c 653a 3a0a 0a20 2020 203e 3e3e 2069  ple::..    >>> i
+00003d90: 6d70 6f72 7420 666c 6178 2e6c 696e 656e  mport flax.linen
+00003da0: 2061 7320 6e6e 0a20 2020 203e 3e3e 2069   as nn.    >>> i
+00003db0: 6d70 6f72 7420 6a61 780a 2020 2020 3e3e  mport jax.    >>
+00003dc0: 3e20 696d 706f 7274 206a 6178 2e6e 756d  > import jax.num
+00003dd0: 7079 2061 7320 6a6e 700a 2020 2020 3e3e  py as jnp.    >>
+00003de0: 3e20 6672 6f6d 2066 6c61 782e 636f 7265  > from flax.core
+00003df0: 2069 6d70 6f72 7420 7072 6574 7479 5f72   import pretty_r
+00003e00: 6570 720a 2020 2020 2e2e 2e0a 2020 2020  epr.    ....    
+00003e10: 3e3e 3e20 636c 6173 7320 466f 6f28 6e6e  >>> class Foo(nn
+00003e20: 2e4d 6f64 756c 6529 3a0a 2020 2020 2e2e  .Module):.    ..
+00003e30: 2e20 2020 406e 6e2e 636f 6d70 6163 745f  .   @nn.compact_
+00003e40: 6e61 6d65 5f73 636f 7065 0a20 2020 202e  name_scope.    .
+00003e50: 2e2e 2020 2064 6566 2075 7028 7365 6c66  ..   def up(self
+00003e60: 2c20 7829 3a0a 2020 2020 2e2e 2e20 2020  , x):.    ...   
+00003e70: 2020 7265 7475 726e 206e 6e2e 4465 6e73    return nn.Dens
+00003e80: 6528 3329 2878 290a 2020 2020 2e2e 2e0a  e(3)(x).    ....
+00003e90: 2020 2020 2e2e 2e20 2020 406e 6e2e 636f      ...   @nn.co
+00003ea0: 6d70 6163 745f 6e61 6d65 5f73 636f 7065  mpact_name_scope
+00003eb0: 0a20 2020 202e 2e2e 2020 2064 6566 2064  .    ...   def d
+00003ec0: 6f77 6e28 7365 6c66 2c20 7829 3a0a 2020  own(self, x):.  
+00003ed0: 2020 2e2e 2e20 2020 2020 7265 7475 726e    ...     return
+00003ee0: 206e 6e2e 4465 6e73 6528 3329 2878 290a   nn.Dense(3)(x).
+00003ef0: 2020 2020 2e2e 2e0a 2020 2020 2e2e 2e20      ....    ... 
+00003f00: 2020 6465 6620 5f5f 6361 6c6c 5f5f 2873    def __call__(s
+00003f10: 656c 662c 2078 293a 0a20 2020 202e 2e2e  elf, x):.    ...
+00003f20: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
+00003f30: 2e75 7028 7829 202b 2073 656c 662e 646f  .up(x) + self.do
+00003f40: 776e 2878 290a 2020 2020 2e2e 2e0a 2020  wn(x).    ....  
+00003f50: 2020 3e3e 3e20 6d6f 6475 6c65 203d 2046    >>> module = F
+00003f60: 6f6f 2829 0a20 2020 203e 3e3e 2076 6172  oo().    >>> var
+00003f70: 6961 626c 6573 203d 206d 6f64 756c 652e  iables = module.
+00003f80: 696e 6974 286a 6178 2e72 616e 646f 6d2e  init(jax.random.
+00003f90: 5052 4e47 4b65 7928 3029 2c20 6a6e 702e  PRNGKey(0), jnp.
+00003fa0: 6f6e 6573 2828 312c 2032 2929 290a 2020  ones((1, 2))).  
+00003fb0: 2020 3e3e 3e20 7061 7261 6d73 203d 2076    >>> params = v
+00003fc0: 6172 6961 626c 6573 5b27 7061 7261 6d73  ariables['params
+00003fd0: 275d 0a20 2020 203e 3e3e 2070 7269 6e74  '].    >>> print
+00003fe0: 2870 7265 7474 795f 7265 7072 286a 6178  (pretty_repr(jax
+00003ff0: 2e74 7265 655f 7574 696c 2e74 7265 655f  .tree_util.tree_
+00004000: 6d61 7028 6a6e 702e 7368 6170 652c 2070  map(jnp.shape, p
+00004010: 6172 616d 7329 2929 0a20 2020 207b 0a20  arams))).    {. 
+00004020: 2020 2020 2020 2064 6f77 6e3a 207b 0a20         down: {. 
+00004030: 2020 2020 2020 2020 2020 2044 656e 7365             Dense
+00004040: 5f30 3a20 7b0a 2020 2020 2020 2020 2020  _0: {.          
+00004050: 2020 2020 2020 6269 6173 3a20 2833 2c29        bias: (3,)
+00004060: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              
+00004070: 2020 6b65 726e 656c 3a20 2832 2c20 3329    kernel: (2, 3)
+00004080: 2c0a 2020 2020 2020 2020 2020 2020 7d2c  ,.            },
+00004090: 0a20 2020 2020 2020 207d 2c0a 2020 2020  .        },.    
+000040a0: 2020 2020 7570 3a20 7b0a 2020 2020 2020      up: {.      
+000040b0: 2020 2020 2020 4465 6e73 655f 303a 207b        Dense_0: {
+000040c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+000040d0: 2062 6961 733a 2028 332c 292c 0a20 2020   bias: (3,),.   
+000040e0: 2020 2020 2020 2020 2020 2020 206b 6572               ker
+000040f0: 6e65 6c3a 2028 322c 2033 292c 0a20 2020  nel: (2, 3),.   
+00004100: 2020 2020 2020 2020 207d 2c0a 2020 2020           },.    
+00004110: 2020 2020 7d2c 0a20 2020 207d 0a0a 2020      },.    }..  
+00004120: 596f 7520 6361 6e20 616c 736f 2075 7365  You can also use
+00004130: 2060 6063 6f6d 7061 6374 5f6e 616d 655f   ``compact_name_
+00004140: 7363 6f70 6560 6020 696e 7369 6465 2060  scope`` inside `
+00004150: 6040 636f 6d70 6163 7460 6020 6d65 7468  `@compact`` meth
+00004160: 6f64 7320 6f72 2065 7665 6e0a 2020 6f74  ods or even.  ot
+00004170: 6865 720a 2020 6060 636f 6d70 6163 745f  her.  ``compact_
+00004180: 6e61 6d65 5f73 636f 7065 6060 206d 6574  name_scope`` met
+00004190: 686f 6473 2e20 4d65 7468 6f64 7320 7468  hods. Methods th
+000041a0: 6174 2061 7265 2064 6563 6f72 6174 6564  at are decorated
+000041b0: 2077 6974 680a 2020 6060 636f 6d70 6163   with.  ``compac
+000041c0: 745f 6e61 6d65 5f73 636f 7065 6060 0a20  t_name_scope``. 
+000041d0: 2063 616e 2061 6c73 6f20 6265 2063 616c   can also be cal
+000041e0: 6c65 6420 6469 7265 6374 6c79 2066 726f  led directly fro
+000041f0: 6d20 6060 696e 6974 6060 206f 7220 6060  m ``init`` or ``
+00004200: 6170 706c 7960 6020 7669 6120 7468 6520  apply`` via the 
+00004210: 6060 6d65 7468 6f64 6060 0a20 2061 7267  ``method``.  arg
+00004220: 756d 656e 743a 3a0a 0a20 2020 203e 3e3e  ument::..    >>>
+00004230: 2079 5f64 6f77 6e20 3d20 6d6f 6475 6c65   y_down = module
+00004240: 2e61 7070 6c79 287b 2770 6172 616d 7327  .apply({'params'
+00004250: 3a20 7061 7261 6d73 7d2c 206a 6e70 2e6f  : params}, jnp.o
+00004260: 6e65 7328 2831 2c20 3229 292c 206d 6574  nes((1, 2)), met
+00004270: 686f 643d 2764 6f77 6e27 290a 2020 2020  hod='down').    
+00004280: 3e3e 3e20 795f 646f 776e 2e73 6861 7065  >>> y_down.shape
+00004290: 0a20 2020 2028 312c 2033 290a 0a20 2041  .    (1, 3)..  A
+000042a0: 7267 733a 0a20 2020 2066 756e 3a20 5468  rgs:.    fun: Th
+000042b0: 6520 4d6f 6475 6c65 206d 6574 686f 6420  e Module method 
+000042c0: 746f 206d 6172 6b20 6173 2063 6f6d 7061  to mark as compa
+000042d0: 6374 5f6e 616d 655f 7363 6f70 652e 0a0a  ct_name_scope...
+000042e0: 2020 5265 7475 726e 733a 0a20 2020 2054    Returns:.    T
+000042f0: 6865 2067 6976 656e 2066 756e 6374 696f  he given functio
+00004300: 6e20 6060 6675 6e60 6020 6d61 726b 6564  n ``fun`` marked
+00004310: 2061 7320 636f 6d70 6163 745f 6e61 6d65   as compact_name
+00004320: 5f73 636f 7065 2e0a 2020 2222 220a 0a20  _scope..  """.. 
+00004330: 2040 6675 6e63 746f 6f6c 732e 7772 6170   @functools.wrap
+00004340: 7328 6675 6e29 0a20 2064 6566 2063 6f6d  s(fun).  def com
+00004350: 7061 6374 5f6e 616d 655f 7363 6f70 655f  pact_name_scope_
+00004360: 7772 6170 7065 7228 7365 6c66 3a20 6e6e  wrapper(self: nn
+00004370: 2e4d 6f64 756c 652c 202a 6172 6773 2c20  .Module, *args, 
+00004380: 2a2a 6b77 6172 6773 293a 0a20 2020 206e  **kwargs):.    n
+00004390: 616d 6520 3d20 6675 6e2e 5f5f 6e61 6d65  ame = fun.__name
+000043a0: 5f5f 0a20 2020 2069 6620 6e6f 7420 6861  __.    if not ha
+000043b0: 7361 7474 7228 7365 6c66 2c20 275f 636f  sattr(self, '_co
+000043c0: 6d70 6163 745f 6e61 6d65 5f73 636f 7065  mpact_name_scope
+000043d0: 5f6d 6f64 756c 6573 2729 3a0a 2020 2020  _modules'):.    
+000043e0: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+000043f0: 6f72 280a 2020 2020 2020 2020 6627 4361  or(.        f'Ca
+00004400: 6e6e 6f74 2063 616c 6c20 636f 6d70 6163  nnot call compac
+00004410: 745f 6e61 6d65 5f73 636f 7065 206d 6574  t_name_scope met
+00004420: 686f 6420 7b6e 616d 6521 727d 206f 6e20  hod {name!r} on 
+00004430: 6120 4d6f 6475 6c65 2074 6861 7420 6861  a Module that ha
+00004440: 7320 6e6f 7420 6265 656e 2027 0a20 2020  s not been '.   
+00004450: 2020 2020 2066 2773 6574 7570 2e20 5468       f'setup. Th
+00004460: 6973 2069 7320 6c69 6b65 6c79 2062 6563  is is likely bec
+00004470: 6175 7365 2079 6f75 2061 7265 2063 616c  ause you are cal
+00004480: 6c69 6e67 207b 6e61 6d65 2172 7d20 270a  ling {name!r} '.
+00004490: 2020 2020 2020 2020 2766 726f 6d20 6f75          'from ou
+000044a0: 7473 6964 6520 6f66 2069 6e69 7420 6f72  tside of init or
+000044b0: 2061 7070 6c79 2e27 0a20 2020 2020 2029   apply.'.      )
+000044c0: 0a20 2020 206d 6f64 756c 6520 3d20 7365  .    module = se
+000044d0: 6c66 2e5f 636f 6d70 6163 745f 6e61 6d65  lf._compact_name
+000044e0: 5f73 636f 7065 5f6d 6f64 756c 6573 5b6e  _scope_modules[n
+000044f0: 616d 655d 0a20 2020 2072 6574 7572 6e20  ame].    return 
+00004500: 6d6f 6475 6c65 282a 6172 6773 2c20 2a2a  module(*args, **
+00004510: 6b77 6172 6773 290a 0a20 2063 6f6d 7061  kwargs)..  compa
+00004520: 6374 5f6e 616d 655f 7363 6f70 655f 7772  ct_name_scope_wr
+00004530: 6170 7065 722e 636f 6d70 6163 745f 6e61  apper.compact_na
+00004540: 6d65 5f73 636f 7065 203d 2054 7275 6520  me_scope = True 
+00004550: 2023 2074 7970 653a 2069 676e 6f72 655b   # type: ignore[
+00004560: 6174 7472 2d64 6566 696e 6564 5d0a 2020  attr-defined].  
+00004570: 636f 6d70 6163 745f 6e61 6d65 5f73 636f  compact_name_sco
+00004580: 7065 5f77 7261 7070 6572 2e69 6e6e 6572  pe_wrapper.inner
+00004590: 5f66 756e 203d 2066 756e 2020 2320 7479  _fun = fun  # ty
+000045a0: 7065 3a20 6967 6e6f 7265 5b61 7474 722d  pe: ignore[attr-
+000045b0: 6465 6669 6e65 645d 0a20 2063 6f6d 7061  defined].  compa
+000045c0: 6374 5f6e 616d 655f 7363 6f70 655f 7772  ct_name_scope_wr
+000045d0: 6170 7065 722e 6e6f 7772 6170 203d 2054  apper.nowrap = T
+000045e0: 7275 6520 2023 2074 7970 653a 2069 676e  rue  # type: ign
+000045f0: 6f72 655b 6174 7472 2d64 6566 696e 6564  ore[attr-defined
+00004600: 5d0a 2020 7265 7475 726e 2063 6f6d 7061  ].  return compa
+00004610: 6374 5f6e 616d 655f 7363 6f70 655f 7772  ct_name_scope_wr
+00004620: 6170 7065 7220 2023 2074 7970 653a 2069  apper  # type: i
+00004630: 676e 6f72 655b 7265 7475 726e 2d76 616c  gnore[return-val
+00004640: 7565 5d0a 0a0a 6465 6620 5f67 6574 5f6c  ue]...def _get_l
+00004650: 6f63 616c 5f6d 6574 686f 645f 6e61 6d65  ocal_method_name
+00004660: 7328 0a20 2063 6c73 3a20 416e 792c 2065  s(.  cls: Any, e
+00004670: 7863 6c75 6465 3a20 4974 6572 6162 6c65  xclude: Iterable
+00004680: 5b73 7472 5d20 3d20 2829 0a29 202d 3e20  [str] = ().) -> 
+00004690: 5475 706c 655b 7374 722c 202e 2e2e 5d3a  Tuple[str, ...]:
+000046a0: 0a20 2022 2222 4765 7473 206d 6574 686f  .  """Gets metho
+000046b0: 6420 6e61 6d65 7320 6f66 2061 2063 6c61  d names of a cla
+000046c0: 7373 2c20 6578 636c 7564 696e 6720 636c  ss, excluding cl
+000046d0: 6173 7320 616e 6420 7374 6174 6963 206d  ass and static m
+000046e0: 6574 686f 6473 2e0a 0a20 2041 7267 733a  ethods...  Args:
+000046f0: 0a20 2020 2063 6c73 3a20 5468 6520 636c  .    cls: The cl
+00004700: 6173 7320 746f 2067 6574 206d 6574 686f  ass to get metho
+00004710: 6420 6e61 6d65 7320 666f 722e 0a20 2020  d names for..   
+00004720: 2065 7863 6c75 6465 3a20 4e61 6d65 7320   exclude: Names 
+00004730: 746f 2065 7863 6c75 6465 2066 726f 6d20  to exclude from 
+00004740: 6f75 7470 7574 2e0a 0a20 2052 6574 7572  output...  Retur
+00004750: 6e73 3a0a 2020 2020 4120 6c69 7374 206f  ns:.    A list o
+00004760: 6620 6d65 7468 6f64 206e 616d 6573 2e0a  f method names..
+00004770: 2020 2222 220a 2020 7472 7565 5f6d 6574    """.  true_met
+00004780: 686f 6473 203d 2073 6574 2829 0a20 2066  hods = set().  f
+00004790: 6f72 206d 2069 6e20 636c 732e 5f5f 6469  or m in cls.__di
+000047a0: 6374 5f5f 3a0a 2020 2020 6966 2063 616c  ct__:.    if cal
+000047b0: 6c61 626c 6528 636c 732e 5f5f 6469 6374  lable(cls.__dict
+000047c0: 5f5f 5b6d 5d29 2061 6e64 206e 6f74 2069  __[m]) and not i
+000047d0: 6e73 7065 6374 2e69 7363 6c61 7373 280a  nspect.isclass(.
+000047e0: 2020 2020 2020 636c 732e 5f5f 6469 6374        cls.__dict
+000047f0: 5f5f 5b6d 5d0a 2020 2020 293a 2020 2320  __[m].    ):  # 
+00004800: 7079 7479 7065 3a20 6469 7361 626c 653d  pytype: disable=
+00004810: 6e6f 742d 7375 7070 6f72 7465 642d 7965  not-supported-ye
+00004820: 740a 2020 2020 2020 6d74 7970 6520 3d20  t.      mtype = 
+00004830: 7479 7065 2863 6c73 2e5f 5f64 6963 745f  type(cls.__dict_
+00004840: 5f5b 6d5d 290a 2020 2020 2020 6966 206d  _[m]).      if m
+00004850: 7479 7065 2021 3d20 7374 6174 6963 6d65  type != staticme
+00004860: 7468 6f64 2061 6e64 206d 7479 7065 2021  thod and mtype !
+00004870: 3d20 636c 6173 736d 6574 686f 643a 0a20  = classmethod:. 
+00004880: 2020 2020 2020 2074 7275 655f 6d65 7468         true_meth
+00004890: 6f64 732e 6164 6428 6d29 0a20 2072 6574  ods.add(m).  ret
+000048a0: 7572 6e20 7475 706c 6528 7472 7565 5f6d  urn tuple(true_m
+000048b0: 6574 686f 6473 2e64 6966 6665 7265 6e63  ethods.differenc
+000048c0: 6528 7365 7428 6578 636c 7564 6529 2929  e(set(exclude)))
+000048d0: 0a0a 0a64 6566 205f 6765 745f 6c6f 6361  ...def _get_loca
+000048e0: 6c5f 6465 7363 7269 7074 6f72 5f6e 616d  l_descriptor_nam
+000048f0: 6573 280a 2020 636c 733a 2041 6e79 2c20  es(.  cls: Any, 
+00004900: 6578 636c 7564 653a 2049 7465 7261 626c  exclude: Iterabl
+00004910: 655b 7374 725d 203d 2028 290a 2920 2d3e  e[str] = ().) ->
+00004920: 2054 7570 6c65 5b73 7472 2c20 2e2e 2e5d   Tuple[str, ...]
+00004930: 3a0a 2020 2222 2247 6574 7320 6465 7363  :.  """Gets desc
+00004940: 7269 7074 6f72 206e 616d 6573 206f 6620  riptor names of 
+00004950: 6120 636c 6173 732e 0a0a 2020 4172 6773  a class...  Args
+00004960: 3a0a 2020 2020 636c 733a 2054 6865 2063  :.    cls: The c
+00004970: 6c61 7373 2074 6f20 6765 7420 7072 6f70  lass to get prop
+00004980: 6572 7479 206e 616d 6573 2066 6f72 2e0a  erty names for..
+00004990: 2020 2020 6578 636c 7564 653a 204e 616d      exclude: Nam
+000049a0: 6573 2074 6f20 6578 636c 7564 6520 6672  es to exclude fr
+000049b0: 6f6d 206f 7574 7075 742e 0a0a 2020 5265  om output...  Re
+000049c0: 7475 726e 733a 0a20 2020 2041 206c 6973  turns:.    A lis
+000049d0: 7420 6f66 2070 726f 7065 7274 7920 6e61  t of property na
+000049e0: 6d65 732e 0a20 2022 2222 0a20 2074 7275  mes..  """.  tru
+000049f0: 655f 7072 6f70 6572 7469 6573 203d 2073  e_properties = s
+00004a00: 6574 2829 0a20 2066 6f72 206d 2c20 6174  et().  for m, at
+00004a10: 7472 2069 6e20 636c 732e 5f5f 6469 6374  tr in cls.__dict
+00004a20: 5f5f 2e69 7465 6d73 2829 3a0a 2020 2020  __.items():.    
+00004a30: 6966 206e 6f74 2063 616c 6c61 626c 6528  if not callable(
+00004a40: 6174 7472 2920 616e 6420 280a 2020 2020  attr) and (.    
+00004a50: 2020 6861 7361 7474 7228 6174 7472 2c20    hasattr(attr, 
+00004a60: 275f 5f67 6574 5f5f 2729 0a20 2020 2020  '__get__').     
+00004a70: 206f 7220 6861 7361 7474 7228 6174 7472   or hasattr(attr
+00004a80: 2c20 275f 5f73 6574 5f5f 2729 0a20 2020  , '__set__').   
+00004a90: 2020 206f 7220 6861 7361 7474 7228 6174     or hasattr(at
+00004aa0: 7472 2c20 275f 5f64 656c 6574 655f 5f27  tr, '__delete__'
+00004ab0: 290a 2020 2020 293a 0a20 2020 2020 206d  ).    ):.      m
+00004ac0: 7479 7065 203d 2074 7970 6528 6174 7472  type = type(attr
+00004ad0: 290a 2020 2020 2020 6966 206d 7479 7065  ).      if mtype
+00004ae0: 2021 3d20 7374 6174 6963 6d65 7468 6f64   != staticmethod
+00004af0: 2061 6e64 206d 7479 7065 2021 3d20 636c   and mtype != cl
+00004b00: 6173 736d 6574 686f 643a 0a20 2020 2020  assmethod:.     
+00004b10: 2020 2074 7275 655f 7072 6f70 6572 7469     true_properti
+00004b20: 6573 2e61 6464 286d 290a 2020 7265 7475  es.add(m).  retu
+00004b30: 726e 2074 7570 6c65 2874 7275 655f 7072  rn tuple(true_pr
+00004b40: 6f70 6572 7469 6573 2e64 6966 6665 7265  operties.differe
+00004b50: 6e63 6528 7365 7428 6578 636c 7564 6529  nce(set(exclude)
+00004b60: 2929 0a0a 0a64 6566 2077 7261 705f 6d65  ))...def wrap_me
+00004b70: 7468 6f64 5f6f 6e63 6528 6675 6e3a 2043  thod_once(fun: C
+00004b80: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
+00004b90: 5d29 202d 3e20 4361 6c6c 6162 6c65 5b2e  ]) -> Callable[.
+00004ba0: 2e2e 2c20 416e 795d 3a0a 2020 2222 224d  .., Any]:.  """M
+00004bb0: 616e 6167 6573 204d 6f64 756c 6520 7374  anages Module st
+00004bc0: 6174 6520 666f 7220 6120 6769 7665 6e20  ate for a given 
+00004bd0: 7573 6572 2d64 6566 696e 6564 206d 6574  user-defined met
+00004be0: 686f 642e 0a0a 2020 4172 6773 3a0a 2020  hod...  Args:.  
+00004bf0: 2020 6675 6e3a 2055 7365 722d 6465 6669    fun: User-defi
+00004c00: 6e65 6420 4d6f 6475 6c65 206d 6574 686f  ned Module metho
+00004c10: 6420 746f 206d 616e 6167 6520 7374 6174  d to manage stat
+00004c20: 6520 666f 722e 0a0a 2020 5265 7475 726e  e for...  Return
+00004c30: 733a 0a20 2020 2057 7261 7070 6564 206d  s:.    Wrapped m
+00004c40: 6574 686f 642e 0a20 2022 2222 0a20 2023  ethod..  """.  #
+00004c50: 2044 6f6e 2774 2072 6577 7261 7020 6d65   Don't rewrap me
+00004c60: 7468 6f64 7320 7468 6174 2068 6176 6520  thods that have 
+00004c70: 616c 7265 6164 7920 6861 6420 7468 6520  already had the 
+00004c80: 7374 6174 6520 6d61 6e61 6765 6d65 6e74  state management
+00004c90: 2077 7261 7070 6572 0a20 2023 2061 7070   wrapper.  # app
+00004ca0: 6c69 6564 2069 6e20 7468 6520 6465 636f  lied in the deco
+00004cb0: 7261 746f 7220 7374 6163 6b2e 2020 5468  rator stack.  Th
+00004cc0: 6973 2077 7261 7070 6572 2073 686f 756c  is wrapper shoul
+00004cd0: 6420 616c 7761 7973 2062 6520 6170 706c  d always be appl
+00004ce0: 6965 640a 2020 2320 6265 666f 7265 2074  ied.  # before t
+00004cf0: 7261 6e73 666f 726d 6174 696f 6e20 7772  ransformation wr
+00004d00: 6170 7065 7273 2e0a 2020 6966 2068 6173  appers..  if has
+00004d10: 6174 7472 2866 756e 2c20 276d 6574 686f  attr(fun, 'metho
+00004d20: 645f 6861 6e64 6c65 725f 7772 6170 7065  d_handler_wrappe
+00004d30: 6427 293a 0a20 2020 2072 6574 7572 6e20  d'):.    return 
+00004d40: 6675 6e0a 0a20 2040 6675 6e63 746f 6f6c  fun..  @functool
+00004d50: 732e 7772 6170 7328 6675 6e29 0a20 2064  s.wraps(fun).  d
+00004d60: 6566 2077 7261 7070 6564 5f6d 6f64 756c  ef wrapped_modul
+00004d70: 655f 6d65 7468 6f64 282a 6172 6773 2c20  e_method(*args, 
+00004d80: 2a2a 6b77 6172 6773 293a 0a20 2020 2023  **kwargs):.    #
+00004d90: 2057 6520 6d69 6768 7420 6861 7665 2069   We might have i
+00004da0: 6e63 6f72 7265 6374 6c79 2077 7261 7070  ncorrectly wrapp
+00004db0: 7065 6420 6120 6361 6c6c 6162 6c65 0a20  ped a callable. 
+00004dc0: 2020 2023 2074 6861 7420 6973 206e 6f74     # that is not
+00004dd0: 2061 206d 6574 686f 642e 2043 6865 636b   a method. Check
+00004de0: 2077 6865 7468 6572 2074 6865 2066 6972   whether the fir
+00004df0: 7374 2061 7267 2069 7320 7365 6c66 2c0a  st arg is self,.
+00004e00: 2020 2020 2320 6f74 6865 7277 6973 6520      # otherwise 
+00004e10: 6361 6c6c 2074 6865 2077 7261 7070 6564  call the wrapped
+00004e20: 2066 756e 6374 696f 6e20 6173 2069 732e   function as is.
+00004e30: 0a20 2020 2069 6620 6172 6773 2061 6e64  .    if args and
+00004e40: 2069 7369 6e73 7461 6e63 6528 6172 6773   isinstance(args
+00004e50: 5b30 5d2c 204d 6f64 756c 6529 3a0a 2020  [0], Module):.  
+00004e60: 2020 2020 7365 6c66 2c20 6172 6773 203d      self, args =
+00004e70: 2061 7267 735b 305d 2c20 6172 6773 5b31   args[0], args[1
+00004e80: 3a5d 0a20 2020 2020 2072 6574 7572 6e20  :].      return 
+00004e90: 7365 6c66 2e5f 6361 6c6c 5f77 7261 7070  self._call_wrapp
+00004ea0: 6564 5f6d 6574 686f 6428 6675 6e2c 2061  ed_method(fun, a
+00004eb0: 7267 732c 206b 7761 7267 7329 0a20 2020  rgs, kwargs).   
+00004ec0: 2065 6c73 653a 0a20 2020 2020 2072 6574   else:.      ret
+00004ed0: 7572 6e20 6675 6e28 2a61 7267 732c 202a  urn fun(*args, *
+00004ee0: 2a6b 7761 7267 7329 0a0a 2020 7772 6170  *kwargs)..  wrap
+00004ef0: 7065 645f 6d6f 6475 6c65 5f6d 6574 686f  ped_module_metho
+00004f00: 642e 6d65 7468 6f64 5f68 616e 646c 6572  d.method_handler
+00004f10: 5f77 7261 7070 6564 203d 2054 7275 6520  _wrapped = True 
+00004f20: 2023 2074 7970 653a 2069 676e 6f72 655b   # type: ignore[
+00004f30: 6174 7472 2d64 6566 696e 6564 5d0a 2020  attr-defined].  
+00004f40: 7265 7475 726e 2077 7261 7070 6564 5f6d  return wrapped_m
+00004f50: 6f64 756c 655f 6d65 7468 6f64 0a0a 0a64  odule_method...d
+00004f60: 6566 2077 7261 705f 6465 7363 7269 7074  ef wrap_descript
+00004f70: 6f72 5f6f 6e63 6528 6465 7363 7269 7074  or_once(descript
+00004f80: 6f72 2920 2d3e 2027 4465 7363 7269 7074  or) -> 'Descript
+00004f90: 6f72 5772 6170 7065 7227 3a0a 2020 2222  orWrapper':.  ""
+00004fa0: 2257 7261 7073 2061 2064 6573 6372 6970  "Wraps a descrip
+00004fb0: 746f 7220 746f 2067 6976 6520 6265 7474  tor to give bett
+00004fc0: 6572 2065 7272 6f72 206d 6573 7361 6765  er error message
+00004fd0: 732e 0a0a 2020 4172 6773 3a0a 2020 2020  s...  Args:.    
+00004fe0: 6465 7363 7269 7074 6f72 3a20 5573 6572  descriptor: User
+00004ff0: 2d64 6566 696e 6564 204d 6f64 756c 6520  -defined Module 
+00005000: 6174 7472 6962 7574 6520 6465 7363 7269  attribute descri
+00005010: 7074 6f72 2e0a 0a20 2052 6574 7572 6e73  ptor...  Returns
+00005020: 3a0a 2020 2020 5772 6170 7065 6420 6465  :.    Wrapped de
+00005030: 7363 7269 7074 6f72 2e0a 2020 2222 220a  scriptor..  """.
+00005040: 2020 2320 446f 6e27 7420 7265 7772 6170    # Don't rewrap
+00005050: 2064 6573 6372 6970 746f 7273 2e0a 2020   descriptors..  
+00005060: 6966 2069 7369 6e73 7461 6e63 6528 6465  if isinstance(de
+00005070: 7363 7269 7074 6f72 2c20 4465 7363 7269  scriptor, Descri
+00005080: 7074 6f72 5772 6170 7065 7229 3a0a 2020  ptorWrapper):.  
+00005090: 2020 7265 7475 726e 2064 6573 6372 6970    return descrip
+000050a0: 746f 720a 0a20 2072 6574 7572 6e20 6372  tor..  return cr
+000050b0: 6561 7465 5f64 6573 6372 6970 746f 725f  eate_descriptor_
+000050c0: 7772 6170 7065 7228 6465 7363 7269 7074  wrapper(descript
+000050d0: 6f72 290a 0a0a 6465 6620 5f77 7261 705f  or)...def _wrap_
+000050e0: 6861 7368 2868 6173 685f 666e 3a20 4361  hash(hash_fn: Ca
+000050f0: 6c6c 6162 6c65 5b2e 2e2e 2c20 416e 795d  llable[..., Any]
+00005100: 2920 2d3e 2043 616c 6c61 626c 655b 2e2e  ) -> Callable[..
+00005110: 2e2c 2041 6e79 5d3a 0a20 2022 2222 5772  ., Any]:.  """Wr
+00005120: 6170 7320 6120 6861 7368 2066 756e 6374  aps a hash funct
+00005130: 696f 6e20 7769 7468 2073 6f6d 6520 6368  ion with some ch
+00005140: 6563 6b20 666f 7220 466c 6178 204d 6f64  eck for Flax Mod
+00005150: 756c 6573 2e22 2222 0a0a 2020 4066 756e  ules."""..  @fun
+00005160: 6374 6f6f 6c73 2e77 7261 7073 2868 6173  ctools.wraps(has
+00005170: 685f 666e 290a 2020 6465 6620 7772 6170  h_fn).  def wrap
+00005180: 7065 6428 7365 6c66 293a 0a20 2020 2069  ped(self):.    i
+00005190: 6620 7365 6c66 2e73 636f 7065 2069 7320  f self.scope is 
+000051a0: 6e6f 7420 4e6f 6e65 3a0a 2020 2020 2020  not None:.      
+000051b0: 7261 6973 6520 5479 7065 4572 726f 7228  raise TypeError(
+000051c0: 2243 616e 2774 2063 616c 6c20 5f5f 6861  "Can't call __ha
+000051d0: 7368 5f5f 206f 6e20 6d6f 6475 6c65 7320  sh__ on modules 
+000051e0: 7468 6174 2068 6f6c 6420 7661 7269 6162  that hold variab
+000051f0: 6c65 732e 2229 0a20 2020 2074 7279 3a0a  les.").    try:.
+00005200: 2020 2020 2020 6861 7368 5f76 616c 7565        hash_value
+00005210: 203d 2068 6173 685f 666e 2873 656c 6629   = hash_fn(self)
+00005220: 0a20 2020 2065 7863 6570 7420 5479 7065  .    except Type
+00005230: 4572 726f 7220 6173 2065 7863 3a0a 2020  Error as exc:.  
+00005240: 2020 2020 7261 6973 6520 5479 7065 4572      raise TypeEr
+00005250: 726f 7228 0a20 2020 2020 2020 2027 4661  ror(.        'Fa
+00005260: 696c 6564 2074 6f20 6861 7368 2046 6c61  iled to hash Fla
+00005270: 7820 4d6f 6475 6c65 2e20 2027 0a20 2020  x Module.  '.   
+00005280: 2020 2020 2027 5468 6520 6d6f 6475 6c65       'The module
+00005290: 2070 726f 6261 626c 7920 636f 6e74 6169   probably contai
+000052a0: 6e73 2075 6e68 6173 6861 626c 6520 6174  ns unhashable at
+000052b0: 7472 6962 7574 6573 2e20 2027 0a20 2020  tributes.  '.   
+000052c0: 2020 2020 2066 274d 6f64 756c 653d 7b73       f'Module={s
+000052d0: 656c 667d 270a 2020 2020 2020 2920 6672  elf}'.      ) fr
+000052e0: 6f6d 2065 7863 0a20 2020 2072 6574 7572  om exc.    retur
+000052f0: 6e20 6861 7368 5f76 616c 7565 0a0a 2020  n hash_value..  
+00005300: 7265 7475 726e 2077 7261 7070 6564 0a0a  return wrapped..
+00005310: 0a64 6566 205f 6765 745f 756e 626f 756e  .def _get_unboun
+00005320: 645f 666e 286d 6574 686f 645f 6f72 5f66  d_fn(method_or_f
+00005330: 6e3a 2043 616c 6c61 626c 655b 2e2e 2e2c  n: Callable[...,
+00005340: 2041 6e79 5d29 202d 3e20 4361 6c6c 6162   Any]) -> Callab
+00005350: 6c65 5b2e 2e2e 2c20 416e 795d 3a0a 2020  le[..., Any]:.  
+00005360: 2222 2252 6574 7572 6e73 2061 6e20 756e  """Returns an un
+00005370: 626f 756e 6420 6675 6e63 7469 6f6e 2066  bound function f
+00005380: 726f 6d20 6120 6d65 7468 6f64 2074 6861  rom a method tha
+00005390: 7420 6973 2070 6f73 7369 626c 7920 626f  t is possibly bo
+000053a0: 756e 642e 0a0a 2020 5468 6973 206d 6561  und...  This mea
+000053b0: 6e73 2074 6861 7420 6966 2074 6865 2070  ns that if the p
+000053c0: 6173 7365 6420 6675 6e63 7469 6f6e 2062  assed function b
+000053d0: 656c 6f6e 6773 206f 6620 616e 2069 6e73  elongs of an ins
+000053e0: 7461 6e63 6520 6f66 2061 2063 6c61 7373  tance of a class
+000053f0: 2c20 7468 656e 0a20 2074 6865 2072 6574  , then.  the ret
+00005400: 7572 6e65 6420 6675 6e63 7469 6f6e 2064  urned function d
+00005410: 6f65 7320 6e6f 206c 6f6e 6765 7220 6465  oes no longer de
+00005420: 7065 6e64 206f 6e20 7468 6520 696e 7374  pend on the inst
+00005430: 616e 6365 2c20 7768 6963 6820 6973 2070  ance, which is p
+00005440: 6173 7365 640a 2020 6173 2074 6865 2066  assed.  as the f
+00005450: 6972 7374 2061 7267 756d 656e 7420 746f  irst argument to
+00005460: 2074 6865 2066 756e 6374 696f 6e2e 0a0a   the function...
+00005470: 2020 4172 6773 3a0a 2020 2020 6d65 7468    Args:.    meth
+00005480: 6f64 5f6f 725f 666e 3a20 4120 636c 6173  od_or_fn: A clas
+00005490: 7320 6d65 7468 6f64 206f 7220 6675 6e63  s method or func
+000054a0: 7469 6f6e 2e0a 0a20 2052 6574 7572 6e73  tion...  Returns
+000054b0: 3a0a 2020 2020 416e 2075 6e62 6f75 6e64  :.    An unbound
+000054c0: 2076 6572 7369 6f6e 206f 6620 696e 7075   version of inpu
+000054d0: 7420 6675 6e63 7469 6f6e 2e0a 2020 2222  t function..  ""
+000054e0: 220a 2020 6966 2069 6e73 7065 6374 2e69  ".  if inspect.i
+000054f0: 736d 6574 686f 6428 6d65 7468 6f64 5f6f  smethod(method_o
+00005500: 725f 666e 2920 616e 6420 6973 696e 7374  r_fn) and isinst
+00005510: 616e 6365 280a 2020 2020 6d65 7468 6f64  ance(.    method
+00005520: 5f6f 725f 666e 2e5f 5f73 656c 665f 5f2c  _or_fn.__self__,
+00005530: 204d 6f64 756c 650a 2020 293a 2020 2320   Module.  ):  # 
+00005540: 7079 7479 7065 3a20 6469 7361 626c 653d  pytype: disable=
+00005550: 6174 7472 6962 7574 652d 6572 726f 720a  attribute-error.
+00005560: 2020 2020 6d65 7468 6f64 5f6f 725f 666e      method_or_fn
+00005570: 203d 206d 6574 686f 645f 6f72 5f66 6e2e   = method_or_fn.
+00005580: 5f5f 6675 6e63 5f5f 2020 2320 7079 7479  __func__  # pyty
+00005590: 7065 3a20 6469 7361 626c 653d 6174 7472  pe: disable=attr
+000055a0: 6962 7574 652d 6572 726f 720a 0a20 2023  ibute-error..  #
+000055b0: 2054 6865 206d 6574 686f 6420 7368 6f75   The method shou
+000055c0: 6c64 2062 6520 6361 6c6c 6162 6c65 2c20  ld be callable, 
+000055d0: 616e 6420 6974 2073 686f 756c 6420 6861  and it should ha
+000055e0: 7665 2061 7420 6c65 6173 7420 6f6e 6520  ve at least one 
+000055f0: 6172 6775 6d65 6e74 0a20 2023 2072 6570  argument.  # rep
+00005600: 7265 7365 6e74 696e 6720 7468 6520 636c  resenting the cl
+00005610: 6173 7320 7468 6174 2069 7320 7061 7373  ass that is pass
+00005620: 6564 2069 6e2e 0a20 2069 6620 280a 2020  ed in..  if (.  
+00005630: 2020 6e6f 7420 6361 6c6c 6162 6c65 286d    not callable(m
+00005640: 6574 686f 645f 6f72 5f66 6e29 0a20 2020  ethod_or_fn).   
+00005650: 206f 7220 6c65 6e28 696e 7370 6563 742e   or len(inspect.
+00005660: 7369 676e 6174 7572 6528 6d65 7468 6f64  signature(method
+00005670: 5f6f 725f 666e 292e 7061 7261 6d65 7465  _or_fn).paramete
+00005680: 7273 2920 3c20 310a 2020 293a 0a20 2020  rs) < 1.  ):.   
+00005690: 2072 6169 7365 2065 7272 6f72 732e 4170   raise errors.Ap
+000056a0: 706c 794d 6f64 756c 6549 6e76 616c 6964  plyModuleInvalid
+000056b0: 4d65 7468 6f64 4572 726f 7228 6d65 7468  MethodError(meth
+000056c0: 6f64 5f6f 725f 666e 290a 0a20 2072 6574  od_or_fn)..  ret
+000056d0: 7572 6e20 6d65 7468 6f64 5f6f 725f 666e  urn method_or_fn
+000056e0: 0a0a 0a64 6566 205f 6d61 705f 7375 626d  ...def _map_subm
+000056f0: 6f64 756c 6573 2866 6e3a 2043 616c 6c61  odules(fn: Calla
+00005700: 626c 655b 5b27 4d6f 6475 6c65 275d 2c20  ble[['Module'], 
+00005710: 416e 795d 2c20 7472 6565 293a 0a20 2022  Any], tree):.  "
+00005720: 2222 4d61 7020 6120 6675 6e63 7469 6f6e  ""Map a function
+00005730: 206f 7665 7220 616c 6c20 7375 626d 6f64   over all submod
+00005740: 756c 6573 2069 6e20 6120 7472 6565 2e22  ules in a tree."
+00005750: 2222 0a20 2067 203d 206c 616d 6264 6120  "".  g = lambda 
+00005760: 5f2c 2078 3a20 666e 2878 2920 6966 2069  _, x: fn(x) if i
+00005770: 7369 6e73 7461 6e63 6528 782c 204d 6f64  sinstance(x, Mod
+00005780: 756c 6529 2065 6c73 6520 780a 2020 7265  ule) else x.  re
+00005790: 7475 726e 205f 6672 6565 7a65 5f61 7474  turn _freeze_att
+000057a0: 7228 5f6d 6170 5f6f 7665 725f 6d6f 6475  r(_map_over_modu
+000057b0: 6c65 735f 696e 5f74 7265 6528 672c 2074  les_in_tree(g, t
+000057c0: 7265 6529 290a 0a0a 636c 6173 7320 5365  ree))...class Se
+000057d0: 7475 7053 7461 7465 2865 6e75 6d2e 496e  tupState(enum.In
+000057e0: 7445 6e75 6d29 3a0a 2020 2320 7365 7475  tEnum):.  # setu
+000057f0: 7028 2920 6861 7320 6e6f 7420 6265 656e  p() has not been
+00005800: 2063 616c 6c65 642e 0a20 204e 4557 203d   called..  NEW =
+00005810: 2030 0a20 2023 2073 6574 7570 2829 2068   0.  # setup() h
+00005820: 6173 2062 6565 6e20 6361 6c6c 6564 206f  as been called o
+00005830: 7574 7369 6465 2061 2074 7261 6e73 666f  utside a transfo
+00005840: 726d 2062 6f75 6e64 6172 792e 0a20 2054  rm boundary..  T
+00005850: 5241 4e53 464f 524d 4544 203d 2031 0a20  RANSFORMED = 1. 
+00005860: 2023 2073 6574 7570 2829 2068 6173 2062   # setup() has b
+00005870: 6565 6e20 6361 6c6c 6564 2e0a 2020 444f  een called..  DO
+00005880: 4e45 203d 2032 0a0a 0a40 6461 7461 636c  NE = 2...@datacl
+00005890: 6173 7365 732e 6461 7461 636c 6173 730a  asses.dataclass.
+000058a0: 636c 6173 7320 5f4d 6f64 756c 6549 6e74  class _ModuleInt
+000058b0: 6572 6e61 6c53 7461 7465 3a0a 2020 2222  ernalState:.  ""
+000058c0: 2245 7068 656d 6572 616c 204d 6f64 756c  "Ephemeral Modul
+000058d0: 6520 4576 616c 7561 7469 6f6e 2053 7461  e Evaluation Sta
+000058e0: 7465 2e0a 0a20 2046 6f72 2063 6c61 7269  te...  For clari
+000058f0: 7479 2c20 7765 2063 6f6c 6c65 6374 2061  ty, we collect a
+00005900: 6c6c 206f 6620 7468 6520 7465 6d70 6f72  ll of the tempor
+00005910: 6172 7920 666c 6167 7320 616e 6420 6570  ary flags and ep
+00005920: 6865 6d65 7261 6c20 7374 6174 6520 7573  hemeral state us
+00005930: 6564 2062 790a 2020 4d6f 6475 6c65 7320  ed by.  Modules 
+00005940: 666f 7220 6175 746f 6e61 6d69 6e67 2061  for autonaming a
+00005950: 6e64 2065 7272 6f72 206d 6573 7361 6765  nd error message
+00005960: 7320 6865 7265 2c20 616c 6f6e 6773 6964  s here, alongsid
+00005970: 6520 7468 6520 7275 6c65 7320 7573 6564  e the rules used
+00005980: 0a20 2074 6f20 7061 7373 2074 6869 7320  .  to pass this 
+00005990: 6570 6865 6d65 7261 6c20 7374 6174 6520  ephemeral state 
+000059a0: 6163 726f 7373 2074 7261 6e73 666f 726d  across transform
+000059b0: 2062 6f75 6e64 6172 6965 732e 0a20 2022   boundaries..  "
+000059c0: 2222 0a0a 2020 696e 5f63 6f6d 7061 6374  ""..  in_compact
+000059d0: 5f6d 6574 686f 643a 2062 6f6f 6c20 3d20  _method: bool = 
+000059e0: 4661 6c73 650a 2020 696e 5f73 6574 7570  False.  in_setup
+000059f0: 3a20 626f 6f6c 203d 2046 616c 7365 0a20  : bool = False. 
+00005a00: 2073 6574 7570 5f63 616c 6c65 643a 2053   setup_called: S
+00005a10: 6574 7570 5374 6174 6520 3d20 5365 7475  etupState = Setu
+00005a20: 7053 7461 7465 2e4e 4557 0a20 2069 735f  pState.NEW.  is_
+00005a30: 696e 6974 6961 6c69 7a65 643a 2062 6f6f  initialized: boo
+00005a40: 6c20 3d20 4661 6c73 650a 2020 6175 746f  l = False.  auto
+00005a50: 6e61 6d65 5f63 7572 736f 723a 2044 6963  name_cursor: Dic
+00005a60: 745b 7374 722c 2069 6e74 5d20 3d20 6461  t[str, int] = da
+00005a70: 7461 636c 6173 7365 732e 6669 656c 6428  taclasses.field(
+00005a80: 6465 6661 756c 745f 6661 6374 6f72 793d  default_factory=
+00005a90: 6469 6374 290a 2020 6368 696c 6472 656e  dict).  children
+00005aa0: 3a20 4469 6374 5b73 7472 2c20 556e 696f  : Dict[str, Unio
+00005ab0: 6e5b 7374 722c 2027 4d6f 6475 6c65 275d  n[str, 'Module']
+00005ac0: 5d20 3d20 6461 7461 636c 6173 7365 732e  ] = dataclasses.
+00005ad0: 6669 656c 6428 0a20 2020 2064 6566 6175  field(.    defau
+00005ae0: 6c74 5f66 6163 746f 7279 3d64 6963 740a  lt_factory=dict.
+00005af0: 2020 290a 0a20 2064 6566 2072 6573 6574    )..  def reset
+00005b00: 2873 656c 6629 202d 3e20 4e6f 6e65 3a0a  (self) -> None:.
+00005b10: 2020 2020 2222 2252 6573 6574 7320 7472      """Resets tr
+00005b20: 616e 7369 656e 7420 7374 6174 652e 0a0a  ansient state...
+00005b30: 2020 2020 5468 6973 2066 756e 6374 696f      This functio
+00005b40: 6e20 6973 2063 616c 6c65 6420 6166 7465  n is called afte
+00005b50: 7220 6561 6368 206d 6f64 756c 6520 6d65  r each module me
+00005b60: 7468 6f64 2c20 736f 206f 6e6c 7920 6174  thod, so only at
+00005b70: 7472 6962 7574 6573 2074 6861 740a 2020  tributes that.  
+00005b80: 2020 6172 6520 6d65 7468 6f64 2d64 6570    are method-dep
+00005b90: 656e 6465 6e74 2061 7265 2072 6573 6574  endent are reset
+00005ba0: 2e0a 2020 2020 2222 220a 2020 2020 7365  ..    """.    se
+00005bb0: 6c66 2e69 6e5f 636f 6d70 6163 745f 6d65  lf.in_compact_me
+00005bc0: 7468 6f64 203d 2046 616c 7365 0a20 2020  thod = False.   
+00005bd0: 2073 656c 662e 696e 5f73 6574 7570 203d   self.in_setup =
+00005be0: 2046 616c 7365 0a20 2020 2073 656c 662e   False.    self.
+00005bf0: 6175 746f 6e61 6d65 5f63 7572 736f 7220  autoname_cursor 
+00005c00: 3d20 6469 6374 2829 0a0a 2020 6465 6620  = dict()..  def 
+00005c10: 6578 706f 7274 2873 656c 6629 202d 3e20  export(self) -> 
+00005c20: 275f 4d6f 6475 6c65 496e 7465 726e 616c  '_ModuleInternal
+00005c30: 5374 6174 6527 3a0a 2020 2020 2222 2245  State':.    """E
+00005c40: 7870 6f72 7473 2074 7261 6e73 666f 726d  xports transform
+00005c50: 2d70 7265 7365 7276 6564 2073 7461 7465  -preserved state
+00005c60: 2061 6372 6f73 7320 7472 616e 7366 6f72   across transfor
+00005c70: 6d20 626f 756e 6461 7279 2e22 2222 0a20  m boundary.""". 
+00005c80: 2020 2073 6574 7570 5f73 7461 7465 203d     setup_state =
+00005c90: 2028 0a20 2020 2020 2053 6574 7570 5374   (.      SetupSt
+00005ca0: 6174 652e 5452 414e 5346 4f52 4d45 4420  ate.TRANSFORMED 
+00005cb0: 6966 2073 656c 662e 7365 7475 705f 6361  if self.setup_ca
+00005cc0: 6c6c 6564 2065 6c73 6520 5365 7475 7053  lled else SetupS
+00005cd0: 7461 7465 2e4e 4557 0a20 2020 2029 0a20  tate.NEW.    ). 
+00005ce0: 2020 2063 6c6f 6e65 6420 3d20 5f4d 6f64     cloned = _Mod
+00005cf0: 756c 6549 6e74 6572 6e61 6c53 7461 7465  uleInternalState
+00005d00: 280a 2020 2020 2020 696e 5f63 6f6d 7061  (.      in_compa
+00005d10: 6374 5f6d 6574 686f 643d 7365 6c66 2e69  ct_method=self.i
+00005d20: 6e5f 636f 6d70 6163 745f 6d65 7468 6f64  n_compact_method
+00005d30: 2c0a 2020 2020 2020 696e 5f73 6574 7570  ,.      in_setup
+00005d40: 3d73 656c 662e 696e 5f73 6574 7570 2c0a  =self.in_setup,.
+00005d50: 2020 2020 2020 7365 7475 705f 6361 6c6c        setup_call
+00005d60: 6564 3d73 6574 7570 5f73 7461 7465 2c0a  ed=setup_state,.
+00005d70: 2020 2020 2020 6973 5f69 6e69 7469 616c        is_initial
+00005d80: 697a 6564 3d73 656c 662e 6973 5f69 6e69  ized=self.is_ini
+00005d90: 7469 616c 697a 6564 2c0a 2020 2020 2020  tialized,.      
+00005da0: 6175 746f 6e61 6d65 5f63 7572 736f 723d  autoname_cursor=
+00005db0: 6469 6374 2873 656c 662e 6175 746f 6e61  dict(self.autona
+00005dc0: 6d65 5f63 7572 736f 7229 2c0a 2020 2020  me_cursor),.    
+00005dd0: 290a 2020 2020 7265 7475 726e 2063 6c6f  ).    return clo
+00005de0: 6e65 640a 0a20 2064 6566 2072 6569 6d70  ned..  def reimp
+00005df0: 6f72 7428 7365 6c66 2c20 6f74 6865 723a  ort(self, other:
+00005e00: 2027 5f4d 6f64 756c 6549 6e74 6572 6e61   '_ModuleInterna
+00005e10: 6c53 7461 7465 2729 202d 3e20 4e6f 6e65  lState') -> None
+00005e20: 3a0a 2020 2020 2222 2252 652d 696d 706f  :.    """Re-impo
+00005e30: 7274 7320 7472 616e 7366 6f72 6d2d 7072  rts transform-pr
+00005e40: 6573 6572 7665 6420 7374 6174 6520 6672  eserved state fr
+00005e50: 6f6d 2061 6372 6f73 7320 7472 616e 7366  om across transf
+00005e60: 6f72 6d20 626f 756e 6461 7279 2e22 2222  orm boundary."""
+00005e70: 0a20 2020 2073 656c 662e 696e 5f63 6f6d  .    self.in_com
+00005e80: 7061 6374 5f6d 6574 686f 6420 3d20 6f74  pact_method = ot
+00005e90: 6865 722e 696e 5f63 6f6d 7061 6374 5f6d  her.in_compact_m
+00005ea0: 6574 686f 640a 2020 2020 7365 6c66 2e69  ethod.    self.i
+00005eb0: 6e5f 7365 7475 7020 3d20 6f74 6865 722e  n_setup = other.
+00005ec0: 696e 5f73 6574 7570 0a20 2020 2073 656c  in_setup.    sel
+00005ed0: 662e 6973 5f69 6e69 7469 616c 697a 6564  f.is_initialized
+00005ee0: 203d 206f 7468 6572 2e69 735f 696e 6974   = other.is_init
+00005ef0: 6961 6c69 7a65 640a 2020 2020 7365 6c66  ialized.    self
+00005f00: 2e61 7574 6f6e 616d 655f 6375 7273 6f72  .autoname_cursor
+00005f10: 203d 2064 6963 7428 6f74 6865 722e 6175   = dict(other.au
+00005f20: 746f 6e61 6d65 5f63 7572 736f 7229 0a0a  toname_cursor)..
+00005f30: 0a5f 756e 696e 6974 6961 6c69 7a65 645f  ._uninitialized_
+00005f40: 6d6f 6475 6c65 5f69 6e74 6572 6e61 6c5f  module_internal_
+00005f50: 7374 6174 6520 3d20 5f4d 6f64 756c 6549  state = _ModuleI
+00005f60: 6e74 6572 6e61 6c53 7461 7465 2829 0a0a  nternalState()..
+00005f70: 0a5f 554e 4445 4649 4e45 445f 434f 5059  ._UNDEFINED_COPY
+00005f80: 5f50 4943 4b4c 455f 4d45 5448 4f44 5320  _PICKLE_METHODS 
+00005f90: 3d20 280a 2020 275f 5f67 6574 7374 6174  = (.  '__getstat
+00005fa0: 655f 5f27 2c0a 2020 275f 5f73 6574 7374  e__',.  '__setst
+00005fb0: 6174 655f 5f27 2c0a 2020 275f 5f67 6574  ate__',.  '__get
+00005fc0: 6e65 7761 7267 735f 6578 5f5f 272c 0a20  newargs_ex__',. 
+00005fd0: 2027 5f5f 7265 6475 6365 5f5f 272c 0a20   '__reduce__',. 
+00005fe0: 2027 5f5f 7265 6475 6365 5f65 785f 5f27   '__reduce_ex__'
+00005ff0: 2c0a 2020 275f 5f63 6f70 795f 5f27 2c0a  ,.  '__copy__',.
+00006000: 2020 275f 5f64 6565 7063 6f70 795f 5f27    '__deepcopy__'
+00006010: 2c0a 290a 0a0a 5f63 6163 6865 733a 2027  ,.)..._caches: '
+00006020: 7765 616b 7265 662e 5765 616b 4b65 7944  weakref.WeakKeyD
+00006030: 6963 7469 6f6e 6172 795b 5363 6f70 652c  ictionary[Scope,
+00006040: 2077 6561 6b72 6566 2e57 6561 6b56 616c   weakref.WeakVal
+00006050: 7565 4469 6374 696f 6e61 7279 5b46 6c61  ueDictionary[Fla
+00006060: 7849 642c 204d 6f64 756c 655d 5d27 203d  xId, Module]]' =
+00006070: 2077 6561 6b72 6566 2e57 6561 6b4b 6579   weakref.WeakKey
+00006080: 4469 6374 696f 6e61 7279 2829 0a0a 0a74  Dictionary()...t
+00006090: 7570 6c65 5f72 6564 7563 6520 3d20 6c61  uple_reduce = la
+000060a0: 6d62 6461 2078 732c 2078 3a20 7873 202b  mbda xs, x: xs +
+000060b0: 2028 782c 290a 7475 706c 655f 696e 6974   (x,).tuple_init
+000060c0: 203d 206c 616d 6264 613a 2028 290a 0a0a   = lambda: ()...
+000060d0: 6361 7074 7572 655f 6361 6c6c 5f69 6e74  capture_call_int
+000060e0: 6572 6d65 6469 6174 6573 203d 206c 616d  ermediates = lam
+000060f0: 6264 6120 5f2c 206d 6574 686f 645f 6e61  bda _, method_na
+00006100: 6d65 3a20 6d65 7468 6f64 5f6e 616d 6520  me: method_name 
+00006110: 3d3d 2027 5f5f 6361 6c6c 5f5f 270a 0a0a  == '__call__'...
+00006120: 636c 6173 7320 5061 7265 6e74 4465 7363  class ParentDesc
+00006130: 7269 7074 6f72 3a0a 2020 2222 2257 7261  riptor:.  """Wra
+00006140: 7073 2070 6172 656e 7420 6d6f 6475 6c65  ps parent module
+00006150: 2072 6566 6572 656e 6365 7320 696e 2077   references in w
+00006160: 6561 6b20 7265 6673 2e0a 0a20 2054 6869  eak refs...  Thi
+00006170: 7320 7072 6576 656e 7473 2072 6566 6572  s prevents refer
+00006180: 656e 6365 2063 7963 6c65 7320 6672 6f6d  ence cycles from
+00006190: 2066 6f72 6d69 6e67 2076 6961 2070 6172   forming via par
+000061a0: 656e 7420 6c69 6e6b 7320 7768 6963 6820  ent links which 
+000061b0: 6361 6e20 6c65 6164 0a20 2074 6f20 6163  can lead.  to ac
+000061c0: 6369 6465 6e74 616c 204f 4f4d 7320 696e  cidental OOMs in
+000061d0: 2065 6167 6572 206d 6f64 6520 6475 6520   eager mode due 
+000061e0: 746f 2073 6c6f 7720 6761 7262 6167 6520  to slow garbage 
+000061f0: 636f 6c6c 6563 7469 6f6e 2061 7320 7765  collection as we
+00006200: 6c6c 2061 730a 2020 7370 7572 696f 7573  ll as.  spurious
+00006210: 2074 7261 6365 7220 6c65 616b 7320 6475   tracer leaks du
+00006220: 7269 6e67 206a 6974 2063 6f6d 7069 6c61  ring jit compila
+00006230: 7469 6f6e 2e0a 0a20 204e 6f74 653a 2022  tion...  Note: "
+00006240: 6465 7363 7269 7074 6f72 7322 2061 7265  descriptors" are
+00006250: 2074 6865 2075 6e64 6572 6c79 696e 6720   the underlying 
+00006260: 7079 7468 6f6e 206d 6563 6861 6e69 736d  python mechanism
+00006270: 2066 6f72 2069 6d70 6c65 6d65 6e74 696e   for implementin
+00006280: 670a 2020 6479 6e61 6d69 6320 4070 726f  g.  dynamic @pro
+00006290: 7065 7274 7920 6465 636f 7261 746f 7273  perty decorators
+000062a0: 2e20 2057 6520 6e65 6564 2074 6f20 7573  .  We need to us
+000062b0: 6520 6120 7261 7720 6465 7363 7269 7074  e a raw descript
+000062c0: 6f72 2069 6e73 7465 6164 206f 6620 7468  or instead of th
+000062d0: 650a 2020 6d6f 7265 2063 6f6d 6d6f 6e20  e.  more common 
+000062e0: 6465 636f 7261 746f 7220 696e 206f 7264  decorator in ord
+000062f0: 6572 2074 6f20 666f 7263 6520 7468 6174  er to force that
+00006300: 2074 6865 2061 7070 726f 7072 6961 7465   the appropriate
+00006310: 2067 6574 7465 722f 7365 7474 6572 0a20   getter/setter. 
+00006320: 206c 6f67 6963 2061 7070 6c69 6573 2069   logic applies i
+00006330: 6e20 7375 6263 6c61 7373 6573 2065 7665  n subclasses eve
+00006340: 6e20 6166 7465 7220 7661 7269 6f75 7320  n after various 
+00006350: 6461 7461 636c 6173 7320 7472 616e 7366  dataclass transf
+00006360: 6f72 6d73 2e0a 2020 2222 220a 0a20 2064  orms..  """..  d
+00006370: 6566 205f 5f67 6574 5f5f 2873 656c 662c  ef __get__(self,
+00006380: 206f 626a 2c20 6f62 6a74 7970 653d 4e6f   obj, objtype=No
+00006390: 6e65 293a 0a20 2020 2023 2063 6865 636b  ne):.    # check
+000063a0: 2069 6620 6f62 6a20 6973 204e 6f6e 652c   if obj is None,
+000063b0: 2068 6170 7065 6e73 2064 7572 696e 6720   happens during 
+000063c0: 2561 7574 6f72 656c 6f61 640a 2020 2020  %autoreload.    
+000063d0: 6966 206f 626a 2069 7320 4e6f 6e65 3a0a  if obj is None:.
+000063e0: 2020 2020 2020 7265 7475 726e 204e 6f6e        return Non
+000063f0: 650a 2020 2020 7061 7265 6e74 203d 206f  e.    parent = o
+00006400: 626a 6563 742e 5f5f 6765 7461 7474 7269  bject.__getattri
+00006410: 6275 7465 5f5f 286f 626a 2c20 275f 7061  bute__(obj, '_pa
+00006420: 7265 6e74 5f72 6566 2729 0a20 2020 2072  rent_ref').    r
+00006430: 6574 7572 6e20 7061 7265 6e74 2829 2069  eturn parent() i
+00006440: 6620 6973 696e 7374 616e 6365 2870 6172  f isinstance(par
+00006450: 656e 742c 2077 6561 6b72 6566 2e52 6566  ent, weakref.Ref
+00006460: 6572 656e 6365 5479 7065 2920 656c 7365  erenceType) else
+00006470: 2070 6172 656e 740a 0a20 2064 6566 205f   parent..  def _
+00006480: 5f73 6574 5f5f 2873 656c 662c 206f 626a  _set__(self, obj
+00006490: 2c20 7661 6c75 6529 3a0a 2020 2020 6d61  , value):.    ma
+000064a0: 7962 655f 7765 616b 203d 2077 6561 6b72  ybe_weak = weakr
+000064b0: 6566 2e72 6566 2876 616c 7565 2920 6966  ef.ref(value) if
+000064c0: 2069 7369 6e73 7461 6e63 6528 7661 6c75   isinstance(valu
+000064d0: 652c 204d 6f64 756c 6529 2065 6c73 6520  e, Module) else 
+000064e0: 7661 6c75 650a 2020 2020 6f62 6a65 6374  value.    object
+000064f0: 2e5f 5f73 6574 6174 7472 5f5f 286f 626a  .__setattr__(obj
+00006500: 2c20 275f 7061 7265 6e74 5f72 6566 272c  , '_parent_ref',
+00006510: 206d 6179 6265 5f77 6561 6b29 0a0a 0a63   maybe_weak)...c
+00006520: 6c61 7373 2044 6573 6372 6970 746f 7228  lass Descriptor(
+00006530: 7470 652e 5072 6f74 6f63 6f6c 293a 0a20  tpe.Protocol):. 
+00006540: 205f 5f69 7361 6273 7472 6163 746d 6574   __isabstractmet
+00006550: 686f 645f 5f3a 2062 6f6f 6c0a 0a20 2064  hod__: bool..  d
+00006560: 6566 205f 5f67 6574 5f5f 2873 656c 662c  ef __get__(self,
+00006570: 206f 626a 2c20 6f62 6a74 7970 653d 4e6f   obj, objtype=No
+00006580: 6e65 2920 2d3e 2041 6e79 3a0a 2020 2020  ne) -> Any:.    
+00006590: 2e2e 2e0a 0a20 2064 6566 205f 5f73 6574  .....  def __set
+000065a0: 5f5f 2873 656c 662c 206f 626a 2c20 7661  __(self, obj, va
+000065b0: 6c75 6529 202d 3e20 4e6f 6e65 3a0a 2020  lue) -> None:.  
+000065c0: 2020 2e2e 2e0a 0a20 2064 6566 205f 5f64    .....  def __d
+000065d0: 656c 6574 655f 5f28 7365 6c66 2c20 6f62  elete__(self, ob
+000065e0: 6a29 202d 3e20 4e6f 6e65 3a0a 2020 2020  j) -> None:.    
+000065f0: 2e2e 2e0a 0a20 2064 6566 205f 5f73 6574  .....  def __set
+00006600: 5f6e 616d 655f 5f28 7365 6c66 2c20 6f77  _name__(self, ow
+00006610: 6e65 722c 206e 616d 6529 202d 3e20 4e6f  ner, name) -> No
+00006620: 6e65 3a0a 2020 2020 2e2e 2e0a 0a0a 636c  ne:.    ......cl
+00006630: 6173 7320 4465 7363 7269 7074 6f72 5772  ass DescriptorWr
+00006640: 6170 7065 723a 0a20 2070 6173 730a 0a0a  apper:.  pass...
+00006650: 6465 6620 6372 6561 7465 5f64 6573 6372  def create_descr
+00006660: 6970 746f 725f 7772 6170 7065 7228 6465  iptor_wrapper(de
+00006670: 7363 7269 7074 6f72 3a20 4465 7363 7269  scriptor: Descri
+00006680: 7074 6f72 293a 0a20 2022 2222 4372 6561  ptor):.  """Crea
+00006690: 7465 7320 6120 6465 7363 7269 7074 6f72  tes a descriptor
+000066a0: 2077 7261 7070 6572 2074 6861 7420 6361   wrapper that ca
+000066b0: 6c6c 7320 6120 6765 745f 666e 206f 6e20  lls a get_fn on 
+000066c0: 7468 6520 6465 7363 7269 7074 6f72 2e22  the descriptor."
+000066d0: 2222 0a0a 2020 636c 6173 7320 5f44 6573  ""..  class _Des
+000066e0: 6372 6970 746f 7257 7261 7070 6572 2844  criptorWrapper(D
+000066f0: 6573 6372 6970 746f 7257 7261 7070 6572  escriptorWrapper
+00006700: 293a 0a20 2020 2022 2222 4120 6465 7363  ):.    """A desc
+00006710: 7269 7074 6f72 2074 6861 7420 6361 6e20  riptor that can 
+00006720: 7772 6170 2061 6e79 2064 6573 6372 6970  wrap any descrip
+00006730: 746f 722e 2222 220a 0a20 2020 2069 6620  tor."""..    if 
+00006740: 6861 7361 7474 7228 6465 7363 7269 7074  hasattr(descript
+00006750: 6f72 2c20 275f 5f69 7361 6273 7472 6163  or, '__isabstrac
+00006760: 746d 6574 686f 645f 5f27 293a 0a20 2020  tmethod__'):.   
+00006770: 2020 205f 5f69 7361 6273 7472 6163 746d     __isabstractm
+00006780: 6574 686f 645f 5f20 3d20 6465 7363 7269  ethod__ = descri
+00006790: 7074 6f72 2e5f 5f69 7361 6273 7472 6163  ptor.__isabstrac
+000067a0: 746d 6574 686f 645f 5f0a 0a20 2020 2064  tmethod__..    d
+000067b0: 6566 205f 5f69 6e69 745f 5f28 7365 6c66  ef __init__(self
+000067c0: 2c20 7772 6170 7065 643a 2044 6573 6372  , wrapped: Descr
+000067d0: 6970 746f 7229 3a0a 2020 2020 2020 7365  iptor):.      se
+000067e0: 6c66 2e77 7261 7070 6564 203d 2077 7261  lf.wrapped = wra
+000067f0: 7070 6564 0a0a 2020 2020 2320 636f 6e64  pped..    # cond
+00006800: 6974 696f 6e61 6c6c 7920 6465 6669 6e65  itionally define
+00006810: 2064 6573 6372 6970 746f 7220 6d65 7468   descriptor meth
+00006820: 6f64 730a 2020 2020 6966 2068 6173 6174  ods.    if hasat
+00006830: 7472 2864 6573 6372 6970 746f 722c 2027  tr(descriptor, '
+00006840: 5f5f 6765 745f 5f27 293a 0a0a 2020 2020  __get__'):..    
+00006850: 2020 6465 6620 5f5f 6765 745f 5f28 7365    def __get__(se
+00006860: 6c66 2c20 2a61 7267 732c 202a 2a6b 7761  lf, *args, **kwa
+00006870: 7267 7329 3a0a 2020 2020 2020 2020 2320  rgs):.        # 
+00006880: 6865 7265 2077 6520 7769 6c6c 2063 6174  here we will cat
+00006890: 6368 2069 6e74 6572 6e61 6c20 4174 7472  ch internal Attr
+000068a0: 6962 7574 6545 7272 6f72 2061 6e64 2072  ibuteError and r
+000068b0: 652d 7261 6973 6520 6974 2061 7320 610a  e-raise it as a.
+000068c0: 2020 2020 2020 2020 2320 6d6f 7265 2069          # more i
+000068d0: 6e66 6f72 6d61 7469 7665 2061 6e64 2063  nformative and c
+000068e0: 6f72 7265 6374 2065 7272 6f72 206d 6573  orrect error mes
+000068f0: 7361 6765 2e0a 2020 2020 2020 2020 7472  sage..        tr
+00006900: 793a 0a20 2020 2020 2020 2020 2072 6574  y:.          ret
+00006910: 7572 6e20 7365 6c66 2e77 7261 7070 6564  urn self.wrapped
+00006920: 2e5f 5f67 6574 5f5f 282a 6172 6773 2c20  .__get__(*args, 
+00006930: 2a2a 6b77 6172 6773 290a 2020 2020 2020  **kwargs).      
+00006940: 2020 6578 6365 7074 2041 7474 7269 6275    except Attribu
+00006950: 7465 4572 726f 7220 6173 2065 3a0a 2020  teError as e:.  
+00006960: 2020 2020 2020 2020 7261 6973 6520 6572          raise er
+00006970: 726f 7273 2e44 6573 6372 6970 746f 7241  rors.DescriptorA
+00006980: 7474 7269 6275 7465 4572 726f 7228 2920  ttributeError() 
+00006990: 6672 6f6d 2065 0a0a 2020 2020 6966 2068  from e..    if h
+000069a0: 6173 6174 7472 2864 6573 6372 6970 746f  asattr(descripto
+000069b0: 722c 2027 5f5f 7365 745f 5f27 293a 0a0a  r, '__set__'):..
+000069c0: 2020 2020 2020 6465 6620 5f5f 7365 745f        def __set_
+000069d0: 5f28 7365 6c66 2c20 2a61 7267 732c 202a  _(self, *args, *
+000069e0: 2a6b 7761 7267 7329 3a0a 2020 2020 2020  *kwargs):.      
+000069f0: 2020 7265 7475 726e 2073 656c 662e 7772    return self.wr
+00006a00: 6170 7065 642e 5f5f 7365 745f 5f28 2a61  apped.__set__(*a
+00006a10: 7267 732c 202a 2a6b 7761 7267 7329 0a0a  rgs, **kwargs)..
+00006a20: 2020 2020 6966 2068 6173 6174 7472 2864      if hasattr(d
+00006a30: 6573 6372 6970 746f 722c 2027 5f5f 6465  escriptor, '__de
+00006a40: 6c65 7465 5f5f 2729 3a0a 0a20 2020 2020  lete__'):..     
+00006a50: 2064 6566 205f 5f64 656c 6574 655f 5f28   def __delete__(
+00006a60: 7365 6c66 2c20 2a61 7267 732c 202a 2a6b  self, *args, **k
+00006a70: 7761 7267 7329 3a0a 2020 2020 2020 2020  wargs):.        
+00006a80: 7265 7475 726e 2073 656c 662e 7772 6170  return self.wrap
+00006a90: 7065 642e 5f5f 6465 6c65 7465 5f5f 282a  ped.__delete__(*
+00006aa0: 6172 6773 2c20 2a2a 6b77 6172 6773 290a  args, **kwargs).
+00006ab0: 0a20 2020 2069 6620 6861 7361 7474 7228  .    if hasattr(
+00006ac0: 6465 7363 7269 7074 6f72 2c20 275f 5f73  descriptor, '__s
+00006ad0: 6574 5f6e 616d 655f 5f27 293a 0a0a 2020  et_name__'):..  
+00006ae0: 2020 2020 6465 6620 5f5f 7365 745f 6e61      def __set_na
+00006af0: 6d65 5f5f 2873 656c 662c 202a 6172 6773  me__(self, *args
+00006b00: 2c20 2a2a 6b77 6172 6773 293a 0a20 2020  , **kwargs):.   
+00006b10: 2020 2020 2073 656c 662e 7772 6170 7065       self.wrappe
+00006b20: 642e 5f5f 7365 745f 6e61 6d65 5f5f 282a  d.__set_name__(*
+00006b30: 6172 6773 2c20 2a2a 6b77 6172 6773 290a  args, **kwargs).
+00006b40: 0a20 2020 2064 6566 205f 5f67 6574 6174  .    def __getat
+00006b50: 7472 5f5f 2873 656c 662c 206e 616d 6529  tr__(self, name)
+00006b60: 3a0a 2020 2020 2020 6966 2027 7772 6170  :.      if 'wrap
+00006b70: 7065 6427 206e 6f74 2069 6e20 7661 7273  ped' not in vars
+00006b80: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
+00006b90: 7261 6973 6520 4174 7472 6962 7574 6545  raise AttributeE
+00006ba0: 7272 6f72 2829 0a20 2020 2020 2072 6574  rror().      ret
+00006bb0: 7572 6e20 6765 7461 7474 7228 7365 6c66  urn getattr(self
+00006bc0: 2e77 7261 7070 6564 2c20 6e61 6d65 290a  .wrapped, name).
+00006bd0: 0a20 2072 6574 7572 6e20 5f44 6573 6372  .  return _Descr
+00006be0: 6970 746f 7257 7261 7070 6572 2864 6573  iptorWrapper(des
+00006bf0: 6372 6970 746f 7229 0a0a 0a23 2042 6173  criptor)...# Bas
+00006c00: 6520 4d6f 6475 6c65 2064 6566 696e 6974  e Module definit
+00006c10: 696f 6e2e 0a23 202d 2d2d 2d2d 2d2d 2d2d  ion..# ---------
+00006c20: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00006c30: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00006c40: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00006c50: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------
+00006c60: 2d2d 2d2d 0a0a 0a64 6566 206d 6f64 756c  ----...def modul
+00006c70: 655f 6669 656c 6428 2a2c 206b 775f 6f6e  e_field(*, kw_on
+00006c80: 6c79 3a20 626f 6f6c 203d 2046 616c 7365  ly: bool = False
+00006c90: 2c20 6465 6661 756c 743a 204f 7074 696f  , default: Optio
+00006ca0: 6e61 6c5b 416e 795d 203d 202e 2e2e 2920  nal[Any] = ...) 
+00006cb0: 2d3e 2041 6e79 3a0a 2020 2e2e 2e0a 0a0a  -> Any:.  ......
+00006cc0: 2320 5468 6520 4d6f 6475 6c65 4261 7365  # The ModuleBase
+00006cd0: 2063 6c61 7373 2069 7320 6372 6561 7465   class is create
+00006ce0: 6420 6f6e 6c79 2074 6f20 6d61 6b65 2073  d only to make s
+00006cf0: 7461 7469 6320 616e 616c 797a 6572 7320  tatic analyzers 
+00006d00: 6861 7070 790a 2320 6d61 696e 6c79 2070  happy.# mainly p
+00006d10: 7974 7970 6520 616e 6420 7079 7269 6768  ytype and pyrigh
+00006d20: 742e 2053 6f6d 6520 6e6f 7465 733a 0a23  t. Some notes:.#
+00006d30: 202a 2070 7972 6967 6874 2028 636f 7272   * pyright (corr
+00006d40: 6563 746c 7929 2063 6f6d 706c 6169 6e73  ectly) complains
+00006d50: 2074 6861 7420 4d6f 6475 6c65 2069 7473   that Module its
+00006d60: 656c 6620 6973 206e 6f74 2061 2064 6174  elf is not a dat
+00006d70: 6163 6c61 7373 2c20 6576 656e 0a23 2020  aclass, even.#  
+00006d80: 2074 686f 7567 6820 616c 6c20 6974 7320   though all its 
+00006d90: 7375 6263 6c61 7373 6573 2061 6e64 2069  subclasses and i
+00006da0: 6e74 616e 6365 7320 4152 4520 6461 7461  ntances ARE data
+00006db0: 636c 6173 7365 732e 2042 6563 6175 7365  classes. Because
+00006dc0: 2074 6865 7265 2069 7320 6e6f 0a23 2020   there is no.#  
+00006dd0: 2077 6179 2074 6f20 616e 6e6f 7461 7465   way to annotate
+00006de0: 2074 6869 7320 696e 2061 2077 6179 2074   this in a way t
+00006df0: 6861 7420 7079 7269 6768 7420 756e 6465  hat pyright unde
+00006e00: 7273 7461 6e64 732c 2077 6520 6372 6561  rstands, we crea
+00006e10: 7465 2061 0a23 2020 204d 6f64 756c 6542  te a.#   ModuleB
+00006e20: 6173 6520 636c 6173 7320 6465 636f 7261  ase class decora
+00006e30: 7465 6420 7769 7468 2060 6461 7461 636c  ted with `datacl
+00006e40: 6173 735f 7472 616e 7366 6f72 6d60 2073  ass_transform` s
+00006e50: 7563 6820 7468 6174 2070 7972 6967 6874  uch that pyright
+00006e60: 0a23 2020 2074 6869 6e6b 7320 4d6f 6475  .#   thinks Modu
+00006e70: 6c65 2069 7320 6120 6461 7461 636c 6173  le is a dataclas
+00006e80: 7320 2869 6e20 7265 616c 6974 7920 6f6e  s (in reality on
+00006e90: 6c79 2073 7562 636c 6173 7365 7320 6172  ly subclasses ar
+00006ea0: 6520 696e 7374 616e 7469 6174 6564 0a23  e instantiated.#
+00006eb0: 2020 2073 6f20 7468 6973 2069 7320 6669     so this is fi
+00006ec0: 6e65 292e 0a23 202a 2054 6865 2060 5f5f  ne)..# * The `__
+00006ed0: 6461 7461 636c 6173 735f 6669 656c 6473  dataclass_fields
+00006ee0: 5f5f 6020 6174 7472 6962 7574 6520 6973  __` attribute is
+00006ef0: 206e 6565 6465 6420 6265 6361 7573 6520   needed because 
+00006f00: 7079 7479 7065 2073 6565 6d73 2074 6f0a  pytype seems to.
+00006f10: 2320 2020 6e6f 7420 756e 6465 7273 7461  #   not understa
+00006f20: 6e64 2074 6865 2060 6461 7461 636c 6173  nd the `dataclas
+00006f30: 735f 7472 616e 7366 6f72 6d60 2064 6563  s_transform` dec
+00006f40: 6f72 6174 6f72 2c20 7468 6572 6566 6f72  orator, therefor
+00006f50: 6520 7765 206e 6565 640a 2320 2020 746f  e we need.#   to
+00006f60: 2061 6464 2074 6865 2061 7474 7269 6275   add the attribu
+00006f70: 7465 206d 616e 7561 6c6c 792e 0a23 202a  te manually..# *
+00006f80: 204f 7468 6572 2061 7474 7269 6275 7465   Other attribute
+00006f90: 7320 6172 6520 616e 6e6f 7461 7465 6420  s are annotated 
+00006fa0: 666f 7220 636f 6d70 6c65 7465 6e65 7373  for completeness
+00006fb0: 2e20 4265 6361 7573 6520 7765 2061 7265  . Because we are
+00006fc0: 2075 7369 6e67 0a23 2020 2074 6865 2060   using.#   the `
+00006fd0: 6966 2074 7970 696e 672e 5459 5045 5f43  if typing.TYPE_C
+00006fe0: 4845 434b 494e 4760 2070 6174 7465 726e  HECKING` pattern
+00006ff0: 2c20 7468 6573 6520 616e 6e6f 7461 7469  , these annotati
+00007000: 6f6e 7320 6172 6520 6e6f 7420 7072 6573  ons are not pres
+00007010: 656e 740a 2320 2020 6174 2072 756e 7469  ent.#   at runti
+00007020: 6d65 2073 6f20 7468 6579 2064 6f6e 2774  me so they don't
+00007030: 2061 6666 6563 7420 7468 6520 6461 7461   affect the data
+00007040: 636c 6173 7320 6265 6861 7669 6f72 2e0a  class behavior..
+00007050: 4074 7065 2e64 6174 6163 6c61 7373 5f74  @tpe.dataclass_t
+00007060: 7261 6e73 666f 726d 2866 6965 6c64 5f73  ransform(field_s
+00007070: 7065 6369 6669 6572 733d 286d 6f64 756c  pecifiers=(modul
+00007080: 655f 6669 656c 642c 2929 2020 2320 7479  e_field,))  # ty
+00007090: 7065 3a20 6967 6e6f 7265 5b6c 6974 6572  pe: ignore[liter
+000070a0: 616c 2d72 6571 7569 7265 645d 0a63 6c61  al-required].cla
+000070b0: 7373 204d 6f64 756c 6542 6173 653a 0a20  ss ModuleBase:. 
+000070c0: 2069 6620 7479 7069 6e67 2e54 5950 455f   if typing.TYPE_
+000070d0: 4348 4543 4b49 4e47 3a0a 2020 2020 7363  CHECKING:.    sc
+000070e0: 6f70 653a 204f 7074 696f 6e61 6c5b 5363  ope: Optional[Sc
+000070f0: 6f70 655d 0a20 2020 205f 7374 6174 653a  ope].    _state:
+00007100: 205f 4d6f 6475 6c65 496e 7465 726e 616c   _ModuleInternal
+00007110: 5374 6174 650a 2020 2020 5f70 6172 656e  State.    _paren
+00007120: 745f 7265 663a 2055 6e69 6f6e 5b27 4d6f  t_ref: Union['Mo
+00007130: 6475 6c65 272c 2077 6561 6b72 6566 2e52  dule', weakref.R
+00007140: 6566 6572 656e 6365 5479 7065 5b27 4d6f  eferenceType['Mo
+00007150: 6475 6c65 275d 2c20 4e6f 6e65 5d0a 2020  dule'], None].  
+00007160: 2020 5f5f 6461 7461 636c 6173 735f 6669    __dataclass_fi
+00007170: 656c 6473 5f5f 3a20 4469 6374 5b73 7472  elds__: Dict[str
+00007180: 2c20 6461 7461 636c 6173 7365 732e 4669  , dataclasses.Fi
+00007190: 656c 645d 0a0a 0a63 6c61 7373 204d 6f64  eld]...class Mod
+000071a0: 756c 6528 4d6f 6475 6c65 4261 7365 293a  ule(ModuleBase):
+000071b0: 0a20 2022 2222 4261 7365 2063 6c61 7373  .  """Base class
+000071c0: 2066 6f72 2061 6c6c 206e 6575 7261 6c20   for all neural 
+000071d0: 6e65 7477 6f72 6b20 6d6f 6475 6c65 732e  network modules.
+000071e0: 0a0a 2020 4c61 7965 7273 2061 6e64 206d  ..  Layers and m
+000071f0: 6f64 656c 7320 7368 6f75 6c64 2073 7562  odels should sub
+00007200: 636c 6173 7320 7468 6973 2063 6c61 7373  class this class
+00007210: 2e0a 0a20 2041 6c6c 2046 6c61 7820 4d6f  ...  All Flax Mo
+00007220: 6475 6c65 7320 6172 6520 5079 7468 6f6e  dules are Python
+00007230: 2033 2e37 0a20 2060 6461 7461 636c 6173   3.7.  `dataclas
+00007240: 7365 7320 3c68 7474 7073 3a2f 2f64 6f63  ses <https://doc
+00007250: 732e 7079 7468 6f6e 2e6f 7267 2f33 2f6c  s.python.org/3/l
+00007260: 6962 7261 7279 2f64 6174 6163 6c61 7373  ibrary/dataclass
+00007270: 6573 2e68 746d 6c3e 605f 2e20 5369 6e63  es.html>`_. Sinc
+00007280: 650a 2020 6461 7461 636c 6173 7365 7320  e.  dataclasses 
+00007290: 7461 6b65 206f 7665 7220 6060 5f5f 696e  take over ``__in
+000072a0: 6974 5f5f 6060 2c20 796f 7520 7368 6f75  it__``, you shou
+000072b0: 6c64 2069 6e73 7465 6164 206f 7665 7272  ld instead overr
+000072c0: 6964 6520 3a6d 6574 683a 6073 6574 7570  ide :meth:`setup
+000072d0: 602c 0a20 2077 6869 6368 2069 7320 6175  `,.  which is au
+000072e0: 746f 6d61 7469 6361 6c6c 7920 6361 6c6c  tomatically call
+000072f0: 6564 2074 6f20 696e 6974 6961 6c69 7a65  ed to initialize
+00007300: 2074 6865 206d 6f64 756c 652e 0a0a 2020   the module...  
+00007310: 4d6f 6475 6c65 7320 6361 6e20 636f 6e74  Modules can cont
+00007320: 6169 6e20 7375 626d 6f64 756c 6573 2c20  ain submodules, 
+00007330: 616e 6420 696e 2074 6869 7320 7761 7920  and in this way 
+00007340: 6361 6e20 6265 206e 6573 7465 6420 696e  can be nested in
+00007350: 2061 2074 7265 650a 2020 7374 7275 6374   a tree.  struct
+00007360: 7572 652e 2053 7562 6d6f 6465 6c73 2063  ure. Submodels c
+00007370: 616e 2062 6520 6173 7369 676e 6564 2061  an be assigned a
+00007380: 7320 7265 6775 6c61 7220 6174 7472 6962  s regular attrib
+00007390: 7574 6573 2069 6e73 6964 6520 7468 650a  utes inside the.
+000073a0: 2020 3a6d 6574 683a 6073 6574 7570 6020    :meth:`setup` 
+000073b0: 6d65 7468 6f64 2e0a 0a20 2059 6f75 2063  method...  You c
+000073c0: 616e 2064 6566 696e 6520 6172 6269 7472  an define arbitr
+000073d0: 6172 7920 2266 6f72 7761 7264 2070 6173  ary "forward pas
+000073e0: 7322 206d 6574 686f 6473 206f 6e20 796f  s" methods on yo
+000073f0: 7572 204d 6f64 756c 6520 7375 6263 6c61  ur Module subcla
+00007400: 7373 2e0a 2020 5768 696c 6520 6e6f 206d  ss..  While no m
+00007410: 6574 686f 6473 2061 7265 2073 7065 6369  ethods are speci
+00007420: 616c 2d63 6173 6564 2c20 6060 5f5f 6361  al-cased, ``__ca
+00007430: 6c6c 5f5f 6060 2069 7320 6120 706f 7075  ll__`` is a popu
+00007440: 6c61 7220 6368 6f69 6365 2062 6563 6175  lar choice becau
+00007450: 7365 0a20 2069 7420 616c 6c6f 7773 2079  se.  it allows y
+00007460: 6f75 2074 6f20 7573 6520 6d6f 6475 6c65  ou to use module
+00007470: 2069 6e73 7461 6e63 6573 2061 7320 6966   instances as if
+00007480: 2074 6865 7920 6172 6520 6675 6e63 7469   they are functi
+00007490: 6f6e 733a 3a0a 0a20 2020 203e 3e3e 2066  ons::..    >>> f
+000074a0: 726f 6d20 666c 6178 2069 6d70 6f72 7420  rom flax import 
+000074b0: 6c69 6e65 6e20 6173 206e 6e0a 2020 2020  linen as nn.    
+000074c0: 3e3e 3e20 6672 6f6d 2074 7970 696e 6720  >>> from typing 
+000074d0: 696d 706f 7274 2054 7570 6c65 0a0a 2020  import Tuple..  
+000074e0: 2020 3e3e 3e20 636c 6173 7320 4d6f 6475    >>> class Modu
+000074f0: 6c65 286e 6e2e 4d6f 6475 6c65 293a 0a20  le(nn.Module):. 
+00007500: 2020 202e 2e2e 2020 2066 6561 7475 7265     ...   feature
+00007510: 733a 2054 7570 6c65 5b69 6e74 2c20 2e2e  s: Tuple[int, ..
+00007520: 2e5d 203d 2028 3136 2c20 3429 0a0a 2020  .] = (16, 4)..  
+00007530: 2020 2e2e 2e20 2020 6465 6620 7365 7475    ...   def setu
+00007540: 7028 7365 6c66 293a 0a20 2020 202e 2e2e  p(self):.    ...
+00007550: 2020 2020 2073 656c 662e 6465 6e73 6531       self.dense1
+00007560: 203d 206e 6e2e 4465 6e73 6528 7365 6c66   = nn.Dense(self
+00007570: 2e66 6561 7475 7265 735b 305d 290a 2020  .features[0]).  
+00007580: 2020 2e2e 2e20 2020 2020 7365 6c66 2e64    ...     self.d
+00007590: 656e 7365 3220 3d20 6e6e 2e44 656e 7365  ense2 = nn.Dense
+000075a0: 2873 656c 662e 6665 6174 7572 6573 5b31  (self.features[1
+000075b0: 5d29 0a0a 2020 2020 2e2e 2e20 2020 6465  ])..    ...   de
+000075c0: 6620 5f5f 6361 6c6c 5f5f 2873 656c 662c  f __call__(self,
+000075d0: 2078 293a 0a20 2020 202e 2e2e 2020 2020   x):.    ...    
+000075e0: 2072 6574 7572 6e20 7365 6c66 2e64 656e   return self.den
+000075f0: 7365 3228 6e6e 2e72 656c 7528 7365 6c66  se2(nn.relu(self
+00007600: 2e64 656e 7365 3128 7829 2929 0a0a 2020  .dense1(x)))..  
+00007610: 4f70 7469 6f6e 616c 6c79 2c20 666f 7220  Optionally, for 
+00007620: 6d6f 7265 2063 6f6e 6369 7365 206d 6f64  more concise mod
+00007630: 756c 6520 696d 706c 656d 656e 7461 7469  ule implementati
+00007640: 6f6e 7320 7768 6572 6520 7375 626d 6f64  ons where submod
+00007650: 756c 6573 0a20 2064 6566 696e 6974 696f  ules.  definitio
+00007660: 6e73 2061 7265 2063 6f2d 6c6f 6361 7465  ns are co-locate
+00007670: 6420 7769 7468 2074 6865 6972 2075 7361  d with their usa
+00007680: 6765 2c20 796f 7520 6361 6e20 7573 6520  ge, you can use 
+00007690: 7468 650a 2020 3a6d 6574 683a 6063 6f6d  the.  :meth:`com
+000076a0: 7061 6374 6020 7772 6170 7065 722e 0a20  pact` wrapper.. 
+000076b0: 2022 2222 0a0a 2020 6966 2074 7970 696e   """..  if typin
+000076c0: 672e 5459 5045 5f43 4845 434b 494e 473a  g.TYPE_CHECKING:
+000076d0: 0a20 2020 206e 616d 653a 204f 7074 696f  .    name: Optio
+000076e0: 6e61 6c5b 7374 725d 203d 206d 6f64 756c  nal[str] = modul
+000076f0: 655f 6669 656c 6428 6b77 5f6f 6e6c 793d  e_field(kw_only=
+00007700: 5472 7565 2c20 6465 6661 756c 743d 4e6f  True, default=No
+00007710: 6e65 290a 2020 2020 7061 7265 6e74 3a20  ne).    parent: 
+00007720: 556e 696f 6e5b 274d 6f64 756c 6527 2c20  Union['Module', 
+00007730: 5f53 656e 7469 6e65 6c2c 204e 6f6e 655d  _Sentinel, None]
+00007740: 203d 206d 6f64 756c 655f 6669 656c 6428   = module_field(
+00007750: 0a20 2020 2020 206b 775f 6f6e 6c79 3d54  .      kw_only=T
+00007760: 7275 652c 2064 6566 6175 6c74 3d4e 6f6e  rue, default=Non
+00007770: 650a 2020 2020 290a 0a20 2020 2064 6566  e.    )..    def
+00007780: 205f 5f69 6e69 745f 5f28 7365 6c66 2c20   __init__(self, 
+00007790: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
+000077a0: 3a0a 2020 2020 2020 2320 7468 6973 2073  :.      # this s
+000077b0: 7475 6220 6d61 6b65 7320 7375 7265 2070  tub makes sure p
+000077c0: 7974 7970 6520 6163 6365 7074 7320 636f  ytype accepts co
+000077d0: 6e73 7472 7563 746f 7220 6172 6775 6d65  nstructor argume
+000077e0: 6e74 732e 0a20 2020 2020 2070 6173 730a  nts..      pass.
+000077f0: 0a20 2020 2064 6566 205f 5f63 616c 6c5f  .    def __call_
+00007800: 5f28 7365 6c66 2c20 2a61 7267 732c 202a  _(self, *args, *
+00007810: 2a6b 7761 7267 7329 202d 3e20 416e 793a  *kwargs) -> Any:
+00007820: 0a20 2020 2020 2023 2074 6869 7320 7374  .      # this st
+00007830: 7562 2061 6c6c 6f77 7320 7079 7479 7065  ub allows pytype
+00007840: 2074 6f20 6163 6365 7074 204d 6f64 756c   to accept Modul
+00007850: 6573 2061 7320 4361 6c6c 6162 6c65 732e  es as Callables.
+00007860: 0a20 2020 2020 2070 6173 730a 0a20 2040  .      pass..  @
+00007870: 636c 6173 736d 6574 686f 640a 2020 6465  classmethod.  de
+00007880: 6620 5f5f 696e 6974 5f73 7562 636c 6173  f __init_subclas
+00007890: 735f 5f28 636c 732c 206b 775f 6f6e 6c79  s__(cls, kw_only
+000078a0: 3a20 626f 6f6c 203d 2046 616c 7365 2c20  : bool = False, 
+000078b0: 2a2a 6b77 6172 6773 3a20 416e 7929 202d  **kwargs: Any) -
+000078c0: 3e20 4e6f 6e65 3a0a 2020 2020 2222 2241  > None:.    """A
+000078d0: 7574 6f6d 6174 6963 616c 6c79 2069 6e69  utomatically ini
+000078e0: 7469 616c 697a 6573 2061 6c6c 2073 7562  tializes all sub
+000078f0: 636c 6173 7365 7320 6173 2063 7573 746f  classes as custo
+00007900: 6d20 6461 7461 636c 6173 7365 732e 2222  m dataclasses.""
+00007910: 220a 2020 2020 7375 7065 7228 292e 5f5f  ".    super().__
+00007920: 696e 6974 5f73 7562 636c 6173 735f 5f28  init_subclass__(
+00007930: 2a2a 6b77 6172 6773 290a 2020 2020 2320  **kwargs).    # 
+00007940: 416c 6c20 466c 6178 204d 6f64 756c 6573  All Flax Modules
+00007950: 2061 7265 2064 6174 6163 6c61 7373 6573   are dataclasses
+00007960: 2e20 2057 6520 666f 7263 6520 7468 6973  .  We force this
+00007970: 2063 6f6e 7665 6e74 696f 6e20 7369 6e63   convention sinc
+00007980: 650a 2020 2020 2320 6974 2065 6e63 6f75  e.    # it encou
+00007990: 7261 6765 7320 7468 6520 7374 6174 656c  rages the statel
+000079a0: 6573 7320 6265 6861 7669 6f72 206e 6565  ess behavior nee
+000079b0: 6465 6420 746f 2063 6c6f 6e65 206d 6f64  ded to clone mod
+000079c0: 756c 6520 696e 7374 616e 6365 7320 666f  ule instances fo
+000079d0: 720a 2020 2020 2320 6675 6e63 7469 6f6e  r.    # function
+000079e0: 616c 2074 7261 6e73 666f 726d 6174 696f  al transformatio
+000079f0: 6e2e 2020 496e 7374 6561 6420 6f66 2075  n.  Instead of u
+00007a00: 7369 6e67 2061 2070 7974 686f 6e20 6d65  sing a python me
+00007a10: 7461 636c 6173 732c 2077 650a 2020 2020  taclass, we.    
+00007a20: 2320 6175 746f 6d61 7469 6361 6c6c 7920  # automatically 
+00007a30: 7472 616e 7366 6f72 6d20 4d6f 6475 6c65  transform Module
+00007a40: 7320 696e 746f 2064 6174 6163 6c61 7373  s into dataclass
+00007a50: 6573 2061 7420 7375 6263 6c61 7373 2063  es at subclass c
+00007a60: 7265 6174 696f 6e0a 2020 2020 2320 7469  reation.    # ti
+00007a70: 6d65 2c20 616e 6420 7765 2073 6574 2074  me, and we set t
+00007a80: 6865 206c 6173 7420 6461 7461 636c 6173  he last dataclas
+00007a90: 7320 6172 6775 6d65 6e74 7320 746f 2060  s arguments to `
+00007aa0: 7061 7265 6e74 6020 616e 6420 606e 616d  parent` and `nam
+00007ab0: 6560 2e0a 2020 2020 636c 732e 5f63 7573  e`..    cls._cus
+00007ac0: 746f 6d69 7a65 645f 6461 7461 636c 6173  tomized_dataclas
+00007ad0: 735f 7472 616e 7366 6f72 6d28 6b77 5f6f  s_transform(kw_o
+00007ae0: 6e6c 7929 0a20 2020 2023 2057 6520 7772  nly).    # We wr
+00007af0: 6170 2075 7365 722d 6465 6669 6e65 6420  ap user-defined 
+00007b00: 6d65 7468 6f64 7320 696e 636c 7564 696e  methods includin
+00007b10: 6720 7365 7475 7020 616e 6420 5f5f 6361  g setup and __ca
+00007b20: 6c6c 5f5f 2074 6f20 656e 666f 7263 650a  ll__ to enforce.
+00007b30: 2020 2020 2320 6120 6e75 6d62 6572 206f      # a number o
+00007b40: 6620 6469 6666 6572 656e 7420 6368 6563  f different chec
+00007b50: 6b73 2061 6e64 2074 6f20 7072 6f76 6964  ks and to provid
+00007b60: 6520 636c 6561 7220 6572 726f 7220 6d65  e clear error me
+00007b70: 7373 6167 6573 2e0a 2020 2020 636c 732e  ssages..    cls.
+00007b80: 5f76 6572 6966 795f 7369 6e67 6c65 5f6f  _verify_single_o
+00007b90: 725f 6e6f 5f63 6f6d 7061 6374 2829 0a20  r_no_compact(). 
+00007ba0: 2020 2063 6c73 2e5f 6669 6e64 5f63 6f6d     cls._find_com
+00007bb0: 7061 6374 5f6e 616d 655f 7363 6f70 655f  pact_name_scope_
+00007bc0: 6d65 7468 6f64 7328 290a 2020 2020 636c  methods().    cl
+00007bd0: 732e 5f77 7261 705f 6d6f 6475 6c65 5f61  s._wrap_module_a
+00007be0: 7474 7269 6275 7465 7328 290a 2020 2020  ttributes().    
+00007bf0: 2320 5365 7420 656d 7074 7920 636c 6173  # Set empty clas
+00007c00: 7320 6465 6661 756c 7473 2e0a 2020 2020  s defaults..    
+00007c10: 636c 732e 5f73 7461 7465 203d 205f 756e  cls._state = _un
+00007c20: 696e 6974 6961 6c69 7a65 645f 6d6f 6475  initialized_modu
+00007c30: 6c65 5f69 6e74 6572 6e61 6c5f 7374 6174  le_internal_stat
+00007c40: 6520 2023 2074 7970 653a 2069 676e 6f72  e  # type: ignor
+00007c50: 655b 6174 7472 2d64 6566 696e 6564 5d0a  e[attr-defined].
+00007c60: 2020 2020 636c 732e 7363 6f70 653a 204f      cls.scope: O
+00007c70: 7074 696f 6e61 6c5b 5363 6f70 655d 203d  ptional[Scope] =
+00007c80: 204e 6f6e 6520 2023 2074 7970 653a 2069   None  # type: i
+00007c90: 676e 6f72 650a 2020 2020 2320 4861 6e64  gnore.    # Hand
+00007ca0: 6c65 7320 7765 616b 2072 6566 6572 656e  les weak referen
+00007cb0: 6369 6e67 206f 6620 7061 7265 6e74 204d  cing of parent M
+00007cc0: 6f64 756c 6573 2074 6f20 7072 6576 656e  odules to preven
+00007cd0: 7420 7265 6665 7265 6e63 6520 6379 636c  t reference cycl
+00007ce0: 6573 2e0a 2020 2020 636c 732e 5f70 6172  es..    cls._par
+00007cf0: 656e 745f 7265 6620 3d20 4e6f 6e65 2020  ent_ref = None  
+00007d00: 2320 7479 7065 3a20 6967 6e6f 7265 5b61  # type: ignore[a
+00007d10: 7474 722d 6465 6669 6e65 645d 0a20 2020  ttr-defined].   
+00007d20: 2063 6c73 2e70 6172 656e 7420 3d20 5061   cls.parent = Pa
+00007d30: 7265 6e74 4465 7363 7269 7074 6f72 2829  rentDescriptor()
+00007d40: 2020 2320 7479 7065 3a20 6967 6e6f 7265    # type: ignore
+00007d50: 5b61 7373 6967 6e6d 656e 745d 0a0a 2020  [assignment]..  
+00007d60: 4063 6c61 7373 6d65 7468 6f64 0a20 2064  @classmethod.  d
+00007d70: 6566 205f 6375 7374 6f6d 697a 6564 5f64  ef _customized_d
+00007d80: 6174 6163 6c61 7373 5f74 7261 6e73 666f  ataclass_transfo
+00007d90: 726d 2863 6c73 2c20 6b77 5f6f 6e6c 793a  rm(cls, kw_only:
+00007da0: 2062 6f6f 6c29 3a0a 2020 2020 2222 2254   bool):.    """T
+00007db0: 7261 6e73 666f 726d 7320 6063 6c73 6020  ransforms `cls` 
+00007dc0: 696e 746f 2061 2064 6174 6163 6c61 7373  into a dataclass
+00007dd0: 2c20 7769 7468 2063 7573 746f 6d20 6164  , with custom ad
+00007de0: 6469 7469 6f6e 616c 2062 6568 6176 696f  ditional behavio
+00007df0: 722e 0a0a 2020 2020 312e 2049 6e6a 6563  r...    1. Injec
+00007e00: 7420 6070 6172 656e 7460 2061 6e64 2060  t `parent` and `
+00007e10: 6e61 6d65 6020 6669 656c 6473 2e20 2028  name` fields.  (
+00007e20: 4966 2074 6865 7920 6172 6520 616c 7265  If they are alre
+00007e30: 6164 7920 7072 6573 656e 742c 0a20 2020  ady present,.   
+00007e40: 2020 2020 7468 656e 2063 6865 636b 2074      then check t
+00007e50: 6861 7420 7468 6579 2068 6176 6520 7468  hat they have th
+00007e60: 6520 6578 7065 6374 6564 2074 7970 6573  e expected types
+00007e70: 2e29 0a20 2020 2032 2e20 5365 7420 636f  .).    2. Set co
+00007e80: 6d70 6172 652c 2068 6173 682c 2061 6e64  mpare, hash, and
+00007e90: 2072 6570 7220 746f 2046 616c 7365 2066   repr to False f
+00007ea0: 6f72 206e 6f6e 2d69 6e69 7420 6669 656c  or non-init fiel
+00007eb0: 6473 2e0a 2020 2020 332e 2047 656e 6572  ds..    3. Gener
+00007ec0: 6174 6520 6120 6861 7368 2066 756e 6374  ate a hash funct
+00007ed0: 696f 6e20 2869 6620 6e6f 7420 7072 6f76  ion (if not prov
+00007ee0: 6964 6564 2062 7920 636c 7329 2e0a 2020  ided by cls)..  
+00007ef0: 2020 2222 220a 2020 2020 2320 4368 6563    """.    # Chec
+00007f00: 6b20 7265 7365 7276 6564 2061 7474 7269  k reserved attri
+00007f10: 6275 7465 7320 6861 7665 2065 7870 6563  butes have expec
+00007f20: 7465 6420 7479 7065 2061 6e6e 6f74 6174  ted type annotat
+00007f30: 696f 6e73 2e0a 2020 2020 616e 6e6f 7461  ions..    annota
+00007f40: 7469 6f6e 7320 3d20 6469 6374 2863 6c73  tions = dict(cls
+00007f50: 2e5f 5f64 6963 745f 5f2e 6765 7428 275f  .__dict__.get('_
+00007f60: 5f61 6e6e 6f74 6174 696f 6e73 5f5f 272c  _annotations__',
+00007f70: 207b 7d29 290a 2020 2020 6966 2061 6e6e   {})).    if ann
+00007f80: 6f74 6174 696f 6e73 2e67 6574 2827 7061  otations.get('pa
+00007f90: 7265 6e74 272c 205f 5061 7265 6e74 5479  rent', _ParentTy
+00007fa0: 7065 2920 213d 205f 5061 7265 6e74 5479  pe) != _ParentTy
+00007fb0: 7065 3a0a 2020 2020 2020 7261 6973 6520  pe:.      raise 
+00007fc0: 6572 726f 7273 2e52 6573 6572 7665 644d  errors.ReservedM
+00007fd0: 6f64 756c 6541 7474 7269 6275 7465 4572  oduleAttributeEr
+00007fe0: 726f 7228 616e 6e6f 7461 7469 6f6e 7329  ror(annotations)
+00007ff0: 0a20 2020 2069 6620 616e 6e6f 7461 7469  .    if annotati
+00008000: 6f6e 732e 6765 7428 276e 616d 6527 2c20  ons.get('name', 
+00008010: 7374 7229 206e 6f74 2069 6e20 2827 7374  str) not in ('st
+00008020: 7227 2c20 7374 722c 204f 7074 696f 6e61  r', str, Optiona
+00008030: 6c5b 7374 725d 293a 0a20 2020 2020 2072  l[str]):.      r
+00008040: 6169 7365 2065 7272 6f72 732e 5265 7365  aise errors.Rese
+00008050: 7276 6564 4d6f 6475 6c65 4174 7472 6962  rvedModuleAttrib
+00008060: 7574 6545 7272 6f72 2861 6e6e 6f74 6174  uteError(annotat
+00008070: 696f 6e73 290a 0a20 2020 2023 2061 6e79  ions)..    # any
+00008080: 206e 6f6e 2d69 6e69 7420 6669 656c 6420   non-init field 
+00008090: 7769 6c6c 206f 6e6c 7920 6265 2073 6574  will only be set
+000080a0: 2069 6e20 7365 7475 700a 2020 2020 2320   in setup.    # 
+000080b0: 4475 7269 6e67 205f 5f68 6173 685f 5f20  During __hash__ 
+000080c0: 616e 6420 5f5f 6571 5f5f 2074 6865 2066  and __eq__ the f
+000080d0: 6965 6c64 2069 7320 6e6f 7420 7365 7420  ield is not set 
+000080e0: 7965 740a 2020 2020 2320 736f 2069 7420  yet.    # so it 
+000080f0: 7368 6f75 6c64 206e 6f74 2062 6520 7573  should not be us
+00008100: 6564 2069 6e20 636f 6d70 6172 652c 2068  ed in compare, h
+00008110: 6173 6820 6f72 2072 6570 722e 0a20 2020  ash or repr..   
+00008120: 2066 6f72 2066 6965 6c64 2069 6e20 616e   for field in an
+00008130: 6e6f 7461 7469 6f6e 733a 0a20 2020 2020  notations:.     
+00008140: 2066 6965 6c64 5f6d 6574 6120 3d20 6765   field_meta = ge
+00008150: 7461 7474 7228 636c 732c 2066 6965 6c64  tattr(cls, field
+00008160: 2c20 4e6f 6e65 290a 2020 2020 2020 6966  , None).      if
+00008170: 2069 7369 6e73 7461 6e63 6528 6669 656c   isinstance(fiel
+00008180: 645f 6d65 7461 2c20 6461 7461 636c 6173  d_meta, dataclas
+00008190: 7365 732e 4669 656c 6429 2061 6e64 206e  ses.Field) and n
+000081a0: 6f74 2066 6965 6c64 5f6d 6574 612e 696e  ot field_meta.in
+000081b0: 6974 3a0a 2020 2020 2020 2020 6669 656c  it:.        fiel
+000081c0: 645f 6d65 7461 2e63 6f6d 7061 7265 203d  d_meta.compare =
+000081d0: 2046 616c 7365 0a20 2020 2020 2020 2066   False.        f
+000081e0: 6965 6c64 5f6d 6574 612e 6861 7368 203d  ield_meta.hash =
+000081f0: 2046 616c 7365 0a20 2020 2020 2020 2066   False.        f
+00008200: 6965 6c64 5f6d 6574 612e 7265 7072 203d  ield_meta.repr =
+00008210: 2046 616c 7365 0a0a 2020 2020 6578 7472   False..    extr
+00008220: 615f 6669 656c 6473 203d 205b 0a20 2020  a_fields = [.   
+00008230: 2020 2028 0a20 2020 2020 2020 2027 7061     (.        'pa
+00008240: 7265 6e74 272c 0a20 2020 2020 2020 205f  rent',.        _
+00008250: 5061 7265 6e74 5479 7065 2c0a 2020 2020  ParentType,.    
+00008260: 2020 2020 6b77 5f6f 6e6c 795f 6461 7461      kw_only_data
+00008270: 636c 6173 7365 732e 6669 656c 6428 0a20  classes.field(. 
+00008280: 2020 2020 2020 2020 2072 6570 723d 4661           repr=Fa
+00008290: 6c73 652c 2064 6566 6175 6c74 3d5f 756e  lse, default=_un
+000082a0: 7370 6563 6966 6965 645f 7061 7265 6e74  specified_parent
+000082b0: 2c20 6b77 5f6f 6e6c 793d 5472 7565 0a20  , kw_only=True. 
+000082c0: 2020 2020 2020 2029 2c0a 2020 2020 2020         ),.      
+000082d0: 292c 0a20 2020 2020 2028 0a20 2020 2020  ),.      (.     
+000082e0: 2020 2027 6e61 6d65 272c 0a20 2020 2020     'name',.     
+000082f0: 2020 204f 7074 696f 6e61 6c5b 7374 725d     Optional[str]
+00008300: 2c0a 2020 2020 2020 2020 6b77 5f6f 6e6c  ,.        kw_onl
+00008310: 795f 6461 7461 636c 6173 7365 732e 6669  y_dataclasses.fi
+00008320: 656c 6428 6465 6661 756c 743d 4e6f 6e65  eld(default=None
+00008330: 2c20 6b77 5f6f 6e6c 793d 5472 7565 292c  , kw_only=True),
+00008340: 0a20 2020 2020 2029 2c0a 2020 2020 5d0a  .      ),.    ].
+00008350: 0a20 2020 2069 6620 6b77 5f6f 6e6c 793a  .    if kw_only:
+00008360: 0a20 2020 2020 2069 6620 7475 706c 6528  .      if tuple(
+00008370: 7379 732e 7665 7273 696f 6e5f 696e 666f  sys.version_info
+00008380: 295b 3a33 5d20 3e3d 2028 332c 2031 302c  )[:3] >= (3, 10,
+00008390: 2030 293a 0a20 2020 2020 2020 2066 6f72   0):.        for
+000083a0: 2028 0a20 2020 2020 2020 2020 206e 616d   (.          nam
+000083b0: 652c 0a20 2020 2020 2020 2020 2061 6e6e  e,.          ann
+000083c0: 6f74 6174 696f 6e2c 2020 2320 7079 7479  otation,  # pyty
+000083d0: 7065 3a20 6469 7361 626c 653d 696e 7661  pe: disable=inva
+000083e0: 6c69 642d 616e 6e6f 7461 7469 6f6e 0a20  lid-annotation. 
+000083f0: 2020 2020 2020 2020 2064 6566 6175 6c74           default
+00008400: 2c0a 2020 2020 2020 2020 2920 696e 2065  ,.        ) in e
+00008410: 7874 7261 5f66 6965 6c64 733a 0a20 2020  xtra_fields:.   
+00008420: 2020 2020 2020 2073 6574 6174 7472 2863         setattr(c
+00008430: 6c73 2c20 6e61 6d65 2c20 6465 6661 756c  ls, name, defaul
+00008440: 7429 0a20 2020 2020 2020 2020 2063 6c73  t).          cls
+00008450: 2e5f 5f61 6e6e 6f74 6174 696f 6e73 5f5f  .__annotations__
+00008460: 5b6e 616d 655d 203d 2061 6e6e 6f74 6174  [name] = annotat
+00008470: 696f 6e0a 2020 2020 2020 2020 6461 7461  ion.        data
+00008480: 636c 6173 7365 732e 6461 7461 636c 6173  classes.dataclas
+00008490: 7328 2020 2320 7479 7065 3a20 6967 6e6f  s(  # type: igno
+000084a0: 7265 5b63 616c 6c2d 6f76 6572 6c6f 6164  re[call-overload
+000084b0: 5d0a 2020 2020 2020 2020 2020 756e 7361  ].          unsa
+000084c0: 6665 5f68 6173 683d 275f 5f68 6173 685f  fe_hash='__hash_
+000084d0: 5f27 206e 6f74 2069 6e20 636c 732e 5f5f  _' not in cls.__
+000084e0: 6469 6374 5f5f 2c0a 2020 2020 2020 2020  dict__,.        
+000084f0: 2020 7265 7072 3d46 616c 7365 2c0a 2020    repr=False,.  
+00008500: 2020 2020 2020 2020 6b77 5f6f 6e6c 793d          kw_only=
+00008510: 5472 7565 2c0a 2020 2020 2020 2020 2928  True,.        )(
+00008520: 636c 7329 0a20 2020 2020 2065 6c73 653a  cls).      else:
+00008530: 0a20 2020 2020 2020 2072 6169 7365 2054  .        raise T
+00008540: 7970 6545 7272 6f72 2827 606b 775f 6f6e  ypeError('`kw_on
+00008550: 6c79 6020 6973 206e 6f74 2061 7661 696c  ly` is not avail
+00008560: 6162 6c65 2062 6566 6f72 6520 5079 2033  able before Py 3
+00008570: 2e31 302e 2729 0a20 2020 2065 6c73 653a  .10.').    else:
+00008580: 0a20 2020 2020 2023 204e 6f77 2061 7070  .      # Now app
+00008590: 6c79 2064 6174 6163 6c61 7373 2074 7261  ly dataclass tra
+000085a0: 6e73 666f 726d 2028 7768 6963 6820 6f70  nsform (which op
+000085b0: 6572 6174 6573 2069 6e2d 706c 6163 6529  erates in-place)
+000085c0: 2e0a 2020 2020 2020 2320 446f 2067 656e  ..      # Do gen
+000085d0: 6572 6174 6520 6120 6861 7368 2066 756e  erate a hash fun
+000085e0: 6374 696f 6e20 6f6e 6c79 2069 6620 6e6f  ction only if no
+000085f0: 7420 7072 6f76 6964 6564 2062 7920 7468  t provided by th
+00008600: 6520 636c 6173 732e 0a20 2020 2020 206b  e class..      k
+00008610: 775f 6f6e 6c79 5f64 6174 6163 6c61 7373  w_only_dataclass
+00008620: 6573 2e64 6174 6163 6c61 7373 280a 2020  es.dataclass(.  
+00008630: 2020 2020 2020 636c 732c 0a20 2020 2020        cls,.     
+00008640: 2020 2075 6e73 6166 655f 6861 7368 3d27     unsafe_hash='
+00008650: 5f5f 6861 7368 5f5f 2720 6e6f 7420 696e  __hash__' not in
+00008660: 2063 6c73 2e5f 5f64 6963 745f 5f2c 0a20   cls.__dict__,. 
+00008670: 2020 2020 2020 2072 6570 723d 4661 6c73         repr=Fals
+00008680: 652c 0a20 2020 2020 2020 2065 7874 7261  e,.        extra
+00008690: 5f66 6965 6c64 733d 6578 7472 615f 6669  _fields=extra_fi
+000086a0: 656c 6473 2c0a 2020 2020 2020 2920 2023  elds,.      )  #
+000086b0: 2070 7974 7970 653a 2064 6973 6162 6c65   pytype: disable
+000086c0: 3d77 726f 6e67 2d6b 6579 776f 7264 2d61  =wrong-keyword-a
+000086d0: 7267 730a 0a20 2020 2063 6c73 2e5f 5f68  rgs..    cls.__h
+000086e0: 6173 685f 5f20 3d20 5f77 7261 705f 6861  ash__ = _wrap_ha
+000086f0: 7368 2863 6c73 2e5f 5f68 6173 685f 5f29  sh(cls.__hash__)
+00008700: 2020 2320 7479 7065 3a20 6967 6e6f 7265    # type: ignore
+00008710: 5b6d 6574 686f 642d 6173 7369 676e 5d0a  [method-assign].
+00008720: 0a20 2040 636c 6173 736d 6574 686f 640a  .  @classmethod.
+00008730: 2020 6465 6620 5f76 6572 6966 795f 7369    def _verify_si
+00008740: 6e67 6c65 5f6f 725f 6e6f 5f63 6f6d 7061  ngle_or_no_compa
+00008750: 6374 2863 6c73 293a 0a20 2020 2022 2222  ct(cls):.    """
+00008760: 5374 6174 6963 616c 6c79 2076 6572 6966  Statically verif
+00008770: 6965 7320 7468 6174 2061 7420 6d6f 7374  ies that at most
+00008780: 2061 2073 696e 676c 6520 6d65 7468 6f64   a single method
+00008790: 2069 7320 6c61 6265 6c6c 6564 2063 6f6d   is labelled com
+000087a0: 7061 6374 2e22 2222 0a20 2020 206d 6574  pact.""".    met
+000087b0: 686f 6473 203d 205b 6d5b 305d 2066 6f72  hods = [m[0] for
+000087c0: 206d 2069 6e20 696e 7370 6563 742e 6765   m in inspect.ge
+000087d0: 746d 656d 6265 7273 2863 6c73 2c20 7072  tmembers(cls, pr
+000087e0: 6564 6963 6174 653d 6361 6c6c 6162 6c65  edicate=callable
+000087f0: 295d 0a20 2020 206e 5f63 6f6d 7061 6374  )].    n_compact
+00008800: 5f66 6e73 203d 206c 656e 280a 2020 2020  _fns = len(.    
+00008810: 2020 5b0a 2020 2020 2020 2020 6d65 7468    [.        meth
+00008820: 6f64 5f6e 616d 650a 2020 2020 2020 2020  od_name.        
+00008830: 666f 7220 6d65 7468 6f64 5f6e 616d 6520  for method_name 
+00008840: 696e 206d 6574 686f 6473 0a20 2020 2020  in methods.     
+00008850: 2020 2069 6620 6861 7361 7474 7228 6765     if hasattr(ge
+00008860: 7461 7474 7228 636c 732c 206d 6574 686f  tattr(cls, metho
+00008870: 645f 6e61 6d65 292c 2027 636f 6d70 6163  d_name), 'compac
+00008880: 7427 290a 2020 2020 2020 5d0a 2020 2020  t').      ].    
+00008890: 290a 2020 2020 6966 206e 5f63 6f6d 7061  ).    if n_compa
+000088a0: 6374 5f66 6e73 203e 2031 3a0a 2020 2020  ct_fns > 1:.    
+000088b0: 2020 7261 6973 6520 6572 726f 7273 2e4d    raise errors.M
+000088c0: 756c 7469 706c 654d 6574 686f 6473 436f  ultipleMethodsCo
+000088d0: 6d70 6163 7445 7272 6f72 2829 0a0a 2020  mpactError()..  
+000088e0: 4063 6c61 7373 6d65 7468 6f64 0a20 2064  @classmethod.  d
+000088f0: 6566 205f 6669 6e64 5f63 6f6d 7061 6374  ef _find_compact
+00008900: 5f6e 616d 655f 7363 6f70 655f 6d65 7468  _name_scope_meth
+00008910: 6f64 7328 636c 7329 3a0a 2020 2020 2222  ods(cls):.    ""
+00008920: 2246 696e 6473 2061 6c6c 2063 6f6d 7061  "Finds all compa
+00008930: 6374 5f6e 616d 655f 7363 6f70 6520 6d65  ct_name_scope me
+00008940: 7468 6f64 7320 696e 2074 6865 2063 6c61  thods in the cla
+00008950: 7373 2e22 2222 0a20 2020 206d 6574 686f  ss.""".    metho
+00008960: 6473 203d 205b 6d5b 305d 2066 6f72 206d  ds = [m[0] for m
+00008970: 2069 6e20 696e 7370 6563 742e 6765 746d   in inspect.getm
+00008980: 656d 6265 7273 2863 6c73 2c20 7072 6564  embers(cls, pred
+00008990: 6963 6174 653d 6361 6c6c 6162 6c65 295d  icate=callable)]
+000089a0: 0a20 2020 2063 6f6d 7061 6374 5f6e 616d  .    compact_nam
+000089b0: 655f 7363 6f70 655f 666e 7320 3d20 7475  e_scope_fns = tu
+000089c0: 706c 6528 0a20 2020 2020 206d 6574 686f  ple(.      metho
+000089d0: 645f 6e61 6d65 0a20 2020 2020 2066 6f72  d_name.      for
+000089e0: 206d 6574 686f 645f 6e61 6d65 2069 6e20   method_name in 
+000089f0: 6d65 7468 6f64 730a 2020 2020 2020 6966  methods.      if
+00008a00: 2068 6173 6174 7472 2867 6574 6174 7472   hasattr(getattr
+00008a10: 2863 6c73 2c20 6d65 7468 6f64 5f6e 616d  (cls, method_nam
+00008a20: 6529 2c20 2763 6f6d 7061 6374 5f6e 616d  e), 'compact_nam
+00008a30: 655f 7363 6f70 6527 290a 2020 2020 290a  e_scope').    ).
+00008a40: 2020 2020 636c 732e 5f63 6f6d 7061 6374      cls._compact
+00008a50: 5f6e 616d 655f 7363 6f70 655f 6d65 7468  _name_scope_meth
+00008a60: 6f64 7320 3d20 636f 6d70 6163 745f 6e61  ods = compact_na
+00008a70: 6d65 5f73 636f 7065 5f66 6e73 0a0a 2020  me_scope_fns..  
+00008a80: 4063 6c61 7373 6d65 7468 6f64 0a20 2064  @classmethod.  d
+00008a90: 6566 205f 7772 6170 5f6d 6f64 756c 655f  ef _wrap_module_
+00008aa0: 6174 7472 6962 7574 6573 2863 6c73 293a  attributes(cls):
+00008ab0: 0a20 2020 2022 2222 5772 6170 7320 7573  .    """Wraps us
+00008ac0: 6572 2d64 6566 696e 6564 206e 6f6e 2d69  er-defined non-i
+00008ad0: 6e68 6572 6974 6564 206d 6574 686f 6473  nherited methods
+00008ae0: 2061 6e64 2064 6573 6372 6970 746f 7273   and descriptors
+00008af0: 2077 6974 6820 7374 6174 650a 0a20 2020   with state..   
+00008b00: 206d 616e 6167 656d 656e 7420 6675 6e63   management func
+00008b10: 7469 6f6e 732e 0a20 2020 2022 2222 0a20  tions..    """. 
+00008b20: 2020 2023 2077 7261 7020 6d65 7468 6f64     # wrap method
+00008b30: 730a 2020 2020 6d65 7468 6f64 5f65 7863  s.    method_exc
+00008b40: 6c75 7369 6f6e 7320 3d20 5b66 2e6e 616d  lusions = [f.nam
+00008b50: 6520 666f 7220 6620 696e 2064 6174 6163  e for f in datac
+00008b60: 6c61 7373 6573 2e66 6965 6c64 7328 636c  lasses.fields(cl
+00008b70: 7329 5d20 2b20 5b0a 2020 2020 2020 275f  s)] + [.      '_
+00008b80: 5f65 715f 5f27 2c0a 2020 2020 2020 275f  _eq__',.      '_
+00008b90: 5f72 6570 725f 5f27 2c0a 2020 2020 2020  _repr__',.      
+00008ba0: 275f 5f69 6e69 745f 5f27 2c0a 2020 2020  '__init__',.    
+00008bb0: 2020 275f 5f68 6173 685f 5f27 2c0a 2020    '__hash__',.  
+00008bc0: 2020 2020 275f 5f70 6f73 745f 696e 6974      '__post_init
+00008bd0: 5f5f 272c 0a20 2020 205d 0a20 2020 2066  __',.    ].    f
+00008be0: 6f72 206b 6579 2069 6e20 5f67 6574 5f6c  or key in _get_l
+00008bf0: 6f63 616c 5f6d 6574 686f 645f 6e61 6d65  ocal_method_name
+00008c00: 7328 636c 732c 2065 7863 6c75 6465 3d6d  s(cls, exclude=m
+00008c10: 6574 686f 645f 6578 636c 7573 696f 6e73  ethod_exclusions
+00008c20: 293a 0a20 2020 2020 206d 6574 686f 6420  ):.      method 
+00008c30: 3d20 6765 7461 7474 7228 636c 732c 206b  = getattr(cls, k
+00008c40: 6579 290a 2020 2020 2020 6966 2068 6173  ey).      if has
+00008c50: 6174 7472 286d 6574 686f 642c 2027 6e6f  attr(method, 'no
+00008c60: 7772 6170 2729 3a0a 2020 2020 2020 2020  wrap'):.        
+00008c70: 636f 6e74 696e 7565 0a20 2020 2020 2073  continue.      s
+00008c80: 6574 6174 7472 2863 6c73 2c20 6b65 792c  etattr(cls, key,
+00008c90: 2077 7261 705f 6d65 7468 6f64 5f6f 6e63   wrap_method_onc
+00008ca0: 6528 6d65 7468 6f64 2929 0a0a 2020 2020  e(method))..    
+00008cb0: 2320 7772 6170 2064 6573 6372 6970 746f  # wrap descripto
+00008cc0: 7273 0a20 2020 2064 6573 6372 6970 746f  rs.    descripto
+00008cd0: 725f 6578 636c 7573 696f 6e73 203d 205b  r_exclusions = [
+00008ce0: 662e 6e61 6d65 2066 6f72 2066 2069 6e20  f.name for f in 
+00008cf0: 6461 7461 636c 6173 7365 732e 6669 656c  dataclasses.fiel
+00008d00: 6473 2863 6c73 295d 202b 205b 0a20 2020  ds(cls)] + [.   
+00008d10: 2020 2027 7061 7265 6e74 272c 0a20 2020     'parent',.   
+00008d20: 2020 2027 5f5f 6469 6374 5f5f 272c 0a20     '__dict__',. 
+00008d30: 2020 205d 0a20 2020 2066 6f72 206b 6579     ].    for key
+00008d40: 2069 6e20 5f67 6574 5f6c 6f63 616c 5f64   in _get_local_d
+00008d50: 6573 6372 6970 746f 725f 6e61 6d65 7328  escriptor_names(
+00008d60: 636c 732c 2064 6573 6372 6970 746f 725f  cls, descriptor_
+00008d70: 6578 636c 7573 696f 6e73 293a 0a20 2020  exclusions):.   
+00008d80: 2020 2023 2064 6f6e 2774 2075 7365 2067     # don't use g
+00008d90: 6574 6174 7472 2068 6572 652c 2073 696e  etattr here, sin
+00008da0: 6365 2069 7420 7769 6c6c 2063 616c 6c20  ce it will call 
+00008db0: 7468 6520 6465 7363 7269 7074 6f72 0a20  the descriptor. 
+00008dc0: 2020 2020 2064 6573 6372 6970 746f 7220       descriptor 
+00008dd0: 3d20 636c 732e 5f5f 6469 6374 5f5f 5b6b  = cls.__dict__[k
+00008de0: 6579 5d0a 2020 2020 2020 6966 2068 6173  ey].      if has
+00008df0: 6174 7472 2864 6573 6372 6970 746f 722c  attr(descriptor,
+00008e00: 2027 6e6f 7772 6170 2729 3a0a 2020 2020   'nowrap'):.    
+00008e10: 2020 2020 636f 6e74 696e 7565 0a20 2020      continue.   
+00008e20: 2020 2073 6574 6174 7472 2863 6c73 2c20     setattr(cls, 
+00008e30: 6b65 792c 2077 7261 705f 6465 7363 7269  key, wrap_descri
+00008e40: 7074 6f72 5f6f 6e63 6528 6465 7363 7269  ptor_once(descri
+00008e50: 7074 6f72 2929 0a20 2020 2072 6574 7572  ptor)).    retur
+00008e60: 6e20 636c 730a 0a20 2064 6566 205f 6361  n cls..  def _ca
+00008e70: 6c6c 5f77 7261 7070 6564 5f6d 6574 686f  ll_wrapped_metho
+00008e80: 6428 7365 6c66 2c20 6675 6e2c 2061 7267  d(self, fun, arg
+00008e90: 732c 206b 7761 7267 7329 3a0a 2020 2020  s, kwargs):.    
+00008ea0: 2222 2243 616c 6c73 2061 2077 7261 7070  """Calls a wrapp
+00008eb0: 6564 206d 6574 686f 642e 0a0a 2020 2020  ed method...    
+00008ec0: 5468 6973 2066 756e 6374 696f 6e20 6973  This function is
+00008ed0: 2072 6573 706f 6e73 6962 6c65 2066 6f72   responsible for
+00008ee0: 2073 6574 7469 6e67 2075 7020 7468 6520   setting up the 
+00008ef0: 7468 7265 6164 206c 6f63 616c 2073 7461  thread local sta
+00008f00: 7465 0a20 2020 2063 6f72 7265 6374 6c79  te.    correctly
+00008f10: 2062 6566 6f72 6520 6361 6c6c 696e 6720   before calling 
+00008f20: 7468 6520 6d65 7468 6f64 2061 6e64 2063  the method and c
+00008f30: 6c65 616e 696e 6720 7570 2061 6674 6572  leaning up after
+00008f40: 7761 7264 732e 0a20 2020 2054 6869 7320  wards..    This 
+00008f50: 696e 636c 7564 6573 2073 746f 7269 6e67  includes storing
+00008f60: 2069 6e74 6572 6d65 6469 6174 6573 2c20   intermediates, 
+00008f70: 7365 7475 7020 6f66 2074 6865 2063 6f6d  setup of the com
+00008f80: 7061 6374 2073 636f 7065 2c0a 2020 2020  pact scope,.    
+00008f90: 616e 6420 6d61 6b69 6e67 2073 7572 6520  and making sure 
+00008fa0: 7365 7475 7020 6973 2063 616c 6c65 6420  setup is called 
+00008fb0: 6265 666f 7265 2061 6e79 206f 7468 6572  before any other
+00008fc0: 206d 6574 686f 642e 0a0a 2020 2020 4172   method...    Ar
+00008fd0: 6773 3a0a 2020 2020 2020 6675 6e3a 2054  gs:.      fun: T
+00008fe0: 6865 2077 7261 7070 6564 206d 6574 686f  he wrapped metho
+00008ff0: 642e 0a20 2020 2020 2061 7267 733a 204e  d..      args: N
+00009000: 616d 6564 2061 7267 756d 656e 7473 2070  amed arguments p
+00009010: 6173 7365 6420 746f 2060 6066 756e 6060  assed to ``fun``
+00009020: 2e0a 2020 2020 2020 6b77 6172 6773 3a20  ..      kwargs: 
+00009030: 4b65 7977 6f72 6420 6172 6775 6d65 6e74  Keyword argument
+00009040: 7320 7061 7373 6564 2074 6f20 6060 6675  s passed to ``fu
+00009050: 6e60 602e 0a0a 2020 2020 5265 7475 726e  n``...    Return
+00009060: 733a 0a20 2020 2020 2054 6865 2072 6573  s:.      The res
+00009070: 756c 7473 206f 6620 6361 6c6c 696e 6720  ults of calling 
+00009080: 6060 6675 6e60 602e 0a20 2020 2022 2222  ``fun``..    """
+00009090: 0a20 2020 2069 735f 636f 6d70 6163 745f  .    is_compact_
+000090a0: 6d65 7468 6f64 203d 2068 6173 6174 7472  method = hasattr
+000090b0: 2866 756e 2c20 2763 6f6d 7061 6374 2729  (fun, 'compact')
+000090c0: 0a20 2020 2066 756e 5f6e 616d 6520 3d20  .    fun_name = 
+000090d0: 5f67 6574 5f66 6e5f 6e61 6d65 2866 756e  _get_fn_name(fun
+000090e0: 290a 2020 2020 6973 5f73 6574 7570 5f6d  ).    is_setup_m
+000090f0: 6574 686f 6420 3d20 6675 6e5f 6e61 6d65  ethod = fun_name
+00009100: 203d 3d20 2773 6574 7570 270a 2020 2020   == 'setup'.    
+00009110: 6164 645f 6361 6c6c 5f69 6e66 6f20 3d20  add_call_info = 
+00009120: 6e6f 7420 6973 5f73 6574 7570 5f6d 6574  not is_setup_met
+00009130: 686f 6420 616e 6420 6c65 6e28 5f63 6f6e  hod and len(_con
+00009140: 7465 7874 2e63 616c 6c5f 696e 666f 5f73  text.call_info_s
+00009150: 7461 636b 2920 3e20 300a 2020 2020 2320  tack) > 0.    # 
+00009160: 5765 206c 617a 696c 7920 6361 6c6c 2073  We lazily call s
+00009170: 6574 7570 2829 206f 6e6c 7920 7768 656e  etup() only when
+00009180: 206e 6565 6465 642e 0a20 2020 2069 6620   needed..    if 
+00009190: 6973 5f73 6574 7570 5f6d 6574 686f 643a  is_setup_method:
+000091a0: 0a20 2020 2020 2069 6620 7365 6c66 2e73  .      if self.s
+000091b0: 636f 7065 2069 7320 4e6f 6e65 3a0a 2020  cope is None:.  
+000091c0: 2020 2020 2020 7261 6973 6520 6572 726f        raise erro
+000091d0: 7273 2e43 616c 6c53 6574 7570 556e 626f  rs.CallSetupUnbo
+000091e0: 756e 644d 6f64 756c 6545 7272 6f72 2829  undModuleError()
+000091f0: 0a20 2020 2020 2069 735f 7265 6375 7272  .      is_recurr
+00009200: 656e 7420 3d20 7365 6c66 2e5f 7374 6174  ent = self._stat
+00009210: 652e 696e 5f73 6574 7570 0a20 2020 2020  e.in_setup.     
+00009220: 2073 656c 662e 5f73 7461 7465 2e69 6e5f   self._state.in_
+00009230: 7365 7475 7020 3d20 5472 7565 0a20 2020  setup = True.   
+00009240: 2065 6c73 653a 0a20 2020 2020 2073 656c   else:.      sel
+00009250: 662e 5f74 7279 5f73 6574 7570 2829 0a0a  f._try_setup()..
+00009260: 2020 2020 6966 2069 735f 636f 6d70 6163      if is_compac
+00009270: 745f 6d65 7468 6f64 3a0a 2020 2020 2020  t_method:.      
+00009280: 6966 2073 656c 662e 7363 6f70 6520 6973  if self.scope is
+00009290: 204e 6f6e 653a 0a20 2020 2020 2020 2072   None:.        r
+000092a0: 6169 7365 2065 7272 6f72 732e 4361 6c6c  aise errors.Call
+000092b0: 436f 6d70 6163 7455 6e62 6f75 6e64 4d6f  CompactUnboundMo
+000092c0: 6475 6c65 4572 726f 7228 290a 2020 2020  duleError().    
+000092d0: 2020 6973 5f72 6563 7572 7265 6e74 203d    is_recurrent =
+000092e0: 2073 656c 662e 5f73 7461 7465 2e69 6e5f   self._state.in_
+000092f0: 636f 6d70 6163 745f 6d65 7468 6f64 0a20  compact_method. 
+00009300: 2020 2020 2073 656c 662e 5f73 7461 7465       self._state
+00009310: 2e69 6e5f 636f 6d70 6163 745f 6d65 7468  .in_compact_meth
+00009320: 6f64 203d 2054 7275 650a 2020 2020 5f63  od = True.    _c
+00009330: 6f6e 7465 7874 2e6d 6f64 756c 655f 7374  ontext.module_st
+00009340: 6163 6b2e 6170 7065 6e64 2873 656c 6629  ack.append(self)
+00009350: 0a20 2020 2074 7279 3a0a 2020 2020 2020  .    try:.      
+00009360: 2320 6765 7420 6361 6c6c 2069 6e66 6f0a  # get call info.
+00009370: 2020 2020 2020 6966 2061 6464 5f63 616c        if add_cal
+00009380: 6c5f 696e 666f 3a0a 2020 2020 2020 2020  l_info:.        
+00009390: 6173 7365 7274 2073 656c 662e 7363 6f70  assert self.scop
+000093a0: 6520 6973 206e 6f74 204e 6f6e 650a 2020  e is not None.  
+000093b0: 2020 2020 2020 6361 6c6c 5f69 6e64 6578        call_index
+000093c0: 203d 205f 636f 6e74 6578 742e 6361 6c6c   = _context.call
+000093d0: 5f69 6e66 6f5f 7374 6163 6b5b 2d31 5d2e  _info_stack[-1].
+000093e0: 6765 745f 6361 6c6c 5f69 6e64 6578 2829  get_call_index()
+000093f0: 0a0a 2020 2020 2020 6966 205f 676c 6f62  ..      if _glob
+00009400: 616c 5f69 6e74 6572 6365 7074 6f72 5f73  al_interceptor_s
+00009410: 7461 636b 3a0a 2020 2020 2020 2020 7275  tack:.        ru
+00009420: 6e5f 6675 6e20 3d20 6675 6e63 746f 6f6c  n_fun = functool
+00009430: 732e 7061 7274 6961 6c28 7275 6e5f 696e  s.partial(run_in
+00009440: 7465 7263 6570 746f 7273 2c20 6675 6e29  terceptors, fun)
+00009450: 0a20 2020 2020 2065 6c73 653a 0a20 2020  .      else:.   
+00009460: 2020 2020 2072 756e 5f66 756e 203d 2066       run_fun = f
+00009470: 756e 0a0a 2020 2020 2020 2320 6361 6c6c  un..      # call
+00009480: 206d 6574 686f 640a 2020 2020 2020 6966   method.      if
+00009490: 205f 7573 655f 6e61 6d65 645f 6361 6c6c   _use_named_call
+000094a0: 3a0a 2020 2020 2020 2020 7769 7468 206a  :.        with j
+000094b0: 6178 2e6e 616d 6564 5f73 636f 7065 285f  ax.named_scope(_
+000094c0: 6465 7269 7665 5f70 726f 6669 6c69 6e67  derive_profiling
+000094d0: 5f6e 616d 6528 7365 6c66 2c20 6675 6e29  _name(self, fun)
+000094e0: 293a 0a20 2020 2020 2020 2020 2079 203d  ):.          y =
+000094f0: 2072 756e 5f66 756e 2873 656c 662c 202a   run_fun(self, *
+00009500: 6172 6773 2c20 2a2a 6b77 6172 6773 290a  args, **kwargs).
+00009510: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
+00009520: 2020 2020 7920 3d20 7275 6e5f 6675 6e28      y = run_fun(
+00009530: 7365 6c66 2c20 2a61 7267 732c 202a 2a6b  self, *args, **k
+00009540: 7761 7267 7329 0a0a 2020 2020 2020 6966  wargs)..      if
+00009550: 205f 636f 6e74 6578 742e 6361 7074 7572   _context.captur
+00009560: 655f 7374 6163 6b3a 0a20 2020 2020 2020  e_stack:.       
+00009570: 2066 696c 7465 725f 666e 203d 205f 636f   filter_fn = _co
+00009580: 6e74 6578 742e 6361 7074 7572 655f 7374  ntext.capture_st
+00009590: 6163 6b5b 2d31 5d0a 2020 2020 2020 2020  ack[-1].        
+000095a0: 6966 2066 696c 7465 725f 666e 2061 6e64  if filter_fn and
+000095b0: 2066 696c 7465 725f 666e 2873 656c 662c   filter_fn(self,
+000095c0: 2066 756e 5f6e 616d 6529 3a0a 2020 2020   fun_name):.    
+000095d0: 2020 2020 2020 7365 6c66 2e73 6f77 2827        self.sow('
+000095e0: 696e 7465 726d 6564 6961 7465 7327 2c20  intermediates', 
+000095f0: 6675 6e5f 6e61 6d65 2c20 7929 0a20 2020  fun_name, y).   
+00009600: 2020 2069 6620 6164 645f 6361 6c6c 5f69     if add_call_i
+00009610: 6e66 6f3a 0a20 2020 2020 2020 205f 6172  nfo:.        _ar
+00009620: 6773 2c20 5f6b 7761 7267 732c 205f 7920  gs, _kwargs, _y 
+00009630: 3d20 666c 6178 2e6c 696e 656e 2e73 756d  = flax.linen.sum
+00009640: 6d61 7279 2e5f 7265 7072 6573 656e 745f  mary._represent_
+00009650: 7472 6565 280a 2020 2020 2020 2020 2020  tree(.          
+00009660: 2861 7267 732c 206b 7761 7267 732c 2079  (args, kwargs, y
+00009670: 290a 2020 2020 2020 2020 290a 2020 2020  ).        ).    
+00009680: 2020 2020 5f63 6f6e 7465 7874 2e63 616c      _context.cal
+00009690: 6c5f 696e 666f 5f73 7461 636b 5b2d 315d  l_info_stack[-1]
+000096a0: 2e63 616c 6c73 2e61 7070 656e 6428 0a20  .calls.append(. 
+000096b0: 2020 2020 2020 2020 205f 4361 6c6c 496e           _CallIn
+000096c0: 666f 280a 2020 2020 2020 2020 2020 2020  fo(.            
+000096d0: 6361 6c6c 5f69 6e64 6578 2c0a 2020 2020  call_index,.    
+000096e0: 2020 2020 2020 2020 7365 6c66 2e70 6174          self.pat
+000096f0: 682c 0a20 2020 2020 2020 2020 2020 2073  h,.            s
+00009700: 656c 662e 636c 6f6e 6528 292c 0a20 2020  elf.clone(),.   
+00009710: 2020 2020 2020 2020 2073 656c 662e 7363           self.sc
+00009720: 6f70 652e 726e 6773 2c0a 2020 2020 2020  ope.rngs,.      
+00009730: 2020 2020 2020 7365 6c66 2e73 636f 7065        self.scope
+00009740: 2e6d 7574 6162 6c65 2c0a 2020 2020 2020  .mutable,.      
+00009750: 2020 2020 2020 6675 6e2e 5f5f 6e61 6d65        fun.__name
+00009760: 5f5f 2c0a 2020 2020 2020 2020 2020 2020  __,.            
+00009770: 5f61 7267 732c 0a20 2020 2020 2020 2020  _args,.         
+00009780: 2020 205f 6b77 6172 6773 2c0a 2020 2020     _kwargs,.    
+00009790: 2020 2020 2020 2020 5f79 2c0a 2020 2020          _y,.    
+000097a0: 2020 2020 2020 290a 2020 2020 2020 2020        ).        
+000097b0: 290a 2020 2020 2020 7265 7475 726e 2079  ).      return y
+000097c0: 0a20 2020 2066 696e 616c 6c79 3a0a 2020  .    finally:.  
+000097d0: 2020 2020 5f63 6f6e 7465 7874 2e6d 6f64      _context.mod
+000097e0: 756c 655f 7374 6163 6b2e 706f 7028 290a  ule_stack.pop().
+000097f0: 2020 2020 2020 6966 2069 735f 636f 6d70        if is_comp
+00009800: 6163 745f 6d65 7468 6f64 3a0a 2020 2020  act_method:.    
+00009810: 2020 2020 6f62 6a65 6374 2e5f 5f73 6574      object.__set
+00009820: 6174 7472 5f5f 2873 656c 662c 2027 7363  attr__(self, 'sc
+00009830: 6f70 6527 2c20 7365 6c66 2e73 636f 7065  ope', self.scope
+00009840: 2e72 6577 6f75 6e64 2829 290a 2020 2020  .rewound()).    
+00009850: 2020 2320 7365 7475 7020 6f72 2063 6f6d    # setup or com
+00009860: 7061 6374 2063 616c 6c73 2063 616e 2062  pact calls can b
+00009870: 6520 7265 6375 7272 656e 7420 666f 7220  e recurrent for 
+00009880: 6578 616d 706c 6520 6475 6520 746f 2073  example due to s
+00009890: 7570 6572 2063 616c 6c73 0a20 2020 2020  uper calls.     
+000098a0: 2023 2072 6573 6574 7469 6e67 2074 6865   # resetting the
+000098b0: 2073 7461 7465 2077 6f75 6c64 2063 6175   state would cau
+000098c0: 7365 2069 7320 636f 6d70 6163 742f 7365  se is compact/se
+000098d0: 7475 7020 6d65 7468 6f64 0a20 2020 2020  tup method.     
+000098e0: 2023 2074 6f20 6265 2073 6574 2074 6f20   # to be set to 
+000098f0: 4661 6c73 6520 7072 656d 6174 7572 656c  False prematurel
+00009900: 792e 0a20 2020 2020 2069 6620 2869 735f  y..      if (is_
+00009910: 636f 6d70 6163 745f 6d65 7468 6f64 206f  compact_method o
+00009920: 7220 6973 5f73 6574 7570 5f6d 6574 686f  r is_setup_metho
+00009930: 6429 2061 6e64 206e 6f74 2069 735f 7265  d) and not is_re
+00009940: 6375 7272 656e 743a 0a20 2020 2020 2020  current:.       
+00009950: 2073 656c 662e 5f73 7461 7465 2e72 6573   self._state.res
+00009960: 6574 2829 0a0a 2020 6465 6620 5f5f 7365  et()..  def __se
+00009970: 7461 7474 725f 5f28 7365 6c66 2c20 6e61  tattr__(self, na
+00009980: 6d65 3a20 7374 722c 2076 616c 3a20 416e  me: str, val: An
+00009990: 7929 3a0a 2020 2020 2222 2253 6574 7320  y):.    """Sets 
+000099a0: 616e 2061 7474 7269 6275 7465 206f 6e20  an attribute on 
+000099b0: 7468 6973 204d 6f64 756c 652e 0a0a 2020  this Module...  
+000099c0: 2020 5765 206f 7665 726c 6f61 6420 7365    We overload se
+000099d0: 7461 7474 7220 736f 6c65 6c79 2074 6f20  tattr solely to 
+000099e0: 7375 7070 6f72 7420 7079 7468 6f6e 6963  support pythonic
+000099f0: 206e 616d 696e 6720 7669 6120 6173 7369   naming via assi
+00009a00: 676e 6d65 6e74 206f 660a 2020 2020 7375  gnment of.    su
+00009a10: 626d 6f64 756c 6573 2069 6e20 7468 6520  bmodules in the 
+00009a20: 7370 6563 6961 6c20 3a6d 6574 683a 6073  special :meth:`s
+00009a30: 6574 7570 6020 6675 6e63 7469 6f6e 3a3a  etup` function::
+00009a40: 0a0a 2020 2020 2020 7365 6c66 2e73 7562  ..      self.sub
+00009a50: 6d6f 6475 6c65 5f6e 616d 6520 3d20 4d79  module_name = My
+00009a60: 4d6f 6475 6c65 282e 2e2e 290a 0a20 2020  Module(...)..   
+00009a70: 2057 6520 616c 736f 2073 7570 706f 7274   We also support
+00009a80: 206c 6973 7473 2061 6e64 206f 7468 6572   lists and other
+00009a90: 2067 656e 6572 616c 2070 7974 7265 6573   general pytrees
+00009aa0: 2c20 652e 672e 3a3a 0a0a 2020 2020 2020  , e.g.::..      
+00009ab0: 7365 6c66 2e73 7562 6d6f 6475 6c65 7320  self.submodules 
+00009ac0: 3d20 5b4d 794d 6f64 756c 6530 282e 2e29  = [MyModule0(..)
+00009ad0: 2c20 4d79 4d6f 6475 6c65 3128 2e2e 292c  , MyModule1(..),
+00009ae0: 202e 2e2e 5d0a 0a20 2020 2041 7267 733a   ...]..    Args:
+00009af0: 0a20 2020 2020 206e 616d 653a 2041 7474  .      name: Att
+00009b00: 7269 6275 7465 2074 6f20 7365 742e 0a20  ribute to set.. 
+00009b10: 2020 2020 2076 616c 3a20 5661 6c75 6520       val: Value 
+00009b20: 6f66 2074 6865 2061 7474 7269 6275 7465  of the attribute
+00009b30: 2e0a 2020 2020 2222 220a 2020 2020 6669  ..    """.    fi
+00009b40: 656c 6473 203d 2073 656c 662e 5f5f 6461  elds = self.__da
+00009b50: 7461 636c 6173 735f 6669 656c 6473 5f5f  taclass_fields__
+00009b60: 2020 2320 7079 7479 7065 3a20 6469 7361    # pytype: disa
+00009b70: 626c 653d 6174 7472 6962 7574 652d 6572  ble=attribute-er
+00009b80: 726f 720a 2020 2020 6973 5f64 6174 6163  ror.    is_datac
+00009b90: 6c61 7373 5f61 7474 7220 3d20 6e61 6d65  lass_attr = name
+00009ba0: 2069 6e20 6669 656c 6473 2061 6e64 2066   in fields and f
+00009bb0: 6965 6c64 735b 6e61 6d65 5d2e 696e 6974  ields[name].init
+00009bc0: 0a0a 2020 2020 6966 206e 6f74 2073 656c  ..    if not sel
+00009bd0: 662e 5f73 7461 7465 2e69 6e5f 7365 7475  f._state.in_setu
+00009be0: 703a 0a20 2020 2020 2069 6620 6e6f 7420  p:.      if not 
+00009bf0: 7365 6c66 2e5f 7374 6174 652e 6973 5f69  self._state.is_i
+00009c00: 6e69 7469 616c 697a 6564 3a0a 2020 2020  nitialized:.    
+00009c10: 2020 2020 2320 5365 7474 696e 6720 6174      # Setting at
+00009c20: 7472 6962 7574 6573 2062 6566 6f72 6520  tributes before 
+00009c30: 656e 6420 6f66 204d 6f64 756c 652e 5f5f  end of Module.__
+00009c40: 706f 7374 5f69 6e69 745f 5f28 290a 2020  post_init__().  
+00009c50: 2020 2020 2020 6f62 6a65 6374 2e5f 5f73        object.__s
+00009c60: 6574 6174 7472 5f5f 2873 656c 662c 206e  etattr__(self, n
+00009c70: 616d 652c 2076 616c 290a 2020 2020 2020  ame, val).      
+00009c80: 2020 7265 7475 726e 0a20 2020 2020 2065    return.      e
+00009c90: 6c73 653a 0a20 2020 2020 2020 2023 2057  lse:.        # W
+00009ca0: 6527 7265 2070 6173 7420 616c 6c20 696e  e're past all in
+00009cb0: 6974 6961 6c69 7a61 7469 6f6e 2061 6e64  itialization and
+00009cc0: 2073 6574 7570 206c 6f67 6963 3a0a 2020   setup logic:.  
+00009cd0: 2020 2020 2020 2320 5261 6973 6573 2061        # Raises a
+00009ce0: 2054 7970 6545 7272 6f72 206a 7573 7420   TypeError just 
+00009cf0: 6c69 6b65 2066 726f 7a65 6e20 7079 7468  like frozen pyth
+00009d00: 6f6e 2064 6174 6163 6c61 7373 6573 2e0a  on dataclasses..
+00009d10: 2020 2020 2020 2020 7261 6973 6520 6572          raise er
+00009d20: 726f 7273 2e53 6574 4174 7472 6962 7574  rors.SetAttribut
+00009d30: 6546 726f 7a65 6e4d 6f64 756c 6545 7272  eFrozenModuleErr
+00009d40: 6f72 280a 2020 2020 2020 2020 2020 7365  or(.          se
+00009d50: 6c66 2e5f 5f63 6c61 7373 5f5f 2e5f 5f6e  lf.__class__.__n
+00009d60: 616d 655f 5f2c 206e 616d 652c 2076 616c  ame__, name, val
+00009d70: 0a20 2020 2020 2020 2029 0a0a 2020 2020  .        )..    
+00009d80: 2320 5765 2772 6520 696e 7369 6465 2074  # We're inside t
+00009d90: 6865 2073 6574 7570 2829 206d 6574 686f  he setup() metho
+00009da0: 643a 0a20 2020 2069 6620 6973 5f64 6174  d:.    if is_dat
+00009db0: 6163 6c61 7373 5f61 7474 723a 0a20 2020  aclass_attr:.   
+00009dc0: 2020 2023 2054 6865 7365 206e 616d 6573     # These names
+00009dd0: 2061 7265 2073 7065 6369 6669 6564 2061   are specified a
+00009de0: 7320 6461 7461 636c 6173 7320 6669 656c  s dataclass fiel
+00009df0: 6473 2e20 5468 6579 2073 686f 756c 6420  ds. They should 
+00009e00: 6e6f 7420 6265 0a20 2020 2020 2023 2069  not be.      # i
+00009e10: 6e69 7469 616c 697a 6564 2077 6974 6869  nitialized withi
+00009e20: 6e20 7468 6520 7365 7475 7028 2920 6d65  n the setup() me
+00009e30: 7468 6f64 2c20 6275 7420 6361 6e20 6265  thod, but can be
+00009e40: 206d 6f64 6966 6965 6420 6672 6565 6c79   modified freely
+00009e50: 0a20 2020 2020 2023 2062 6566 6f72 6520  .      # before 
+00009e60: 6974 2e0a 2020 2020 2020 7261 6973 6520  it..      raise 
+00009e70: 6572 726f 7273 2e53 6574 4174 7472 6962  errors.SetAttrib
+00009e80: 7574 6549 6e4d 6f64 756c 6553 6574 7570  uteInModuleSetup
+00009e90: 4572 726f 7228 290a 0a20 2020 2023 2056  Error()..    # V
+00009ea0: 616c 7565 7320 2874 6861 7420 6d61 7920  alues (that may 
+00009eb0: 6265 2076 6172 6961 626c 6573 206f 7220  be variables or 
+00009ec0: 7375 626d 6f64 756c 6573 2920 6172 6520  submodules) are 
+00009ed0: 6265 696e 6720 6465 6669 6e65 6420 616e  being defined an
+00009ee0: 640a 2020 2020 2320 6174 7461 6368 6564  d.    # attached
+00009ef0: 2069 6e20 7365 7475 7028 292c 2077 6520   in setup(), we 
+00009f00: 7275 6e20 736f 6d65 2065 7874 7261 206c  run some extra l
+00009f10: 6f67 6963 2069 6e20 7468 6174 2063 6173  ogic in that cas
+00009f20: 652e 0a20 2020 2073 656c 662e 5f72 6567  e..    self._reg
+00009f30: 6973 7465 725f 7375 626d 6f64 756c 6573  ister_submodules
+00009f40: 286e 616d 652c 2076 616c 290a 0a20 2064  (name, val)..  d
+00009f50: 6566 205f 5f67 6574 6174 7472 5f5f 2873  ef __getattr__(s
+00009f60: 656c 662c 206e 616d 653a 2073 7472 2920  elf, name: str) 
+00009f70: 2d3e 2041 6e79 3a0a 2020 2020 2222 2243  -> Any:.    """C
+00009f80: 616c 6c20 7365 7475 7028 2920 6265 666f  all setup() befo
+00009f90: 7265 2067 6574 7469 6e67 2061 6e79 2073  re getting any s
+00009fa0: 6574 7570 2d64 6566 696e 6564 2061 7474  etup-defined att
+00009fb0: 7269 6275 7465 732e 2222 220a 2020 2020  ributes.""".    
+00009fc0: 2320 5765 2064 6f6e 2774 2077 616e 7420  # We don't want 
+00009fd0: 746f 2072 6574 7572 6e20 616e 7974 6869  to return anythi
+00009fe0: 6e67 2066 6f72 2070 7974 686f 6e20 636f  ng for python co
+00009ff0: 7079 202f 2070 6963 6b6c 6520 6d65 7468  py / pickle meth
+0000a000: 6f64 732e 0a20 2020 2069 6620 6e61 6d65  ods..    if name
+0000a010: 2069 6e20 5f55 4e44 4546 494e 4544 5f43   in _UNDEFINED_C
+0000a020: 4f50 595f 5049 434b 4c45 5f4d 4554 484f  OPY_PICKLE_METHO
+0000a030: 4453 3a0a 2020 2020 2020 7261 6973 6520  DS:.      raise 
+0000a040: 4174 7472 6962 7574 6545 7272 6f72 2829  AttributeError()
+0000a050: 0a20 2020 2073 656c 662e 5f74 7279 5f73  .    self._try_s
+0000a060: 6574 7570 2829 0a20 2020 2069 6620 6e61  etup().    if na
+0000a070: 6d65 2069 6e20 7365 6c66 2e5f 5f64 6963  me in self.__dic
+0000a080: 745f 5f3a 0a20 2020 2020 2072 6574 7572  t__:.      retur
+0000a090: 6e20 7365 6c66 2e5f 5f64 6963 745f 5f5b  n self.__dict__[
+0000a0a0: 6e61 6d65 5d0a 2020 2020 656c 7365 3a0a  name].    else:.
+0000a0b0: 2020 2020 2020 6d73 6720 3d20 6627 227b        msg = f'"{
+0000a0c0: 7365 6c66 2e5f 5f63 6c61 7373 5f5f 2e5f  self.__class__._
+0000a0d0: 5f6e 616d 655f 5f7d 2220 6f62 6a65 6374  _name__}" object
+0000a0e0: 2068 6173 206e 6f20 6174 7472 6962 7574   has no attribut
+0000a0f0: 6520 227b 6e61 6d65 7d22 2e27 0a20 2020  e "{name}".'.   
+0000a100: 2020 2069 6620 7365 6c66 2e73 636f 7065     if self.scope
+0000a110: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
+0000a120: 2020 6d73 6720 2b3d 2028 0a20 2020 2020    msg += (.     
+0000a130: 2020 2020 2066 2720 4966 2022 7b6e 616d       f' If "{nam
+0000a140: 657d 2220 6973 2064 6566 696e 6564 2069  e}" is defined i
+0000a150: 6e20 5c27 2e73 6574 7570 2829 5c27 2c20  n \'.setup()\', 
+0000a160: 7265 6d65 6d62 6572 2074 6865 7365 2066  remember these f
+0000a170: 6965 6c64 7320 270a 2020 2020 2020 2020  ields '.        
+0000a180: 2020 2261 7265 206f 6e6c 7920 6163 6365    "are only acce
+0000a190: 7373 6962 6c65 2066 726f 6d20 696e 7369  ssible from insi
+0000a1a0: 6465 2027 696e 6974 2720 6f72 2027 6170  de 'init' or 'ap
+0000a1b0: 706c 7927 2e22 0a20 2020 2020 2020 2029  ply'.".        )
+0000a1c0: 0a20 2020 2020 2072 6169 7365 2041 7474  .      raise Att
+0000a1d0: 7269 6275 7465 4572 726f 7228 6d73 6729  ributeError(msg)
+0000a1e0: 0a0a 2020 6465 6620 5f5f 6469 725f 5f28  ..  def __dir__(
+0000a1f0: 7365 6c66 2920 2d3e 204c 6973 745b 7374  self) -> List[st
+0000a200: 725d 3a0a 2020 2020 2222 2243 616c 6c20  r]:.    """Call 
+0000a210: 7365 7475 7028 2920 6265 666f 7265 206c  setup() before l
+0000a220: 6973 7469 6e67 2061 7474 7269 6275 7465  isting attribute
+0000a230: 732e 2222 220a 2020 2020 7365 6c66 2e5f  s.""".    self._
+0000a240: 7472 795f 7365 7475 7028 290a 2020 2020  try_setup().    
+0000a250: 7265 7475 726e 206f 626a 6563 742e 5f5f  return object.__
+0000a260: 6469 725f 5f28 7365 6c66 2920 2023 2074  dir__(self)  # t
+0000a270: 7970 653a 2069 676e 6f72 650a 0a20 2064  ype: ignore..  d
+0000a280: 6566 205f 5f70 6f73 745f 696e 6974 5f5f  ef __post_init__
+0000a290: 2873 656c 6629 202d 3e20 4e6f 6e65 3a0a  (self) -> None:.
+0000a2a0: 2020 2020 2320 444f 204e 4f54 2052 454d      # DO NOT REM
+0000a2b0: 4f56 4520 2d20 4d61 726b 6572 2066 6f72  OVE - Marker for
+0000a2c0: 2069 6e74 6572 6e61 6c20 6c6f 6767 696e   internal loggin
+0000a2d0: 672e 0a20 2020 2023 2049 6e20 6461 7461  g..    # In data
+0000a2e0: 636c 6173 7365 732c 205f 5f69 6e69 745f  classes, __init_
+0000a2f0: 5f20 6973 206f 7665 7272 6964 6465 6e20  _ is overridden 
+0000a300: 746f 2070 726f 6365 7373 2064 6174 6163  to process datac
+0000a310: 6c61 7373 2061 7267 756d 656e 7473 2c0a  lass arguments,.
+0000a320: 2020 2020 2320 616e 6420 5f5f 706f 7374      # and __post
+0000a330: 5f69 6e69 745f 5f20 6973 2063 616c 6c65  _init__ is calle
+0000a340: 6420 696d 6d65 6469 6174 656c 7920 6166  d immediately af
+0000a350: 7465 7277 6172 6473 2e20 4865 7265 2c20  terwards. Here, 
+0000a360: 6465 7065 6e64 696e 6720 6f6e 2074 6865  depending on the
+0000a370: 0a20 2020 2023 2074 7970 6520 6f66 2060  .    # type of `
+0000a380: 7061 7265 6e74 6020 7061 7373 6564 2074  parent` passed t
+0000a390: 6f20 696e 6974 6961 6c69 7a65 2074 6865  o initialize the
+0000a3a0: 204d 6f64 756c 652c 2077 6520 6569 7468   Module, we eith
+0000a3b0: 6572 2064 6566 6572 0a20 2020 2023 2069  er defer.    # i
+0000a3c0: 6e69 7469 616c 697a 6174 696f 6e2c 2061  nitialization, a
+0000a3d0: 7474 6163 6820 7468 6973 204d 6f64 756c  ttach this Modul
+0000a3e0: 6520 6173 2061 2073 7562 6d6f 6475 6c65  e as a submodule
+0000a3f0: 206f 6620 6120 7061 7265 6e74 2c20 6f72   of a parent, or
+0000a400: 2062 696e 640a 2020 2020 2320 7468 6973   bind.    # this
+0000a410: 204d 6f64 756c 6520 6174 2074 6865 2074   Module at the t
+0000a420: 6f70 2d6c 6576 656c 2074 6f20 7661 7269  op-level to vari
+0000a430: 6162 6c65 7320 616e 6420 726e 6773 2e0a  ables and rngs..
+0000a440: 0a20 2020 206f 626a 6563 742e 5f5f 7365  .    object.__se
+0000a450: 7461 7474 725f 5f28 7365 6c66 2c20 275f  tattr__(self, '_
+0000a460: 6964 272c 2075 7569 6428 2929 0a20 2020  id', uuid()).   
+0000a470: 206f 626a 6563 742e 5f5f 7365 7461 7474   object.__setatt
+0000a480: 725f 5f28 7365 6c66 2c20 275f 7374 6174  r__(self, '_stat
+0000a490: 6527 2c20 5f4d 6f64 756c 6549 6e74 6572  e', _ModuleInter
+0000a4a0: 6e61 6c53 7461 7465 2829 290a 0a20 2020  nalState())..   
+0000a4b0: 2023 2054 7970 6963 616c 6c79 2077 6520   # Typically we 
+0000a4c0: 7365 7420 7468 6520 7061 7265 6e74 2062  set the parent b
+0000a4d0: 6173 6564 206f 6e20 7468 6520 6479 6e61  ased on the dyna
+0000a4e0: 6d69 6320 6d6f 6475 6c65 2063 6f6e 7465  mic module conte
+0000a4f0: 7874 2e0a 2020 2020 6966 2073 656c 662e  xt..    if self.
+0000a500: 7061 7265 6e74 2069 7320 5f75 6e73 7065  parent is _unspe
+0000a510: 6369 6669 6564 5f70 6172 656e 743a 2020  cified_parent:  
+0000a520: 2320 7079 7479 7065 3a20 6469 7361 626c  # pytype: disabl
+0000a530: 653d 6174 7472 6962 7574 652d 6572 726f  e=attribute-erro
+0000a540: 720a 2020 2020 2020 6f62 6a65 6374 2e5f  r.      object._
+0000a550: 5f73 6574 6174 7472 5f5f 2873 656c 662c  _setattr__(self,
+0000a560: 2027 7061 7265 6e74 272c 205f 636f 6e74   'parent', _cont
+0000a570: 6578 742e 6d6f 6475 6c65 5f73 7461 636b  ext.module_stack
+0000a580: 5b2d 315d 290a 0a20 2020 2023 2049 6e69  [-1])..    # Ini
+0000a590: 7469 616c 697a 6174 696f 6e20 6973 2064  tialization is d
+0000a5a0: 6566 6572 7265 6420 666f 7220 746f 7020  eferred for top 
+0000a5b0: 6c65 7665 6c20 4d6f 6475 6c65 7320 6f72  level Modules or
+0000a5c0: 2061 6e79 206f 7468 6572 2022 6f72 7068   any other "orph
+0000a5d0: 616e 220a 2020 2020 2320 4d6f 6475 6c65  an".    # Module
+0000a5e0: 7320 756e 7469 6c20 6174 7461 6368 6d65  s until attachme
+0000a5f0: 6e74 2062 7920 5f5f 7365 7461 7474 725f  nt by __setattr_
+0000a600: 5f20 692e 652e 204d 794d 6f64 756c 6528  _ i.e. MyModule(
+0000a610: 2e2e 2e2c 2070 6172 656e 743d 4e6f 6e65  ..., parent=None
+0000a620: 290a 2020 2020 6966 2073 656c 662e 7061  ).    if self.pa
+0000a630: 7265 6e74 2069 7320 4e6f 6e65 3a0a 2020  rent is None:.  
+0000a640: 2020 2020 7265 7475 726e 0a0a 2020 2020      return..    
+0000a650: 2320 5265 6769 7374 6572 2073 7562 6d6f  # Register submo
+0000a660: 6475 6c65 206f 6e20 7061 7265 6e74 204d  dule on parent M
+0000a670: 6f64 756c 652e 0a20 2020 2069 6620 6973  odule..    if is
+0000a680: 696e 7374 616e 6365 2873 656c 662e 7061  instance(self.pa
+0000a690: 7265 6e74 2c20 4d6f 6475 6c65 293a 0a20  rent, Module):. 
+0000a6a0: 2020 2020 2023 2057 6865 6e20 696e 6974       # When init
+0000a6b0: 6961 6c69 7a69 6e67 2061 6e20 756e 6e61  ializing an unna
+0000a6c0: 6d65 6420 4d6f 6475 6c65 2069 6e73 6964  med Module insid
+0000a6d0: 6520 7365 7475 7028 290a 2020 2020 2020  e setup().      
+0000a6e0: 2320 696e 6974 6961 6c69 7a61 7469 6f6e  # initialization
+0000a6f0: 2069 7320 6465 6665 7272 6564 2075 6e74   is deferred unt
+0000a700: 696c 2061 7474 6163 686d 656e 7420 6279  il attachment by
+0000a710: 205f 5f73 6574 6174 7472 5f5f 0a20 2020   __setattr__.   
+0000a720: 2020 2023 2069 2e65 2e20 7365 6c66 2e6d     # i.e. self.m
+0000a730: 796d 6f64 756c 6520 3d20 4d79 4d6f 6475  ymodule = MyModu
+0000a740: 6c65 282e 2e2e 290a 2020 2020 2020 7365  le(...).      se
+0000a750: 6c66 2e6e 616d 653a 204f 7074 696f 6e61  lf.name: Optiona
+0000a760: 6c5b 7374 725d 0a20 2020 2020 2069 6620  l[str].      if 
+0000a770: 280a 2020 2020 2020 2020 7365 6c66 2e70  (.        self.p
+0000a780: 6172 656e 742e 5f73 7461 7465 2e69 6e5f  arent._state.in_
+0000a790: 7365 7475 7020 616e 6420 7365 6c66 2e6e  setup and self.n
+0000a7a0: 616d 6520 6973 204e 6f6e 650a 2020 2020  ame is None.    
+0000a7b0: 2020 293a 2020 2320 7079 7479 7065 3a20    ):  # pytype: 
+0000a7c0: 6469 7361 626c 653d 6174 7472 6962 7574  disable=attribut
+0000a7d0: 652d 6572 726f 720a 2020 2020 2020 2020  e-error.        
+0000a7e0: 7265 7475 726e 0a20 2020 2020 2069 6620  return.      if 
+0000a7f0: 6e6f 7420 7365 6c66 2e70 6172 656e 742e  not self.parent.
+0000a800: 5f69 6e69 7469 616c 697a 6174 696f 6e5f  _initialization_
+0000a810: 616c 6c6f 7765 643a 0a20 2020 2020 2020  allowed:.       
+0000a820: 2072 6169 7365 2065 7272 6f72 732e 4173   raise errors.As
+0000a830: 7369 676e 5375 624d 6f64 756c 6545 7272  signSubModuleErr
+0000a840: 6f72 2873 656c 662e 5f5f 636c 6173 735f  or(self.__class_
+0000a850: 5f2e 5f5f 6e61 6d65 5f5f 290a 2020 2020  _.__name__).    
+0000a860: 2020 2320 4175 746f 6e61 6d69 6e67 206f    # Autonaming o
+0000a870: 6620 7375 626d 6f64 756c 6573 2e0a 2020  f submodules..  
+0000a880: 2020 2020 6966 2073 656c 662e 6e61 6d65      if self.name
+0000a890: 2069 7320 4e6f 6e65 3a20 2023 2070 7974   is None:  # pyt
+0000a8a0: 7970 653a 2064 6973 6162 6c65 3d61 7474  ype: disable=att
+0000a8b0: 7269 6275 7465 2d65 7272 6f72 0a20 2020  ribute-error.   
+0000a8c0: 2020 2020 2070 7265 6669 7820 3d20 6627       prefix = f'
+0000a8d0: 7b73 656c 662e 5f5f 636c 6173 735f 5f2e  {self.__class__.
+0000a8e0: 5f5f 6e61 6d65 5f5f 7d27 0a20 2020 2020  __name__}'.     
+0000a8f0: 2020 2063 7572 736f 7220 3d20 7365 6c66     cursor = self
+0000a900: 2e70 6172 656e 742e 5f73 7461 7465 2e61  .parent._state.a
+0000a910: 7574 6f6e 616d 655f 6375 7273 6f72 2e67  utoname_cursor.g
+0000a920: 6574 2870 7265 6669 782c 2030 290a 2020  et(prefix, 0).  
+0000a930: 2020 2020 2020 7365 6c66 2e6e 616d 6520        self.name 
+0000a940: 3d20 6627 7b70 7265 6669 787d 5f7b 6375  = f'{prefix}_{cu
+0000a950: 7273 6f72 7d27 0a20 2020 2020 2020 2073  rsor}'.        s
+0000a960: 656c 662e 7061 7265 6e74 2e5f 7374 6174  elf.parent._stat
+0000a970: 652e 6175 746f 6e61 6d65 5f63 7572 736f  e.autoname_curso
+0000a980: 725b 7072 6566 6978 5d20 3d20 6375 7273  r[prefix] = curs
+0000a990: 6f72 202b 2031 0a20 2020 2020 2023 2041  or + 1.      # A
+0000a9a0: 6c6c 6f77 2073 636f 7065 2061 6c69 6173  llow scope alias
+0000a9b0: 696e 6720 756e 6465 7220 7472 616e 7366  ing under transf
+0000a9c0: 6f72 6d73 2066 6f72 2073 7562 6d6f 6475  orms for submodu
+0000a9d0: 6c65 7320 6465 6669 6e65 6420 696e 2073  les defined in s
+0000a9e0: 6574 7570 2e0a 2020 2020 2020 7265 7573  etup..      reus
+0000a9f0: 655f 7363 6f70 6573 203d 2028 0a20 2020  e_scopes = (.   
+0000aa00: 2020 2020 2073 656c 662e 7061 7265 6e74       self.parent
+0000aa10: 2e5f 7374 6174 652e 696e 5f73 6574 7570  ._state.in_setup
+0000aa20: 0a20 2020 2020 2020 2061 6e64 2073 656c  .        and sel
+0000aa30: 662e 7061 7265 6e74 2e5f 7374 6174 652e  f.parent._state.
+0000aa40: 7365 7475 705f 6361 6c6c 6564 203d 3d20  setup_called == 
+0000aa50: 5365 7475 7053 7461 7465 2e54 5241 4e53  SetupState.TRANS
+0000aa60: 464f 524d 4544 0a20 2020 2020 2029 0a20  FORMED.      ). 
+0000aa70: 2020 2020 2023 2050 6572 666f 726d 206e       # Perform n
+0000aa80: 616d 652d 636f 6c6c 6973 696f 6e20 6368  ame-collision ch
+0000aa90: 6563 6b2e 0a20 2020 2020 2069 6620 7365  eck..      if se
+0000aaa0: 6c66 2e70 6172 656e 742e 5f6e 616d 655f  lf.parent._name_
+0000aab0: 7461 6b65 6e28 7365 6c66 2e6e 616d 652c  taken(self.name,
+0000aac0: 2072 6575 7365 5f73 636f 7065 733d 7265   reuse_scopes=re
+0000aad0: 7573 655f 7363 6f70 6573 293a 0a20 2020  use_scopes):.   
+0000aae0: 2020 2020 2070 6172 656e 745f 636c 6173       parent_clas
+0000aaf0: 7320 3d20 7365 6c66 2e70 6172 656e 742e  s = self.parent.
+0000ab00: 5f5f 636c 6173 735f 5f2e 5f5f 6e61 6d65  __class__.__name
+0000ab10: 5f5f 0a20 2020 2020 2020 2072 6169 7365  __.        raise
+0000ab20: 2065 7272 6f72 732e 4e61 6d65 496e 5573   errors.NameInUs
+0000ab30: 6545 7272 6f72 2827 7375 626d 6f64 756c  eError('submodul
+0000ab40: 6527 2c20 7365 6c66 2e6e 616d 652c 2070  e', self.name, p
+0000ab50: 6172 656e 745f 636c 6173 7329 0a20 2020  arent_class).   
+0000ab60: 2020 2023 2046 696e 616c 697a 6520 6174     # Finalize at
+0000ab70: 7461 6368 6d65 6e74 2074 6f20 7061 7265  tachment to pare
+0000ab80: 6e74 2061 6e64 2073 636f 7065 2069 6e69  nt and scope ini
+0000ab90: 7469 616c 697a 6174 696f 6e2e 0a20 2020  tialization..   
+0000aba0: 2020 2073 656c 662e 7061 7265 6e74 2e5f     self.parent._
+0000abb0: 7374 6174 652e 6368 696c 6472 656e 5b73  state.children[s
+0000abc0: 656c 662e 6e61 6d65 5d20 3d20 7365 6c66  elf.name] = self
+0000abd0: 0a20 2020 2020 2061 7373 6572 7420 7365  .      assert se
+0000abe0: 6c66 2e70 6172 656e 742e 7363 6f70 6520  lf.parent.scope 
+0000abf0: 6973 206e 6f74 204e 6f6e 650a 2020 2020  is not None.    
+0000ac00: 2020 6f62 6a65 6374 2e5f 5f73 6574 6174    object.__setat
+0000ac10: 7472 5f5f 280a 2020 2020 2020 2020 7365  tr__(.        se
+0000ac20: 6c66 2c20 2773 636f 7065 272c 2073 656c  lf, 'scope', sel
+0000ac30: 662e 7061 7265 6e74 2e73 636f 7065 2e70  f.parent.scope.p
+0000ac40: 7573 6828 7365 6c66 2e6e 616d 652c 2072  ush(self.name, r
+0000ac50: 6575 7365 3d72 6575 7365 5f73 636f 7065  euse=reuse_scope
+0000ac60: 7329 0a20 2020 2020 2029 0a0a 2020 2020  s).      )..    
+0000ac70: 2320 546f 702d 6c65 7665 6c20 696e 766f  # Top-level invo
+0000ac80: 6361 7469 6f6e 2077 6974 6820 6120 6675  cation with a fu
+0000ac90: 6e63 7469 6f6e 616c 2053 636f 7065 2e0a  nctional Scope..
+0000aca0: 2020 2020 656c 6966 2069 7369 6e73 7461      elif isinsta
+0000acb0: 6e63 6528 7365 6c66 2e70 6172 656e 742c  nce(self.parent,
+0000acc0: 2053 636f 7065 293a 0a20 2020 2020 206f   Scope):.      o
+0000acd0: 626a 6563 742e 5f5f 7365 7461 7474 725f  bject.__setattr_
+0000ace0: 5f28 7365 6c66 2c20 2773 636f 7065 272c  _(self, 'scope',
+0000acf0: 2073 656c 662e 7061 7265 6e74 290a 2020   self.parent).  
+0000ad00: 2020 656c 7365 3a0a 2020 2020 2020 7261    else:.      ra
+0000ad10: 6973 6520 5661 6c75 6545 7272 6f72 2827  ise ValueError('
+0000ad20: 7061 7265 6e74 206d 7573 7420 6265 204e  parent must be N
+0000ad30: 6f6e 652c 204d 6f64 756c 6520 6f72 2053  one, Module or S
+0000ad40: 636f 7065 2729 0a0a 2020 2020 2320 6561  cope')..    # ea
+0000ad50: 6765 726c 7920 6269 6e64 2073 7562 6d6f  gerly bind submo
+0000ad60: 6475 6c65 7320 6966 2073 636f 7065 2069  dules if scope i
+0000ad70: 7320 6176 6169 6c61 626c 650a 2020 2020  s available.    
+0000ad80: 6966 2073 656c 662e 7363 6f70 6520 6973  if self.scope is
+0000ad90: 206e 6f74 204e 6f6e 653a 0a20 2020 2020   not None:.     
+0000ada0: 2066 6f72 2066 6965 6c64 2069 6e20 6461   for field in da
+0000adb0: 7461 636c 6173 7365 732e 6669 656c 6473  taclasses.fields
+0000adc0: 2873 656c 6629 3a0a 2020 2020 2020 2020  (self):.        
+0000add0: 6966 2066 6965 6c64 2e6e 616d 6520 6e6f  if field.name no
+0000ade0: 7420 696e 2028 2770 6172 656e 7427 2c20  t in ('parent', 
+0000adf0: 276e 616d 6527 2920 616e 6420 6669 656c  'name') and fiel
+0000ae00: 642e 696e 6974 3a0a 2020 2020 2020 2020  d.init:.        
+0000ae10: 2020 7365 6c66 2e5f 7265 6769 7374 6572    self._register
+0000ae20: 5f73 7562 6d6f 6475 6c65 7328 6669 656c  _submodules(fiel
+0000ae30: 642e 6e61 6d65 2c20 6765 7461 7474 7228  d.name, getattr(
+0000ae40: 7365 6c66 2c20 6669 656c 642e 6e61 6d65  self, field.name
+0000ae50: 2929 0a0a 2020 2020 7365 6c66 2e5f 7374  ))..    self._st
+0000ae60: 6174 652e 6973 5f69 6e69 7469 616c 697a  ate.is_initializ
+0000ae70: 6564 203d 2054 7275 650a 0a20 2064 6566  ed = True..  def
+0000ae80: 205f 5f72 6570 725f 5f28 7365 6c66 2920   __repr__(self) 
+0000ae90: 2d3e 2073 7472 3a0a 2020 2020 7265 7475  -> str:.    retu
+0000aea0: 726e 205f 6d6f 6475 6c65 5f72 6570 7228  rn _module_repr(
+0000aeb0: 7365 6c66 290a 0a20 2064 6566 2073 6574  self)..  def set
+0000aec0: 7570 2873 656c 6629 202d 3e20 4e6f 6e65  up(self) -> None
+0000aed0: 3a0a 2020 2020 2222 2249 6e69 7469 616c  :.    """Initial
+0000aee0: 697a 6573 2061 204d 6f64 756c 6520 6c61  izes a Module la
+0000aef0: 7a69 6c79 2028 7369 6d69 6c61 7220 746f  zily (similar to
+0000af00: 2061 206c 617a 7920 6060 5f5f 696e 6974   a lazy ``__init
+0000af10: 5f5f 6060 292e 0a0a 2020 2020 6060 7365  __``)...    ``se
+0000af20: 7475 7060 6020 6973 2063 616c 6c65 6420  tup`` is called 
+0000af30: 6f6e 6365 206c 617a 696c 7920 6f6e 2061  once lazily on a
+0000af40: 206d 6f64 756c 6520 696e 7374 616e 6365   module instance
+0000af50: 2077 6865 6e20 6120 6d6f 6475 6c65 0a20   when a module. 
+0000af60: 2020 2069 7320 626f 756e 642c 2069 6d6d     is bound, imm
+0000af70: 6564 6961 7465 6c79 2062 6566 6f72 6520  ediately before 
+0000af80: 616e 7920 6f74 6865 7220 6d65 7468 6f64  any other method
+0000af90: 7320 6c69 6b65 2060 605f 5f63 616c 6c5f  s like ``__call_
+0000afa0: 5f60 6020 6172 650a 2020 2020 696e 766f  _`` are.    invo
+0000afb0: 6b65 642c 206f 7220 6265 666f 7265 2061  ked, or before a
+0000afc0: 2060 6073 6574 7570 6060 2d64 6566 696e   ``setup``-defin
+0000afd0: 6564 2061 7474 7269 6275 7465 206f 6e20  ed attribute on 
+0000afe0: 6060 7365 6c66 6060 2069 7320 6163 6365  ``self`` is acce
+0000aff0: 7373 6564 2e0a 0a20 2020 2054 6869 7320  ssed...    This 
+0000b000: 6361 6e20 6861 7070 656e 2069 6e20 7468  can happen in th
+0000b010: 7265 6520 6361 7365 733a 0a0a 2020 2020  ree cases:..    
+0000b020: 2020 312e 2049 6d6d 6564 6961 7465 6c79    1. Immediately
+0000b030: 2077 6865 6e20 696e 766f 6b69 6e67 203a   when invoking :
+0000b040: 6d65 7468 3a60 6170 706c 7960 2c20 3a6d  meth:`apply`, :m
+0000b050: 6574 683a 6069 6e69 7460 206f 720a 2020  eth:`init` or.  
+0000b060: 2020 2020 2020 203a 6d65 7468 3a60 696e         :meth:`in
+0000b070: 6974 5f61 6e64 5f6f 7574 7075 7460 2e0a  it_and_output`..
+0000b080: 0a20 2020 2020 2032 2e20 4f6e 6365 2074  .      2. Once t
+0000b090: 6865 206d 6f64 756c 6520 6973 2067 6976  he module is giv
+0000b0a0: 656e 2061 206e 616d 6520 6279 2062 6569  en a name by bei
+0000b0b0: 6e67 2061 7373 6967 6e65 6420 746f 2061  ng assigned to a
+0000b0c0: 6e20 6174 7472 6962 7574 6520 6f66 0a20  n attribute of. 
+0000b0d0: 2020 2020 2020 2020 616e 6f74 6865 7220          another 
+0000b0e0: 6d6f 6475 6c65 2069 6e73 6964 6520 7468  module inside th
+0000b0f0: 6520 6f74 6865 7220 6d6f 6475 6c65 2773  e other module's
+0000b100: 2060 6073 6574 7570 6060 206d 6574 686f   ``setup`` metho
+0000b110: 640a 2020 2020 2020 2020 2028 7365 6520  d.         (see 
+0000b120: 3a6d 6574 683a 605f 5f73 6574 6174 7472  :meth:`__setattr
+0000b130: 5f5f 6029 3a3a 0a0a 2020 2020 2020 2020  __`)::..        
+0000b140: 2020 2020 3e3e 3e20 636c 6173 7320 4d79      >>> class My
+0000b150: 4d6f 6475 6c65 286e 6e2e 4d6f 6475 6c65  Module(nn.Module
+0000b160: 293a 0a20 2020 2020 2020 2020 2020 202e  ):.            .
+0000b170: 2e2e 2020 2064 6566 2073 6574 7570 2873  ..   def setup(s
+0000b180: 656c 6629 3a0a 2020 2020 2020 2020 2020  elf):.          
+0000b190: 2020 2e2e 2e20 2020 2020 7375 626d 6f64    ...     submod
+0000b1a0: 756c 6520 3d20 6e6e 2e43 6f6e 7628 2e2e  ule = nn.Conv(..
+0000b1b0: 2e29 0a0a 2020 2020 2020 2020 2020 2020  .)..            
+0000b1c0: 2e2e 2e20 2020 2020 2320 4163 6365 7373  ...     # Access
+0000b1d0: 696e 6720 6073 7562 6d6f 6475 6c65 6020  ing `submodule` 
+0000b1e0: 6174 7472 6962 7574 6573 2064 6f65 7320  attributes does 
+0000b1f0: 6e6f 7420 7965 7420 776f 726b 2068 6572  not yet work her
+0000b200: 652e 0a0a 2020 2020 2020 2020 2020 2020  e...            
+0000b210: 2e2e 2e20 2020 2020 2320 5468 6520 666f  ...     # The fo
+0000b220: 6c6c 6f77 696e 6720 6c69 6e65 2069 6e76  llowing line inv
+0000b230: 6f6b 6573 2060 7365 6c66 2e5f 5f73 6574  okes `self.__set
+0000b240: 6174 7472 5f5f 602c 2077 6869 6368 2067  attr__`, which g
+0000b250: 6976 6573 0a20 2020 2020 2020 2020 2020  ives.           
+0000b260: 202e 2e2e 2020 2020 2023 2060 7375 626d   ...     # `subm
+0000b270: 6f64 756c 6560 2074 6865 206e 616d 6520  odule` the name 
+0000b280: 2263 6f6e 7631 222e 0a20 2020 2020 2020  "conv1"..       
+0000b290: 2020 2020 202e 2e2e 2020 2020 2073 656c       ...     sel
+0000b2a0: 662e 636f 6e76 3120 3d20 7375 626d 6f64  f.conv1 = submod
+0000b2b0: 756c 650a 0a20 2020 2020 2020 2020 2020  ule..           
+0000b2c0: 202e 2e2e 2020 2020 2023 2041 6363 6573   ...     # Acces
+0000b2d0: 7369 6e67 2060 7375 626d 6f64 756c 6560  sing `submodule`
+0000b2e0: 2061 7474 7269 6275 7465 7320 6f72 206d   attributes or m
+0000b2f0: 6574 686f 6473 2069 7320 6e6f 7720 7361  ethods is now sa
+0000b300: 6665 2061 6e64 0a20 2020 2020 2020 2020  fe and.         
+0000b310: 2020 202e 2e2e 2020 2020 2023 2065 6974     ...     # eit
+0000b320: 6865 7220 6361 7573 6573 2073 6574 7570  her causes setup
+0000b330: 2829 2074 6f20 6265 2063 616c 6c65 6420  () to be called 
+0000b340: 6f6e 6365 2e0a 0a20 2020 2020 2033 2e20  once...      3. 
+0000b350: 4f6e 6365 2061 206d 6f64 756c 6520 6973  Once a module is
+0000b360: 2063 6f6e 7374 7275 6374 6564 2069 6e73   constructed ins
+0000b370: 6964 6520 6120 6d65 7468 6f64 2077 7261  ide a method wra
+0000b380: 7070 6564 2077 6974 680a 2020 2020 2020  pped with.      
+0000b390: 2020 203a 6d65 7468 3a60 636f 6d70 6163     :meth:`compac
+0000b3a0: 7460 2c20 696d 6d65 6469 6174 656c 7920  t`, immediately 
+0000b3b0: 6265 666f 7265 2061 6e6f 7468 6572 206d  before another m
+0000b3c0: 6574 686f 6420 6973 2063 616c 6c65 6420  ethod is called 
+0000b3d0: 6f72 0a20 2020 2020 2020 2020 6060 7365  or.         ``se
+0000b3e0: 7475 7060 6020 6465 6669 6e65 6420 6174  tup`` defined at
+0000b3f0: 7472 6962 7574 6520 6973 2061 6363 6573  tribute is acces
+0000b400: 7365 642e 0a20 2020 2022 2222 0a20 2020  sed..    """.   
+0000b410: 2070 6173 730a 0a20 2064 6566 205f 7265   pass..  def _re
+0000b420: 6769 7374 6572 5f73 7562 6d6f 6475 6c65  gister_submodule
+0000b430: 7328 7365 6c66 2c20 6e61 6d65 2c20 7661  s(self, name, va
+0000b440: 6c29 3a0a 2020 2020 2222 2252 6567 6973  l):.    """Regis
+0000b450: 7465 7273 2061 2073 7562 6d6f 6475 6c65  ters a submodule
+0000b460: 2e22 2222 0a20 2020 2061 7373 6572 7420  .""".    assert 
+0000b470: 7365 6c66 2e73 636f 7065 2c20 2754 7279  self.scope, 'Try
+0000b480: 696e 6720 746f 2072 6567 6973 7465 7220  ing to register 
+0000b490: 7375 626d 6f64 756c 6573 206f 6e20 756e  submodules on un
+0000b4a0: 626f 756e 6420 7363 6f70 652e 270a 2020  bound scope.'.  
+0000b4b0: 2020 726f 6f74 203d 2073 656c 662e 7363    root = self.sc
+0000b4c0: 6f70 652e 726f 6f74 0a20 2020 2063 6163  ope.root.    cac
+0000b4d0: 6865 203d 205f 6361 6368 6573 2e67 6574  he = _caches.get
+0000b4e0: 2872 6f6f 742c 2077 6561 6b72 6566 2e57  (root, weakref.W
+0000b4f0: 6561 6b56 616c 7565 4469 6374 696f 6e61  eakValueDictiona
+0000b500: 7279 2829 290a 2020 2020 5f63 6163 6865  ry()).    _cache
+0000b510: 735b 726f 6f74 5d20 3d20 6361 6368 650a  s[root] = cache.
+0000b520: 2020 2020 7175 6575 6520 3d20 5b5d 0a20      queue = []. 
+0000b530: 2020 2070 7265 7365 7276 655f 6164 6f70     preserve_adop
+0000b540: 7465 645f 6e61 6d65 7320 3d20 636f 6e66  ted_names = conf
+0000b550: 6967 2e66 6c61 785f 7072 6573 6572 7665  ig.flax_preserve
+0000b560: 5f61 646f 7074 6564 5f6e 616d 6573 0a20  _adopted_names. 
+0000b570: 2020 2069 6620 6861 7361 7474 7228 7479     if hasattr(ty
+0000b580: 7065 2873 656c 6629 2c20 2770 7265 7365  pe(self), 'prese
+0000b590: 7276 655f 6164 6f70 7465 645f 6e61 6d65  rve_adopted_name
+0000b5a0: 7327 293a 0a20 2020 2020 2070 7265 7365  s'):.      prese
+0000b5b0: 7276 655f 6164 6f70 7465 645f 6e61 6d65  rve_adopted_name
+0000b5c0: 7320 3d20 7479 7065 2873 656c 6629 2e70  s = type(self).p
+0000b5d0: 7265 7365 7276 655f 6164 6f70 7465 645f  reserve_adopted_
+0000b5e0: 6e61 6d65 730a 0a20 2020 2064 6566 2061  names..    def a
+0000b5f0: 646f 7074 5f61 7474 725f 6d6f 6475 6c65  dopt_attr_module
+0000b600: 7328 6361 6368 652c 2071 7565 7565 2c20  s(cache, queue, 
+0000b610: 7375 6666 6978 2c20 7375 6276 616c 7565  suffix, subvalue
+0000b620: 293a 0a20 2020 2020 2069 6620 6973 696e  ):.      if isin
+0000b630: 7374 616e 6365 2873 7562 7661 6c75 652c  stance(subvalue,
+0000b640: 204d 6f64 756c 6529 3a0a 2020 2020 2020   Module):.      
+0000b650: 2020 6375 7272 656e 745f 6e61 6d65 203d    current_name =
+0000b660: 2073 7562 7661 6c75 652e 6e61 6d65 0a20   subvalue.name. 
+0000b670: 2020 2020 2020 2061 646f 7074 6564 5f6e         adopted_n
+0000b680: 616d 6520 3d20 4e6f 6e65 0a20 2020 2020  ame = None.     
+0000b690: 2020 2069 6620 7375 6276 616c 7565 2e70     if subvalue.p
+0000b6a0: 6172 656e 7420 6973 204e 6f6e 653a 0a20  arent is None:. 
+0000b6b0: 2020 2020 2020 2020 2023 2050 7265 7365           # Prese
+0000b6c0: 7276 6520 7368 6172 696e 672d 6279 2d72  rve sharing-by-r
+0000b6d0: 6566 6572 656e 6365 2072 656c 6174 696f  eference relatio
+0000b6e0: 6e73 6869 7073 2064 7572 696e 6720 6164  nships during ad
+0000b6f0: 6f70 7469 6f6e 0a20 2020 2020 2020 2020  option.         
+0000b700: 2023 2076 6961 2063 6163 6865 206b 6579   # via cache key
+0000b710: 6564 206f 6e20 756e 6971 7565 2069 6e73  ed on unique ins
+0000b720: 7461 6e63 6520 6964 732e 0a20 2020 2020  tance ids..     
+0000b730: 2020 2020 206b 6579 203d 2073 7562 7661       key = subva
+0000b740: 6c75 652e 5f69 640a 2020 2020 2020 2020  lue._id.        
+0000b750: 2020 2320 4d6f 6475 6c65 2077 6173 2070    # Module was p
+0000b760: 6173 7365 6420 6672 6f6d 206f 7574 7369  assed from outsi
+0000b770: 6465 2e20 4974 206e 6565 6473 2074 6f20  de. It needs to 
+0000b780: 6265 2063 6c6f 6e65 642e 0a20 2020 2020  be cloned..     
+0000b790: 2020 2020 2023 204f 7574 7369 6465 206d       # Outside m
+0000b7a0: 6f64 756c 6573 2061 7265 206e 616d 6564  odules are named
+0000b7b0: 2062 7920 6174 7461 6368 6d65 6e74 2c20   by attachment, 
+0000b7c0: 6e6f 7420 616e 206f 7574 6572 206e 616d  not an outer nam
+0000b7d0: 652c 0a20 2020 2020 2020 2020 2023 2055  e,.          # U
+0000b7e0: 4e4c 4553 5320 7765 2772 6520 7573 696e  NLESS we're usin
+0000b7f0: 6720 6e65 7720 6164 6f70 7465 6420 6e61  g new adopted na
+0000b800: 6d65 2070 6f6c 6963 792c 2069 6e20 7768  me policy, in wh
+0000b810: 6963 6820 6361 7365 2061 6e20 6578 6973  ich case an exis
+0000b820: 7469 6e67 0a20 2020 2020 2020 2020 2023  ting.          #
+0000b830: 206e 616d 6520 7769 6c6c 2062 6520 7573   name will be us
+0000b840: 6564 2c20 6173 2069 7320 6f66 7465 6e20  ed, as is often 
+0000b850: 7375 7070 6c69 6564 2062 7920 636f 6e66  supplied by conf
+0000b860: 6967 2073 7973 7465 6d73 2e0a 2020 2020  ig systems..    
+0000b870: 2020 2020 2020 6966 2070 7265 7365 7276        if preserv
+0000b880: 655f 6164 6f70 7465 645f 6e61 6d65 733a  e_adopted_names:
+0000b890: 0a20 2020 2020 2020 2020 2020 2061 646f  .            ado
+0000b8a0: 7074 6564 5f6e 616d 6520 3d20 6f62 6a65  pted_name = obje
+0000b8b0: 6374 2e5f 5f67 6574 6174 7472 6962 7574  ct.__getattribut
+0000b8c0: 655f 5f28 7375 6276 616c 7565 2c20 276e  e__(subvalue, 'n
+0000b8d0: 616d 6527 290a 2020 2020 2020 2020 2020  ame').          
+0000b8e0: 6966 206b 6579 2069 6e20 6361 6368 653a  if key in cache:
+0000b8f0: 0a20 2020 2020 2020 2020 2020 2073 7562  .            sub
+0000b900: 7661 6c75 6520 3d20 6361 6368 655b 6b65  value = cache[ke
+0000b910: 795d 0a20 2020 2020 2020 2020 2065 6c73  y].          els
+0000b920: 653a 0a20 2020 2020 2020 2020 2020 2073  e:.            s
+0000b930: 7562 7661 6c75 6520 3d20 7375 6276 616c  ubvalue = subval
+0000b940: 7565 2e63 6c6f 6e65 286e 616d 653d 4e6f  ue.clone(name=No
+0000b950: 6e65 290a 2020 2020 2020 2020 2020 2020  ne).            
+0000b960: 6361 6368 655b 6b65 795d 203d 2073 7562  cache[key] = sub
+0000b970: 7661 6c75 650a 2020 2020 2020 2020 6966  value.        if
+0000b980: 2073 7562 7661 6c75 652e 6e61 6d65 2069   subvalue.name i
+0000b990: 7320 4e6f 6e65 3a0a 2020 2020 2020 2020  s None:.        
+0000b9a0: 2020 6f62 6a65 6374 2e5f 5f73 6574 6174    object.__setat
+0000b9b0: 7472 5f5f 2873 7562 7661 6c75 652c 2027  tr__(subvalue, '
+0000b9c0: 7061 7265 6e74 272c 2073 656c 6629 0a20  parent', self). 
+0000b9d0: 2020 2020 2020 2020 2069 6620 6164 6f70           if adop
+0000b9e0: 7465 645f 6e61 6d65 2069 7320 4e6f 6e65  ted_name is None
+0000b9f0: 3a0a 2020 2020 2020 2020 2020 2020 6164  :.            ad
+0000ba00: 6f70 7465 645f 6e61 6d65 203d 2028 0a20  opted_name = (. 
+0000ba10: 2020 2020 2020 2020 2020 2020 2066 277b               f'{
+0000ba20: 6e61 6d65 7d7b 7375 6666 6978 7d27 0a20  name}{suffix}'. 
+0000ba30: 2020 2020 2020 2020 2020 2020 2069 6620               if 
+0000ba40: 6e6f 7420 6973 696e 7374 616e 6365 2873  not isinstance(s
+0000ba50: 7562 7661 6c75 652c 2043 6f6d 7061 6374  ubvalue, Compact
+0000ba60: 4e61 6d65 5363 6f70 6529 0a20 2020 2020  NameScope).     
+0000ba70: 2020 2020 2020 2020 2065 6c73 6520 6375           else cu
+0000ba80: 7272 656e 745f 6e61 6d65 0a20 2020 2020  rrent_name.     
+0000ba90: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       
+0000baa0: 2020 206f 626a 6563 742e 5f5f 7365 7461     object.__seta
+0000bab0: 7474 725f 5f28 7375 6276 616c 7565 2c20  ttr__(subvalue, 
+0000bac0: 276e 616d 6527 2c20 6164 6f70 7465 645f  'name', adopted_
+0000bad0: 6e61 6d65 290a 2020 2020 2020 2020 2020  name).          
+0000bae0: 7175 6575 652e 6170 7065 6e64 2873 7562  queue.append(sub
+0000baf0: 7661 6c75 6529 0a20 2020 2020 2072 6574  value).      ret
+0000bb00: 7572 6e20 7375 6276 616c 7565 0a0a 2020  urn subvalue..  
+0000bb10: 2020 7661 6c20 3d20 5f66 7265 657a 655f    val = _freeze_
+0000bb20: 6174 7472 280a 2020 2020 2020 5f6d 6170  attr(.      _map
+0000bb30: 5f6f 7665 725f 6d6f 6475 6c65 735f 696e  _over_modules_in
+0000bb40: 5f74 7265 6528 0a20 2020 2020 2020 2066  _tree(.        f
+0000bb50: 756e 6374 6f6f 6c73 2e70 6172 7469 616c  unctools.partial
+0000bb60: 2861 646f 7074 5f61 7474 725f 6d6f 6475  (adopt_attr_modu
+0000bb70: 6c65 732c 2063 6163 6865 2c20 7175 6575  les, cache, queu
+0000bb80: 6529 2c20 7661 6c0a 2020 2020 2020 290a  e), val.      ).
+0000bb90: 2020 2020 290a 2020 2020 6f62 6a65 6374      ).    object
+0000bba0: 2e5f 5f73 6574 6174 7472 5f5f 2873 656c  .__setattr__(sel
+0000bbb0: 662c 206e 616d 652c 2076 616c 290a 2020  f, name, val).  
+0000bbc0: 2020 666f 7220 7820 696e 2071 7565 7565    for x in queue
+0000bbd0: 3a0a 2020 2020 2020 782e 5f5f 706f 7374  :.      x.__post
+0000bbe0: 5f69 6e69 745f 5f28 290a 0a20 2064 6566  _init__()..  def
+0000bbf0: 205f 7472 795f 7365 7475 7028 7365 6c66   _try_setup(self
+0000bc00: 2c20 7368 616c 6c6f 773a 2062 6f6f 6c20  , shallow: bool 
+0000bc10: 3d20 4661 6c73 6529 202d 3e20 4e6f 6e65  = False) -> None
+0000bc20: 3a0a 2020 2020 2222 2254 7269 6573 2074  :.    """Tries t
+0000bc30: 6f20 7365 7475 7020 6d6f 6475 6c65 2069  o setup module i
+0000bc40: 6620 7363 6f70 6520 6973 2061 7661 696c  f scope is avail
+0000bc50: 6162 6c65 2061 6e64 2073 6574 7570 2068  able and setup h
+0000bc60: 6173 206e 6f74 2062 6565 6e20 6361 6c6c  as not been call
+0000bc70: 6564 2079 6574 2e22 2222 0a20 2020 2069  ed yet.""".    i
+0000bc80: 6620 280a 2020 2020 2020 7365 6c66 2e73  f (.      self.s
+0000bc90: 636f 7065 0a20 2020 2020 2061 6e64 206e  cope.      and n
+0000bca0: 6f74 2073 656c 662e 5f73 7461 7465 2e69  ot self._state.i
+0000bcb0: 6e5f 7365 7475 700a 2020 2020 2020 616e  n_setup.      an
+0000bcc0: 6420 7365 6c66 2e5f 7374 6174 652e 7365  d self._state.se
+0000bcd0: 7475 705f 6361 6c6c 6564 2021 3d20 5365  tup_called != Se
+0000bce0: 7475 7053 7461 7465 2e44 4f4e 450a 2020  tupState.DONE.  
+0000bcf0: 2020 293a 0a20 2020 2020 2074 7279 3a0a    ):.      try:.
+0000bd00: 2020 2020 2020 2020 7365 6c66 2e5f 7374          self._st
+0000bd10: 6174 652e 696e 5f73 6574 7570 203d 2054  ate.in_setup = T
+0000bd20: 7275 650a 2020 2020 2020 2020 2320 4120  rue.        # A 
+0000bd30: 7368 616c 6c6f 7720 7365 7475 7020 7769  shallow setup wi
+0000bd40: 6c6c 206f 6e6c 7920 7265 6769 7374 6572  ll only register
+0000bd50: 2061 7474 7269 6275 7465 2073 7562 6d6f   attribute submo
+0000bd60: 6475 6c65 7320 6275 7420 6974 2064 6f65  dules but it doe
+0000bd70: 730a 2020 2020 2020 2020 2320 6e6f 7420  s.        # not 
+0000bd80: 6361 6c6c 2074 6865 2075 7365 7227 7320  call the user's 
+0000bd90: 7365 7475 702e 2054 6869 7320 6176 6f69  setup. This avoi
+0000bda0: 6473 2072 756e 6e69 6e67 2062 6566 6f72  ds running befor
+0000bdb0: 6520 610a 2020 2020 2020 2020 2320 7472  e a.        # tr
+0000bdc0: 616e 7366 6f72 6d61 7469 6f6e 2e0a 2020  ansformation..  
+0000bdd0: 2020 2020 2020 666f 7220 6669 656c 6420        for field 
+0000bde0: 696e 2064 6174 6163 6c61 7373 6573 2e66  in dataclasses.f
+0000bdf0: 6965 6c64 7328 7365 6c66 293a 0a20 2020  ields(self):.   
+0000be00: 2020 2020 2020 2069 6620 6669 656c 642e         if field.
+0000be10: 6e61 6d65 206e 6f74 2069 6e20 2827 7061  name not in ('pa
+0000be20: 7265 6e74 272c 2027 6e61 6d65 2729 2061  rent', 'name') a
+0000be30: 6e64 2066 6965 6c64 2e69 6e69 743a 0a20  nd field.init:. 
+0000be40: 2020 2020 2020 2020 2020 2073 656c 662e             self.
+0000be50: 5f72 6567 6973 7465 725f 7375 626d 6f64  _register_submod
+0000be60: 756c 6573 2866 6965 6c64 2e6e 616d 652c  ules(field.name,
+0000be70: 2067 6574 6174 7472 2873 656c 662c 2066   getattr(self, f
+0000be80: 6965 6c64 2e6e 616d 6529 290a 2020 2020  ield.name)).    
+0000be90: 2020 2020 6966 206e 6f74 2073 6861 6c6c      if not shall
+0000bea0: 6f77 3a0a 2020 2020 2020 2020 2020 7365  ow:.          se
+0000beb0: 6c66 2e73 6574 7570 2829 0a20 2020 2020  lf.setup().     
+0000bec0: 2020 2020 2023 2063 7265 6174 6520 4e6f       # create No
+0000bed0: 6e54 7261 6e73 7061 7265 6e74 204d 6f64  nTransparent Mod
+0000bee0: 756c 6573 0a20 2020 2020 2020 2020 2073  ules.          s
+0000bef0: 656c 662e 5f63 6f6d 7061 6374 5f6e 616d  elf._compact_nam
+0000bf00: 655f 7363 6f70 655f 6d6f 6475 6c65 7320  e_scope_modules 
+0000bf10: 3d20 7b0a 2020 2020 2020 2020 2020 2020  = {.            
+0000bf20: 6e61 6d65 3a20 436f 6d70 6163 744e 616d  name: CompactNam
+0000bf30: 6553 636f 7065 280a 2020 2020 2020 2020  eScope(.        
+0000bf40: 2020 2020 2020 6765 7461 7474 7228 7479        getattr(ty
+0000bf50: 7065 2873 656c 6629 2c20 6e61 6d65 292e  pe(self), name).
+0000bf60: 696e 6e65 725f 6675 6e2c 206c 616d 6264  inner_fun, lambd
+0000bf70: 613a 2073 656c 662c 206e 616d 653d 6e61  a: self, name=na
+0000bf80: 6d65 0a20 2020 2020 2020 2020 2020 2029  me.            )
+0000bf90: 0a20 2020 2020 2020 2020 2020 2066 6f72  .            for
+0000bfa0: 206e 616d 6520 696e 2073 656c 662e 5f63   name in self._c
+0000bfb0: 6f6d 7061 6374 5f6e 616d 655f 7363 6f70  ompact_name_scop
+0000bfc0: 655f 6d65 7468 6f64 730a 2020 2020 2020  e_methods.      
+0000bfd0: 2020 2020 7d0a 0a20 2020 2020 2020 2023      }..        #
+0000bfe0: 2057 6520 7275 6e20 7374 6174 6963 2063   We run static c
+0000bff0: 6865 636b 7320 6162 7374 7261 6374 6c79  hecks abstractly
+0000c000: 206f 6e63 6520 666f 7220 7365 7475 7020   once for setup 
+0000c010: 6265 666f 7265 2061 6e79 2074 7261 6e73  before any trans
+0000c020: 666f 726d 730a 2020 2020 2020 2020 2320  forms.        # 
+0000c030: 746f 2064 6574 6563 7420 6e61 6d65 2063  to detect name c
+0000c040: 6f6c 6c69 7369 6f6e 7320 616e 6420 6f74  ollisions and ot
+0000c050: 6865 7220 7079 7468 6f6e 2065 7272 6f72  her python error
+0000c060: 732e 0a20 2020 2020 2020 2065 6c69 6620  s..        elif 
+0000c070: 7365 6c66 2e5f 7374 6174 652e 7365 7475  self._state.setu
+0000c080: 705f 6361 6c6c 6564 203d 3d20 5365 7475  p_called == Setu
+0000c090: 7053 7461 7465 2e4e 4557 3a0a 2020 2020  pState.NEW:.    
+0000c0a0: 2020 2020 2020 7365 6c66 2e5f 7661 6c69        self._vali
+0000c0b0: 6461 7465 5f73 6574 7570 2829 0a20 2020  date_setup().   
+0000c0c0: 2020 2066 696e 616c 6c79 3a0a 2020 2020     finally:.    
+0000c0d0: 2020 2020 7365 6c66 2e5f 7374 6174 652e      self._state.
+0000c0e0: 696e 5f73 6574 7570 203d 2046 616c 7365  in_setup = False
+0000c0f0: 0a20 2020 2020 2020 2069 6620 6e6f 7420  .        if not 
+0000c100: 7368 616c 6c6f 773a 0a20 2020 2020 2020  shallow:.       
+0000c110: 2020 2073 656c 662e 5f73 7461 7465 2e73     self._state.s
+0000c120: 6574 7570 5f63 616c 6c65 6420 3d20 5365  etup_called = Se
+0000c130: 7475 7053 7461 7465 2e44 4f4e 450a 0a20  tupState.DONE.. 
+0000c140: 2064 6566 205f 7661 6c69 6461 7465 5f73   def _validate_s
+0000c150: 6574 7570 2873 656c 6629 202d 3e20 4e6f  etup(self) -> No
+0000c160: 6e65 3a0a 2020 2020 2222 2241 6273 7472  ne:.    """Abstr
+0000c170: 6163 746c 7920 6576 616c 7561 7465 7320  actly evaluates 
+0000c180: 7365 7475 7020 6f6e 6c79 2074 6f20 7275  setup only to ru
+0000c190: 6e20 7374 6174 6963 2063 6865 636b 732e  n static checks.
+0000c1a0: 2222 220a 0a20 2020 2064 6566 2072 756e  """..    def run
+0000c1b0: 5f73 6574 7570 5f6f 6e6c 7928 7829 3a0a  _setup_only(x):.
+0000c1c0: 2020 2020 2020 7772 6170 7065 645f 6964        wrapped_id
+0000c1d0: 203d 2077 7261 705f 6d65 7468 6f64 5f6f   = wrap_method_o
+0000c1e0: 6e63 6528 6c61 6d62 6461 206d 2c20 783a  nce(lambda m, x:
+0000c1f0: 2078 290a 2020 2020 2020 7769 7468 2054   x).      with T
+0000c200: 6573 7453 636f 7065 287b 7d2c 2072 6e67  estScope({}, rng
+0000c210: 733d 7b7d 2c20 6d75 7461 626c 653d 5472  s={}, mutable=Tr
+0000c220: 7565 292e 7465 6d70 6f72 6172 7928 2920  ue).temporary() 
+0000c230: 6173 2072 6f6f 743a 0a20 2020 2020 2020  as root:.       
+0000c240: 2072 6574 7572 6e20 7772 6170 7065 645f   return wrapped_
+0000c250: 6964 2873 656c 662e 636c 6f6e 6528 7061  id(self.clone(pa
+0000c260: 7265 6e74 3d72 6f6f 7429 2c20 7829 0a0a  rent=root), x)..
+0000c270: 2020 2020 5f20 3d20 6a61 782e 6576 616c      _ = jax.eval
+0000c280: 5f73 6861 7065 2872 756e 5f73 6574 7570  _shape(run_setup
+0000c290: 5f6f 6e6c 792c 2030 290a 0a20 2064 6566  _only, 0)..  def
+0000c2a0: 205f 6e61 6d65 5f74 616b 656e 280a 2020   _name_taken(.  
+0000c2b0: 2020 7365 6c66 2c0a 2020 2020 6e61 6d65    self,.    name
+0000c2c0: 3a20 7374 722c 0a20 2020 2072 6575 7365  : str,.    reuse
+0000c2d0: 5f73 636f 7065 733a 2062 6f6f 6c20 3d20  _scopes: bool = 
+0000c2e0: 4661 6c73 652c 0a20 2020 2063 6f6c 6c65  False,.    colle
+0000c2f0: 6374 696f 6e3a 204f 7074 696f 6e61 6c5b  ction: Optional[
+0000c300: 7374 725d 203d 204e 6f6e 652c 0a20 2029  str] = None,.  )
+0000c310: 202d 3e20 626f 6f6c 3a0a 2020 2020 6173   -> bool:.    as
+0000c320: 7365 7274 2073 656c 662e 7363 6f70 6520  sert self.scope 
+0000c330: 6973 206e 6f74 204e 6f6e 650a 2020 2020  is not None.    
+0000c340: 6966 2072 6575 7365 5f73 636f 7065 733a  if reuse_scopes:
+0000c350: 0a20 2020 2020 2072 6574 7572 6e20 4661  .      return Fa
+0000c360: 6c73 650a 2020 2020 7265 7475 726e 2073  lse.    return s
+0000c370: 656c 662e 7363 6f70 652e 6e61 6d65 5f72  elf.scope.name_r
+0000c380: 6573 6572 7665 6428 6e61 6d65 2c20 636f  eserved(name, co
+0000c390: 6c6c 6563 7469 6f6e 290a 0a20 2040 7072  llection)..  @pr
+0000c3a0: 6f70 6572 7479 0a20 2064 6566 205f 696e  operty.  def _in
+0000c3b0: 6974 6961 6c69 7a61 7469 6f6e 5f61 6c6c  itialization_all
+0000c3c0: 6f77 6564 2873 656c 6629 3a0a 2020 2020  owed(self):.    
+0000c3d0: 7265 7475 726e 2028 0a20 2020 2020 206e  return (.      n
+0000c3e0: 6f74 2073 656c 662e 5f73 7461 7465 2e69  ot self._state.i
+0000c3f0: 735f 696e 6974 6961 6c69 7a65 6420 2023  s_initialized  #
+0000c400: 2061 6c6c 6f77 2065 6167 6572 2061 7474   allow eager att
+0000c410: 6163 686d 656e 7420 696e 2070 6f73 742d  achment in post-
+0000c420: 696e 6974 0a20 2020 2020 206f 7220 7365  init.      or se
+0000c430: 6c66 2e5f 7374 6174 652e 696e 5f73 6574  lf._state.in_set
+0000c440: 7570 0a20 2020 2020 206f 7220 7365 6c66  up.      or self
+0000c450: 2e5f 7374 6174 652e 696e 5f63 6f6d 7061  ._state.in_compa
+0000c460: 6374 5f6d 6574 686f 640a 2020 2020 290a  ct_method.    ).
+0000c470: 0a20 2040 7072 6f70 6572 7479 0a20 2064  .  @property.  d
+0000c480: 6566 2070 6174 6828 7365 6c66 293a 0a20  ef path(self):. 
+0000c490: 2020 2069 6620 7365 6c66 2e73 636f 7065     if self.scope
+0000c4a0: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
+0000c4b0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+0000c4c0: 2822 4361 6e27 7420 6163 6365 7373 206d  ("Can't access m
+0000c4d0: 6f64 756c 6520 7061 7468 7320 6f6e 2075  odule paths on u
+0000c4e0: 6e62 6f75 6e64 206d 6f64 756c 6573 2e22  nbound modules."
+0000c4f0: 290a 0a20 2020 2072 6574 7572 6e20 7365  )..    return se
+0000c500: 6c66 2e73 636f 7065 2e70 6174 680a 0a20  lf.scope.path.. 
+0000c510: 2064 6566 2063 6c6f 6e65 280a 2020 2020   def clone(.    
+0000c520: 7365 6c66 3a20 4d2c 0a20 2020 202a 2c0a  self: M,.    *,.
+0000c530: 2020 2020 7061 7265 6e74 3a20 4f70 7469      parent: Opti
+0000c540: 6f6e 616c 5b55 6e69 6f6e 5b53 636f 7065  onal[Union[Scope
+0000c550: 2c20 274d 6f64 756c 6527 2c20 5f53 656e  , 'Module', _Sen
+0000c560: 7469 6e65 6c5d 5d20 3d20 4e6f 6e65 2c0a  tinel]] = None,.
+0000c570: 2020 2020 5f64 6565 705f 636c 6f6e 653a      _deep_clone:
+0000c580: 2055 6e69 6f6e 5b62 6f6f 6c2c 2077 6561   Union[bool, wea
+0000c590: 6b72 6566 2e57 6561 6b56 616c 7565 4469  kref.WeakValueDi
+0000c5a0: 6374 696f 6e61 7279 5d20 3d20 4661 6c73  ctionary] = Fals
+0000c5b0: 652c 0a20 2020 205f 7265 7365 745f 6e61  e,.    _reset_na
+0000c5c0: 6d65 733a 2062 6f6f 6c20 3d20 4661 6c73  mes: bool = Fals
+0000c5d0: 652c 0a20 2020 202a 2a75 7064 6174 6573  e,.    **updates
+0000c5e0: 2c0a 2020 2920 2d3e 204d 3a0a 2020 2020  ,.  ) -> M:.    
+0000c5f0: 2222 2243 7265 6174 6573 2061 2063 6c6f  """Creates a clo
+0000c600: 6e65 206f 6620 7468 6973 204d 6f64 756c  ne of this Modul
+0000c610: 652c 2077 6974 6820 6f70 7469 6f6e 616c  e, with optional
+0000c620: 6c79 2075 7064 6174 6564 2061 7267 756d  ly updated argum
+0000c630: 656e 7473 2e0a 0a20 2020 204e 4f54 453a  ents...    NOTE:
+0000c640: 2065 6e64 2075 7365 7273 2061 7265 2065   end users are e
+0000c650: 6e63 6f75 7261 6765 6420 746f 2075 7365  ncouraged to use
+0000c660: 2074 6865 2060 6063 6f70 7960 6020 6d65   the ``copy`` me
+0000c670: 7468 6f64 2e20 2060 6063 6c6f 6e65 6060  thod.  ``clone``
+0000c680: 2069 7320 7573 6564 0a20 2020 2020 2070   is used.      p
+0000c690: 7269 6d61 7269 6c79 2066 6f72 2069 6e74  rimarily for int
+0000c6a0: 6572 6e61 6c20 726f 7574 696e 6573 2c20  ernal routines, 
+0000c6b0: 616e 6420 6060 636f 7079 6060 206f 6666  and ``copy`` off
+0000c6c0: 6572 7320 7369 6d70 6c65 7220 6172 6775  ers simpler argu
+0000c6d0: 6d65 6e74 7320 616e 640a 2020 2020 2020  ments and.      
+0000c6e0: 6265 7474 6572 2064 6566 6175 6c74 732e  better defaults.
+0000c6f0: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    
+0000c700: 2020 7061 7265 6e74 3a20 5468 6520 7061    parent: The pa
+0000c710: 7265 6e74 206f 6620 7468 6520 636c 6f6e  rent of the clon
+0000c720: 652e 2054 6865 2063 6c6f 6e65 2077 696c  e. The clone wil
+0000c730: 6c20 6861 7665 206e 6f20 7061 7265 6e74  l have no parent
+0000c740: 2069 6620 6e6f 0a20 2020 2020 2020 2065   if no.        e
+0000c750: 7870 6c69 6369 7420 7061 7265 6e74 2069  xplicit parent i
+0000c760: 7320 7370 6563 6966 6965 642e 0a20 2020  s specified..   
+0000c770: 2020 205f 6465 6570 5f63 6c6f 6e65 3a20     _deep_clone: 
+0000c780: 4120 626f 6f6c 6561 6e20 6f72 2061 2077  A boolean or a w
+0000c790: 6561 6b20 7661 6c75 6520 6469 6374 696f  eak value dictio
+0000c7a0: 6e61 7279 2074 6f20 636f 6e74 726f 6c20  nary to control 
+0000c7b0: 6465 6570 2063 6c6f 6e69 6e67 0a20 2020  deep cloning.   
+0000c7c0: 2020 2020 206f 6620 7375 626d 6f64 756c       of submodul
+0000c7d0: 6573 2e20 4966 2054 7275 652c 2073 7562  es. If True, sub
+0000c7e0: 6d6f 6475 6c65 7320 7769 6c6c 2062 6520  modules will be 
+0000c7f0: 636c 6f6e 6564 2072 6563 7572 7369 7665  cloned recursive
+0000c800: 6c79 2e20 4966 2061 2077 6561 6b0a 2020  ly. If a weak.  
+0000c810: 2020 2020 2020 7661 6c75 6520 6469 6374        value dict
+0000c820: 696f 6e61 7279 2069 7320 7061 7373 6564  ionary is passed
+0000c830: 2c20 6974 2077 696c 6c20 6265 2075 7365  , it will be use
+0000c840: 6420 746f 2063 6163 6865 2063 6c6f 6e65  d to cache clone
+0000c850: 6420 7375 626d 6f64 756c 6573 2e0a 2020  d submodules..  
+0000c860: 2020 2020 2020 5468 6973 2066 6c61 6720        This flag 
+0000c870: 6973 2075 7365 6420 6279 2069 6e69 742f  is used by init/
+0000c880: 6170 706c 792f 6269 6e64 2074 6f20 6176  apply/bind to av
+0000c890: 6f69 6420 7363 6f70 6520 6c65 616b 6167  oid scope leakag
+0000c8a0: 652e 0a20 2020 2020 205f 7265 7365 745f  e..      _reset_
+0000c8b0: 6e61 6d65 733a 2049 6620 5472 7565 2c20  names: If True, 
+0000c8c0: 6060 6e61 6d65 3d4e 6f6e 6560 6020 6973  ``name=None`` is
+0000c8d0: 2061 6c73 6f20 7061 7373 6564 2074 6f20   also passed to 
+0000c8e0: 7375 626d 6f64 756c 6573 2077 6865 6e0a  submodules when.
+0000c8f0: 2020 2020 2020 2020 636c 6f6e 696e 672e          cloning.
+0000c900: 2052 6573 6574 7469 6e67 206e 616d 6573   Resetting names
+0000c910: 2069 6e20 7375 626d 6f64 756c 6573 2069   in submodules i
+0000c920: 7320 6e65 6365 7373 6172 7920 7768 656e  s necessary when
+0000c930: 2063 616c 6c69 6e67 2060 602e 756e 6269   calling ``.unbi
+0000c940: 6e64 6060 2e0a 2020 2020 2020 2a2a 7570  nd``..      **up
+0000c950: 6461 7465 733a 2041 7474 7269 6275 7465  dates: Attribute
+0000c960: 2075 7064 6174 6573 2e0a 0a20 2020 2052   updates...    R
+0000c970: 6574 7572 6e73 3a0a 2020 2020 2020 4120  eturns:.      A 
+0000c980: 636c 6f6e 6520 6f66 2074 6865 2074 6869  clone of the thi
+0000c990: 7320 4d6f 6475 6c65 2077 6974 6820 7468  s Module with th
+0000c9a0: 6520 7570 6461 7465 6420 6174 7472 6962  e updated attrib
+0000c9b0: 7574 6573 2061 6e64 2070 6172 656e 742e  utes and parent.
+0000c9c0: 0a20 2020 2022 2222 0a20 2020 2061 7474  .    """.    att
+0000c9d0: 7273 203d 207b 0a20 2020 2020 2066 2e6e  rs = {.      f.n
+0000c9e0: 616d 653a 2067 6574 6174 7472 2873 656c  ame: getattr(sel
+0000c9f0: 662c 2066 2e6e 616d 6529 2066 6f72 2066  f, f.name) for f
+0000ca00: 2069 6e20 6461 7461 636c 6173 7365 732e   in dataclasses.
+0000ca10: 6669 656c 6473 2873 656c 6629 2069 6620  fields(self) if 
+0000ca20: 662e 696e 6974 0a20 2020 207d 0a0a 2020  f.init.    }..  
+0000ca30: 2020 6174 7472 732e 7570 6461 7465 2870    attrs.update(p
+0000ca40: 6172 656e 743d 7061 7265 6e74 2c20 2a2a  arent=parent, **
+0000ca50: 7570 6461 7465 7329 0a0a 2020 2020 2320  updates)..    # 
+0000ca60: 4865 7265 2077 6520 696d 706c 656d 656e  Here we implemen
+0000ca70: 7420 6465 6570 2063 6c6f 6e69 6e67 206f  t deep cloning o
+0000ca80: 6620 7375 626d 6f64 756c 6573 2c20 7468  f submodules, th
+0000ca90: 6973 2069 7320 6e65 6365 7373 6172 7920  is is necessary 
+0000caa0: 746f 2061 766f 6964 2073 636f 7065 206c  to avoid scope l
+0000cab0: 6561 6b61 6765 0a20 2020 2023 2066 726f  eakage.    # fro
+0000cac0: 6d20 6578 7465 726e 616c 2073 7562 6d6f  m external submo
+0000cad0: 6475 6c65 7320 696e 746f 2069 6e69 742f  dules into init/
+0000cae0: 6170 706c 792f 6269 6e64 2077 6869 6c65  apply/bind while
+0000caf0: 2070 7265 7365 7276 696e 6720 7368 6172   preserving shar
+0000cb00: 696e 672d 6279 2d72 6566 6572 656e 6365  ing-by-reference
+0000cb10: 0a20 2020 2023 2072 656c 6174 696f 6e73  .    # relations
+0000cb20: 6869 7073 2062 6574 7765 656e 2073 7562  hips between sub
+0000cb30: 6d6f 6475 6c65 732e 0a20 2020 2069 6620  modules..    if 
+0000cb40: 5f64 6565 705f 636c 6f6e 6520 213d 2046  _deep_clone != F
+0000cb50: 616c 7365 3a0a 2020 2020 2020 2320 5765  alse:.      # We
+0000cb60: 2075 7365 2061 2077 6561 6b20 7661 6c75   use a weak valu
+0000cb70: 6520 6469 6374 696f 6e61 7279 2074 6f20  e dictionary to 
+0000cb80: 6361 6368 6520 636c 6f6e 6564 2073 7562  cache cloned sub
+0000cb90: 6d6f 6475 6c65 732e 2057 6865 6e20 6120  modules. When a 
+0000cba0: 7368 6172 6564 0a20 2020 2020 2023 2073  shared.      # s
+0000cbb0: 7562 6d6f 6475 6c65 2069 7320 636c 6f6e  ubmodule is clon
+0000cbc0: 6564 2c20 6974 7320 6f6e 6c79 2063 6c6f  ed, its only clo
+0000cbd0: 6e65 6420 6f6e 6365 2065 6c73 6520 6974  ned once else it
+0000cbe0: 7320 6665 7463 6865 6420 6672 6f6d 2074  s fetched from t
+0000cbf0: 6865 2063 6163 6865 2e0a 2020 2020 2020  he cache..      
+0000cc00: 6361 6368 6520 3d20 280a 2020 2020 2020  cache = (.      
+0000cc10: 2020 7765 616b 7265 662e 5765 616b 5661    weakref.WeakVa
+0000cc20: 6c75 6544 6963 7469 6f6e 6172 7928 290a  lueDictionary().
+0000cc30: 2020 2020 2020 2020 6966 2069 7369 6e73          if isins
+0000cc40: 7461 6e63 6528 5f64 6565 705f 636c 6f6e  tance(_deep_clon
+0000cc50: 652c 2062 6f6f 6c29 0a20 2020 2020 2020  e, bool).       
+0000cc60: 2065 6c73 6520 5f64 6565 705f 636c 6f6e   else _deep_clon
+0000cc70: 650a 2020 2020 2020 290a 0a20 2020 2020  e.      )..     
+0000cc80: 2064 6566 2063 6c6f 6e65 5f66 6e28 6d3a   def clone_fn(m:
+0000cc90: 204d 6f64 756c 6529 202d 3e20 4d6f 6475   Module) -> Modu
+0000cca0: 6c65 3a0a 2020 2020 2020 2020 6966 2068  le:.        if h
+0000ccb0: 6173 6174 7472 286d 2c20 275f 6964 2729  asattr(m, '_id')
+0000ccc0: 3a0a 2020 2020 2020 2020 2020 6b65 7920  :.          key 
+0000ccd0: 3d20 6d2e 5f69 640a 2020 2020 2020 2020  = m._id.        
+0000cce0: 2020 6966 206b 6579 2069 6e20 6361 6368    if key in cach
+0000ccf0: 653a 0a20 2020 2020 2020 2020 2020 2072  e:.            r
+0000cd00: 6574 7572 6e20 6361 6368 655b 6b65 795d  eturn cache[key]
+0000cd10: 0a20 2020 2020 2020 2020 2065 6c73 653a  .          else:
+0000cd20: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
+0000cd30: 5f72 6573 6574 5f6e 616d 6573 3a0a 2020  _reset_names:.  
+0000cd40: 2020 2020 2020 2020 2020 2020 636c 6f6e              clon
+0000cd50: 6520 3d20 6d2e 636c 6f6e 6528 0a20 2020  e = m.clone(.   
+0000cd60: 2020 2020 2020 2020 2020 2020 205f 6465               _de
+0000cd70: 6570 5f63 6c6f 6e65 3d63 6163 6865 2c20  ep_clone=cache, 
+0000cd80: 5f72 6573 6574 5f6e 616d 6573 3d5f 7265  _reset_names=_re
+0000cd90: 7365 745f 6e61 6d65 732c 206e 616d 653d  set_names, name=
+0000cda0: 4e6f 6e65 0a20 2020 2020 2020 2020 2020  None.           
+0000cdb0: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
+0000cdc0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
+0000cdd0: 2020 2020 2063 6c6f 6e65 203d 206d 2e63       clone = m.c
+0000cde0: 6c6f 6e65 285f 6465 6570 5f63 6c6f 6e65  lone(_deep_clone
+0000cdf0: 3d63 6163 6865 290a 2020 2020 2020 2020  =cache).        
+0000ce00: 2020 2020 6361 6368 655b 6b65 795d 203d      cache[key] =
+0000ce10: 2063 6c6f 6e65 0a20 2020 2020 2020 2020   clone.         
+0000ce20: 2020 2072 6574 7572 6e20 636c 6f6e 650a     return clone.
+0000ce30: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
+0000ce40: 2020 2020 2020 2020 2320 4966 2074 6865          # If the
+0000ce50: 206d 6f64 756c 6520 646f 6573 6e27 7420   module doesn't 
+0000ce60: 6861 7665 2061 6e20 5f69 6420 6174 7472  have an _id attr
+0000ce70: 6962 7574 6520 6974 2063 6f75 6c64 2062  ibute it could b
+0000ce80: 6520 6120 6d6f 636b 206f 626a 6563 740a  e a mock object.
+0000ce90: 2020 2020 2020 2020 2020 2320 736f 2077            # so w
+0000cea0: 6520 7265 7475 726e 2069 7420 6173 2069  e return it as i
+0000ceb0: 732e 0a20 2020 2020 2020 2020 2072 6574  s..          ret
+0000cec0: 7572 6e20 6d0a 0a20 2020 2020 2023 205f  urn m..      # _
+0000ced0: 6d61 705f 7375 626d 6f64 756c 6573 2077  map_submodules w
+0000cee0: 696c 6c20 6d61 7020 6f76 6572 2061 6c6c  ill map over all
+0000cef0: 2073 7562 6d6f 6475 6c65 7320 696e 7369   submodules insi
+0000cf00: 6465 2061 7474 7273 0a20 2020 2020 2023  de attrs.      #
+0000cf10: 2076 616c 7565 2068 6572 6520 6361 6e20   value here can 
+0000cf20: 6265 2061 6e79 2070 7974 7265 652c 206e  be any pytree, n
+0000cf30: 6f6e 2d6d 6f64 756c 6520 7661 6c75 6573  on-module values
+0000cf40: 2061 7265 2069 676e 6f72 6564 0a20 2020   are ignored.   
+0000cf50: 2020 2066 6f72 2066 6965 6c64 5f6e 616d     for field_nam
+0000cf60: 652c 2076 616c 7565 2069 6e20 6174 7472  e, value in attr
+0000cf70: 732e 6974 656d 7328 293a 0a20 2020 2020  s.items():.     
+0000cf80: 2020 2069 6620 6669 656c 645f 6e61 6d65     if field_name
+0000cf90: 203d 3d20 2770 6172 656e 7427 3a0a 2020   == 'parent':.  
+0000cfa0: 2020 2020 2020 2020 636f 6e74 696e 7565          continue
+0000cfb0: 0a20 2020 2020 2020 2061 7474 7273 5b66  .        attrs[f
+0000cfc0: 6965 6c64 5f6e 616d 655d 203d 205f 6d61  ield_name] = _ma
+0000cfd0: 705f 7375 626d 6f64 756c 6573 2863 6c6f  p_submodules(clo
+0000cfe0: 6e65 5f66 6e2c 2076 616c 7565 290a 0a20  ne_fn, value).. 
+0000cff0: 2020 206d 6f64 756c 6520 3d20 7365 6c66     module = self
+0000d000: 2e5f 5f63 6c61 7373 5f5f 282a 2a61 7474  .__class__(**att
+0000d010: 7273 290a 0a20 2020 2072 6574 7572 6e20  rs)..    return 
+0000d020: 6d6f 6475 6c65 0a0a 2020 6465 6620 636f  module..  def co
+0000d030: 7079 280a 2020 2020 7365 6c66 3a20 4d2c  py(.    self: M,
+0000d040: 0a20 2020 202a 2c0a 2020 2020 7061 7265  .    *,.    pare
+0000d050: 6e74 3a20 4f70 7469 6f6e 616c 5b55 6e69  nt: Optional[Uni
+0000d060: 6f6e 5b53 636f 7065 2c20 274d 6f64 756c  on[Scope, 'Modul
+0000d070: 6527 2c20 5f53 656e 7469 6e65 6c5d 5d20  e', _Sentinel]] 
+0000d080: 3d20 5f75 6e73 7065 6369 6669 6564 5f70  = _unspecified_p
+0000d090: 6172 656e 742c 0a20 2020 206e 616d 653a  arent,.    name:
+0000d0a0: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
+0000d0b0: 204e 6f6e 652c 0a20 2020 202a 2a75 7064   None,.    **upd
+0000d0c0: 6174 6573 2c0a 2020 2920 2d3e 204d 3a0a  ates,.  ) -> M:.
+0000d0d0: 2020 2020 2222 2243 7265 6174 6573 2061      """Creates a
+0000d0e0: 2063 6f70 7920 6f66 2074 6869 7320 4d6f   copy of this Mo
+0000d0f0: 6475 6c65 2c20 7769 7468 206f 7074 696f  dule, with optio
+0000d100: 6e61 6c6c 7920 7570 6461 7465 6420 6172  nally updated ar
+0000d110: 6775 6d65 6e74 732e 0a0a 2020 2020 4172  guments...    Ar
+0000d120: 6773 3a0a 2020 2020 2020 7061 7265 6e74  gs:.      parent
+0000d130: 3a20 5468 6520 7061 7265 6e74 206f 6620  : The parent of 
+0000d140: 7468 6520 636f 7079 2e20 2042 7920 6465  the copy.  By de
+0000d150: 6661 756c 7420 7468 6520 6375 7272 656e  fault the curren
+0000d160: 7420 6d6f 6475 6c65 2069 7320 7461 6b65  t module is take
+0000d170: 6e0a 2020 2020 2020 2020 6173 2070 6172  n.        as par
+0000d180: 656e 7420 6966 206e 6f74 2065 7870 6c69  ent if not expli
+0000d190: 6369 746c 7920 7370 6563 6966 6965 642e  citly specified.
+0000d1a0: 0a20 2020 2020 206e 616d 653a 2041 206e  .      name: A n
+0000d1b0: 6577 206e 616d 6520 666f 7220 7468 6520  ew name for the 
+0000d1c0: 636f 7069 6564 204d 6f64 756c 652c 2062  copied Module, b
+0000d1d0: 7920 6465 6661 756c 7420 6120 6e65 7720  y default a new 
+0000d1e0: 6175 746f 6d61 7469 6320 6e61 6d65 0a20  automatic name. 
+0000d1f0: 2020 2020 2020 2077 696c 6c20 6265 2067         will be g
+0000d200: 6976 656e 2e0a 2020 2020 2020 2a2a 7570  iven..      **up
+0000d210: 6461 7465 733a 2041 7474 7269 6275 7465  dates: Attribute
+0000d220: 2075 7064 6174 6573 2e0a 0a20 2020 2052   updates...    R
+0000d230: 6574 7572 6e73 3a0a 2020 2020 2020 4120  eturns:.      A 
+0000d240: 636f 7079 206f 6620 7468 6520 7468 6973  copy of the this
+0000d250: 204d 6f64 756c 6520 7769 7468 2074 6865   Module with the
+0000d260: 2075 7064 6174 6564 206e 616d 652c 2070   updated name, p
+0000d270: 6172 656e 742c 2061 6e64 2061 7474 7269  arent, and attri
+0000d280: 6275 7465 732e 0a20 2020 2022 2222 0a20  butes..    """. 
+0000d290: 2020 2072 6574 7572 6e20 7365 6c66 2e63     return self.c
+0000d2a0: 6c6f 6e65 280a 2020 2020 2020 7061 7265  lone(.      pare
+0000d2b0: 6e74 3d70 6172 656e 742c 206e 616d 653d  nt=parent, name=
+0000d2c0: 6e61 6d65 2c20 5f64 6565 705f 636c 6f6e  name, _deep_clon
+0000d2d0: 653d 5472 7565 2c20 5f72 6573 6574 5f6e  e=True, _reset_n
+0000d2e0: 616d 6573 3d46 616c 7365 2c20 2a2a 7570  ames=False, **up
+0000d2f0: 6461 7465 730a 2020 2020 290a 0a20 2040  dates.    )..  @
+0000d300: 6f76 6572 6c6f 6164 0a20 2064 6566 2076  overload.  def v
+0000d310: 6172 6961 626c 6528 0a20 2020 2073 656c  ariable(.    sel
+0000d320: 662c 0a20 2020 2063 6f6c 3a20 7374 722c  f,.    col: str,
+0000d330: 0a20 2020 206e 616d 653a 2073 7472 2c0a  .    name: str,.
+0000d340: 2020 2020 696e 6974 5f66 6e3a 204f 7074      init_fn: Opt
+0000d350: 696f 6e61 6c5b 4361 6c6c 6162 6c65 5b2e  ional[Callable[.
+0000d360: 2e2e 2c20 545d 5d20 3d20 4e6f 6e65 2c0a  .., T]] = None,.
+0000d370: 2020 2020 2a69 6e69 745f 6172 6773 2c0a      *init_args,.
+0000d380: 2020 2920 2d3e 2056 6172 6961 626c 655b    ) -> Variable[
+0000d390: 545d 3a0a 2020 2020 2e2e 2e0a 0a20 2040  T]:.    .....  @
+0000d3a0: 6f76 6572 6c6f 6164 0a20 2064 6566 2076  overload.  def v
+0000d3b0: 6172 6961 626c 6528 0a20 2020 2073 656c  ariable(.    sel
+0000d3c0: 662c 0a20 2020 2063 6f6c 3a20 7374 722c  f,.    col: str,
+0000d3d0: 0a20 2020 206e 616d 653a 2073 7472 2c0a  .    name: str,.
+0000d3e0: 2020 2020 696e 6974 5f66 6e3a 204f 7074      init_fn: Opt
+0000d3f0: 696f 6e61 6c5b 4361 6c6c 6162 6c65 5b2e  ional[Callable[.
+0000d400: 2e2e 2c20 545d 5d20 3d20 4e6f 6e65 2c0a  .., T]] = None,.
+0000d410: 2020 2020 2a69 6e69 745f 6172 6773 2c0a      *init_args,.
+0000d420: 2020 2020 756e 626f 783a 204c 6974 6572      unbox: Liter
+0000d430: 616c 5b54 7275 655d 2c0a 2020 2020 2a2a  al[True],.    **
+0000d440: 696e 6974 5f6b 7761 7267 732c 0a20 2029  init_kwargs,.  )
+0000d450: 202d 3e20 5661 7269 6162 6c65 5b54 5d3a   -> Variable[T]:
+0000d460: 0a20 2020 202e 2e2e 0a0a 2020 406f 7665  .    .....  @ove
+0000d470: 726c 6f61 640a 2020 6465 6620 7661 7269  rload.  def vari
+0000d480: 6162 6c65 280a 2020 2020 7365 6c66 2c0a  able(.    self,.
+0000d490: 2020 2020 636f 6c3a 2073 7472 2c0a 2020      col: str,.  
+0000d4a0: 2020 6e61 6d65 3a20 7374 722c 0a20 2020    name: str,.   
+0000d4b0: 2069 6e69 745f 666e 3a20 4f70 7469 6f6e   init_fn: Option
+0000d4c0: 616c 5b43 616c 6c61 626c 655b 2e2e 2e2c  al[Callable[...,
+0000d4d0: 2054 5d5d 203d 204e 6f6e 652c 0a20 2020   T]] = None,.   
+0000d4e0: 202a 696e 6974 5f61 7267 732c 0a20 2020   *init_args,.   
+0000d4f0: 2075 6e62 6f78 3a20 4c69 7465 7261 6c5b   unbox: Literal[
+0000d500: 4661 6c73 655d 2c0a 2020 2020 2a2a 696e  False],.    **in
+0000d510: 6974 5f6b 7761 7267 732c 0a20 2029 202d  it_kwargs,.  ) -
+0000d520: 3e20 5661 7269 6162 6c65 5b6d 6574 612e  > Variable[meta.
+0000d530: 4178 6973 4d65 7461 6461 7461 5b54 5d5d  AxisMetadata[T]]
+0000d540: 3a0a 2020 2020 2e2e 2e0a 0a20 2040 6f76  :.    .....  @ov
+0000d550: 6572 6c6f 6164 0a20 2064 6566 2076 6172  erload.  def var
+0000d560: 6961 626c 6528 0a20 2020 2073 656c 662c  iable(.    self,
+0000d570: 0a20 2020 2063 6f6c 3a20 7374 722c 0a20  .    col: str,. 
+0000d580: 2020 206e 616d 653a 2073 7472 2c0a 2020     name: str,.  
+0000d590: 2020 696e 6974 5f66 6e3a 204f 7074 696f    init_fn: Optio
+0000d5a0: 6e61 6c5b 4361 6c6c 6162 6c65 5b2e 2e2e  nal[Callable[...
+0000d5b0: 2c20 545d 5d20 3d20 4e6f 6e65 2c0a 2020  , T]] = None,.  
+0000d5c0: 2020 2a69 6e69 745f 6172 6773 2c0a 2020    *init_args,.  
+0000d5d0: 2020 756e 626f 783a 2062 6f6f 6c20 3d20    unbox: bool = 
+0000d5e0: 5472 7565 2c0a 2020 2020 2a2a 696e 6974  True,.    **init
+0000d5f0: 5f6b 7761 7267 732c 0a20 2029 202d 3e20  _kwargs,.  ) -> 
+0000d600: 556e 696f 6e5b 5661 7269 6162 6c65 5b54  Union[Variable[T
+0000d610: 5d2c 2056 6172 6961 626c 655b 6d65 7461  ], Variable[meta
+0000d620: 2e41 7869 734d 6574 6164 6174 615b 545d  .AxisMetadata[T]
+0000d630: 5d5d 3a0a 2020 2020 2e2e 2e0a 0a20 2064  ]]:.    .....  d
+0000d640: 6566 2076 6172 6961 626c 6528 0a20 2020  ef variable(.   
+0000d650: 2073 656c 662c 0a20 2020 2063 6f6c 3a20   self,.    col: 
+0000d660: 7374 722c 0a20 2020 206e 616d 653a 2073  str,.    name: s
+0000d670: 7472 2c0a 2020 2020 696e 6974 5f66 6e3a  tr,.    init_fn:
+0000d680: 204f 7074 696f 6e61 6c5b 4361 6c6c 6162   Optional[Callab
+0000d690: 6c65 5b2e 2e2e 2c20 545d 5d20 3d20 4e6f  le[..., T]] = No
+0000d6a0: 6e65 2c0a 2020 2020 2a69 6e69 745f 6172  ne,.    *init_ar
+0000d6b0: 6773 2c0a 2020 2020 756e 626f 783a 2062  gs,.    unbox: b
+0000d6c0: 6f6f 6c20 3d20 5472 7565 2c0a 2020 2020  ool = True,.    
+0000d6d0: 2a2a 696e 6974 5f6b 7761 7267 732c 0a20  **init_kwargs,. 
+0000d6e0: 2029 202d 3e20 556e 696f 6e5b 5661 7269   ) -> Union[Vari
+0000d6f0: 6162 6c65 5b54 5d2c 2056 6172 6961 626c  able[T], Variabl
+0000d700: 655b 6d65 7461 2e41 7869 734d 6574 6164  e[meta.AxisMetad
+0000d710: 6174 615b 545d 5d5d 3a0a 2020 2020 2222  ata[T]]]:.    ""
+0000d720: 2244 6563 6c61 7265 7320 616e 6420 7265  "Declares and re
+0000d730: 7475 726e 7320 6120 7661 7269 6162 6c65  turns a variable
+0000d740: 2069 6e20 7468 6973 204d 6f64 756c 652e   in this Module.
+0000d750: 0a0a 2020 2020 5365 6520 3a6d 6f64 3a60  ..    See :mod:`
+0000d760: 666c 6178 2e63 6f72 652e 7661 7269 6162  flax.core.variab
+0000d770: 6c65 7360 2066 6f72 206d 6f72 6520 696e  les` for more in
+0000d780: 666f 726d 6174 696f 6e2e 2053 6565 2061  formation. See a
+0000d790: 6c73 6f20 3a6d 6574 683a 6070 6172 616d  lso :meth:`param
+0000d7a0: 600a 2020 2020 666f 7220 6120 7368 6f72  `.    for a shor
+0000d7b0: 7468 616e 6420 7761 7920 746f 2064 6566  thand way to def
+0000d7c0: 696e 6520 7265 6164 2d6f 6e6c 7920 7661  ine read-only va
+0000d7d0: 7269 6162 6c65 7320 696e 2074 6865 2022  riables in the "
+0000d7e0: 7061 7261 6d73 220a 2020 2020 636f 6c6c  params".    coll
+0000d7f0: 6563 7469 6f6e 2e0a 0a20 2020 2043 6f6e  ection...    Con
+0000d800: 7472 6172 7920 746f 203a 6d65 7468 3a60  trary to :meth:`
+0000d810: 7061 7261 6d60 2c20 616c 6c20 6172 6775  param`, all argu
+0000d820: 6d65 6e74 7320 7061 7373 696e 6720 7573  ments passing us
+0000d830: 696e 6720 6060 696e 6974 5f66 6e60 6020  ing ``init_fn`` 
+0000d840: 7368 6f75 6c64 2062 650a 2020 2020 7061  should be.    pa
+0000d850: 7373 6564 206f 6e20 6578 706c 6963 6974  ssed on explicit
+0000d860: 6c79 3a3a 0a0a 2020 2020 2020 3e3e 3e20  ly::..      >>> 
+0000d870: 636c 6173 7320 466f 6f28 6e6e 2e4d 6f64  class Foo(nn.Mod
+0000d880: 756c 6529 3a0a 2020 2020 2020 2e2e 2e20  ule):.      ... 
+0000d890: 2020 406e 6e2e 636f 6d70 6163 740a 2020    @nn.compact.  
+0000d8a0: 2020 2020 2e2e 2e20 2020 6465 6620 5f5f      ...   def __
+0000d8b0: 6361 6c6c 5f5f 2873 656c 662c 2078 293a  call__(self, x):
+0000d8c0: 0a20 2020 2020 202e 2e2e 2020 2020 2078  .      ...     x
+0000d8d0: 203d 206e 6e2e 4465 6e73 6528 3429 2878   = nn.Dense(4)(x
+0000d8e0: 290a 2020 2020 2020 2e2e 2e20 2020 2020  ).      ...     
+0000d8f0: 6b65 7920 3d20 7365 6c66 2e6d 616b 655f  key = self.make_
+0000d900: 726e 6728 2773 7461 7473 2729 0a20 2020  rng('stats').   
+0000d910: 2020 202e 2e2e 2020 2020 206d 6561 6e20     ...     mean 
+0000d920: 3d20 7365 6c66 2e76 6172 6961 626c 6528  = self.variable(
+0000d930: 2773 7461 7473 272c 2027 6d65 616e 272c  'stats', 'mean',
+0000d940: 206e 6e2e 696e 6974 6961 6c69 7a65 7273   nn.initializers
+0000d950: 2e6c 6563 756e 5f6e 6f72 6d61 6c28 292c  .lecun_normal(),
+0000d960: 206b 6579 2c20 782e 7368 6170 6529 0a20   key, x.shape). 
+0000d970: 2020 2020 202e 2e2e 2020 2020 202e 2e2e       ...     ...
+0000d980: 0a20 2020 2020 202e 2e2e 2020 2020 2072  .      ...     r
+0000d990: 6574 7572 6e20 7820 2a20 6d65 616e 2e76  eturn x * mean.v
+0000d9a0: 616c 7565 0a20 2020 2020 203e 3e3e 2076  alue.      >>> v
+0000d9b0: 6172 6961 626c 6573 203d 2046 6f6f 2829  ariables = Foo()
+0000d9c0: 2e69 6e69 7428 7b27 7061 7261 6d73 273a  .init({'params':
+0000d9d0: 206a 6178 2e72 616e 646f 6d2e 6b65 7928   jax.random.key(
+0000d9e0: 3029 2c20 2773 7461 7473 273a 206a 6178  0), 'stats': jax
+0000d9f0: 2e72 616e 646f 6d2e 6b65 7928 3129 7d2c  .random.key(1)},
+0000da00: 206a 6e70 2e6f 6e65 7328 2832 2c20 3329   jnp.ones((2, 3)
+0000da10: 2929 0a20 2020 2020 203e 3e3e 206a 6178  )).      >>> jax
+0000da20: 2e74 7265 655f 7574 696c 2e74 7265 655f  .tree_util.tree_
+0000da30: 6d61 7028 6a6e 702e 7368 6170 652c 2076  map(jnp.shape, v
+0000da40: 6172 6961 626c 6573 290a 2020 2020 2020  ariables).      
+0000da50: 7b27 7061 7261 6d73 273a 207b 2744 656e  {'params': {'Den
+0000da60: 7365 5f30 273a 207b 2762 6961 7327 3a20  se_0': {'bias': 
+0000da70: 2834 2c29 2c20 276b 6572 6e65 6c27 3a20  (4,), 'kernel': 
+0000da80: 2833 2c20 3429 7d7d 2c20 2773 7461 7473  (3, 4)}}, 'stats
+0000da90: 273a 207b 276d 6561 6e27 3a20 2832 2c20  ': {'mean': (2, 
+0000daa0: 3429 7d7d 0a0a 2020 2020 496e 2074 6865  4)}}..    In the
+0000dab0: 2065 7861 6d70 6c65 2061 626f 7665 2c20   example above, 
+0000dac0: 7468 6520 6675 6e63 7469 6f6e 2060 606c  the function ``l
+0000dad0: 6563 756e 5f6e 6f72 6d61 6c60 6020 6578  ecun_normal`` ex
+0000dae0: 7065 6374 7320 7477 6f20 6172 6775 6d65  pects two argume
+0000daf0: 6e74 733a 0a20 2020 2060 606b 6579 6060  nts:.    ``key``
+0000db00: 2061 6e64 2060 6073 6861 7065 6060 2c20   and ``shape``, 
+0000db10: 616e 6420 626f 7468 2068 6176 6520 746f  and both have to
+0000db20: 2062 6520 7061 7373 6564 206f 6e2e 2054   be passed on. T
+0000db30: 6865 2050 524e 4720 666f 7220 6060 7374  he PRNG for ``st
+0000db40: 6174 7360 600a 2020 2020 6861 7320 746f  ats``.    has to
+0000db50: 2062 6520 7072 6f76 6964 6564 2065 7870   be provided exp
+0000db60: 6c69 6369 746c 7920 7768 656e 2063 616c  licitly when cal
+0000db70: 6c69 6e67 203a 6d65 7468 3a60 696e 6974  ling :meth:`init
+0000db80: 6020 616e 6420 3a6d 6574 683a 6061 7070  ` and :meth:`app
+0000db90: 6c79 602e 0a0a 2020 2020 4172 6773 3a0a  ly`...    Args:.
+0000dba0: 2020 2020 2020 636f 6c3a 2054 6865 2076        col: The v
+0000dbb0: 6172 6961 626c 6520 636f 6c6c 6563 7469  ariable collecti
+0000dbc0: 6f6e 206e 616d 652e 0a20 2020 2020 206e  on name..      n
+0000dbd0: 616d 653a 2054 6865 2076 6172 6961 626c  ame: The variabl
+0000dbe0: 6520 6e61 6d65 2e0a 2020 2020 2020 696e  e name..      in
+0000dbf0: 6974 5f66 6e3a 2054 6865 2066 756e 6374  it_fn: The funct
+0000dc00: 696f 6e20 7468 6174 2077 696c 6c20 6265  ion that will be
+0000dc10: 2063 616c 6c65 6420 746f 2063 6f6d 7075   called to compu
+0000dc20: 7465 2074 6865 2069 6e69 7469 616c 2076  te the initial v
+0000dc30: 616c 7565 206f 660a 2020 2020 2020 2020  alue of.        
+0000dc40: 7468 6973 2076 6172 6961 626c 652e 2054  this variable. T
+0000dc50: 6869 7320 6675 6e63 7469 6f6e 2077 696c  his function wil
+0000dc60: 6c20 6f6e 6c79 2062 6520 6361 6c6c 6564  l only be called
+0000dc70: 2074 6865 2066 6972 7374 2074 696d 6520   the first time 
+0000dc80: 7468 6973 0a20 2020 2020 2020 2076 6172  this.        var
+0000dc90: 6961 626c 6520 6973 2075 7365 6420 696e  iable is used in
+0000dca0: 2074 6869 7320 6d6f 6475 6c65 2e20 4966   this module. If
+0000dcb0: 204e 6f6e 652c 2074 6865 2076 6172 6961   None, the varia
+0000dcc0: 626c 6520 6d75 7374 2061 6c72 6561 6479  ble must already
+0000dcd0: 2062 650a 2020 2020 2020 2020 696e 6974   be.        init
+0000dce0: 6961 6c69 7a65 6420 6f74 6865 7277 6973  ialized otherwis
+0000dcf0: 6520 616e 2065 7272 6f72 2069 7320 7261  e an error is ra
+0000dd00: 6973 6564 2e0a 2020 2020 2020 2a69 6e69  ised..      *ini
+0000dd10: 745f 6172 6773 3a20 5468 6520 706f 7369  t_args: The posi
+0000dd20: 7469 6f6e 616c 2061 7267 756d 656e 7473  tional arguments
+0000dd30: 2074 6f20 7061 7373 2074 6f20 696e 6974   to pass to init
+0000dd40: 5f66 6e2e 0a20 2020 2020 2075 6e62 6f78  _fn..      unbox
+0000dd50: 3a20 4966 2054 7275 652c 2060 6041 7869  : If True, ``Axi
+0000dd60: 734d 6574 6164 6174 6160 6020 696e 7374  sMetadata`` inst
+0000dd70: 616e 6365 7320 6172 6520 7265 706c 6163  ances are replac
+0000dd80: 6564 2062 7920 7468 6569 7220 756e 626f  ed by their unbo
+0000dd90: 7865 640a 2020 2020 2020 2020 7661 6c75  xed.        valu
+0000dda0: 652c 2073 6565 2060 6066 6c61 782e 6e6e  e, see ``flax.nn
+0000ddb0: 2e6d 6574 612e 756e 626f 7860 6020 2864  .meta.unbox`` (d
+0000ddc0: 6566 6175 6c74 3a20 5472 7565 292e 0a20  efault: True).. 
+0000ddd0: 2020 2020 202a 2a69 6e69 745f 6b77 6172       **init_kwar
+0000dde0: 6773 3a20 5468 6520 6b65 792d 776f 7264  gs: The key-word
+0000ddf0: 2061 7267 756d 656e 7473 2074 6f20 7061   arguments to pa
+0000de00: 7373 2074 6f20 696e 6974 5f66 6e0a 0a20  ss to init_fn.. 
+0000de10: 2020 2052 6574 7572 6e73 3a0a 2020 2020     Returns:.    
+0000de20: 2020 4120 3a63 6c61 7373 3a60 666c 6178    A :class:`flax
+0000de30: 2e63 6f72 652e 7661 7269 6162 6c65 732e  .core.variables.
+0000de40: 5661 7269 6162 6c65 6020 7468 6174 2063  Variable` that c
+0000de50: 616e 2062 6520 7265 6164 206f 7220 7365  an be read or se
+0000de60: 7420 7669 610a 2020 2020 2020 222e 7661  t via.      ".va
+0000de70: 6c75 6522 2061 7474 7269 6275 7465 2e20  lue" attribute. 
+0000de80: 5468 726f 7773 2061 6e20 6572 726f 7220  Throws an error 
+0000de90: 6966 2074 6865 2076 6172 6961 626c 6520  if the variable 
+0000dea0: 6578 6973 7473 2061 6c72 6561 6479 2e0a  exists already..
+0000deb0: 2020 2020 2222 220a 2020 2020 6966 206e      """.    if n
+0000dec0: 6f74 2073 656c 662e 5f69 6e69 7469 616c  ot self._initial
+0000ded0: 697a 6174 696f 6e5f 616c 6c6f 7765 643a  ization_allowed:
+0000dee0: 0a20 2020 2020 2072 6169 7365 2056 616c  .      raise Val
+0000def0: 7565 4572 726f 7228 0a20 2020 2020 2020  ueError(.       
+0000df00: 2027 5661 7269 6162 6c65 7320 6d75 7374   'Variables must
+0000df10: 2062 6520 696e 6974 6961 6c69 7a65 6420   be initialized 
+0000df20: 696e 2060 7365 7475 7028 2960 206f 7220  in `setup()` or 
+0000df30: 696e 2061 206d 6574 686f 6420 270a 2020  in a method '.  
+0000df40: 2020 2020 2020 2777 7261 7070 6564 2069        'wrapped i
+0000df50: 6e20 6040 636f 6d70 6163 7460 270a 2020  n `@compact`'.  
+0000df60: 2020 2020 290a 2020 2020 6966 2073 656c      ).    if sel
+0000df70: 662e 5f6e 616d 655f 7461 6b65 6e28 6e61  f._name_taken(na
+0000df80: 6d65 2c20 636f 6c6c 6563 7469 6f6e 3d63  me, collection=c
+0000df90: 6f6c 293a 0a20 2020 2020 2072 6169 7365  ol):.      raise
+0000dfa0: 2065 7272 6f72 732e 4e61 6d65 496e 5573   errors.NameInUs
+0000dfb0: 6545 7272 6f72 2827 7661 7269 6162 6c65  eError('variable
+0000dfc0: 272c 206e 616d 652c 2073 656c 662e 5f5f  ', name, self.__
+0000dfd0: 636c 6173 735f 5f2e 5f5f 6e61 6d65 5f5f  class__.__name__
+0000dfe0: 290a 2020 2020 6173 7365 7274 2073 656c  ).    assert sel
+0000dff0: 662e 7363 6f70 6520 6973 206e 6f74 204e  f.scope is not N
+0000e000: 6f6e 650a 2020 2020 7620 3d20 7365 6c66  one.    v = self
+0000e010: 2e73 636f 7065 2e76 6172 6961 626c 6528  .scope.variable(
+0000e020: 0a20 2020 2020 2063 6f6c 2c20 6e61 6d65  .      col, name
+0000e030: 2c20 696e 6974 5f66 6e2c 202a 696e 6974  , init_fn, *init
+0000e040: 5f61 7267 732c 2075 6e62 6f78 3d75 6e62  _args, unbox=unb
+0000e050: 6f78 2c20 2a2a 696e 6974 5f6b 7761 7267  ox, **init_kwarg
+0000e060: 730a 2020 2020 290a 2020 2020 7365 6c66  s.    ).    self
+0000e070: 2e5f 7374 6174 652e 6368 696c 6472 656e  ._state.children
+0000e080: 5b6e 616d 655d 203d 2063 6f6c 0a20 2020  [name] = col.   
+0000e090: 2072 6574 7572 6e20 760a 0a20 2040 6f76   return v..  @ov
+0000e0a0: 6572 6c6f 6164 0a20 2064 6566 2070 6172  erload.  def par
+0000e0b0: 616d 280a 2020 2020 7365 6c66 2c20 6e61  am(.    self, na
+0000e0c0: 6d65 3a20 7374 722c 2069 6e69 745f 666e  me: str, init_fn
+0000e0d0: 3a20 4361 6c6c 6162 6c65 5b2e 2e2e 2c20  : Callable[..., 
+0000e0e0: 545d 2c20 2a69 6e69 745f 6172 6773 2c0a  T], *init_args,.
+0000e0f0: 2020 2920 2d3e 2054 3a0a 2020 2020 2e2e    ) -> T:.    ..
+0000e100: 2e0a 0a20 2040 6f76 6572 6c6f 6164 0a20  ...  @overload. 
+0000e110: 2064 6566 2070 6172 616d 280a 2020 2020   def param(.    
+0000e120: 7365 6c66 2c0a 2020 2020 6e61 6d65 3a20  self,.    name: 
+0000e130: 7374 722c 0a20 2020 2069 6e69 745f 666e  str,.    init_fn
+0000e140: 3a20 4361 6c6c 6162 6c65 5b2e 2e2e 2c20  : Callable[..., 
+0000e150: 545d 2c0a 2020 2020 2a69 6e69 745f 6172  T],.    *init_ar
+0000e160: 6773 2c0a 2020 2020 756e 626f 783a 204c  gs,.    unbox: L
+0000e170: 6974 6572 616c 5b54 7275 655d 2c0a 2020  iteral[True],.  
+0000e180: 2020 2a2a 696e 6974 5f6b 7761 7267 732c    **init_kwargs,
+0000e190: 0a20 2029 202d 3e20 543a 0a20 2020 202e  .  ) -> T:.    .
+0000e1a0: 2e2e 0a0a 2020 406f 7665 726c 6f61 640a  ....  @overload.
+0000e1b0: 2020 6465 6620 7061 7261 6d28 0a20 2020    def param(.   
+0000e1c0: 2073 656c 662c 0a20 2020 206e 616d 653a   self,.    name:
+0000e1d0: 2073 7472 2c0a 2020 2020 696e 6974 5f66   str,.    init_f
+0000e1e0: 6e3a 2043 616c 6c61 626c 655b 2e2e 2e2c  n: Callable[...,
+0000e1f0: 2054 5d2c 0a20 2020 202a 696e 6974 5f61   T],.    *init_a
+0000e200: 7267 732c 0a20 2020 2075 6e62 6f78 3a20  rgs,.    unbox: 
+0000e210: 4c69 7465 7261 6c5b 4661 6c73 655d 2c0a  Literal[False],.
+0000e220: 2020 2020 2a2a 696e 6974 5f6b 7761 7267      **init_kwarg
+0000e230: 732c 0a20 2029 202d 3e20 6d65 7461 2e41  s,.  ) -> meta.A
+0000e240: 7869 734d 6574 6164 6174 615b 545d 3a0a  xisMetadata[T]:.
+0000e250: 2020 2020 2e2e 2e0a 0a20 2040 6f76 6572      .....  @over
+0000e260: 6c6f 6164 0a20 2064 6566 2070 6172 616d  load.  def param
+0000e270: 280a 2020 2020 7365 6c66 2c0a 2020 2020  (.    self,.    
+0000e280: 6e61 6d65 3a20 7374 722c 0a20 2020 2069  name: str,.    i
+0000e290: 6e69 745f 666e 3a20 4361 6c6c 6162 6c65  nit_fn: Callable
+0000e2a0: 5b2e 2e2e 2c20 545d 2c0a 2020 2020 2a69  [..., T],.    *i
+0000e2b0: 6e69 745f 6172 6773 2c0a 2020 2020 756e  nit_args,.    un
+0000e2c0: 626f 783a 2062 6f6f 6c2c 0a20 2020 202a  box: bool,.    *
+0000e2d0: 2a69 6e69 745f 6b77 6172 6773 2c0a 2020  *init_kwargs,.  
+0000e2e0: 2920 2d3e 2055 6e69 6f6e 5b54 2c20 6d65  ) -> Union[T, me
+0000e2f0: 7461 2e41 7869 734d 6574 6164 6174 615b  ta.AxisMetadata[
+0000e300: 545d 5d3a 0a20 2020 202e 2e2e 0a0a 2020  T]]:.    .....  
+0000e310: 6465 6620 7061 7261 6d28 0a20 2020 2073  def param(.    s
+0000e320: 656c 662c 0a20 2020 206e 616d 653a 2073  elf,.    name: s
+0000e330: 7472 2c0a 2020 2020 696e 6974 5f66 6e3a  tr,.    init_fn:
+0000e340: 2043 616c 6c61 626c 655b 2e2e 2e2c 2054   Callable[..., T
+0000e350: 5d2c 0a20 2020 202a 696e 6974 5f61 7267  ],.    *init_arg
+0000e360: 732c 0a20 2020 2075 6e62 6f78 3a20 626f  s,.    unbox: bo
+0000e370: 6f6c 203d 2054 7275 652c 0a20 2020 202a  ol = True,.    *
+0000e380: 2a69 6e69 745f 6b77 6172 6773 2c0a 2020  *init_kwargs,.  
+0000e390: 2920 2d3e 2055 6e69 6f6e 5b54 2c20 6d65  ) -> Union[T, me
+0000e3a0: 7461 2e41 7869 734d 6574 6164 6174 615b  ta.AxisMetadata[
+0000e3b0: 545d 5d3a 0a20 2020 2022 2222 4465 636c  T]]:.    """Decl
+0000e3c0: 6172 6573 2061 6e64 2072 6574 7572 6e73  ares and returns
+0000e3d0: 2061 2070 6172 616d 6574 6572 2069 6e20   a parameter in 
+0000e3e0: 7468 6973 204d 6f64 756c 652e 0a0a 2020  this Module...  
+0000e3f0: 2020 5061 7261 6d65 7465 7273 2061 7265    Parameters are
+0000e400: 2072 6561 642d 6f6e 6c79 2076 6172 6961   read-only varia
+0000e410: 626c 6573 2069 6e20 7468 6520 636f 6c6c  bles in the coll
+0000e420: 6563 7469 6f6e 206e 616d 6564 2022 7061  ection named "pa
+0000e430: 7261 6d73 222e 2053 6565 0a20 2020 203a  rams". See.    :
+0000e440: 6d6f 643a 6066 6c61 782e 636f 7265 2e76  mod:`flax.core.v
+0000e450: 6172 6961 626c 6573 6020 666f 7220 6d6f  ariables` for mo
+0000e460: 7265 2064 6574 6169 6c73 206f 6e20 7661  re details on va
+0000e470: 7269 6162 6c65 732e 0a0a 2020 2020 5468  riables...    Th
+0000e480: 6520 6669 7273 7420 6172 6775 6d65 6e74  e first argument
+0000e490: 206f 6620 6060 696e 6974 5f66 6e60 6020   of ``init_fn`` 
+0000e4a0: 6973 2061 7373 756d 6564 2074 6f20 6265  is assumed to be
+0000e4b0: 2061 2050 524e 4720 6b65 792c 2077 6869   a PRNG key, whi
+0000e4c0: 6368 2069 730a 2020 2020 7072 6f76 6964  ch is.    provid
+0000e4d0: 6564 2061 7574 6f6d 6174 6963 616c 6c79  ed automatically
+0000e4e0: 2061 6e64 2064 6f65 7320 6e6f 7420 6861   and does not ha
+0000e4f0: 7665 2074 6f20 6265 2070 6173 7365 6420  ve to be passed 
+0000e500: 7573 696e 6720 6060 696e 6974 5f61 7267  using ``init_arg
+0000e510: 7360 600a 2020 2020 6f72 2060 6069 6e69  s``.    or ``ini
+0000e520: 745f 6b77 6172 6773 6060 3a3a 0a0a 2020  t_kwargs``::..  
+0000e530: 2020 2020 3e3e 3e20 636c 6173 7320 466f      >>> class Fo
+0000e540: 6f28 6e6e 2e4d 6f64 756c 6529 3a0a 2020  o(nn.Module):.  
+0000e550: 2020 2020 2e2e 2e20 2020 406e 6e2e 636f      ...   @nn.co
+0000e560: 6d70 6163 740a 2020 2020 2020 2e2e 2e20  mpact.      ... 
+0000e570: 2020 6465 6620 5f5f 6361 6c6c 5f5f 2873    def __call__(s
+0000e580: 656c 662c 2078 293a 0a20 2020 2020 202e  elf, x):.      .
+0000e590: 2e2e 2020 2020 2078 203d 206e 6e2e 4465  ..     x = nn.De
+0000e5a0: 6e73 6528 3429 2878 290a 2020 2020 2020  nse(4)(x).      
+0000e5b0: 2e2e 2e20 2020 2020 6d65 616e 203d 2073  ...     mean = s
+0000e5c0: 656c 662e 7061 7261 6d28 276d 6561 6e27  elf.param('mean'
+0000e5d0: 2c20 6e6e 2e69 6e69 7469 616c 697a 6572  , nn.initializer
+0000e5e0: 732e 6c65 6375 6e5f 6e6f 726d 616c 2829  s.lecun_normal()
+0000e5f0: 2c20 782e 7368 6170 6529 0a20 2020 2020  , x.shape).     
+0000e600: 202e 2e2e 2020 2020 202e 2e2e 0a20 2020   ...     ....   
+0000e610: 2020 202e 2e2e 2020 2020 2072 6574 7572     ...     retur
+0000e620: 6e20 7820 2a20 6d65 616e 0a20 2020 2020  n x * mean.     
+0000e630: 203e 3e3e 2076 6172 6961 626c 6573 203d   >>> variables =
+0000e640: 2046 6f6f 2829 2e69 6e69 7428 7b27 7061   Foo().init({'pa
+0000e650: 7261 6d73 273a 206a 6178 2e72 616e 646f  rams': jax.rando
+0000e660: 6d2e 6b65 7928 3029 2c20 2773 7461 7473  m.key(0), 'stats
+0000e670: 273a 206a 6178 2e72 616e 646f 6d2e 6b65  ': jax.random.ke
+0000e680: 7928 3129 7d2c 206a 6e70 2e6f 6e65 7328  y(1)}, jnp.ones(
+0000e690: 2832 2c20 3329 2929 0a20 2020 2020 203e  (2, 3))).      >
+0000e6a0: 3e3e 206a 6178 2e74 7265 655f 7574 696c  >> jax.tree_util
+0000e6b0: 2e74 7265 655f 6d61 7028 6a6e 702e 7368  .tree_map(jnp.sh
+0000e6c0: 6170 652c 2076 6172 6961 626c 6573 290a  ape, variables).
+0000e6d0: 2020 2020 2020 7b27 7061 7261 6d73 273a        {'params':
+0000e6e0: 207b 2744 656e 7365 5f30 273a 207b 2762   {'Dense_0': {'b
+0000e6f0: 6961 7327 3a20 2834 2c29 2c20 276b 6572  ias': (4,), 'ker
+0000e700: 6e65 6c27 3a20 2833 2c20 3429 7d2c 2027  nel': (3, 4)}, '
+0000e710: 6d65 616e 273a 2028 322c 2034 297d 7d0a  mean': (2, 4)}}.
+0000e720: 0a20 2020 2049 6e20 7468 6520 6578 616d  .    In the exam
+0000e730: 706c 6520 6162 6f76 652c 2074 6865 2066  ple above, the f
+0000e740: 756e 6374 696f 6e20 6060 6c65 6375 6e5f  unction ``lecun_
+0000e750: 6e6f 726d 616c 6060 2065 7870 6563 7473  normal`` expects
+0000e760: 2074 776f 2061 7267 756d 656e 7473 3a0a   two arguments:.
+0000e770: 2020 2020 6060 6b65 7960 6020 616e 6420      ``key`` and 
+0000e780: 6060 7368 6170 6560 602c 2062 7574 206f  ``shape``, but o
+0000e790: 6e6c 7920 6060 7368 6170 6560 6020 6861  nly ``shape`` ha
+0000e7a0: 7320 746f 2062 6520 7072 6f76 6964 6564  s to be provided
+0000e7b0: 2065 7870 6c69 6369 746c 793b 0a20 2020   explicitly;.   
+0000e7c0: 2060 606b 6579 6060 2069 7320 7365 7420   ``key`` is set 
+0000e7d0: 6175 746f 6d61 7469 6361 6c6c 7920 7573  automatically us
+0000e7e0: 696e 6720 7468 6520 5052 4e47 2066 6f72  ing the PRNG for
+0000e7f0: 2060 6070 6172 616d 7360 6020 7468 6174   ``params`` that
+0000e800: 2069 7320 7061 7373 6564 0a20 2020 2077   is passed.    w
+0000e810: 6865 6e20 696e 6974 6961 6c69 7a69 6e67  hen initializing
+0000e820: 2074 6865 206d 6f64 756c 6520 7573 696e   the module usin
+0000e830: 6720 3a6d 6574 683a 6069 6e69 7460 2e0a  g :meth:`init`..
+0000e840: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     
+0000e850: 206e 616d 653a 2054 6865 2070 6172 616d   name: The param
+0000e860: 6574 6572 206e 616d 652e 0a20 2020 2020  eter name..     
+0000e870: 2069 6e69 745f 666e 3a20 5468 6520 6675   init_fn: The fu
+0000e880: 6e63 7469 6f6e 2074 6861 7420 7769 6c6c  nction that will
+0000e890: 2062 6520 6361 6c6c 6564 2074 6f20 636f   be called to co
+0000e8a0: 6d70 7574 6520 7468 6520 696e 6974 6961  mpute the initia
+0000e8b0: 6c20 7661 6c75 6520 6f66 0a20 2020 2020  l value of.     
+0000e8c0: 2020 2074 6869 7320 7661 7269 6162 6c65     this variable
+0000e8d0: 2e20 5468 6973 2066 756e 6374 696f 6e20  . This function 
+0000e8e0: 7769 6c6c 206f 6e6c 7920 6265 2063 616c  will only be cal
+0000e8f0: 6c65 6420 7468 6520 6669 7273 7420 7469  led the first ti
+0000e900: 6d65 2074 6869 730a 2020 2020 2020 2020  me this.        
+0000e910: 7061 7261 6d65 7465 7220 6973 2075 7365  parameter is use
+0000e920: 6420 696e 2074 6869 7320 6d6f 6475 6c65  d in this module
+0000e930: 2e0a 2020 2020 2020 2a69 6e69 745f 6172  ..      *init_ar
+0000e940: 6773 3a20 5468 6520 706f 7369 7469 6f6e  gs: The position
+0000e950: 616c 2061 7267 756d 656e 7473 2074 6f20  al arguments to 
+0000e960: 7061 7373 2074 6f20 696e 6974 5f66 6e2e  pass to init_fn.
+0000e970: 0a20 2020 2020 2075 6e62 6f78 3a20 4966  .      unbox: If
+0000e980: 2054 7275 652c 2060 6041 7869 734d 6574   True, ``AxisMet
+0000e990: 6164 6174 6160 6020 696e 7374 616e 6365  adata`` instance
+0000e9a0: 7320 6172 6520 7265 706c 6163 6564 2062  s are replaced b
+0000e9b0: 7920 7468 6569 7220 756e 626f 7865 640a  y their unboxed.
+0000e9c0: 2020 2020 2020 2020 7661 6c75 652c 2073          value, s
+0000e9d0: 6565 2060 6066 6c61 782e 6e6e 2e6d 6574  ee ``flax.nn.met
+0000e9e0: 612e 756e 626f 7860 6020 2864 6566 6175  a.unbox`` (defau
+0000e9f0: 6c74 3a20 5472 7565 292e 0a20 2020 2020  lt: True)..     
+0000ea00: 202a 2a69 6e69 745f 6b77 6172 6773 3a20   **init_kwargs: 
+0000ea10: 5468 6520 6b65 792d 776f 7264 2061 7267  The key-word arg
+0000ea20: 756d 656e 7473 2074 6f20 7061 7373 2074  uments to pass t
+0000ea30: 6f20 696e 6974 5f66 6e2e 0a0a 2020 2020  o init_fn...    
+0000ea40: 5265 7475 726e 733a 0a20 2020 2020 2054  Returns:.      T
+0000ea50: 6865 2076 616c 7565 206f 6620 7468 6520  he value of the 
+0000ea60: 696e 6974 6961 6c69 7a65 6420 7061 7261  initialized para
+0000ea70: 6d65 7465 722e 2054 6872 6f77 7320 616e  meter. Throws an
+0000ea80: 2065 7272 6f72 2069 6620 7468 6520 7061   error if the pa
+0000ea90: 7261 6d65 7465 720a 2020 2020 2020 6578  rameter.      ex
+0000eaa0: 6973 7473 2061 6c72 6561 6479 2e0a 2020  ists already..  
+0000eab0: 2020 2222 220a 2020 2020 6966 206e 6f74    """.    if not
+0000eac0: 2073 656c 662e 5f69 6e69 7469 616c 697a   self._initializ
+0000ead0: 6174 696f 6e5f 616c 6c6f 7765 643a 0a20  ation_allowed:. 
+0000eae0: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
+0000eaf0: 4572 726f 7228 0a20 2020 2020 2020 2027  Error(.        '
+0000eb00: 5061 7261 6d65 7465 7273 206d 7573 7420  Parameters must 
+0000eb10: 6265 2069 6e69 7469 616c 697a 6564 2069  be initialized i
+0000eb20: 6e20 6073 6574 7570 2829 6020 6f72 2069  n `setup()` or i
+0000eb30: 6e20 6120 6d65 7468 6f64 2027 0a20 2020  n a method '.   
+0000eb40: 2020 2020 2027 7772 6170 7065 6420 696e       'wrapped in
+0000eb50: 2060 4063 6f6d 7061 6374 6027 0a20 2020   `@compact`'.   
+0000eb60: 2020 2029 0a20 2020 2069 6620 7365 6c66     ).    if self
+0000eb70: 2e5f 6e61 6d65 5f74 616b 656e 286e 616d  ._name_taken(nam
+0000eb80: 652c 2063 6f6c 6c65 6374 696f 6e3d 2770  e, collection='p
+0000eb90: 6172 616d 7327 293a 0a20 2020 2020 2072  arams'):.      r
+0000eba0: 6169 7365 2065 7272 6f72 732e 4e61 6d65  aise errors.Name
+0000ebb0: 496e 5573 6545 7272 6f72 2827 7061 7261  InUseError('para
+0000ebc0: 6d27 2c20 6e61 6d65 2c20 7365 6c66 2e5f  m', name, self._
+0000ebd0: 5f63 6c61 7373 5f5f 2e5f 5f6e 616d 655f  _class__.__name_
+0000ebe0: 5f29 0a20 2020 2061 7373 6572 7420 7365  _).    assert se
+0000ebf0: 6c66 2e73 636f 7065 2069 7320 6e6f 7420  lf.scope is not 
+0000ec00: 4e6f 6e65 0a20 2020 2076 203d 2073 656c  None.    v = sel
+0000ec10: 662e 7363 6f70 652e 7061 7261 6d28 6e61  f.scope.param(na
+0000ec20: 6d65 2c20 696e 6974 5f66 6e2c 202a 696e  me, init_fn, *in
+0000ec30: 6974 5f61 7267 732c 2075 6e62 6f78 3d75  it_args, unbox=u
+0000ec40: 6e62 6f78 2c20 2a2a 696e 6974 5f6b 7761  nbox, **init_kwa
+0000ec50: 7267 7329 0a20 2020 2073 656c 662e 5f73  rgs).    self._s
+0000ec60: 7461 7465 2e63 6869 6c64 7265 6e5b 6e61  tate.children[na
+0000ec70: 6d65 5d20 3d20 2770 6172 616d 7327 0a20  me] = 'params'. 
+0000ec80: 2020 2072 6574 7572 6e20 760a 0a20 2064     return v..  d
+0000ec90: 6566 2068 6173 5f76 6172 6961 626c 6528  ef has_variable(
+0000eca0: 7365 6c66 2c20 636f 6c3a 2073 7472 2c20  self, col: str, 
+0000ecb0: 6e61 6d65 3a20 7374 7229 202d 3e20 626f  name: str) -> bo
+0000ecc0: 6f6c 3a0a 2020 2020 2222 2243 6865 636b  ol:.    """Check
+0000ecd0: 7320 6966 2061 2076 6172 6961 626c 6520  s if a variable 
+0000ece0: 6f66 2067 6976 656e 2063 6f6c 6c65 6374  of given collect
+0000ecf0: 696f 6e20 616e 6420 6e61 6d65 2065 7869  ion and name exi
+0000ed00: 7374 7320 696e 2074 6869 7320 4d6f 6475  sts in this Modu
+0000ed10: 6c65 2e0a 0a20 2020 2053 6565 203a 6d6f  le...    See :mo
+0000ed20: 643a 6066 6c61 782e 636f 7265 2e76 6172  d:`flax.core.var
+0000ed30: 6961 626c 6573 6020 666f 7220 6d6f 7265  iables` for more
+0000ed40: 2065 7870 6c61 6e61 7469 6f6e 206f 6e20   explanation on 
+0000ed50: 7661 7269 6162 6c65 7320 616e 640a 2020  variables and.  
+0000ed60: 2020 636f 6c6c 6563 7469 6f6e 732e 0a0a    collections...
+0000ed70: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
+0000ed80: 636f 6c3a 2054 6865 2076 6172 6961 626c  col: The variabl
+0000ed90: 6520 636f 6c6c 6563 7469 6f6e 206e 616d  e collection nam
+0000eda0: 652e 0a20 2020 2020 206e 616d 653a 2054  e..      name: T
+0000edb0: 6865 206e 616d 6520 6f66 2074 6865 2076  he name of the v
+0000edc0: 6172 6961 626c 652e 0a0a 2020 2020 5265  ariable...    Re
+0000edd0: 7475 726e 733a 0a20 2020 2020 2054 7275  turns:.      Tru
+0000ede0: 6520 6966 2074 6865 2076 6172 6961 626c  e if the variabl
+0000edf0: 6520 6578 6973 7473 2e0a 2020 2020 2222  e exists..    ""
+0000ee00: 220a 2020 2020 6966 2073 656c 662e 7363  ".    if self.sc
+0000ee10: 6f70 6520 6973 204e 6f6e 653a 0a20 2020  ope is None:.   
+0000ee20: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
+0000ee30: 726f 7228 2243 616e 2774 2061 6363 6573  ror("Can't acces
+0000ee40: 7320 7661 7269 6162 6c65 7320 6f6e 2075  s variables on u
+0000ee50: 6e62 6f75 6e64 206d 6f64 756c 6573 2229  nbound modules")
+0000ee60: 0a20 2020 2072 6574 7572 6e20 7365 6c66  .    return self
+0000ee70: 2e73 636f 7065 2e68 6173 5f76 6172 6961  .scope.has_varia
+0000ee80: 626c 6528 636f 6c2c 206e 616d 6529 0a0a  ble(col, name)..
+0000ee90: 2020 6465 6620 6973 5f6d 7574 6162 6c65    def is_mutable
+0000eea0: 5f63 6f6c 6c65 6374 696f 6e28 7365 6c66  _collection(self
+0000eeb0: 2c20 636f 6c3a 2073 7472 2920 2d3e 2062  , col: str) -> b
+0000eec0: 6f6f 6c3a 0a20 2020 2022 2222 5265 7475  ool:.    """Retu
+0000eed0: 726e 7320 7472 7565 2069 6620 7468 6520  rns true if the 
+0000eee0: 636f 6c6c 6563 7469 6f6e 2060 6063 6f6c  collection ``col
+0000eef0: 6060 2069 7320 6d75 7461 626c 652e 2222  `` is mutable.""
+0000ef00: 220a 2020 2020 6966 2073 656c 662e 7363  ".    if self.sc
+0000ef10: 6f70 6520 6973 204e 6f6e 653a 0a20 2020  ope is None:.   
+0000ef20: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
+0000ef30: 726f 7228 2243 616e 2774 2063 6865 636b  ror("Can't check
+0000ef40: 206d 7574 6162 696c 6974 7920 6f6e 2075   mutability on u
+0000ef50: 6e62 6f75 6e64 206d 6f64 756c 6573 2229  nbound modules")
+0000ef60: 0a20 2020 2072 6574 7572 6e20 7365 6c66  .    return self
+0000ef70: 2e73 636f 7065 2e69 735f 6d75 7461 626c  .scope.is_mutabl
+0000ef80: 655f 636f 6c6c 6563 7469 6f6e 2863 6f6c  e_collection(col
+0000ef90: 290a 0a20 2064 6566 2068 6173 5f72 6e67  )..  def has_rng
+0000efa0: 2873 656c 662c 206e 616d 653a 2073 7472  (self, name: str
+0000efb0: 2920 2d3e 2062 6f6f 6c3a 0a20 2020 2022  ) -> bool:.    "
+0000efc0: 2222 5265 7475 726e 7320 7472 7565 2069  ""Returns true i
+0000efd0: 6620 6120 5052 4e47 5365 7175 656e 6365  f a PRNGSequence
+0000efe0: 2077 6974 6820 6e61 6d65 2060 606e 616d   with name ``nam
+0000eff0: 6560 6020 6578 6973 7473 2e22 2222 0a20  e`` exists.""". 
+0000f000: 2020 2069 6620 7365 6c66 2e73 636f 7065     if self.scope
+0000f010: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
+0000f020: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+0000f030: 2822 4361 6e27 7420 7175 6572 7920 666f  ("Can't query fo
+0000f040: 7220 524e 4773 206f 6e20 756e 626f 756e  r RNGs on unboun
+0000f050: 6420 6d6f 6475 6c65 7322 290a 2020 2020  d modules").    
+0000f060: 7265 7475 726e 2073 656c 662e 7363 6f70  return self.scop
+0000f070: 652e 6861 735f 726e 6728 6e61 6d65 290a  e.has_rng(name).
+0000f080: 0a20 2064 6566 206d 616b 655f 726e 6728  .  def make_rng(
+0000f090: 7365 6c66 2c20 6e61 6d65 3a20 7374 7220  self, name: str 
+0000f0a0: 3d20 2770 6172 616d 7327 2920 2d3e 2050  = 'params') -> P
+0000f0b0: 524e 474b 6579 3a0a 2020 2020 2222 2252  RNGKey:.    """R
+0000f0c0: 6574 7572 6e73 2061 206e 6577 2052 4e47  eturns a new RNG
+0000f0d0: 206b 6579 2066 726f 6d20 6120 6769 7665   key from a give
+0000f0e0: 6e20 524e 4720 7365 7175 656e 6365 2066  n RNG sequence f
+0000f0f0: 6f72 2074 6869 7320 4d6f 6475 6c65 2e0a  or this Module..
+0000f100: 0a20 2020 2054 6865 206e 6577 2052 4e47  .    The new RNG
+0000f110: 206b 6579 2069 7320 7370 6c69 7420 6672   key is split fr
+0000f120: 6f6d 2074 6865 2070 7265 7669 6f75 7320  om the previous 
+0000f130: 6f6e 652e 2054 6875 732c 2065 7665 7279  one. Thus, every
+0000f140: 2063 616c 6c20 746f 0a20 2020 2060 606d   call to.    ``m
+0000f150: 616b 655f 726e 6760 6020 7265 7475 726e  ake_rng`` return
+0000f160: 7320 6120 6e65 7720 524e 4720 6b65 792c  s a new RNG key,
+0000f170: 2077 6869 6c65 2073 7469 6c6c 2067 7561   while still gua
+0000f180: 7261 6e74 6565 696e 6720 6675 6c6c 0a20  ranteeing full. 
+0000f190: 2020 2072 6570 726f 6475 6369 6269 6c69     reproducibili
+0000f1a0: 7479 2e0a 0a20 2020 202e 2e20 6e6f 7465  ty...    .. note
+0000f1b0: 3a3a 0a20 2020 2020 2049 6620 616e 2069  ::.      If an i
+0000f1c0: 6e76 616c 6964 206e 616d 6520 6973 2070  nvalid name is p
+0000f1d0: 6173 7365 6420 2869 2e65 2e20 6e6f 2052  assed (i.e. no R
+0000f1e0: 4e47 206b 6579 2077 6173 2070 6173 7365  NG key was passe
+0000f1f0: 6420 6279 0a20 2020 2020 2074 6865 2075  d by.      the u
+0000f200: 7365 7220 696e 2060 602e 696e 6974 6060  ser in ``.init``
+0000f210: 206f 7220 6060 2e61 7070 6c79 6060 2066   or ``.apply`` f
+0000f220: 6f72 2074 6869 7320 6e61 6d65 292c 2074  or this name), t
+0000f230: 6865 6e20 6060 6e61 6d65 6060 0a20 2020  hen ``name``.   
+0000f240: 2020 2077 696c 6c20 6465 6661 756c 7420     will default 
+0000f250: 746f 2060 6027 7061 7261 6d73 2760 602e  to ``'params'``.
+0000f260: 0a0a 2020 2020 4578 616d 706c 653a 3a0a  ..    Example::.
+0000f270: 0a20 2020 2020 203e 3e3e 2069 6d70 6f72  .      >>> impor
+0000f280: 7420 6a61 780a 2020 2020 2020 3e3e 3e20  t jax.      >>> 
+0000f290: 696d 706f 7274 2066 6c61 782e 6c69 6e65  import flax.line
+0000f2a0: 6e20 6173 206e 6e0a 0a20 2020 2020 203e  n as nn..      >
+0000f2b0: 3e3e 2063 6c61 7373 2050 6172 616d 734d  >> class ParamsM
+0000f2c0: 6f64 756c 6528 6e6e 2e4d 6f64 756c 6529  odule(nn.Module)
+0000f2d0: 3a0a 2020 2020 2020 2e2e 2e20 2020 6465  :.      ...   de
+0000f2e0: 6620 5f5f 6361 6c6c 5f5f 2873 656c 6629  f __call__(self)
+0000f2f0: 3a0a 2020 2020 2020 2e2e 2e20 2020 2020  :.      ...     
+0000f300: 7265 7475 726e 2073 656c 662e 6d61 6b65  return self.make
+0000f310: 5f72 6e67 2827 7061 7261 6d73 2729 0a20  _rng('params'). 
+0000f320: 2020 2020 203e 3e3e 2063 6c61 7373 204f       >>> class O
+0000f330: 7468 6572 4d6f 6475 6c65 286e 6e2e 4d6f  therModule(nn.Mo
+0000f340: 6475 6c65 293a 0a20 2020 2020 202e 2e2e  dule):.      ...
+0000f350: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
+0000f360: 7365 6c66 293a 0a20 2020 2020 202e 2e2e  self):.      ...
+0000f370: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
+0000f380: 2e6d 616b 655f 726e 6728 276f 7468 6572  .make_rng('other
+0000f390: 2729 0a0a 2020 2020 2020 3e3e 3e20 6b65  ')..      >>> ke
+0000f3a0: 7920 3d20 6a61 782e 7261 6e64 6f6d 2e6b  y = jax.random.k
+0000f3b0: 6579 2830 290a 2020 2020 2020 3e3e 3e20  ey(0).      >>> 
+0000f3c0: 7061 7261 6d73 5f6f 7574 2c20 5f20 3d20  params_out, _ = 
+0000f3d0: 5061 7261 6d73 4d6f 6475 6c65 2829 2e69  ParamsModule().i
+0000f3e0: 6e69 745f 7769 7468 5f6f 7574 7075 7428  nit_with_output(
+0000f3f0: 7b27 7061 7261 6d73 273a 206b 6579 7d29  {'params': key})
+0000f400: 0a20 2020 2020 203e 3e3e 2023 2073 656c  .      >>> # sel
+0000f410: 662e 6d61 6b65 5f72 6e67 2827 6f74 6865  f.make_rng('othe
+0000f420: 7227 2920 7769 6c6c 2064 6566 6175 6c74  r') will default
+0000f430: 2074 6f20 7573 696e 6720 7468 6520 2770   to using the 'p
+0000f440: 6172 616d 7327 2052 4e47 2073 7472 6561  arams' RNG strea
+0000f450: 6d0a 2020 2020 2020 3e3e 3e20 6f74 6865  m.      >>> othe
+0000f460: 725f 6f75 742c 205f 203d 204f 7468 6572  r_out, _ = Other
+0000f470: 4d6f 6475 6c65 2829 2e69 6e69 745f 7769  Module().init_wi
+0000f480: 7468 5f6f 7574 7075 7428 7b27 7061 7261  th_output({'para
+0000f490: 6d73 273a 206b 6579 7d29 0a20 2020 2020  ms': key}).     
+0000f4a0: 203e 3e3e 2061 7373 6572 7420 7061 7261   >>> assert para
+0000f4b0: 6d73 5f6f 7574 203d 3d20 6f74 6865 725f  ms_out == other_
+0000f4c0: 6f75 740a 0a20 2020 204c 6561 726e 206d  out..    Learn m
+0000f4d0: 6f72 6520 6162 6f75 7420 524e 4727 7320  ore about RNG's 
+0000f4e0: 6279 2072 6561 6469 6e67 2074 6865 2046  by reading the F
+0000f4f0: 6c61 7820 524e 4720 6775 6964 653a 0a20  lax RNG guide:. 
+0000f500: 2020 2068 7474 7073 3a2f 2f66 6c61 782e     https://flax.
+0000f510: 7265 6164 7468 6564 6f63 732e 696f 2f65  readthedocs.io/e
+0000f520: 6e2f 6c61 7465 7374 2f67 7569 6465 732f  n/latest/guides/
+0000f530: 666c 6178 5f66 756e 6461 6d65 6e74 616c  flax_fundamental
+0000f540: 732f 726e 675f 6775 6964 652e 6874 6d6c  s/rng_guide.html
+0000f550: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    
+0000f560: 2020 6e61 6d65 3a20 5468 6520 524e 4720    name: The RNG 
+0000f570: 7365 7175 656e 6365 206e 616d 652e 0a0a  sequence name...
+0000f580: 2020 2020 5265 7475 726e 733a 0a20 2020      Returns:.   
+0000f590: 2020 2054 6865 206e 6577 6c79 2067 656e     The newly gen
+0000f5a0: 6572 6174 6564 2052 4e47 206b 6579 2e0a  erated RNG key..
+0000f5b0: 2020 2020 2222 220a 2020 2020 6966 2073      """.    if s
+0000f5c0: 656c 662e 7363 6f70 6520 6973 204e 6f6e  elf.scope is Non
+0000f5d0: 653a 0a20 2020 2020 2072 6169 7365 2056  e:.      raise V
+0000f5e0: 616c 7565 4572 726f 7228 2243 616e 2774  alueError("Can't
+0000f5f0: 2075 7365 2052 4e47 7320 6f6e 2075 6e62   use RNGs on unb
+0000f600: 6f75 6e64 206d 6f64 756c 6573 2229 0a20  ound modules"). 
+0000f610: 2020 2072 6574 7572 6e20 7365 6c66 2e73     return self.s
+0000f620: 636f 7065 2e6d 616b 655f 726e 6728 6e61  cope.make_rng(na
+0000f630: 6d65 290a 0a20 2064 6566 2069 735f 696e  me)..  def is_in
+0000f640: 6974 6961 6c69 7a69 6e67 2873 656c 6629  itializing(self)
+0000f650: 202d 3e20 626f 6f6c 3a0a 2020 2020 2222   -> bool:.    ""
+0000f660: 2252 6574 7572 6e73 2054 7275 6520 6966  "Returns True if
+0000f670: 2072 756e 6e69 6e67 2075 6e64 6572 2073   running under s
+0000f680: 656c 662e 696e 6974 282e 2e2e 2920 6f72  elf.init(...) or
+0000f690: 206e 6e2e 696e 6974 282e 2e2e 2928 292e   nn.init(...)().
+0000f6a0: 0a0a 2020 2020 5468 6973 2069 7320 6120  ..    This is a 
+0000f6b0: 6865 6c70 6572 206d 6574 686f 6420 746f  helper method to
+0000f6c0: 2068 616e 646c 6520 7468 6520 636f 6d6d   handle the comm
+0000f6d0: 6f6e 2063 6173 6520 6f66 2073 696d 706c  on case of simpl
+0000f6e0: 6520 696e 6974 6961 6c69 7a61 7469 6f6e  e initialization
+0000f6f0: 0a20 2020 2077 6865 7265 2077 6520 7769  .    where we wi
+0000f700: 7368 2074 6f20 6861 7665 2073 6574 7570  sh to have setup
+0000f710: 206c 6f67 6963 206f 6363 7572 2077 6865   logic occur whe
+0000f720: 6e20 6f6e 6c79 2063 616c 6c65 6420 756e  n only called un
+0000f730: 6465 720a 2020 2020 6060 6d6f 6475 6c65  der.    ``module
+0000f740: 2e69 6e69 7460 6020 6f72 2060 606e 6e2e  .init`` or ``nn.
+0000f750: 696e 6974 6060 2e20 2046 6f72 206d 6f72  init``.  For mor
+0000f760: 6520 636f 6d70 6c69 6361 7465 6420 6d75  e complicated mu
+0000f770: 6c74 692d 7068 6173 650a 2020 2020 696e  lti-phase.    in
+0000f780: 6974 6961 6c69 7a61 7469 6f6e 2073 6365  itialization sce
+0000f790: 6e61 7269 6f73 2069 7420 6973 2062 6574  narios it is bet
+0000f7a0: 7465 7220 746f 2074 6573 7420 666f 7220  ter to test for 
+0000f7b0: 7468 6520 6d75 7461 6269 6c69 7479 206f  the mutability o
+0000f7c0: 660a 2020 2020 7061 7274 6963 756c 6172  f.    particular
+0000f7d0: 2076 6172 6961 626c 6520 636f 6c6c 6563   variable collec
+0000f7e0: 7469 6f6e 7320 6f72 2066 6f72 2074 6865  tions or for the
+0000f7f0: 2070 7265 7365 6e63 6520 6f66 2070 6172   presence of par
+0000f800: 7469 6375 6c61 720a 2020 2020 7661 7269  ticular.    vari
+0000f810: 6162 6c65 7320 7468 6174 2070 6f74 656e  ables that poten
+0000f820: 7469 616c 6c79 206e 6565 6420 746f 2062  tially need to b
+0000f830: 6520 696e 6974 6961 6c69 7a65 642e 0a20  e initialized.. 
+0000f840: 2020 2022 2222 0a20 2020 2069 6620 7365     """.    if se
+0000f850: 6c66 2e73 636f 7065 2069 7320 4e6f 6e65  lf.scope is None
+0000f860: 3a0a 2020 2020 2020 7261 6973 6520 5661  :.      raise Va
+0000f870: 6c75 6545 7272 6f72 2822 4361 6e27 7420  lueError("Can't 
+0000f880: 6368 6563 6b20 6966 2072 756e 6e69 6e67  check if running
+0000f890: 2075 6e64 6572 2069 6e69 7428 2920 6f6e   under init() on
+0000f8a0: 2075 6e62 6f75 6e64 206d 6f64 756c 6573   unbound modules
+0000f8b0: 2229 0a20 2020 2072 6574 7572 6e20 7365  ").    return se
+0000f8c0: 6c66 2e73 636f 7065 2e67 6574 5f66 6c61  lf.scope.get_fla
+0000f8d0: 6728 2769 6e69 7469 616c 697a 696e 6727  g('initializing'
+0000f8e0: 2c20 4661 6c73 6529 0a0a 2020 6465 6620  , False)..  def 
+0000f8f0: 5f6d 6f64 756c 655f 6368 6563 6b73 2873  _module_checks(s
+0000f900: 656c 6629 3a0a 2020 2020 2222 2252 756e  elf):.    """Run
+0000f910: 2073 7461 6e64 6172 6420 7275 6e74 696d   standard runtim
+0000f920: 6520 6368 6563 6b73 2e22 2222 0a0a 2020  e checks."""..  
+0000f930: 2020 6966 206e 6f74 2069 7369 6e73 7461    if not isinsta
+0000f940: 6e63 6528 7365 6c66 2c20 4d6f 6475 6c65  nce(self, Module
+0000f950: 293a 0a20 2020 2020 2072 6169 7365 2065  ):.      raise e
+0000f960: 7272 6f72 732e 496e 7661 6c69 6449 6e73  rrors.InvalidIns
+0000f970: 7461 6e63 654d 6f64 756c 6545 7272 6f72  tanceModuleError
+0000f980: 2829 0a0a 2020 2020 6f76 6572 7269 6464  ()..    overridd
+0000f990: 656e 5f70 6f73 745f 696e 6974 203d 2073  en_post_init = s
+0000f9a0: 656c 662e 5f5f 706f 7374 5f69 6e69 745f  elf.__post_init_
+0000f9b0: 5f20 213d 204d 6f64 756c 652e 5f5f 706f  _ != Module.__po
+0000f9c0: 7374 5f69 6e69 745f 5f0a 2020 2020 6966  st_init__.    if
+0000f9d0: 206f 7665 7272 6964 6465 6e5f 706f 7374   overridden_post
+0000f9e0: 5f69 6e69 7420 616e 6420 6e6f 7420 6861  _init and not ha
+0000f9f0: 7361 7474 7228 7365 6c66 2c20 275f 6964  sattr(self, '_id
+0000fa00: 2729 3a0a 2020 2020 2020 7261 6973 6520  '):.      raise 
+0000fa10: 6572 726f 7273 2e49 6e63 6f72 7265 6374  errors.Incorrect
+0000fa20: 506f 7374 496e 6974 4f76 6572 7269 6465  PostInitOverride
+0000fa30: 4572 726f 7228 290a 0a20 2040 7472 6163  Error()..  @trac
+0000fa40: 6562 6163 6b5f 7574 696c 2e61 7069 5f62  eback_util.api_b
+0000fa50: 6f75 6e64 6172 790a 2020 6465 6620 6269  oundary.  def bi
+0000fa60: 6e64 280a 2020 2020 7365 6c66 3a20 4d2c  nd(.    self: M,
+0000fa70: 0a20 2020 2076 6172 6961 626c 6573 3a20  .    variables: 
+0000fa80: 5661 7269 6162 6c65 4469 6374 2c0a 2020  VariableDict,.  
+0000fa90: 2020 2a61 7267 732c 0a20 2020 2072 6e67    *args,.    rng
+0000faa0: 733a 204f 7074 696f 6e61 6c5b 524e 4753  s: Optional[RNGS
+0000fab0: 6571 7565 6e63 6573 5d20 3d20 4e6f 6e65  equences] = None
+0000fac0: 2c0a 2020 2020 6d75 7461 626c 653a 2043  ,.    mutable: C
+0000fad0: 6f6c 6c65 6374 696f 6e46 696c 7465 7220  ollectionFilter 
+0000fae0: 3d20 4661 6c73 652c 0a20 2029 202d 3e20  = False,.  ) -> 
+0000faf0: 4d3a 0a20 2020 2022 2222 4372 6561 7465  M:.    """Create
+0000fb00: 7320 616e 2069 6e74 6572 6163 7469 7665  s an interactive
+0000fb10: 204d 6f64 756c 6520 696e 7374 616e 6365   Module instance
+0000fb20: 2062 7920 6269 6e64 696e 6720 7661 7269   by binding vari
+0000fb30: 6162 6c65 7320 616e 6420 524e 4773 2e0a  ables and RNGs..
+0000fb40: 0a20 2020 2060 6062 696e 6460 6020 7072  .    ``bind`` pr
+0000fb50: 6f76 6964 6573 2061 6e20 2269 6e74 6572  ovides an "inter
+0000fb60: 6163 7469 7665 2220 696e 7374 616e 6365  active" instance
+0000fb70: 206f 6620 6120 4d6f 6475 6c65 2064 6972   of a Module dir
+0000fb80: 6563 746c 7920 7769 7468 6f75 740a 2020  ectly without.  
+0000fb90: 2020 7472 616e 7366 6f72 6d69 6e67 2061    transforming a
+0000fba0: 2066 756e 6374 696f 6e20 7769 7468 2060   function with `
+0000fbb0: 6061 7070 6c79 6060 2e20 5468 6973 2069  `apply``. This i
+0000fbc0: 7320 7061 7274 6963 756c 6172 6c79 2075  s particularly u
+0000fbd0: 7365 6675 6c20 666f 720a 2020 2020 6465  seful for.    de
+0000fbe0: 6275 6767 696e 6720 616e 6420 696e 7465  bugging and inte
+0000fbf0: 7261 6374 6976 6520 7573 6520 6361 7365  ractive use case
+0000fc00: 7320 6c69 6b65 206e 6f74 6562 6f6f 6b73  s like notebooks
+0000fc10: 2077 6865 7265 2061 2066 756e 6374 696f   where a functio
+0000fc20: 6e20 776f 756c 640a 2020 2020 6c69 6d69  n would.    limi
+0000fc30: 7420 7468 6520 6162 696c 6974 7920 746f  t the ability to
+0000fc40: 2073 706c 6974 2075 7020 636f 6465 2069   split up code i
+0000fc50: 6e74 6f20 6469 6666 6572 656e 7420 6365  nto different ce
+0000fc60: 6c6c 732e 0a0a 2020 2020 4f6e 6365 2074  lls...    Once t
+0000fc70: 6865 2076 6172 6961 626c 6573 2028 616e  he variables (an
+0000fc80: 6420 6f70 7469 6f6e 616c 6c79 2052 4e47  d optionally RNG
+0000fc90: 7329 2061 7265 2062 6f75 6e64 2074 6f20  s) are bound to 
+0000fca0: 6120 6060 4d6f 6475 6c65 6060 2069 740a  a ``Module`` it.
+0000fcb0: 2020 2020 6265 636f 6d65 7320 6120 7374      becomes a st
+0000fcc0: 6174 6566 756c 206f 626a 6563 742e 204e  ateful object. N
+0000fcd0: 6f74 6520 7468 6174 2069 6469 6f6d 6174  ote that idiomat
+0000fce0: 6963 204a 4158 2069 7320 6675 6e63 7469  ic JAX is functi
+0000fcf0: 6f6e 616c 2061 6e64 0a20 2020 2074 6865  onal and.    the
+0000fd00: 7265 666f 7265 2061 6e20 696e 7465 7261  refore an intera
+0000fd10: 6374 6976 6520 696e 7374 616e 6365 2064  ctive instance d
+0000fd20: 6f65 7320 6e6f 7420 6d69 7820 7765 6c6c  oes not mix well
+0000fd30: 2077 6974 6820 7661 6e69 6c6c 6120 4a41   with vanilla JA
+0000fd40: 5820 4150 4973 2e0a 2020 2020 6060 6269  X APIs..    ``bi
+0000fd50: 6e64 2829 6060 2073 686f 756c 6420 6f6e  nd()`` should on
+0000fd60: 6c79 2062 6520 7573 6564 2066 6f72 2069  ly be used for i
+0000fd70: 6e74 6572 6163 7469 7665 2065 7870 6572  nteractive exper
+0000fd80: 696d 656e 7461 7469 6f6e 2c20 616e 6420  imentation, and 
+0000fd90: 696e 2061 6c6c 0a20 2020 206f 7468 6572  in all.    other
+0000fda0: 2063 6173 6573 2077 6520 7374 726f 6e67   cases we strong
+0000fdb0: 6c79 2065 6e63 6f75 7261 6765 2075 7365  ly encourage use
+0000fdc0: 7273 2074 6f20 7573 6520 6060 6170 706c  rs to use ``appl
+0000fdd0: 7928 2960 6020 696e 7374 6561 642e 0a0a  y()`` instead...
+0000fde0: 2020 2020 4578 616d 706c 653a 3a0a 0a20      Example::.. 
+0000fdf0: 2020 2020 203e 3e3e 2069 6d70 6f72 7420       >>> import 
+0000fe00: 6a61 780a 2020 2020 2020 3e3e 3e20 696d  jax.      >>> im
+0000fe10: 706f 7274 206a 6178 2e6e 756d 7079 2061  port jax.numpy a
+0000fe20: 7320 6a6e 700a 2020 2020 2020 3e3e 3e20  s jnp.      >>> 
+0000fe30: 696d 706f 7274 2066 6c61 782e 6c69 6e65  import flax.line
+0000fe40: 6e20 6173 206e 6e0a 0a20 2020 2020 203e  n as nn..      >
+0000fe50: 3e3e 2063 6c61 7373 2041 7574 6f45 6e63  >> class AutoEnc
+0000fe60: 6f64 6572 286e 6e2e 4d6f 6475 6c65 293a  oder(nn.Module):
+0000fe70: 0a20 2020 2020 202e 2e2e 2020 2064 6566  .      ...   def
+0000fe80: 2073 6574 7570 2873 656c 6629 3a0a 2020   setup(self):.  
+0000fe90: 2020 2020 2e2e 2e20 2020 2020 7365 6c66      ...     self
+0000fea0: 2e65 6e63 6f64 6572 203d 206e 6e2e 4465  .encoder = nn.De
+0000feb0: 6e73 6528 3329 0a20 2020 2020 202e 2e2e  nse(3).      ...
+0000fec0: 2020 2020 2073 656c 662e 6465 636f 6465       self.decode
+0000fed0: 7220 3d20 6e6e 2e44 656e 7365 2835 290a  r = nn.Dense(5).
+0000fee0: 2020 2020 2020 2e2e 2e0a 2020 2020 2020        ....      
+0000fef0: 2e2e 2e20 2020 6465 6620 5f5f 6361 6c6c  ...   def __call
+0000ff00: 5f5f 2873 656c 662c 2078 293a 0a20 2020  __(self, x):.   
+0000ff10: 2020 202e 2e2e 2020 2020 2072 6574 7572     ...     retur
+0000ff20: 6e20 7365 6c66 2e64 6563 6f64 6572 2873  n self.decoder(s
+0000ff30: 656c 662e 656e 636f 6465 7228 7829 290a  elf.encoder(x)).
+0000ff40: 0a20 2020 2020 203e 3e3e 2078 203d 206a  .      >>> x = j
+0000ff50: 6e70 2e6f 6e65 7328 2831 362c 2039 2929  np.ones((16, 9))
+0000ff60: 0a20 2020 2020 203e 3e3e 2061 6520 3d20  .      >>> ae = 
+0000ff70: 4175 746f 456e 636f 6465 7228 290a 2020  AutoEncoder().  
+0000ff80: 2020 2020 3e3e 3e20 7661 7269 6162 6c65      >>> variable
+0000ff90: 7320 3d20 6165 2e69 6e69 7428 6a61 782e  s = ae.init(jax.
+0000ffa0: 7261 6e64 6f6d 2e6b 6579 2830 292c 2078  random.key(0), x
+0000ffb0: 290a 2020 2020 2020 3e3e 3e20 6d6f 6465  ).      >>> mode
+0000ffc0: 6c20 3d20 6165 2e62 696e 6428 7661 7269  l = ae.bind(vari
+0000ffd0: 6162 6c65 7329 0a20 2020 2020 203e 3e3e  ables).      >>>
+0000ffe0: 207a 203d 206d 6f64 656c 2e65 6e63 6f64   z = model.encod
+0000fff0: 6572 2878 290a 2020 2020 2020 3e3e 3e20  er(x).      >>> 
+00010000: 785f 7265 636f 6e73 7472 7563 7465 6420  x_reconstructed 
+00010010: 3d20 6d6f 6465 6c2e 6465 636f 6465 7228  = model.decoder(
+00010020: 7a29 0a0a 2020 2020 4172 6773 3a0a 2020  z)..    Args:.  
+00010030: 2020 2020 7661 7269 6162 6c65 733a 2041      variables: A
+00010040: 2064 6963 7469 6f6e 6172 7920 636f 6e74   dictionary cont
+00010050: 6169 6e69 6e67 2076 6172 6961 626c 6573  aining variables
+00010060: 206b 6579 6564 2062 7920 7661 7269 6162   keyed by variab
+00010070: 6c65 0a20 2020 2020 2020 2063 6f6c 6c65  le.        colle
+00010080: 6374 696f 6e73 2e20 5365 6520 3a6d 6f64  ctions. See :mod
+00010090: 3a60 666c 6178 2e63 6f72 652e 7661 7269  :`flax.core.vari
+000100a0: 6162 6c65 7360 2066 6f72 206d 6f72 6520  ables` for more 
+000100b0: 6465 7461 696c 7320 6162 6f75 740a 2020  details about.  
+000100c0: 2020 2020 2020 7661 7269 6162 6c65 732e        variables.
+000100d0: 0a20 2020 2020 202a 6172 6773 3a20 4e61  .      *args: Na
+000100e0: 6d65 6420 6172 6775 6d65 6e74 7320 286e  med arguments (n
+000100f0: 6f74 2075 7365 6429 2e0a 2020 2020 2020  ot used)..      
+00010100: 726e 6773 3a20 6120 6469 6374 206f 6620  rngs: a dict of 
+00010110: 5052 4e47 4b65 7973 2074 6f20 696e 6974  PRNGKeys to init
+00010120: 6961 6c69 7a65 2074 6865 2050 524e 4720  ialize the PRNG 
+00010130: 7365 7175 656e 6365 732e 0a20 2020 2020  sequences..     
+00010140: 206d 7574 6162 6c65 3a20 4361 6e20 6265   mutable: Can be
+00010150: 2062 6f6f 6c2c 2073 7472 2c20 6f72 206c   bool, str, or l
+00010160: 6973 742e 2053 7065 6369 6669 6573 2077  ist. Specifies w
+00010170: 6869 6368 2063 6f6c 6c65 6374 696f 6e73  hich collections
+00010180: 2073 686f 756c 6420 6265 0a20 2020 2020   should be.     
+00010190: 2020 2074 7265 6174 6564 2061 7320 6d75     treated as mu
+000101a0: 7461 626c 653a 2060 6062 6f6f 6c60 603a  table: ``bool``:
+000101b0: 2061 6c6c 2f6e 6f20 636f 6c6c 6563 7469   all/no collecti
+000101c0: 6f6e 7320 6172 6520 6d75 7461 626c 652e  ons are mutable.
+000101d0: 2060 6073 7472 6060 3a0a 2020 2020 2020   ``str``:.      
+000101e0: 2020 5468 6520 6e61 6d65 206f 6620 6120    The name of a 
+000101f0: 7369 6e67 6c65 206d 7574 6162 6c65 2063  single mutable c
+00010200: 6f6c 6c65 6374 696f 6e2e 2060 606c 6973  ollection. ``lis
+00010210: 7460 603a 2041 206c 6973 7420 6f66 206e  t``: A list of n
+00010220: 616d 6573 206f 660a 2020 2020 2020 2020  ames of.        
+00010230: 6d75 7461 626c 6520 636f 6c6c 6563 7469  mutable collecti
+00010240: 6f6e 732e 0a0a 2020 2020 5265 7475 726e  ons...    Return
+00010250: 733a 0a20 2020 2020 2041 2063 6f70 7920  s:.      A copy 
+00010260: 6f66 2074 6869 7320 696e 7374 616e 6365  of this instance
+00010270: 2077 6974 6820 626f 756e 6420 7661 7269   with bound vari
+00010280: 6162 6c65 7320 616e 6420 524e 4773 2e0a  ables and RNGs..
+00010290: 2020 2020 2222 220a 2020 2020 4d6f 6475      """.    Modu
+000102a0: 6c65 2e5f 6d6f 6475 6c65 5f63 6865 636b  le._module_check
+000102b0: 7328 7365 6c66 290a 0a20 2020 2064 656c  s(self)..    del
+000102c0: 2061 7267 730a 2020 2020 7363 6f70 6520   args.    scope 
+000102d0: 3d20 636f 7265 2e62 696e 6428 7661 7269  = core.bind(vari
+000102e0: 6162 6c65 732c 2072 6e67 733d 726e 6773  ables, rngs=rngs
+000102f0: 2c20 6d75 7461 626c 653d 6d75 7461 626c  , mutable=mutabl
+00010300: 6529 0a20 2020 2072 6574 7572 6e20 7365  e).    return se
+00010310: 6c66 2e63 6c6f 6e65 2870 6172 656e 743d  lf.clone(parent=
+00010320: 7363 6f70 652c 205f 6465 6570 5f63 6c6f  scope, _deep_clo
+00010330: 6e65 3d54 7275 6529 0a0a 2020 6465 6620  ne=True)..  def 
+00010340: 756e 6269 6e64 2873 656c 663a 204d 2920  unbind(self: M) 
+00010350: 2d3e 2054 7570 6c65 5b4d 2c20 5661 7269  -> Tuple[M, Vari
+00010360: 6162 6c65 4469 6374 5d3a 0a20 2020 2022  ableDict]:.    "
+00010370: 2222 5265 7475 726e 7320 616e 2075 6e62  ""Returns an unb
+00010380: 6f75 6e64 2063 6f70 7920 6f66 2061 204d  ound copy of a M
+00010390: 6f64 756c 6520 616e 6420 6974 7320 7661  odule and its va
+000103a0: 7269 6162 6c65 732e 0a0a 2020 2020 6060  riables...    ``
+000103b0: 756e 6269 6e64 6060 2068 656c 7073 2063  unbind`` helps c
+000103c0: 7265 6174 6520 6120 7374 6174 656c 6573  reate a stateles
+000103d0: 7320 7665 7273 696f 6e20 6f66 2061 2062  s version of a b
+000103e0: 6f75 6e64 204d 6f64 756c 652e 0a0a 2020  ound Module...  
+000103f0: 2020 416e 2065 7861 6d70 6c65 206f 6620    An example of 
+00010400: 6120 636f 6d6d 6f6e 2075 7365 2063 6173  a common use cas
+00010410: 653a 2074 6f20 6578 7472 6163 7420 6120  e: to extract a 
+00010420: 7375 622d 4d6f 6475 6c65 2064 6566 696e  sub-Module defin
+00010430: 6564 2069 6e73 6964 650a 2020 2020 6060  ed inside.    ``
+00010440: 7365 7475 7028 2960 6020 616e 6420 6974  setup()`` and it
+00010450: 7320 636f 7272 6573 706f 6e64 696e 6720  s corresponding 
+00010460: 7661 7269 6162 6c65 733a 2031 2920 7465  variables: 1) te
+00010470: 6d70 6f72 6172 696c 7920 6060 6269 6e64  mporarily ``bind
+00010480: 6060 2074 6865 0a20 2020 2070 6172 656e  `` the.    paren
+00010490: 7420 4d6f 6475 6c65 3b20 616e 6420 7468  t Module; and th
+000104a0: 656e 2032 2920 6060 756e 6269 6e64 6060  en 2) ``unbind``
+000104b0: 2074 6865 2064 6573 6972 6564 2073 7562   the desired sub
+000104c0: 2d4d 6f64 756c 652e 2028 5265 6361 6c6c  -Module. (Recall
+000104d0: 2074 6861 740a 2020 2020 6060 7365 7475   that.    ``setu
+000104e0: 7028 2960 6020 6973 206f 6e6c 7920 6361  p()`` is only ca
+000104f0: 6c6c 6564 2077 6865 6e20 7468 6520 4d6f  lled when the Mo
+00010500: 6475 6c65 2069 7320 626f 756e 642e 293a  dule is bound.):
+00010510: 3a0a 0a20 2020 2020 203e 3e3e 2063 6c61  :..      >>> cla
+00010520: 7373 2045 6e63 6f64 6572 286e 6e2e 4d6f  ss Encoder(nn.Mo
+00010530: 6475 6c65 293a 0a20 2020 2020 202e 2e2e  dule):.      ...
+00010540: 2020 2040 6e6e 2e63 6f6d 7061 6374 0a20     @nn.compact. 
+00010550: 2020 2020 202e 2e2e 2020 2064 6566 205f       ...   def _
+00010560: 5f63 616c 6c5f 5f28 7365 6c66 2c20 7829  _call__(self, x)
+00010570: 3a0a 2020 2020 2020 2e2e 2e20 2020 2020  :.      ...     
+00010580: 2e2e 2e0a 2020 2020 2020 2e2e 2e20 2020  ....      ...   
+00010590: 2020 7265 7475 726e 206e 6e2e 4465 6e73    return nn.Dens
+000105a0: 6528 3235 3629 2878 290a 0a20 2020 2020  e(256)(x)..     
+000105b0: 203e 3e3e 2063 6c61 7373 2044 6563 6f64   >>> class Decod
+000105c0: 6572 286e 6e2e 4d6f 6475 6c65 293a 0a20  er(nn.Module):. 
+000105d0: 2020 2020 202e 2e2e 2020 2040 6e6e 2e63       ...   @nn.c
+000105e0: 6f6d 7061 6374 0a20 2020 2020 202e 2e2e  ompact.      ...
+000105f0: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
+00010600: 7365 6c66 2c20 7829 3a0a 2020 2020 2020  self, x):.      
+00010610: 2e2e 2e20 2020 2020 2e2e 2e0a 2020 2020  ...     ....    
+00010620: 2020 2e2e 2e20 2020 2020 7265 7475 726e    ...     return
+00010630: 206e 6e2e 4465 6e73 6528 3738 3429 2878   nn.Dense(784)(x
+00010640: 290a 0a20 2020 2020 203e 3e3e 2063 6c61  )..      >>> cla
+00010650: 7373 2041 7574 6f45 6e63 6f64 6572 286e  ss AutoEncoder(n
+00010660: 6e2e 4d6f 6475 6c65 293a 0a20 2020 2020  n.Module):.     
+00010670: 202e 2e2e 2020 2064 6566 2073 6574 7570   ...   def setup
+00010680: 2873 656c 6629 3a0a 2020 2020 2020 2e2e  (self):.      ..
+00010690: 2e20 2020 2020 7365 6c66 2e65 6e63 6f64  .     self.encod
+000106a0: 6572 203d 2045 6e63 6f64 6572 2829 0a20  er = Encoder(). 
+000106b0: 2020 2020 202e 2e2e 2020 2020 2073 656c       ...     sel
+000106c0: 662e 6465 636f 6465 7220 3d20 4465 636f  f.decoder = Deco
+000106d0: 6465 7228 290a 2020 2020 2020 2e2e 2e0a  der().      ....
+000106e0: 2020 2020 2020 2e2e 2e20 2020 6465 6620        ...   def 
+000106f0: 5f5f 6361 6c6c 5f5f 2873 656c 662c 2078  __call__(self, x
+00010700: 293a 0a20 2020 2020 202e 2e2e 2020 2020  ):.      ...    
+00010710: 2072 6574 7572 6e20 7365 6c66 2e64 6563   return self.dec
+00010720: 6f64 6572 2873 656c 662e 656e 636f 6465  oder(self.encode
+00010730: 7228 7829 290a 0a20 2020 2020 203e 3e3e  r(x))..      >>>
+00010740: 206d 6f64 756c 6520 3d20 4175 746f 456e   module = AutoEn
+00010750: 636f 6465 7228 290a 2020 2020 2020 3e3e  coder().      >>
+00010760: 3e20 7661 7269 6162 6c65 7320 3d20 6d6f  > variables = mo
+00010770: 6475 6c65 2e69 6e69 7428 6a61 782e 7261  dule.init(jax.ra
+00010780: 6e64 6f6d 2e6b 6579 2830 292c 206a 6e70  ndom.key(0), jnp
+00010790: 2e6f 6e65 7328 2831 2c20 3738 3429 2929  .ones((1, 784)))
+000107a0: 0a0a 2020 2020 2020 3e3e 3e20 2320 4578  ..      >>> # Ex
+000107b0: 7472 6163 7420 7468 6520 456e 636f 6465  tract the Encode
+000107c0: 7220 7375 622d 4d6f 6475 6c65 2061 6e64  r sub-Module and
+000107d0: 2069 7473 2076 6172 6961 626c 6573 0a20   its variables. 
+000107e0: 2020 2020 203e 3e3e 2065 6e63 6f64 6572       >>> encoder
+000107f0: 2c20 656e 636f 6465 725f 7661 7273 203d  , encoder_vars =
+00010800: 206d 6f64 756c 652e 6269 6e64 2876 6172   module.bind(var
+00010810: 6961 626c 6573 292e 656e 636f 6465 722e  iables).encoder.
+00010820: 756e 6269 6e64 2829 0a0a 2020 2020 5265  unbind()..    Re
+00010830: 7475 726e 733a 0a20 2020 2020 2041 2074  turns:.      A t
+00010840: 7570 6c65 2077 6974 6820 616e 2075 6e62  uple with an unb
+00010850: 6f75 6e64 2063 6f70 7920 6f66 2074 6869  ound copy of thi
+00010860: 7320 4d6f 6475 6c65 2061 6e64 2069 7473  s Module and its
+00010870: 2076 6172 6961 626c 6573 2e0a 2020 2020   variables..    
+00010880: 2222 220a 2020 2020 4d6f 6475 6c65 2e5f  """.    Module._
+00010890: 6d6f 6475 6c65 5f63 6865 636b 7328 7365  module_checks(se
+000108a0: 6c66 290a 0a20 2020 2069 6620 7365 6c66  lf)..    if self
+000108b0: 2e73 636f 7065 2069 7320 4e6f 6e65 3a0a  .scope is None:.
+000108c0: 2020 2020 2020 7261 6973 6520 6572 726f        raise erro
+000108d0: 7273 2e43 616c 6c55 6e62 696e 644f 6e55  rs.CallUnbindOnU
+000108e0: 6e62 6f75 6e64 4d6f 6475 6c65 4572 726f  nboundModuleErro
+000108f0: 7228 290a 0a20 2020 2076 6172 6961 626c  r()..    variabl
+00010900: 6573 203d 2073 656c 662e 7661 7269 6162  es = self.variab
+00010910: 6c65 730a 2020 2020 6d6f 6475 6c65 203d  les.    module =
+00010920: 2073 656c 662e 636c 6f6e 6528 5f64 6565   self.clone(_dee
+00010930: 705f 636c 6f6e 653d 5472 7565 2c20 5f72  p_clone=True, _r
+00010940: 6573 6574 5f6e 616d 6573 3d54 7275 652c  eset_names=True,
+00010950: 206e 616d 653d 4e6f 6e65 290a 2020 2020   name=None).    
+00010960: 7265 7475 726e 206d 6f64 756c 652c 2076  return module, v
+00010970: 6172 6961 626c 6573 0a0a 2020 4074 7261  ariables..  @tra
+00010980: 6365 6261 636b 5f75 7469 6c2e 6170 695f  ceback_util.api_
+00010990: 626f 756e 6461 7279 0a20 2064 6566 2061  boundary.  def a
+000109a0: 7070 6c79 280a 2020 2020 7365 6c66 2c0a  pply(.    self,.
+000109b0: 2020 2020 7661 7269 6162 6c65 733a 2056      variables: V
+000109c0: 6172 6961 626c 6544 6963 742c 0a20 2020  ariableDict,.   
+000109d0: 202a 6172 6773 2c0a 2020 2020 726e 6773   *args,.    rngs
+000109e0: 3a20 4f70 7469 6f6e 616c 5b55 6e69 6f6e  : Optional[Union
+000109f0: 5b50 524e 474b 6579 2c20 524e 4753 6571  [PRNGKey, RNGSeq
+00010a00: 7565 6e63 6573 5d5d 203d 204e 6f6e 652c  uences]] = None,
+00010a10: 0a20 2020 206d 6574 686f 643a 2055 6e69  .    method: Uni
+00010a20: 6f6e 5b43 616c 6c61 626c 655b 2e2e 2e2c  on[Callable[...,
+00010a30: 2041 6e79 5d2c 2073 7472 2c20 4e6f 6e65   Any], str, None
+00010a40: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6d75  ] = None,.    mu
+00010a50: 7461 626c 653a 2043 6f6c 6c65 6374 696f  table: Collectio
+00010a60: 6e46 696c 7465 7220 3d20 4661 6c73 652c  nFilter = False,
+00010a70: 0a20 2020 2063 6170 7475 7265 5f69 6e74  .    capture_int
+00010a80: 6572 6d65 6469 6174 6573 3a20 556e 696f  ermediates: Unio
+00010a90: 6e5b 626f 6f6c 2c20 4361 6c6c 6162 6c65  n[bool, Callable
+00010aa0: 5b5b 274d 6f64 756c 6527 2c20 7374 725d  [['Module', str]
+00010ab0: 2c20 626f 6f6c 5d5d 203d 2046 616c 7365  , bool]] = False
+00010ac0: 2c0a 2020 2020 2a2a 6b77 6172 6773 2c0a  ,.    **kwargs,.
+00010ad0: 2020 2920 2d3e 2055 6e69 6f6e 5b41 6e79    ) -> Union[Any
+00010ae0: 2c20 5475 706c 655b 416e 792c 2055 6e69  , Tuple[Any, Uni
+00010af0: 6f6e 5b46 726f 7a65 6e56 6172 6961 626c  on[FrozenVariabl
+00010b00: 6544 6963 742c 2044 6963 745b 7374 722c  eDict, Dict[str,
+00010b10: 2041 6e79 5d5d 5d5d 3a0a 2020 2020 2222   Any]]]]:.    ""
+00010b20: 2241 7070 6c69 6573 2061 206d 6f64 756c  "Applies a modul
+00010b30: 6520 6d65 7468 6f64 2074 6f20 7661 7269  e method to vari
+00010b40: 6162 6c65 7320 616e 6420 7265 7475 726e  ables and return
+00010b50: 7320 6f75 7470 7574 2061 6e64 206d 6f64  s output and mod
+00010b60: 6966 6965 6420 7661 7269 6162 6c65 732e  ified variables.
+00010b70: 0a0a 2020 2020 4e6f 7465 2074 6861 7420  ..    Note that 
+00010b80: 6060 6d65 7468 6f64 6060 2073 686f 756c  ``method`` shoul
+00010b90: 6420 6265 2073 6574 2069 6620 6f6e 6520  d be set if one 
+00010ba0: 776f 756c 6420 6c69 6b65 2074 6f20 6361  would like to ca
+00010bb0: 6c6c 2060 6061 7070 6c79 6060 206f 6e20  ll ``apply`` on 
+00010bc0: 610a 2020 2020 6469 6666 6572 656e 7420  a.    different 
+00010bd0: 636c 6173 7320 6d65 7468 6f64 2074 6861  class method tha
+00010be0: 6e20 6060 5f5f 6361 6c6c 5f5f 6060 2e20  n ``__call__``. 
+00010bf0: 466f 7220 696e 7374 616e 6365 2c20 7375  For instance, su
+00010c00: 7070 6f73 6520 610a 2020 2020 5472 616e  ppose a.    Tran
+00010c10: 7366 6f72 6d65 7220 6d6f 6475 6c65 7320  sformer modules 
+00010c20: 6861 7320 6120 6d65 7468 6f64 2063 616c  has a method cal
+00010c30: 6c65 6420 6060 656e 636f 6465 6060 2c20  led ``encode``, 
+00010c40: 7468 656e 2074 6865 2066 6f6c 6c6f 7769  then the followi
+00010c50: 6e67 2063 616c 6c73 0a20 2020 2060 6061  ng calls.    ``a
+00010c60: 7070 6c79 6060 206f 6e20 7468 6174 206d  pply`` on that m
+00010c70: 6574 686f 643a 3a0a 0a20 2020 2020 203e  ethod::..      >
+00010c80: 3e3e 2069 6d70 6f72 7420 666c 6178 2e6c  >> import flax.l
+00010c90: 696e 656e 2061 7320 6e6e 0a20 2020 2020  inen as nn.     
+00010ca0: 203e 3e3e 2069 6d70 6f72 7420 6a61 782c   >>> import jax,
+00010cb0: 206a 6178 2e6e 756d 7079 2061 7320 6a6e   jax.numpy as jn
+00010cc0: 700a 2020 2020 2020 3e3e 3e20 696d 706f  p.      >>> impo
+00010cd0: 7274 206e 756d 7079 2061 7320 6e70 0a0a  rt numpy as np..
+00010ce0: 2020 2020 2020 3e3e 3e20 636c 6173 7320        >>> class 
+00010cf0: 5472 616e 7366 6f72 6d65 7228 6e6e 2e4d  Transformer(nn.M
+00010d00: 6f64 756c 6529 3a0a 2020 2020 2020 2e2e  odule):.      ..
+00010d10: 2e20 2020 6465 6620 656e 636f 6465 2873  .   def encode(s
+00010d20: 656c 662c 2078 293a 0a20 2020 2020 202e  elf, x):.      .
+00010d30: 2e2e 2020 2020 202e 2e2e 0a0a 2020 2020  ..     .....    
+00010d40: 2020 3e3e 3e20 7820 3d20 6a6e 702e 6f6e    >>> x = jnp.on
+00010d50: 6573 2828 3136 2c20 3929 290a 2020 2020  es((16, 9)).    
+00010d60: 2020 3e3e 3e20 6d6f 6465 6c20 3d20 5472    >>> model = Tr
+00010d70: 616e 7366 6f72 6d65 7228 290a 2020 2020  ansformer().    
+00010d80: 2020 3e3e 3e20 7661 7269 6162 6c65 7320    >>> variables 
+00010d90: 3d20 6d6f 6465 6c2e 696e 6974 286a 6178  = model.init(jax
+00010da0: 2e72 616e 646f 6d2e 6b65 7928 3029 2c20  .random.key(0), 
+00010db0: 782c 206d 6574 686f 643d 5472 616e 7366  x, method=Transf
+00010dc0: 6f72 6d65 722e 656e 636f 6465 290a 0a20  ormer.encode).. 
+00010dd0: 2020 2020 203e 3e3e 2065 6e63 6f64 6564       >>> encoded
+00010de0: 203d 206d 6f64 656c 2e61 7070 6c79 2876   = model.apply(v
+00010df0: 6172 6961 626c 6573 2c20 782c 206d 6574  ariables, x, met
+00010e00: 686f 643d 5472 616e 7366 6f72 6d65 722e  hod=Transformer.
+00010e10: 656e 636f 6465 290a 0a20 2020 2049 6620  encode)..    If 
+00010e20: 6120 6675 6e63 7469 6f6e 2069 6e73 7461  a function insta
+00010e30: 6e63 6520 6973 2070 726f 7669 6465 642c  nce is provided,
+00010e40: 2074 6865 2075 6e62 6f75 6e64 2066 756e   the unbound fun
+00010e50: 6374 696f 6e20 6973 2075 7365 642e 2046  ction is used. F
+00010e60: 6f72 0a20 2020 2069 6e73 7461 6e63 652c  or.    instance,
+00010e70: 2074 6865 2065 7861 6d70 6c65 2062 656c   the example bel
+00010e80: 6f77 2069 7320 6571 7569 7661 6c65 6e74  ow is equivalent
+00010e90: 2074 6f20 7468 6520 6f6e 6520 6162 6f76   to the one abov
+00010ea0: 653a 3a0a 0a20 2020 2020 203e 3e3e 2065  e::..      >>> e
+00010eb0: 6e63 6f64 6564 203d 206d 6f64 656c 2e61  ncoded = model.a
+00010ec0: 7070 6c79 2876 6172 6961 626c 6573 2c20  pply(variables, 
+00010ed0: 782c 206d 6574 686f 643d 6d6f 6465 6c2e  x, method=model.
+00010ee0: 656e 636f 6465 290a 0a20 2020 2059 6f75  encode)..    You
+00010ef0: 2063 616e 2061 6c73 6f20 7061 7373 2061   can also pass a
+00010f00: 2073 7472 696e 6720 746f 2061 2063 616c   string to a cal
+00010f10: 6c61 626c 6520 6174 7472 6962 7574 6520  lable attribute 
+00010f20: 6f66 2074 6865 206d 6f64 756c 652e 2046  of the module. F
+00010f30: 6f72 0a20 2020 2065 7861 6d70 6c65 2c20  or.    example, 
+00010f40: 7468 6520 7072 6576 696f 7573 2063 616e  the previous can
+00010f50: 2062 6520 7772 6974 7465 6e20 6173 3a3a   be written as::
+00010f60: 0a0a 2020 2020 2020 3e3e 3e20 656e 636f  ..      >>> enco
+00010f70: 6465 6420 3d20 6d6f 6465 6c2e 6170 706c  ded = model.appl
+00010f80: 7928 7661 7269 6162 6c65 732c 2078 2c20  y(variables, x, 
+00010f90: 6d65 7468 6f64 3d27 656e 636f 6465 2729  method='encode')
+00010fa0: 0a0a 2020 2020 4e6f 7465 2060 606d 6574  ..    Note ``met
+00010fb0: 686f 6460 6020 6361 6e20 616c 736f 2062  hod`` can also b
+00010fc0: 6520 6120 6675 6e63 7469 6f6e 2074 6861  e a function tha
+00010fd0: 7420 6973 206e 6f74 2064 6566 696e 6564  t is not defined
+00010fe0: 2069 6e0a 2020 2020 6060 5472 616e 7366   in.    ``Transf
+00010ff0: 6f72 6d65 7260 602e 2049 6e20 7468 6174  ormer``. In that
+00011000: 2063 6173 652c 2074 6865 2066 756e 6374   case, the funct
+00011010: 696f 6e20 7368 6f75 6c64 2068 6176 6520  ion should have 
+00011020: 6174 206c 6561 7374 206f 6e65 0a20 2020  at least one.   
+00011030: 2061 7267 756d 656e 7420 7265 7072 6573   argument repres
+00011040: 656e 7469 6e67 2061 6e20 696e 7374 616e  enting an instan
+00011050: 6365 206f 6620 7468 6520 4d6f 6475 6c65  ce of the Module
+00011060: 2063 6c61 7373 3a3a 0a0a 2020 2020 2020   class::..      
+00011070: 3e3e 3e20 6465 6620 6f74 6865 725f 666e  >>> def other_fn
+00011080: 2869 6e73 7461 6e63 652c 2078 293a 0a20  (instance, x):. 
+00011090: 2020 2020 202e 2e2e 2020 2023 2069 6e73       ...   # ins
+000110a0: 7461 6e63 652e 736f 6d65 5f6d 6f64 756c  tance.some_modul
+000110b0: 655f 6174 7472 282e 2e2e 290a 2020 2020  e_attr(...).    
+000110c0: 2020 2e2e 2e20 2020 696e 7374 616e 6365    ...   instance
+000110d0: 2e65 6e63 6f64 650a 2020 2020 2020 2e2e  .encode.      ..
+000110e0: 2e20 2020 2e2e 2e0a 0a20 2020 2020 203e  .   .....      >
+000110f0: 3e3e 206d 6f64 656c 2e61 7070 6c79 2876  >> model.apply(v
+00011100: 6172 6961 626c 6573 2c20 782c 206d 6574  ariables, x, met
+00011110: 686f 643d 6f74 6865 725f 666e 290a 0a20  hod=other_fn).. 
+00011120: 2020 2049 6620 796f 7520 7061 7373 2061     If you pass a
+00011130: 2073 696e 676c 6520 6060 5052 4e47 4b65   single ``PRNGKe
+00011140: 7960 602c 2046 6c61 7820 7769 6c6c 2075  y``, Flax will u
+00011150: 7365 2069 7420 746f 2066 6565 6420 7468  se it to feed th
+00011160: 6520 6060 2770 6172 616d 7327 6060 0a20  e ``'params'``. 
+00011170: 2020 2052 4e47 2073 7472 6561 6d2e 2020     RNG stream.  
+00011180: 4966 2079 6f75 2077 616e 7420 746f 2075  If you want to u
+00011190: 7365 2061 2064 6966 6665 7265 6e74 2052  se a different R
+000111a0: 4e47 2073 7472 6561 6d20 6f72 206e 6565  NG stream or nee
+000111b0: 6420 746f 2075 7365 0a20 2020 206d 756c  d to use.    mul
+000111c0: 7469 706c 6520 7374 7265 616d 732c 2079  tiple streams, y
+000111d0: 6f75 2063 616e 2070 6173 7320 6120 6469  ou can pass a di
+000111e0: 6374 696f 6e61 7279 206d 6170 7069 6e67  ctionary mapping
+000111f0: 2065 6163 6820 524e 4720 7374 7265 616d   each RNG stream
+00011200: 206e 616d 650a 2020 2020 746f 2069 7473   name.    to its
+00011210: 2063 6f72 7265 7370 6f6e 6469 6e67 2060   corresponding `
+00011220: 6050 524e 474b 6579 6060 2074 6f20 6060  `PRNGKey`` to ``
+00011230: 6170 706c 7960 602e 2049 6620 6060 7365  apply``. If ``se
+00011240: 6c66 2e6d 616b 655f 726e 6728 6e61 6d65  lf.make_rng(name
+00011250: 2960 600a 2020 2020 6973 2063 616c 6c65  )``.    is calle
+00011260: 6420 6f6e 2061 6e20 524e 4720 7374 7265  d on an RNG stre
+00011270: 616d 206e 616d 6520 7468 6174 2069 736e  am name that isn
+00011280: 2774 2070 6173 7365 6420 6279 2074 6865  't passed by the
+00011290: 2075 7365 722c 2069 7420 7769 6c6c 0a20   user, it will. 
+000112a0: 2020 2064 6566 6175 6c74 2074 6f20 7573     default to us
+000112b0: 696e 6720 7468 6520 6060 2770 6172 616d  ing the ``'param
+000112c0: 7327 6060 2052 4e47 2073 7472 6561 6d2e  s'`` RNG stream.
+000112d0: 0a0a 2020 2020 4578 616d 706c 653a 3a0a  ..    Example::.
+000112e0: 0a20 2020 2020 203e 3e3e 2063 6c61 7373  .      >>> class
+000112f0: 2046 6f6f 286e 6e2e 4d6f 6475 6c65 293a   Foo(nn.Module):
+00011300: 0a20 2020 2020 202e 2e2e 2020 2040 6e6e  .      ...   @nn
+00011310: 2e63 6f6d 7061 6374 0a20 2020 2020 202e  .compact.      .
+00011320: 2e2e 2020 2064 6566 205f 5f63 616c 6c5f  ..   def __call_
+00011330: 5f28 7365 6c66 2c20 782c 2061 6464 5f6e  _(self, x, add_n
+00011340: 6f69 7365 3d46 616c 7365 293a 0a20 2020  oise=False):.   
+00011350: 2020 202e 2e2e 2020 2020 2078 203d 206e     ...     x = n
+00011360: 6e2e 4465 6e73 6528 3136 2928 7829 0a20  n.Dense(16)(x). 
+00011370: 2020 2020 202e 2e2e 2020 2020 2078 203d       ...     x =
+00011380: 206e 6e2e 7265 6c75 2878 290a 2020 2020   nn.relu(x).    
+00011390: 2020 2e2e 2e0a 2020 2020 2020 2e2e 2e20    ....      ... 
+000113a0: 2020 2020 6966 2061 6464 5f6e 6f69 7365      if add_noise
+000113b0: 3a0a 2020 2020 2020 2e2e 2e20 2020 2020  :.      ...     
+000113c0: 2020 2320 4164 6420 6761 7573 7369 616e    # Add gaussian
+000113d0: 206e 6f69 7365 0a20 2020 2020 202e 2e2e   noise.      ...
+000113e0: 2020 2020 2020 206e 6f69 7365 5f6b 6579         noise_key
+000113f0: 203d 2073 656c 662e 6d61 6b65 5f72 6e67   = self.make_rng
+00011400: 2827 6e6f 6973 6527 290a 2020 2020 2020  ('noise').      
+00011410: 2e2e 2e20 2020 2020 2020 7820 3d20 7820  ...       x = x 
+00011420: 2b20 6a61 782e 7261 6e64 6f6d 2e6e 6f72  + jax.random.nor
+00011430: 6d61 6c28 6e6f 6973 655f 6b65 792c 2078  mal(noise_key, x
+00011440: 2e73 6861 7065 290a 2020 2020 2020 2e2e  .shape).      ..
+00011450: 2e0a 2020 2020 2020 2e2e 2e20 2020 2020  ..      ...     
+00011460: 7265 7475 726e 206e 6e2e 4465 6e73 6528  return nn.Dense(
+00011470: 3129 2878 290a 0a20 2020 2020 203e 3e3e  1)(x)..      >>>
+00011480: 2078 203d 206a 6e70 2e65 6d70 7479 2828   x = jnp.empty((
+00011490: 312c 2037 2929 0a20 2020 2020 203e 3e3e  1, 7)).      >>>
+000114a0: 206d 6f64 756c 6520 3d20 466f 6f28 290a   module = Foo().
+000114b0: 2020 2020 2020 3e3e 3e20 726e 6773 203d        >>> rngs =
+000114c0: 207b 2770 6172 616d 7327 3a20 6a61 782e   {'params': jax.
+000114d0: 7261 6e64 6f6d 2e6b 6579 2830 292c 2027  random.key(0), '
+000114e0: 6e6f 6973 6527 3a20 6a61 782e 7261 6e64  noise': jax.rand
+000114f0: 6f6d 2e6b 6579 2831 297d 0a20 2020 2020  om.key(1)}.     
+00011500: 203e 3e3e 2076 6172 6961 626c 6573 203d   >>> variables =
+00011510: 206d 6f64 756c 652e 696e 6974 2872 6e67   module.init(rng
+00011520: 732c 2078 290a 2020 2020 2020 3e3e 3e20  s, x).      >>> 
+00011530: 6f75 7430 203d 206d 6f64 756c 652e 6170  out0 = module.ap
+00011540: 706c 7928 7661 7269 6162 6c65 732c 2078  ply(variables, x
+00011550: 2c20 6164 645f 6e6f 6973 653d 5472 7565  , add_noise=True
+00011560: 2c20 726e 6773 3d72 6e67 7329 0a0a 2020  , rngs=rngs)..  
+00011570: 2020 2020 3e3e 3e20 726e 6773 5b27 6e6f      >>> rngs['no
+00011580: 6973 6527 5d20 3d20 6a61 782e 7261 6e64  ise'] = jax.rand
+00011590: 6f6d 2e6b 6579 2830 290a 2020 2020 2020  om.key(0).      
+000115a0: 3e3e 3e20 6f75 7431 203d 206d 6f64 756c  >>> out1 = modul
+000115b0: 652e 6170 706c 7928 7661 7269 6162 6c65  e.apply(variable
+000115c0: 732c 2078 2c20 6164 645f 6e6f 6973 653d  s, x, add_noise=
+000115d0: 5472 7565 2c20 726e 6773 3d72 6e67 7329  True, rngs=rngs)
+000115e0: 0a20 2020 2020 203e 3e3e 2023 2064 6966  .      >>> # dif
+000115f0: 6665 7265 6e74 206f 7574 7075 7420 286b  ferent output (k
+00011600: 6579 2831 2920 7673 206b 6579 2830 2929  ey(1) vs key(0))
+00011610: 0a20 2020 2020 203e 3e3e 206e 702e 7465  .      >>> np.te
+00011620: 7374 696e 672e 6173 7365 7274 5f72 6169  sting.assert_rai
+00011630: 7365 7328 4173 7365 7274 696f 6e45 7272  ses(AssertionErr
+00011640: 6f72 2c20 6e70 2e74 6573 7469 6e67 2e61  or, np.testing.a
+00011650: 7373 6572 745f 616c 6c63 6c6f 7365 2c20  ssert_allclose, 
+00011660: 6f75 7430 2c20 6f75 7431 290a 0a20 2020  out0, out1)..   
+00011670: 2020 203e 3e3e 2064 656c 2072 6e67 735b     >>> del rngs[
+00011680: 276e 6f69 7365 275d 0a20 2020 2020 203e  'noise'].      >
+00011690: 3e3e 2023 2073 656c 662e 6d61 6b65 5f72  >> # self.make_r
+000116a0: 6e67 2827 6e6f 6973 6527 2920 7769 6c6c  ng('noise') will
+000116b0: 2064 6566 6175 6c74 2074 6f20 7573 696e   default to usin
+000116c0: 6720 7468 6520 2770 6172 616d 7327 2052  g the 'params' R
+000116d0: 4e47 2073 7472 6561 6d0a 2020 2020 2020  NG stream.      
+000116e0: 3e3e 3e20 6f75 7432 203d 206d 6f64 756c  >>> out2 = modul
+000116f0: 652e 6170 706c 7928 7661 7269 6162 6c65  e.apply(variable
+00011700: 732c 2078 2c20 6164 645f 6e6f 6973 653d  s, x, add_noise=
+00011710: 5472 7565 2c20 726e 6773 3d72 6e67 7329  True, rngs=rngs)
+00011720: 0a20 2020 2020 203e 3e3e 2023 2073 616d  .      >>> # sam
+00011730: 6520 6f75 7470 7574 2028 6b65 7928 3029  e output (key(0)
+00011740: 290a 2020 2020 2020 3e3e 3e20 6e70 2e74  ).      >>> np.t
+00011750: 6573 7469 6e67 2e61 7373 6572 745f 616c  esting.assert_al
+00011760: 6c63 6c6f 7365 286f 7574 312c 206f 7574  lclose(out1, out
+00011770: 3229 0a0a 2020 2020 2020 3e3e 3e20 2320  2)..      >>> # 
+00011780: 7061 7373 696e 6720 696e 2061 2073 696e  passing in a sin
+00011790: 676c 6520 6b65 7920 6973 2065 7175 6976  gle key is equiv
+000117a0: 616c 656e 7420 746f 2070 6173 7369 6e67  alent to passing
+000117b0: 2069 6e20 7b27 7061 7261 6d73 273a 206b   in {'params': k
+000117c0: 6579 7d0a 2020 2020 2020 3e3e 3e20 6f75  ey}.      >>> ou
+000117d0: 7433 203d 206d 6f64 756c 652e 6170 706c  t3 = module.appl
+000117e0: 7928 7661 7269 6162 6c65 732c 2078 2c20  y(variables, x, 
+000117f0: 6164 645f 6e6f 6973 653d 5472 7565 2c20  add_noise=True, 
+00011800: 726e 6773 3d6a 6178 2e72 616e 646f 6d2e  rngs=jax.random.
+00011810: 6b65 7928 3029 290a 2020 2020 2020 3e3e  key(0)).      >>
+00011820: 3e20 2320 7361 6d65 206f 7574 7075 7420  > # same output 
+00011830: 286b 6579 2830 2929 0a20 2020 2020 203e  (key(0)).      >
+00011840: 3e3e 206e 702e 7465 7374 696e 672e 6173  >> np.testing.as
+00011850: 7365 7274 5f61 6c6c 636c 6f73 6528 6f75  sert_allclose(ou
+00011860: 7432 2c20 6f75 7433 290a 0a20 2020 2041  t2, out3)..    A
+00011870: 7267 733a 0a20 2020 2020 2076 6172 6961  rgs:.      varia
+00011880: 626c 6573 3a20 4120 6469 6374 696f 6e61  bles: A dictiona
+00011890: 7279 2063 6f6e 7461 696e 696e 6720 7661  ry containing va
+000118a0: 7269 6162 6c65 7320 6b65 7965 6420 6279  riables keyed by
+000118b0: 2076 6172 6961 626c 650a 2020 2020 2020   variable.      
+000118c0: 2020 636f 6c6c 6563 7469 6f6e 732e 2053    collections. S
+000118d0: 6565 203a 6d6f 643a 6066 6c61 782e 636f  ee :mod:`flax.co
+000118e0: 7265 2e76 6172 6961 626c 6573 6020 666f  re.variables` fo
+000118f0: 7220 6d6f 7265 2064 6574 6169 6c73 2061  r more details a
+00011900: 626f 7574 0a20 2020 2020 2020 2076 6172  bout.        var
+00011910: 6961 626c 6573 2e0a 2020 2020 2020 2a61  iables..      *a
+00011920: 7267 733a 204e 616d 6564 2061 7267 756d  rgs: Named argum
+00011930: 656e 7473 2070 6173 7365 6420 746f 2074  ents passed to t
+00011940: 6865 2073 7065 6369 6669 6564 2061 7070  he specified app
+00011950: 6c79 206d 6574 686f 642e 0a20 2020 2020  ly method..     
+00011960: 2072 6e67 733a 2061 2064 6963 7420 6f66   rngs: a dict of
+00011970: 2050 524e 474b 6579 7320 746f 2069 6e69   PRNGKeys to ini
+00011980: 7469 616c 697a 6520 7468 6520 5052 4e47  tialize the PRNG
+00011990: 2073 6571 7565 6e63 6573 2e20 5468 6520   sequences. The 
+000119a0: 2270 6172 616d 7322 0a20 2020 2020 2020  "params".       
+000119b0: 2050 524e 4720 7365 7175 656e 6365 2069   PRNG sequence i
+000119c0: 7320 7573 6564 2074 6f20 696e 6974 6961  s used to initia
+000119d0: 6c69 7a65 2070 6172 616d 6574 6572 732e  lize parameters.
+000119e0: 0a20 2020 2020 206d 6574 686f 643a 2041  .      method: A
+000119f0: 2066 756e 6374 696f 6e20 746f 2063 616c   function to cal
+00011a00: 6c20 6170 706c 7920 6f6e 2e20 5468 6973  l apply on. This
+00011a10: 2069 7320 6765 6e65 7261 6c6c 7920 6120   is generally a 
+00011a20: 6675 6e63 7469 6f6e 2069 6e20 7468 650a  function in the.
+00011a30: 2020 2020 2020 2020 6d6f 6475 6c65 2e20          module. 
+00011a40: 4966 2070 726f 7669 6465 642c 2061 7070  If provided, app
+00011a50: 6c69 6573 2074 6869 7320 6d65 7468 6f64  lies this method
+00011a60: 2e20 4966 206e 6f74 2070 726f 7669 6465  . If not provide
+00011a70: 642c 2061 7070 6c69 6573 2074 6865 0a20  d, applies the. 
+00011a80: 2020 2020 2020 2060 605f 5f63 616c 6c5f         ``__call_
+00011a90: 5f60 6020 6d65 7468 6f64 206f 6620 7468  _`` method of th
+00011aa0: 6520 6d6f 6475 6c65 2e20 4120 7374 7269  e module. A stri
+00011ab0: 6e67 2063 616e 2061 6c73 6f20 6265 2070  ng can also be p
+00011ac0: 726f 7669 6465 6420 746f 0a20 2020 2020  rovided to.     
+00011ad0: 2020 2073 7065 6369 6679 2061 206d 6574     specify a met
+00011ae0: 686f 6420 6279 206e 616d 652e 0a20 2020  hod by name..   
+00011af0: 2020 206d 7574 6162 6c65 3a20 4361 6e20     mutable: Can 
+00011b00: 6265 2062 6f6f 6c2c 2073 7472 2c20 6f72  be bool, str, or
+00011b10: 206c 6973 742e 2053 7065 6369 6669 6573   list. Specifies
+00011b20: 2077 6869 6368 2063 6f6c 6c65 6374 696f   which collectio
+00011b30: 6e73 2073 686f 756c 6420 6265 0a20 2020  ns should be.   
+00011b40: 2020 2020 2074 7265 6174 6564 2061 7320       treated as 
+00011b50: 6d75 7461 626c 653a 2060 6062 6f6f 6c60  mutable: ``bool`
+00011b60: 603a 2061 6c6c 2f6e 6f20 636f 6c6c 6563  `: all/no collec
+00011b70: 7469 6f6e 7320 6172 6520 6d75 7461 626c  tions are mutabl
+00011b80: 652e 2060 6073 7472 6060 3a0a 2020 2020  e. ``str``:.    
+00011b90: 2020 2020 5468 6520 6e61 6d65 206f 6620      The name of 
+00011ba0: 6120 7369 6e67 6c65 206d 7574 6162 6c65  a single mutable
+00011bb0: 2063 6f6c 6c65 6374 696f 6e2e 2060 606c   collection. ``l
+00011bc0: 6973 7460 603a 2041 206c 6973 7420 6f66  ist``: A list of
+00011bd0: 206e 616d 6573 206f 660a 2020 2020 2020   names of.      
+00011be0: 2020 6d75 7461 626c 6520 636f 6c6c 6563    mutable collec
+00011bf0: 7469 6f6e 732e 0a20 2020 2020 2063 6170  tions..      cap
+00011c00: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
+00011c10: 6573 3a20 4966 2060 6054 7275 6560 602c  es: If ``True``,
+00011c20: 2063 6170 7475 7265 7320 696e 7465 726d   captures interm
+00011c30: 6564 6961 7465 2072 6574 7572 6e20 7661  ediate return va
+00011c40: 6c75 6573 206f 660a 2020 2020 2020 2020  lues of.        
+00011c50: 616c 6c20 4d6f 6475 6c65 7320 696e 7369  all Modules insi
+00011c60: 6465 2074 6865 2022 696e 7465 726d 6564  de the "intermed
+00011c70: 6961 7465 7322 2063 6f6c 6c65 6374 696f  iates" collectio
+00011c80: 6e2e 2042 7920 6465 6661 756c 742c 206f  n. By default, o
+00011c90: 6e6c 7920 7468 650a 2020 2020 2020 2020  nly the.        
+00011ca0: 7265 7475 726e 2076 616c 7565 7320 6f66  return values of
+00011cb0: 2061 6c6c 2060 605f 5f63 616c 6c5f 5f60   all ``__call__`
+00011cc0: 6020 6d65 7468 6f64 7320 6172 6520 7374  ` methods are st
+00011cd0: 6f72 6564 2e20 4120 6675 6e63 7469 6f6e  ored. A function
+00011ce0: 2063 616e 2062 650a 2020 2020 2020 2020   can be.        
+00011cf0: 7061 7373 6564 2074 6f20 6368 616e 6765  passed to change
+00011d00: 2074 6865 2066 696c 7465 7220 6265 6861   the filter beha
+00011d10: 7669 6f72 2e20 5468 6520 6669 6c74 6572  vior. The filter
+00011d20: 2066 756e 6374 696f 6e20 7461 6b65 7320   function takes 
+00011d30: 7468 650a 2020 2020 2020 2020 4d6f 6475  the.        Modu
+00011d40: 6c65 2069 6e73 7461 6e63 6520 616e 6420  le instance and 
+00011d50: 6d65 7468 6f64 206e 616d 6520 616e 6420  method name and 
+00011d60: 7265 7475 726e 7320 6120 626f 6f6c 2069  returns a bool i
+00011d70: 6e64 6963 6174 696e 6720 7768 6574 6865  ndicating whethe
+00011d80: 720a 2020 2020 2020 2020 7468 6520 6f75  r.        the ou
+00011d90: 7470 7574 206f 6620 7468 6174 206d 6574  tput of that met
+00011da0: 686f 6420 696e 766f 6361 7469 6f6e 2073  hod invocation s
+00011db0: 686f 756c 6420 6265 2073 746f 7265 642e  hould be stored.
+00011dc0: 0a20 2020 2020 202a 2a6b 7761 7267 733a  .      **kwargs:
+00011dd0: 204b 6579 776f 7264 2061 7267 756d 656e   Keyword argumen
+00011de0: 7473 2070 6173 7365 6420 746f 2074 6865  ts passed to the
+00011df0: 2073 7065 6369 6669 6564 2061 7070 6c79   specified apply
+00011e00: 206d 6574 686f 642e 0a0a 2020 2020 5265   method...    Re
+00011e10: 7475 726e 733a 0a20 2020 2020 2049 6620  turns:.      If 
+00011e20: 6060 6d75 7461 626c 6560 6020 6973 2046  ``mutable`` is F
+00011e30: 616c 7365 2c20 7265 7475 726e 7320 6f75  alse, returns ou
+00011e40: 7470 7574 2e20 4966 2061 6e79 2063 6f6c  tput. If any col
+00011e50: 6c65 6374 696f 6e73 2061 7265 0a20 2020  lections are.   
+00011e60: 2020 206d 7574 6162 6c65 2c20 7265 7475     mutable, retu
+00011e70: 726e 7320 6060 286f 7574 7075 742c 2076  rns ``(output, v
+00011e80: 6172 7329 6060 2c20 7768 6572 6520 6060  ars)``, where ``
+00011e90: 7661 7273 6060 2061 7265 2069 7320 6120  vars`` are is a 
+00011ea0: 6469 6374 0a20 2020 2020 206f 6620 7468  dict.      of th
+00011eb0: 6520 6d6f 6469 6669 6564 2063 6f6c 6c65  e modified colle
+00011ec0: 6374 696f 6e73 2e0a 2020 2020 2222 220a  ctions..    """.
+00011ed0: 2020 2020 4d6f 6475 6c65 2e5f 6d6f 6475      Module._modu
+00011ee0: 6c65 5f63 6865 636b 7328 7365 6c66 290a  le_checks(self).
+00011ef0: 0a20 2020 2069 6620 726e 6773 2069 7320  .    if rngs is 
+00011f00: 6e6f 7420 4e6f 6e65 2061 6e64 206e 6f74  not None and not
+00011f10: 2069 7369 6e73 7461 6e63 6528 726e 6773   isinstance(rngs
+00011f20: 2c20 6469 6374 293a 0a20 2020 2020 2069  , dict):.      i
+00011f30: 6620 6e6f 7420 636f 7265 2e73 636f 7065  f not core.scope
+00011f40: 2e5f 6973 5f76 616c 6964 5f72 6e67 2872  ._is_valid_rng(r
+00011f50: 6e67 7329 3a0a 2020 2020 2020 2020 7261  ngs):.        ra
+00011f60: 6973 6520 6572 726f 7273 2e49 6e76 616c  ise errors.Inval
+00011f70: 6964 526e 6745 7272 6f72 280a 2020 2020  idRngError(.    
+00011f80: 2020 2020 2020 2752 4e47 7320 7368 6f75        'RNGs shou
+00011f90: 6c64 2062 6520 6f66 2073 6861 7065 2028  ld be of shape (
+00011fa0: 322c 2920 6f72 2050 524e 474b 6579 2069  2,) or PRNGKey i
+00011fb0: 6e20 4d6f 6475 6c65 2027 0a20 2020 2020  n Module '.     
+00011fc0: 2020 2020 2066 277b 7365 6c66 2e5f 5f63       f'{self.__c
+00011fd0: 6c61 7373 5f5f 2e5f 5f6e 616d 655f 5f7d  lass__.__name__}
+00011fe0: 2c20 6275 7420 726e 6773 2061 7265 3a20  , but rngs are: 
+00011ff0: 7b72 6e67 737d 270a 2020 2020 2020 2020  {rngs}'.        
+00012000: 290a 2020 2020 2020 726e 6773 203d 207b  ).      rngs = {
+00012010: 2770 6172 616d 7327 3a20 726e 6773 7d0a  'params': rngs}.
+00012020: 0a20 2020 2069 6620 6973 696e 7374 616e  .    if isinstan
+00012030: 6365 286d 6574 686f 642c 2073 7472 293a  ce(method, str):
+00012040: 0a20 2020 2020 2061 7474 7269 6275 7465  .      attribute
+00012050: 5f6e 616d 6520 3d20 6d65 7468 6f64 0a20  _name = method. 
+00012060: 2020 2020 206d 6574 686f 6420 3d20 6765       method = ge
+00012070: 7461 7474 7228 7365 6c66 2c20 6174 7472  tattr(self, attr
+00012080: 6962 7574 655f 6e61 6d65 290a 2020 2020  ibute_name).    
+00012090: 2020 6966 206e 6f74 2063 616c 6c61 626c    if not callabl
+000120a0: 6528 6d65 7468 6f64 293a 0a20 2020 2020  e(method):.     
+000120b0: 2020 2063 6c61 7373 5f6e 616d 6520 3d20     class_name = 
+000120c0: 7479 7065 2873 656c 6629 2e5f 5f6e 616d  type(self).__nam
+000120d0: 655f 5f0a 2020 2020 2020 2020 7261 6973  e__.        rais
+000120e0: 6520 5479 7065 4572 726f 7228 0a20 2020  e TypeError(.   
+000120f0: 2020 2020 2020 2066 2227 7b63 6c61 7373         f"'{class
+00012100: 5f6e 616d 657d 2e7b 6174 7472 6962 7574  _name}.{attribut
+00012110: 655f 6e61 6d65 7d27 206d 7573 7420 6265  e_name}' must be
+00012120: 2061 2063 616c 6c61 626c 652c 2067 6f74   a callable, got
+00012130: 220a 2020 2020 2020 2020 2020 6627 207b  ".          f' {
+00012140: 7479 7065 286d 6574 686f 6429 7d2e 270a  type(method)}.'.
+00012150: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
+00012160: 2320 6966 2074 6865 2060 6d65 7468 6f64  # if the `method
+00012170: 6020 7374 7269 6e67 2069 7320 6120 7375  ` string is a su
+00012180: 626d 6f64 756c 652c 2077 6520 6372 6561  bmodule, we crea
+00012190: 7465 2061 206c 616d 6264 6120 6675 6e63  te a lambda func
+000121a0: 7469 6f6e 0a20 2020 2020 2023 2074 6861  tion.      # tha
+000121b0: 7420 6361 6c6c 7320 7468 6520 7375 626d  t calls the subm
+000121c0: 6f64 756c 652c 2066 6f72 7761 7264 696e  odule, forwardin
+000121d0: 6720 616c 6c20 6172 6775 6d65 6e74 732e  g all arguments.
+000121e0: 0a20 2020 2020 2069 6620 6973 696e 7374  .      if isinst
+000121f0: 616e 6365 286d 6574 686f 642c 204d 6f64  ance(method, Mod
+00012200: 756c 6529 3a0a 2020 2020 2020 2020 6d65  ule):.        me
+00012210: 7468 6f64 203d 206c 616d 6264 6120 7365  thod = lambda se
+00012220: 6c66 2c20 2a61 7267 732c 202a 2a6b 7761  lf, *args, **kwa
+00012230: 7267 733a 2067 6574 6174 7472 2873 656c  rgs: getattr(sel
+00012240: 662c 2061 7474 7269 6275 7465 5f6e 616d  f, attribute_nam
+00012250: 6529 280a 2020 2020 2020 2020 2020 2a61  e)(.          *a
+00012260: 7267 732c 202a 2a6b 7761 7267 730a 2020  rgs, **kwargs.  
+00012270: 2020 2020 2020 290a 2020 2020 656c 6966        ).    elif
+00012280: 206d 6574 686f 6420 6973 204e 6f6e 653a   method is None:
+00012290: 0a20 2020 2020 206d 6574 686f 6420 3d20  .      method = 
+000122a0: 7365 6c66 2e5f 5f63 616c 6c5f 5f0a 2020  self.__call__.  
+000122b0: 2020 6d65 7468 6f64 203d 205f 6765 745f    method = _get_
+000122c0: 756e 626f 756e 645f 666e 286d 6574 686f  unbound_fn(metho
+000122d0: 6429 0a20 2020 2072 6574 7572 6e20 6170  d).    return ap
+000122e0: 706c 7928 0a20 2020 2020 206d 6574 686f  ply(.      metho
+000122f0: 642c 0a20 2020 2020 2073 656c 662c 0a20  d,.      self,. 
+00012300: 2020 2020 206d 7574 6162 6c65 3d6d 7574       mutable=mut
+00012310: 6162 6c65 2c0a 2020 2020 2020 6361 7074  able,.      capt
+00012320: 7572 655f 696e 7465 726d 6564 6961 7465  ure_intermediate
+00012330: 733d 6361 7074 7572 655f 696e 7465 726d  s=capture_interm
+00012340: 6564 6961 7465 732c 0a20 2020 2029 2876  ediates,.    )(v
+00012350: 6172 6961 626c 6573 2c20 2a61 7267 732c  ariables, *args,
+00012360: 202a 2a6b 7761 7267 732c 2072 6e67 733d   **kwargs, rngs=
+00012370: 726e 6773 290a 0a20 2040 7472 6163 6562  rngs)..  @traceb
+00012380: 6163 6b5f 7574 696c 2e61 7069 5f62 6f75  ack_util.api_bou
+00012390: 6e64 6172 790a 2020 6465 6620 696e 6974  ndary.  def init
+000123a0: 5f77 6974 685f 6f75 7470 7574 280a 2020  _with_output(.  
+000123b0: 2020 7365 6c66 2c0a 2020 2020 726e 6773    self,.    rngs
+000123c0: 3a20 556e 696f 6e5b 5052 4e47 4b65 792c  : Union[PRNGKey,
+000123d0: 2052 4e47 5365 7175 656e 6365 735d 2c0a   RNGSequences],.
+000123e0: 2020 2020 2a61 7267 732c 0a20 2020 206d      *args,.    m
+000123f0: 6574 686f 643a 2055 6e69 6f6e 5b43 616c  ethod: Union[Cal
+00012400: 6c61 626c 655b 2e2e 2e2c 2041 6e79 5d2c  lable[..., Any],
+00012410: 2073 7472 2c20 4e6f 6e65 5d20 3d20 4e6f   str, None] = No
+00012420: 6e65 2c0a 2020 2020 6d75 7461 626c 653a  ne,.    mutable:
+00012430: 2043 6f6c 6c65 6374 696f 6e46 696c 7465   CollectionFilte
+00012440: 7220 3d20 4465 6e79 4c69 7374 2827 696e  r = DenyList('in
+00012450: 7465 726d 6564 6961 7465 7327 292c 0a20  termediates'),. 
+00012460: 2020 2063 6170 7475 7265 5f69 6e74 6572     capture_inter
+00012470: 6d65 6469 6174 6573 3a20 556e 696f 6e5b  mediates: Union[
+00012480: 626f 6f6c 2c20 4361 6c6c 6162 6c65 5b5b  bool, Callable[[
+00012490: 274d 6f64 756c 6527 2c20 7374 725d 2c20  'Module', str], 
+000124a0: 626f 6f6c 5d5d 203d 2046 616c 7365 2c0a  bool]] = False,.
+000124b0: 2020 2020 2a2a 6b77 6172 6773 2c0a 2020      **kwargs,.  
+000124c0: 2920 2d3e 2054 7570 6c65 5b41 6e79 2c20  ) -> Tuple[Any, 
+000124d0: 556e 696f 6e5b 4672 6f7a 656e 5661 7269  Union[FrozenVari
+000124e0: 6162 6c65 4469 6374 2c20 4469 6374 5b73  ableDict, Dict[s
+000124f0: 7472 2c20 416e 795d 5d5d 3a0a 2020 2020  tr, Any]]]:.    
+00012500: 2222 2249 6e69 7469 616c 697a 6573 2061  """Initializes a
+00012510: 206d 6f64 756c 6520 6d65 7468 6f64 2077   module method w
+00012520: 6974 6820 7661 7269 6162 6c65 7320 616e  ith variables an
+00012530: 6420 7265 7475 726e 7320 6f75 7470 7574  d returns output
+00012540: 2061 6e64 206d 6f64 6966 6965 6420 7661   and modified va
+00012550: 7269 6162 6c65 732e 0a0a 2020 2020 4172  riables...    Ar
+00012560: 6773 3a0a 2020 2020 2020 726e 6773 3a20  gs:.      rngs: 
+00012570: 5468 6520 726e 6773 2066 6f72 2074 6865  The rngs for the
+00012580: 2076 6172 6961 626c 6520 636f 6c6c 6563   variable collec
+00012590: 7469 6f6e 732e 0a20 2020 2020 202a 6172  tions..      *ar
+000125a0: 6773 3a20 4e61 6d65 6420 6172 6775 6d65  gs: Named argume
+000125b0: 6e74 7320 7061 7373 6564 2074 6f20 7468  nts passed to th
+000125c0: 6520 696e 6974 2066 756e 6374 696f 6e2e  e init function.
+000125d0: 0a20 2020 2020 206d 6574 686f 643a 2041  .      method: A
+000125e0: 6e20 6f70 7469 6f6e 616c 206d 6574 686f  n optional metho
+000125f0: 642e 2049 6620 7072 6f76 6964 6564 2c20  d. If provided, 
+00012600: 6170 706c 6965 7320 7468 6973 206d 6574  applies this met
+00012610: 686f 642e 2049 6620 6e6f 740a 2020 2020  hod. If not.    
+00012620: 2020 2020 7072 6f76 6964 6564 2c20 6170      provided, ap
+00012630: 706c 6965 7320 7468 6520 6060 5f5f 6361  plies the ``__ca
+00012640: 6c6c 5f5f 6060 206d 6574 686f 642e 2041  ll__`` method. A
+00012650: 2073 7472 696e 6720 6361 6e20 616c 736f   string can also
+00012660: 2062 650a 2020 2020 2020 2020 7072 6f76   be.        prov
+00012670: 6964 6564 2074 6f20 7370 6563 6966 7920  ided to specify 
+00012680: 6120 6d65 7468 6f64 2062 7920 6e61 6d65  a method by name
+00012690: 2e0a 2020 2020 2020 6d75 7461 626c 653a  ..      mutable:
+000126a0: 2043 616e 2062 6520 626f 6f6c 2c20 7374   Can be bool, st
+000126b0: 722c 206f 7220 6c69 7374 2e20 5370 6563  r, or list. Spec
+000126c0: 6966 6965 7320 7768 6963 6820 636f 6c6c  ifies which coll
+000126d0: 6563 7469 6f6e 7320 7368 6f75 6c64 2062  ections should b
+000126e0: 650a 2020 2020 2020 2020 7472 6561 7465  e.        treate
+000126f0: 6420 6173 206d 7574 6162 6c65 3a20 6060  d as mutable: ``
+00012700: 626f 6f6c 6060 3a20 616c 6c2f 6e6f 2063  bool``: all/no c
+00012710: 6f6c 6c65 6374 696f 6e73 2061 7265 206d  ollections are m
+00012720: 7574 6162 6c65 2e20 6060 7374 7260 603a  utable. ``str``:
+00012730: 0a20 2020 2020 2020 2054 6865 206e 616d  .        The nam
+00012740: 6520 6f66 2061 2073 696e 676c 6520 6d75  e of a single mu
+00012750: 7461 626c 6520 636f 6c6c 6563 7469 6f6e  table collection
+00012760: 2e20 6060 6c69 7374 6060 3a20 4120 6c69  . ``list``: A li
+00012770: 7374 206f 6620 6e61 6d65 7320 6f66 0a20  st of names of. 
+00012780: 2020 2020 2020 206d 7574 6162 6c65 2063         mutable c
+00012790: 6f6c 6c65 6374 696f 6e73 2e20 4279 2064  ollections. By d
+000127a0: 6566 6175 6c74 2c20 616c 6c20 636f 6c6c  efault, all coll
+000127b0: 6563 7469 6f6e 7320 6578 6365 7074 2022  ections except "
+000127c0: 696e 7465 726d 6564 6961 7465 7322 0a20  intermediates". 
+000127d0: 2020 2020 2020 2061 7265 206d 7574 6162         are mutab
+000127e0: 6c65 2e0a 2020 2020 2020 6361 7074 7572  le..      captur
+000127f0: 655f 696e 7465 726d 6564 6961 7465 733a  e_intermediates:
+00012800: 2049 6620 6060 5472 7565 6060 2c20 6361   If ``True``, ca
+00012810: 7074 7572 6573 2069 6e74 6572 6d65 6469  ptures intermedi
+00012820: 6174 6520 7265 7475 726e 2076 616c 7565  ate return value
+00012830: 7320 6f66 0a20 2020 2020 2020 2061 6c6c  s of.        all
+00012840: 204d 6f64 756c 6573 2069 6e73 6964 6520   Modules inside 
+00012850: 7468 6520 2269 6e74 6572 6d65 6469 6174  the "intermediat
+00012860: 6573 2220 636f 6c6c 6563 7469 6f6e 2e20  es" collection. 
+00012870: 4279 2064 6566 6175 6c74 206f 6e6c 7920  By default only 
+00012880: 7468 650a 2020 2020 2020 2020 7265 7475  the.        retu
+00012890: 726e 2076 616c 7565 7320 6f66 2061 6c6c  rn values of all
+000128a0: 2060 605f 5f63 616c 6c5f 5f60 6020 6d65   ``__call__`` me
+000128b0: 7468 6f64 7320 6172 6520 7374 6f72 6564  thods are stored
+000128c0: 2e20 4120 6675 6e63 7469 6f6e 2063 616e  . A function can
+000128d0: 2062 650a 2020 2020 2020 2020 7061 7373   be.        pass
+000128e0: 6564 2074 6f20 6368 616e 6765 2074 6865  ed to change the
+000128f0: 2066 696c 7465 7220 6265 6861 7669 6f72   filter behavior
+00012900: 2e20 5468 6520 6669 6c74 6572 2066 756e  . The filter fun
+00012910: 6374 696f 6e20 7461 6b65 7320 7468 650a  ction takes the.
+00012920: 2020 2020 2020 2020 4d6f 6475 6c65 2069          Module i
+00012930: 6e73 7461 6e63 6520 616e 6420 6d65 7468  nstance and meth
+00012940: 6f64 206e 616d 6520 616e 6420 7265 7475  od name and retu
+00012950: 726e 7320 6120 626f 6f6c 2069 6e64 6963  rns a bool indic
+00012960: 6174 696e 6720 7768 6574 6865 720a 2020  ating whether.  
+00012970: 2020 2020 2020 7468 6520 6f75 7470 7574        the output
+00012980: 206f 6620 7468 6174 206d 6574 686f 6420   of that method 
+00012990: 696e 766f 6361 7469 6f6e 2073 686f 756c  invocation shoul
+000129a0: 6420 6265 2073 746f 7265 642e 0a20 2020  d be stored..   
+000129b0: 2020 202a 2a6b 7761 7267 733a 204b 6579     **kwargs: Key
+000129c0: 776f 7264 2061 7267 756d 656e 7473 2070  word arguments p
+000129d0: 6173 7365 6420 746f 2074 6865 2069 6e69  assed to the ini
+000129e0: 7420 6675 6e63 7469 6f6e 2e0a 0a20 2020  t function...   
+000129f0: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
+00012a00: 6060 286f 7574 7075 742c 2076 6172 7329  ``(output, vars)
+00012a10: 6060 2c20 7768 6572 6520 6060 7661 7273  ``, where ``vars
+00012a20: 6060 2061 7265 2069 7320 6120 6469 6374  `` are is a dict
+00012a30: 206f 6620 7468 6520 6d6f 6469 6669 6564   of the modified
+00012a40: 0a20 2020 2020 2063 6f6c 6c65 6374 696f  .      collectio
+00012a50: 6e73 2e0a 2020 2020 2222 220a 2020 2020  ns..    """.    
+00012a60: 4d6f 6475 6c65 2e5f 6d6f 6475 6c65 5f63  Module._module_c
+00012a70: 6865 636b 7328 7365 6c66 290a 0a20 2020  hecks(self)..   
+00012a80: 2069 6620 6e6f 7420 6973 696e 7374 616e   if not isinstan
+00012a90: 6365 2872 6e67 732c 2064 6963 7429 3a0a  ce(rngs, dict):.
+00012aa0: 2020 2020 2020 6966 206e 6f74 2063 6f72        if not cor
+00012ab0: 652e 7363 6f70 652e 5f69 735f 7661 6c69  e.scope._is_vali
+00012ac0: 645f 726e 6728 726e 6773 293a 0a20 2020  d_rng(rngs):.   
+00012ad0: 2020 2020 2072 6169 7365 2065 7272 6f72       raise error
+00012ae0: 732e 496e 7661 6c69 6452 6e67 4572 726f  s.InvalidRngErro
+00012af0: 7228 0a20 2020 2020 2020 2020 2027 524e  r(.          'RN
+00012b00: 4773 2073 686f 756c 6420 6265 206f 6620  Gs should be of 
+00012b10: 7368 6170 6520 2832 2c29 206f 7220 5052  shape (2,) or PR
+00012b20: 4e47 4b65 7920 696e 204d 6f64 756c 6520  NGKey in Module 
+00012b30: 270a 2020 2020 2020 2020 2020 6627 7b73  '.          f'{s
+00012b40: 656c 662e 5f5f 636c 6173 735f 5f2e 5f5f  elf.__class__.__
+00012b50: 6e61 6d65 5f5f 7d2c 2062 7574 2072 6e67  name__}, but rng
+00012b60: 7320 6172 653a 207b 726e 6773 7d27 0a20  s are: {rngs}'. 
+00012b70: 2020 2020 2020 2029 0a20 2020 2020 2072         ).      r
+00012b80: 6e67 7320 3d20 7b27 7061 7261 6d73 273a  ngs = {'params':
+00012b90: 2072 6e67 737d 0a0a 2020 2020 6966 2069   rngs}..    if i
+00012ba0: 7369 6e73 7461 6e63 6528 6d65 7468 6f64  sinstance(method
+00012bb0: 2c20 7374 7229 3a0a 2020 2020 2020 6174  , str):.      at
+00012bc0: 7472 6962 7574 655f 6e61 6d65 203d 206d  tribute_name = m
+00012bd0: 6574 686f 640a 2020 2020 2020 6d65 7468  ethod.      meth
+00012be0: 6f64 203d 2067 6574 6174 7472 2873 656c  od = getattr(sel
+00012bf0: 662c 2061 7474 7269 6275 7465 5f6e 616d  f, attribute_nam
+00012c00: 6529 0a20 2020 2020 2069 6620 6e6f 7420  e).      if not 
+00012c10: 6361 6c6c 6162 6c65 286d 6574 686f 6429  callable(method)
+00012c20: 3a0a 2020 2020 2020 2020 636c 6173 735f  :.        class_
+00012c30: 6e61 6d65 203d 2074 7970 6528 7365 6c66  name = type(self
+00012c40: 292e 5f5f 6e61 6d65 5f5f 0a20 2020 2020  ).__name__.     
+00012c50: 2020 2072 6169 7365 2054 7970 6545 7272     raise TypeErr
+00012c60: 6f72 280a 2020 2020 2020 2020 2020 6622  or(.          f"
+00012c70: 277b 636c 6173 735f 6e61 6d65 7d2e 7b61  '{class_name}.{a
+00012c80: 7474 7269 6275 7465 5f6e 616d 657d 2720  ttribute_name}' 
+00012c90: 6d75 7374 2062 6520 6120 6361 6c6c 6162  must be a callab
+00012ca0: 6c65 2c20 676f 7422 0a20 2020 2020 2020  le, got".       
+00012cb0: 2020 2066 2720 7b74 7970 6528 6d65 7468     f' {type(meth
+00012cc0: 6f64 297d 2e27 0a20 2020 2020 2020 2029  od)}.'.        )
+00012cd0: 0a20 2020 2065 6c69 6620 6d65 7468 6f64  .    elif method
+00012ce0: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
+00012cf0: 6d65 7468 6f64 203d 2073 656c 662e 5f5f  method = self.__
+00012d00: 6361 6c6c 5f5f 0a20 2020 206d 6574 686f  call__.    metho
+00012d10: 6420 3d20 5f67 6574 5f75 6e62 6f75 6e64  d = _get_unbound
+00012d20: 5f66 6e28 6d65 7468 6f64 290a 2020 2020  _fn(method).    
+00012d30: 7265 7475 726e 2069 6e69 745f 7769 7468  return init_with
+00012d40: 5f6f 7574 7075 7428 0a20 2020 2020 206d  _output(.      m
+00012d50: 6574 686f 642c 0a20 2020 2020 2073 656c  ethod,.      sel
+00012d60: 662c 0a20 2020 2020 206d 7574 6162 6c65  f,.      mutable
+00012d70: 3d6d 7574 6162 6c65 2c0a 2020 2020 2020  =mutable,.      
+00012d80: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
+00012d90: 6961 7465 733d 6361 7074 7572 655f 696e  iates=capture_in
+00012da0: 7465 726d 6564 6961 7465 732c 0a20 2020  termediates,.   
+00012db0: 2029 2872 6e67 732c 202a 6172 6773 2c20   )(rngs, *args, 
+00012dc0: 2a2a 6b77 6172 6773 290a 0a20 2040 7472  **kwargs)..  @tr
+00012dd0: 6163 6562 6163 6b5f 7574 696c 2e61 7069  aceback_util.api
+00012de0: 5f62 6f75 6e64 6172 790a 2020 6465 6620  _boundary.  def 
+00012df0: 696e 6974 280a 2020 2020 7365 6c66 2c0a  init(.    self,.
+00012e00: 2020 2020 726e 6773 3a20 556e 696f 6e5b      rngs: Union[
+00012e10: 5052 4e47 4b65 792c 2052 4e47 5365 7175  PRNGKey, RNGSequ
+00012e20: 656e 6365 735d 2c0a 2020 2020 2a61 7267  ences],.    *arg
+00012e30: 732c 0a20 2020 206d 6574 686f 643a 2055  s,.    method: U
+00012e40: 6e69 6f6e 5b43 616c 6c61 626c 655b 2e2e  nion[Callable[..
+00012e50: 2e2c 2041 6e79 5d2c 2073 7472 2c20 4e6f  ., Any], str, No
+00012e60: 6e65 5d20 3d20 4e6f 6e65 2c0a 2020 2020  ne] = None,.    
+00012e70: 6d75 7461 626c 653a 2043 6f6c 6c65 6374  mutable: Collect
+00012e80: 696f 6e46 696c 7465 7220 3d20 4465 6e79  ionFilter = Deny
+00012e90: 4c69 7374 2827 696e 7465 726d 6564 6961  List('intermedia
+00012ea0: 7465 7327 292c 0a20 2020 2063 6170 7475  tes'),.    captu
+00012eb0: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
+00012ec0: 3a20 556e 696f 6e5b 626f 6f6c 2c20 4361  : Union[bool, Ca
+00012ed0: 6c6c 6162 6c65 5b5b 274d 6f64 756c 6527  llable[['Module'
+00012ee0: 2c20 7374 725d 2c20 626f 6f6c 5d5d 203d  , str], bool]] =
+00012ef0: 2046 616c 7365 2c0a 2020 2020 2a2a 6b77   False,.    **kw
+00012f00: 6172 6773 2c0a 2020 2920 2d3e 2055 6e69  args,.  ) -> Uni
+00012f10: 6f6e 5b46 726f 7a65 6e56 6172 6961 626c  on[FrozenVariabl
+00012f20: 6544 6963 742c 2044 6963 745b 7374 722c  eDict, Dict[str,
+00012f30: 2041 6e79 5d5d 3a0a 2020 2020 2222 2249   Any]]:.    """I
+00012f40: 6e69 7469 616c 697a 6573 2061 206d 6f64  nitializes a mod
+00012f50: 756c 6520 6d65 7468 6f64 2077 6974 6820  ule method with 
+00012f60: 7661 7269 6162 6c65 7320 616e 6420 7265  variables and re
+00012f70: 7475 726e 7320 6d6f 6469 6669 6564 2076  turns modified v
+00012f80: 6172 6961 626c 6573 2e0a 0a20 2020 2060  ariables...    `
+00012f90: 6069 6e69 7460 6020 7461 6b65 7320 6173  `init`` takes as
+00012fa0: 2066 6972 7374 2061 7267 756d 656e 7420   first argument 
+00012fb0: 6569 7468 6572 2061 2073 696e 676c 6520  either a single 
+00012fc0: 6060 5052 4e47 4b65 7960 602c 206f 7220  ``PRNGKey``, or 
+00012fd0: 610a 2020 2020 6469 6374 696f 6e61 7279  a.    dictionary
+00012fe0: 206d 6170 7069 6e67 2076 6172 6961 626c   mapping variabl
+00012ff0: 6520 636f 6c6c 6563 7469 6f6e 7320 6e61  e collections na
+00013000: 6d65 7320 746f 2074 6865 6972 2060 6050  mes to their ``P
+00013010: 524e 474b 6579 7360 602c 2061 6e64 0a20  RNGKeys``, and. 
+00013020: 2020 2077 696c 6c20 6361 6c6c 2060 606d     will call ``m
+00013030: 6574 686f 6460 6020 2877 6869 6368 2069  ethod`` (which i
+00013040: 7320 7468 6520 6d6f 6475 6c65 2773 2060  s the module's `
+00013050: 605f 5f63 616c 6c5f 5f60 6020 6675 6e63  `__call__`` func
+00013060: 7469 6f6e 2062 790a 2020 2020 6465 6661  tion by.    defa
+00013070: 756c 7429 2070 6173 7369 6e67 2060 602a  ult) passing ``*
+00013080: 6172 6773 6060 2061 6e64 2060 602a 2a6b  args`` and ``**k
+00013090: 7761 7267 7360 602c 2061 6e64 2072 6574  wargs``, and ret
+000130a0: 7572 6e73 0a20 2020 2061 2064 6963 7469  urns.    a dicti
+000130b0: 6f6e 6172 7920 6f66 2069 6e69 7469 616c  onary of initial
+000130c0: 697a 6564 2076 6172 6961 626c 6573 2e0a  ized variables..
+000130d0: 0a20 2020 2045 7861 6d70 6c65 3a3a 0a0a  .    Example::..
+000130e0: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
+000130f0: 2066 6c61 782e 6c69 6e65 6e20 6173 206e   flax.linen as n
+00013100: 6e0a 2020 2020 2020 3e3e 3e20 696d 706f  n.      >>> impo
+00013110: 7274 206a 6178 2c20 6a61 782e 6e75 6d70  rt jax, jax.nump
+00013120: 7920 6173 206a 6e70 0a20 2020 2020 203e  y as jnp.      >
+00013130: 3e3e 2069 6d70 6f72 7420 6e75 6d70 7920  >> import numpy 
+00013140: 6173 206e 700a 0a20 2020 2020 203e 3e3e  as np..      >>>
+00013150: 2063 6c61 7373 2046 6f6f 286e 6e2e 4d6f   class Foo(nn.Mo
+00013160: 6475 6c65 293a 0a20 2020 2020 202e 2e2e  dule):.      ...
+00013170: 2020 2040 6e6e 2e63 6f6d 7061 6374 0a20     @nn.compact. 
+00013180: 2020 2020 202e 2e2e 2020 2064 6566 205f       ...   def _
+00013190: 5f63 616c 6c5f 5f28 7365 6c66 2c20 782c  _call__(self, x,
+000131a0: 2074 7261 696e 293a 0a20 2020 2020 202e   train):.      .
+000131b0: 2e2e 2020 2020 2078 203d 206e 6e2e 4465  ..     x = nn.De
+000131c0: 6e73 6528 3136 2928 7829 0a20 2020 2020  nse(16)(x).     
+000131d0: 202e 2e2e 2020 2020 2078 203d 206e 6e2e   ...     x = nn.
+000131e0: 4261 7463 684e 6f72 6d28 7573 655f 7275  BatchNorm(use_ru
+000131f0: 6e6e 696e 675f 6176 6572 6167 653d 6e6f  nning_average=no
+00013200: 7420 7472 6169 6e29 2878 290a 2020 2020  t train)(x).    
+00013210: 2020 2e2e 2e20 2020 2020 7820 3d20 6e6e    ...     x = nn
+00013220: 2e72 656c 7528 7829 0a20 2020 2020 202e  .relu(x).      .
+00013230: 2e2e 2020 2020 2072 6574 7572 6e20 6e6e  ..     return nn
+00013240: 2e44 656e 7365 2831 2928 7829 0a0a 2020  .Dense(1)(x)..  
+00013250: 2020 2020 3e3e 3e20 7820 3d20 6a6e 702e      >>> x = jnp.
+00013260: 656d 7074 7928 2831 2c20 3729 290a 2020  empty((1, 7)).  
+00013270: 2020 2020 3e3e 3e20 6d6f 6475 6c65 203d      >>> module =
+00013280: 2046 6f6f 2829 0a20 2020 2020 203e 3e3e   Foo().      >>>
+00013290: 206b 6579 203d 206a 6178 2e72 616e 646f   key = jax.rando
+000132a0: 6d2e 6b65 7928 3029 0a20 2020 2020 203e  m.key(0).      >
+000132b0: 3e3e 2076 6172 6961 626c 6573 203d 206d  >> variables = m
+000132c0: 6f64 756c 652e 696e 6974 286b 6579 2c20  odule.init(key, 
+000132d0: 782c 2074 7261 696e 3d46 616c 7365 290a  x, train=False).
+000132e0: 0a20 2020 2049 6620 796f 7520 7061 7373  .    If you pass
+000132f0: 2061 2073 696e 676c 6520 6060 5052 4e47   a single ``PRNG
+00013300: 4b65 7960 602c 2046 6c61 7820 7769 6c6c  Key``, Flax will
+00013310: 2075 7365 2069 7420 746f 2066 6565 6420   use it to feed 
+00013320: 7468 6520 6060 2770 6172 616d 7327 6060  the ``'params'``
+00013330: 0a20 2020 2052 4e47 2073 7472 6561 6d2e  .    RNG stream.
+00013340: 2020 4966 2079 6f75 2077 616e 7420 746f    If you want to
+00013350: 2075 7365 2061 2064 6966 6665 7265 6e74   use a different
+00013360: 2052 4e47 2073 7472 6561 6d20 6f72 206e   RNG stream or n
+00013370: 6565 6420 746f 2075 7365 0a20 2020 206d  eed to use.    m
+00013380: 756c 7469 706c 6520 7374 7265 616d 732c  ultiple streams,
+00013390: 2079 6f75 2063 616e 2070 6173 7320 6120   you can pass a 
+000133a0: 6469 6374 696f 6e61 7279 206d 6170 7069  dictionary mappi
+000133b0: 6e67 2065 6163 6820 524e 4720 7374 7265  ng each RNG stre
+000133c0: 616d 206e 616d 650a 2020 2020 746f 2069  am name.    to i
+000133d0: 7473 2063 6f72 7265 7370 6f6e 6469 6e67  ts corresponding
+000133e0: 2060 6050 524e 474b 6579 6060 2074 6f20   ``PRNGKey`` to 
+000133f0: 6060 696e 6974 6060 2e20 4966 2060 6073  ``init``. If ``s
+00013400: 656c 662e 6d61 6b65 5f72 6e67 286e 616d  elf.make_rng(nam
+00013410: 6529 6060 0a20 2020 2069 7320 6361 6c6c  e)``.    is call
+00013420: 6564 206f 6e20 616e 2052 4e47 2073 7472  ed on an RNG str
+00013430: 6561 6d20 6e61 6d65 2074 6861 7420 6973  eam name that is
+00013440: 6e27 7420 7061 7373 6564 2062 7920 7468  n't passed by th
+00013450: 6520 7573 6572 2c20 6974 2077 696c 6c0a  e user, it will.
+00013460: 2020 2020 6465 6661 756c 7420 746f 2075      default to u
+00013470: 7369 6e67 2074 6865 2060 6027 7061 7261  sing the ``'para
+00013480: 6d73 2760 6020 524e 4720 7374 7265 616d  ms'`` RNG stream
+00013490: 2e0a 0a20 2020 2045 7861 6d70 6c65 3a3a  ...    Example::
+000134a0: 0a0a 2020 2020 2020 3e3e 3e20 636c 6173  ..      >>> clas
+000134b0: 7320 466f 6f28 6e6e 2e4d 6f64 756c 6529  s Foo(nn.Module)
+000134c0: 3a0a 2020 2020 2020 2e2e 2e20 2020 406e  :.      ...   @n
+000134d0: 6e2e 636f 6d70 6163 740a 2020 2020 2020  n.compact.      
+000134e0: 2e2e 2e20 2020 6465 6620 5f5f 6361 6c6c  ...   def __call
+000134f0: 5f5f 2873 656c 662c 2078 293a 0a20 2020  __(self, x):.   
+00013500: 2020 202e 2e2e 2020 2020 2078 203d 206e     ...     x = n
+00013510: 6e2e 4465 6e73 6528 3136 2928 7829 0a20  n.Dense(16)(x). 
+00013520: 2020 2020 202e 2e2e 2020 2020 2078 203d       ...     x =
+00013530: 206e 6e2e 7265 6c75 2878 290a 2020 2020   nn.relu(x).    
+00013540: 2020 2e2e 2e0a 2020 2020 2020 2e2e 2e20    ....      ... 
+00013550: 2020 2020 6f74 6865 725f 7661 7269 6162      other_variab
+00013560: 6c65 203d 2073 656c 662e 7661 7269 6162  le = self.variab
+00013570: 6c65 280a 2020 2020 2020 2e2e 2e20 2020  le(.      ...   
+00013580: 2020 2020 276f 7468 6572 5f63 6f6c 6c65      'other_colle
+00013590: 6374 696f 6e27 2c0a 2020 2020 2020 2e2e  ction',.      ..
+000135a0: 2e20 2020 2020 2020 276f 7468 6572 5f76  .       'other_v
+000135b0: 6172 6961 626c 6527 2c0a 2020 2020 2020  ariable',.      
+000135c0: 2e2e 2e20 2020 2020 2020 6c61 6d62 6461  ...       lambda
+000135d0: 2078 3a20 6a61 782e 7261 6e64 6f6d 2e6e   x: jax.random.n
+000135e0: 6f72 6d61 6c28 7365 6c66 2e6d 616b 655f  ormal(self.make_
+000135f0: 726e 6728 276f 7468 6572 5f72 6e67 2729  rng('other_rng')
+00013600: 2c20 782e 7368 6170 6529 2c0a 2020 2020  , x.shape),.    
+00013610: 2020 2e2e 2e20 2020 2020 2020 782c 0a20    ...       x,. 
+00013620: 2020 2020 202e 2e2e 2020 2020 2029 0a20       ...     ). 
+00013630: 2020 2020 202e 2e2e 2020 2020 2078 203d       ...     x =
+00013640: 2078 202b 206f 7468 6572 5f76 6172 6961   x + other_varia
+00013650: 626c 652e 7661 6c75 650a 2020 2020 2020  ble.value.      
+00013660: 2e2e 2e0a 2020 2020 2020 2e2e 2e20 2020  ....      ...   
+00013670: 2020 7265 7475 726e 206e 6e2e 4465 6e73    return nn.Dens
+00013680: 6528 3129 2878 290a 0a20 2020 2020 203e  e(1)(x)..      >
+00013690: 3e3e 206d 6f64 756c 6520 3d20 466f 6f28  >> module = Foo(
+000136a0: 290a 2020 2020 2020 3e3e 3e20 726e 6773  ).      >>> rngs
+000136b0: 203d 207b 2770 6172 616d 7327 3a20 6a61   = {'params': ja
+000136c0: 782e 7261 6e64 6f6d 2e6b 6579 2830 292c  x.random.key(0),
+000136d0: 2027 6f74 6865 725f 726e 6727 3a20 6a61   'other_rng': ja
+000136e0: 782e 7261 6e64 6f6d 2e6b 6579 2831 297d  x.random.key(1)}
+000136f0: 0a20 2020 2020 203e 3e3e 2076 6172 6961  .      >>> varia
+00013700: 626c 6573 3020 3d20 6d6f 6475 6c65 2e69  bles0 = module.i
+00013710: 6e69 7428 726e 6773 2c20 7829 0a0a 2020  nit(rngs, x)..  
+00013720: 2020 2020 3e3e 3e20 726e 6773 5b27 6f74      >>> rngs['ot
+00013730: 6865 725f 726e 6727 5d20 3d20 6a61 782e  her_rng'] = jax.
+00013740: 7261 6e64 6f6d 2e6b 6579 2830 290a 2020  random.key(0).  
+00013750: 2020 2020 3e3e 3e20 7661 7269 6162 6c65      >>> variable
+00013760: 7331 203d 206d 6f64 756c 652e 696e 6974  s1 = module.init
+00013770: 2872 6e67 732c 2078 290a 2020 2020 2020  (rngs, x).      
+00013780: 3e3e 3e20 2320 6571 7569 7661 6c65 6e74  >>> # equivalent
+00013790: 2070 6172 616d 7320 286b 6579 2830 2929   params (key(0))
+000137a0: 0a20 2020 2020 203e 3e3e 205f 203d 206a  .      >>> _ = j
+000137b0: 6178 2e74 7265 655f 7574 696c 2e74 7265  ax.tree_util.tre
+000137c0: 655f 6d61 7028 0a20 2020 2020 202e 2e2e  e_map(.      ...
+000137d0: 2020 206e 702e 7465 7374 696e 672e 6173     np.testing.as
+000137e0: 7365 7274 5f61 6c6c 636c 6f73 652c 2076  sert_allclose, v
+000137f0: 6172 6961 626c 6573 305b 2770 6172 616d  ariables0['param
+00013800: 7327 5d2c 2076 6172 6961 626c 6573 315b  s'], variables1[
+00013810: 2770 6172 616d 7327 5d0a 2020 2020 2020  'params'].      
+00013820: 2e2e 2e20 290a 2020 2020 2020 3e3e 3e20  ... ).      >>> 
+00013830: 2320 6469 6666 6572 656e 7420 6f74 6865  # different othe
+00013840: 725f 7661 7269 6162 6c65 2028 6b65 7928  r_variable (key(
+00013850: 3129 2076 7320 6b65 7928 3029 290a 2020  1) vs key(0)).  
+00013860: 2020 2020 3e3e 3e20 6e70 2e74 6573 7469      >>> np.testi
+00013870: 6e67 2e61 7373 6572 745f 7261 6973 6573  ng.assert_raises
+00013880: 280a 2020 2020 2020 2e2e 2e20 2020 4173  (.      ...   As
+00013890: 7365 7274 696f 6e45 7272 6f72 2c0a 2020  sertionError,.  
+000138a0: 2020 2020 2e2e 2e20 2020 6e70 2e74 6573      ...   np.tes
+000138b0: 7469 6e67 2e61 7373 6572 745f 616c 6c63  ting.assert_allc
+000138c0: 6c6f 7365 2c0a 2020 2020 2020 2e2e 2e20  lose,.      ... 
+000138d0: 2020 7661 7269 6162 6c65 7330 5b27 6f74    variables0['ot
+000138e0: 6865 725f 636f 6c6c 6563 7469 6f6e 275d  her_collection']
+000138f0: 5b27 6f74 6865 725f 7661 7269 6162 6c65  ['other_variable
+00013900: 275d 2c0a 2020 2020 2020 2e2e 2e20 2020  '],.      ...   
+00013910: 7661 7269 6162 6c65 7331 5b27 6f74 6865  variables1['othe
+00013920: 725f 636f 6c6c 6563 7469 6f6e 275d 5b27  r_collection']['
+00013930: 6f74 6865 725f 7661 7269 6162 6c65 275d  other_variable']
+00013940: 2c0a 2020 2020 2020 2e2e 2e20 290a 0a20  ,.      ... ).. 
+00013950: 2020 2020 203e 3e3e 2064 656c 2072 6e67       >>> del rng
+00013960: 735b 276f 7468 6572 5f72 6e67 275d 0a20  s['other_rng']. 
+00013970: 2020 2020 203e 3e3e 2023 2073 656c 662e       >>> # self.
+00013980: 6d61 6b65 5f72 6e67 2827 6f74 6865 725f  make_rng('other_
+00013990: 726e 6727 2920 7769 6c6c 2064 6566 6175  rng') will defau
+000139a0: 6c74 2074 6f20 7573 696e 6720 7468 6520  lt to using the 
+000139b0: 2770 6172 616d 7327 2052 4e47 2073 7472  'params' RNG str
+000139c0: 6561 6d0a 2020 2020 2020 3e3e 3e20 7661  eam.      >>> va
+000139d0: 7269 6162 6c65 7332 203d 206d 6f64 756c  riables2 = modul
+000139e0: 652e 696e 6974 2872 6e67 732c 2078 290a  e.init(rngs, x).
+000139f0: 2020 2020 2020 3e3e 3e20 2320 6571 7569        >>> # equi
+00013a00: 7661 6c65 6e74 2070 6172 616d 7320 286b  valent params (k
+00013a10: 6579 2830 2929 0a20 2020 2020 203e 3e3e  ey(0)).      >>>
+00013a20: 205f 203d 206a 6178 2e74 7265 655f 7574   _ = jax.tree_ut
+00013a30: 696c 2e74 7265 655f 6d61 7028 0a20 2020  il.tree_map(.   
+00013a40: 2020 202e 2e2e 2020 206e 702e 7465 7374     ...   np.test
+00013a50: 696e 672e 6173 7365 7274 5f61 6c6c 636c  ing.assert_allcl
+00013a60: 6f73 652c 2076 6172 6961 626c 6573 315b  ose, variables1[
+00013a70: 2770 6172 616d 7327 5d2c 2076 6172 6961  'params'], varia
+00013a80: 626c 6573 325b 2770 6172 616d 7327 5d0a  bles2['params'].
+00013a90: 2020 2020 2020 2e2e 2e20 290a 2020 2020        ... ).    
+00013aa0: 2020 3e3e 3e20 2320 6571 7569 7661 6c65    >>> # equivale
+00013ab0: 6e74 206f 7468 6572 5f76 6172 6961 626c  nt other_variabl
+00013ac0: 6520 286b 6579 2830 2929 0a20 2020 2020  e (key(0)).     
+00013ad0: 203e 3e3e 206e 702e 7465 7374 696e 672e   >>> np.testing.
+00013ae0: 6173 7365 7274 5f61 6c6c 636c 6f73 6528  assert_allclose(
+00013af0: 0a20 2020 2020 202e 2e2e 2020 2076 6172  .      ...   var
+00013b00: 6961 626c 6573 315b 276f 7468 6572 5f63  iables1['other_c
+00013b10: 6f6c 6c65 6374 696f 6e27 5d5b 276f 7468  ollection']['oth
+00013b20: 6572 5f76 6172 6961 626c 6527 5d2c 0a20  er_variable'],. 
+00013b30: 2020 2020 202e 2e2e 2020 2076 6172 6961       ...   varia
+00013b40: 626c 6573 325b 276f 7468 6572 5f63 6f6c  bles2['other_col
+00013b50: 6c65 6374 696f 6e27 5d5b 276f 7468 6572  lection']['other
+00013b60: 5f76 6172 6961 626c 6527 5d2c 0a20 2020  _variable'],.   
+00013b70: 2020 202e 2e2e 2029 0a0a 2020 2020 2020     ... )..      
+00013b80: 3e3e 3e20 2320 7061 7373 696e 6720 696e  >>> # passing in
+00013b90: 2061 2073 696e 676c 6520 6b65 7920 6973   a single key is
+00013ba0: 2065 7175 6976 616c 656e 7420 746f 2070   equivalent to p
+00013bb0: 6173 7369 6e67 2069 6e20 7b27 7061 7261  assing in {'para
+00013bc0: 6d73 273a 206b 6579 7d0a 2020 2020 2020  ms': key}.      
+00013bd0: 3e3e 3e20 7661 7269 6162 6c65 7333 203d  >>> variables3 =
+00013be0: 206d 6f64 756c 652e 696e 6974 286a 6178   module.init(jax
+00013bf0: 2e72 616e 646f 6d2e 6b65 7928 3029 2c20  .random.key(0), 
+00013c00: 7829 0a20 2020 2020 203e 3e3e 2023 2065  x).      >>> # e
+00013c10: 7175 6976 616c 656e 7420 7061 7261 6d73  quivalent params
+00013c20: 2028 6b65 7928 3029 290a 2020 2020 2020   (key(0)).      
+00013c30: 3e3e 3e20 5f20 3d20 6a61 782e 7472 6565  >>> _ = jax.tree
+00013c40: 5f75 7469 6c2e 7472 6565 5f6d 6170 280a  _util.tree_map(.
+00013c50: 2020 2020 2020 2e2e 2e20 2020 6e70 2e74        ...   np.t
+00013c60: 6573 7469 6e67 2e61 7373 6572 745f 616c  esting.assert_al
+00013c70: 6c63 6c6f 7365 2c20 7661 7269 6162 6c65  lclose, variable
+00013c80: 7332 5b27 7061 7261 6d73 275d 2c20 7661  s2['params'], va
+00013c90: 7269 6162 6c65 7333 5b27 7061 7261 6d73  riables3['params
+00013ca0: 275d 0a20 2020 2020 202e 2e2e 2029 0a20  '].      ... ). 
+00013cb0: 2020 2020 203e 3e3e 2023 2065 7175 6976       >>> # equiv
+00013cc0: 616c 656e 7420 6f74 6865 725f 7661 7269  alent other_vari
+00013cd0: 6162 6c65 2028 6b65 7928 3029 290a 2020  able (key(0)).  
+00013ce0: 2020 2020 3e3e 3e20 6e70 2e74 6573 7469      >>> np.testi
+00013cf0: 6e67 2e61 7373 6572 745f 616c 6c63 6c6f  ng.assert_allclo
+00013d00: 7365 280a 2020 2020 2020 2e2e 2e20 2020  se(.      ...   
+00013d10: 7661 7269 6162 6c65 7332 5b27 6f74 6865  variables2['othe
+00013d20: 725f 636f 6c6c 6563 7469 6f6e 275d 5b27  r_collection']['
+00013d30: 6f74 6865 725f 7661 7269 6162 6c65 275d  other_variable']
+00013d40: 2c0a 2020 2020 2020 2e2e 2e20 2020 7661  ,.      ...   va
+00013d50: 7269 6162 6c65 7333 5b27 6f74 6865 725f  riables3['other_
+00013d60: 636f 6c6c 6563 7469 6f6e 275d 5b27 6f74  collection']['ot
+00013d70: 6865 725f 7661 7269 6162 6c65 275d 2c0a  her_variable'],.
+00013d80: 2020 2020 2020 2e2e 2e20 290a 0a20 2020        ... )..   
+00013d90: 204a 6974 7469 6e67 2060 6069 6e69 7460   Jitting ``init`
+00013da0: 6020 696e 6974 6961 6c69 7a65 7320 6120  ` initializes a 
+00013db0: 6d6f 6465 6c20 6c61 7a69 6c79 2075 7369  model lazily usi
+00013dc0: 6e67 206f 6e6c 7920 7468 6520 7368 6170  ng only the shap
+00013dd0: 6573 206f 6620 7468 650a 2020 2020 7072  es of the.    pr
+00013de0: 6f76 6964 6564 2061 7267 756d 656e 7473  ovided arguments
+00013df0: 2c20 616e 6420 6176 6f69 6473 2063 6f6d  , and avoids com
+00013e00: 7075 7469 6e67 2074 6865 2066 6f72 7761  puting the forwa
+00013e10: 7264 2070 6173 7320 7769 7468 2061 6374  rd pass with act
+00013e20: 7561 6c0a 2020 2020 7661 6c75 6573 2e20  ual.    values. 
+00013e30: 4578 616d 706c 653a 3a0a 0a20 2020 2020  Example::..     
+00013e40: 203e 3e3e 206d 6f64 756c 6520 3d20 6e6e   >>> module = nn
+00013e50: 2e44 656e 7365 2831 290a 2020 2020 2020  .Dense(1).      
+00013e60: 3e3e 3e20 696e 6974 5f6a 6974 203d 206a  >>> init_jit = j
+00013e70: 6178 2e6a 6974 286d 6f64 756c 652e 696e  ax.jit(module.in
+00013e80: 6974 290a 2020 2020 2020 3e3e 3e20 7661  it).      >>> va
+00013e90: 7269 6162 6c65 7320 3d20 696e 6974 5f6a  riables = init_j
+00013ea0: 6974 286a 6178 2e72 616e 646f 6d2e 6b65  it(jax.random.ke
+00013eb0: 7928 3029 2c20 7829 0a0a 2020 2020 6060  y(0), x)..    ``
+00013ec0: 696e 6974 6060 2069 7320 6120 6c69 6768  init`` is a ligh
+00013ed0: 7420 7772 6170 7065 7220 6f76 6572 2060  t wrapper over `
+00013ee0: 6061 7070 6c79 6060 2c20 736f 206f 7468  `apply``, so oth
+00013ef0: 6572 2060 6061 7070 6c79 6060 2061 7267  er ``apply`` arg
+00013f00: 756d 656e 7473 0a20 2020 206c 696b 6520  uments.    like 
+00013f10: 6060 6d65 7468 6f64 6060 2c20 6060 6d75  ``method``, ``mu
+00013f20: 7461 626c 6560 602c 2061 6e64 2060 6063  table``, and ``c
+00013f30: 6170 7475 7265 5f69 6e74 6572 6d65 6469  apture_intermedi
+00013f40: 6174 6573 6060 2061 7265 2061 6c73 6f0a  ates`` are also.
+00013f50: 2020 2020 6176 6169 6c61 626c 652e 0a0a      available...
+00013f60: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
+00013f70: 726e 6773 3a20 5468 6520 726e 6773 2066  rngs: The rngs f
+00013f80: 6f72 2074 6865 2076 6172 6961 626c 6520  or the variable 
+00013f90: 636f 6c6c 6563 7469 6f6e 732e 0a20 2020  collections..   
+00013fa0: 2020 202a 6172 6773 3a20 4e61 6d65 6420     *args: Named 
+00013fb0: 6172 6775 6d65 6e74 7320 7061 7373 6564  arguments passed
+00013fc0: 2074 6f20 7468 6520 696e 6974 2066 756e   to the init fun
+00013fd0: 6374 696f 6e2e 0a20 2020 2020 206d 6574  ction..      met
+00013fe0: 686f 643a 2041 6e20 6f70 7469 6f6e 616c  hod: An optional
+00013ff0: 206d 6574 686f 642e 2049 6620 7072 6f76   method. If prov
+00014000: 6964 6564 2c20 6170 706c 6965 7320 7468  ided, applies th
+00014010: 6973 206d 6574 686f 642e 2049 6620 6e6f  is method. If no
+00014020: 740a 2020 2020 2020 2020 7072 6f76 6964  t.        provid
+00014030: 6564 2c20 6170 706c 6965 7320 7468 6520  ed, applies the 
+00014040: 6060 5f5f 6361 6c6c 5f5f 6060 206d 6574  ``__call__`` met
+00014050: 686f 642e 2041 2073 7472 696e 6720 6361  hod. A string ca
+00014060: 6e20 616c 736f 2062 6520 7072 6f76 6964  n also be provid
+00014070: 6564 0a20 2020 2020 2020 2074 6f20 7370  ed.        to sp
+00014080: 6563 6966 7920 6120 6d65 7468 6f64 2062  ecify a method b
+00014090: 7920 6e61 6d65 2e0a 2020 2020 2020 6d75  y name..      mu
+000140a0: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
+000140b0: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
+000140c0: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
+000140d0: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
+000140e0: 6f75 6c64 2062 650a 2020 2020 2020 2020  ould be.        
+000140f0: 7472 6561 7465 6420 6173 206d 7574 6162  treated as mutab
+00014100: 6c65 3a20 6060 626f 6f6c 6060 3a20 616c  le: ``bool``: al
+00014110: 6c2f 6e6f 2063 6f6c 6c65 6374 696f 6e73  l/no collections
+00014120: 2061 7265 206d 7574 6162 6c65 2e20 6060   are mutable. ``
+00014130: 7374 7260 603a 0a20 2020 2020 2020 2054  str``:.        T
+00014140: 6865 206e 616d 6520 6f66 2061 2073 696e  he name of a sin
+00014150: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
+00014160: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
+00014170: 3a20 4120 6c69 7374 206f 6620 6e61 6d65  : A list of name
+00014180: 7320 6f66 0a20 2020 2020 2020 206d 7574  s of.        mut
+00014190: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
+000141a0: 2e20 4279 2064 6566 6175 6c74 2061 6c6c  . By default all
+000141b0: 2063 6f6c 6c65 6374 696f 6e73 2065 7863   collections exc
+000141c0: 6570 7420 2269 6e74 6572 6d65 6469 6174  ept "intermediat
+000141d0: 6573 220a 2020 2020 2020 2020 6172 6520  es".        are 
+000141e0: 6d75 7461 626c 652e 0a20 2020 2020 2063  mutable..      c
+000141f0: 6170 7475 7265 5f69 6e74 6572 6d65 6469  apture_intermedi
+00014200: 6174 6573 3a20 4966 2060 6054 7275 6560  ates: If ``True`
+00014210: 602c 2063 6170 7475 7265 7320 696e 7465  `, captures inte
+00014220: 726d 6564 6961 7465 2072 6574 7572 6e20  rmediate return 
+00014230: 7661 6c75 6573 206f 660a 2020 2020 2020  values of.      
+00014240: 2020 616c 6c20 4d6f 6475 6c65 7320 696e    all Modules in
+00014250: 7369 6465 2074 6865 2022 696e 7465 726d  side the "interm
+00014260: 6564 6961 7465 7322 2063 6f6c 6c65 6374  ediates" collect
+00014270: 696f 6e2e 2042 7920 6465 6661 756c 7420  ion. By default 
+00014280: 6f6e 6c79 2074 6865 0a20 2020 2020 2020  only the.       
+00014290: 2072 6574 7572 6e20 7661 6c75 6573 206f   return values o
+000142a0: 6620 616c 6c20 6060 5f5f 6361 6c6c 5f5f  f all ``__call__
+000142b0: 6060 206d 6574 686f 6473 2061 7265 2073  `` methods are s
+000142c0: 746f 7265 642e 2041 2066 756e 6374 696f  tored. A functio
+000142d0: 6e20 6361 6e20 6265 0a20 2020 2020 2020  n can be.       
+000142e0: 2070 6173 7365 6420 746f 2063 6861 6e67   passed to chang
+000142f0: 6520 7468 6520 6669 6c74 6572 2062 6568  e the filter beh
+00014300: 6176 696f 722e 2054 6865 2066 696c 7465  avior. The filte
+00014310: 7220 6675 6e63 7469 6f6e 2074 616b 6573  r function takes
+00014320: 2074 6865 0a20 2020 2020 2020 204d 6f64   the.        Mod
+00014330: 756c 6520 696e 7374 616e 6365 2061 6e64  ule instance and
+00014340: 206d 6574 686f 6420 6e61 6d65 2061 6e64   method name and
+00014350: 2072 6574 7572 6e73 2061 2062 6f6f 6c20   returns a bool 
+00014360: 696e 6469 6361 7469 6e67 2077 6865 7468  indicating wheth
+00014370: 6572 0a20 2020 2020 2020 2074 6865 206f  er.        the o
+00014380: 7574 7075 7420 6f66 2074 6861 7420 6d65  utput of that me
+00014390: 7468 6f64 2069 6e76 6f63 6174 696f 6e20  thod invocation 
+000143a0: 7368 6f75 6c64 2062 6520 7374 6f72 6564  should be stored
+000143b0: 2e0a 2020 2020 2020 2a2a 6b77 6172 6773  ..      **kwargs
+000143c0: 3a20 4b65 7977 6f72 6420 6172 6775 6d65  : Keyword argume
+000143d0: 6e74 7320 7061 7373 6564 2074 6f20 7468  nts passed to th
+000143e0: 6520 696e 6974 2066 756e 6374 696f 6e2e  e init function.
+000143f0: 0a0a 2020 2020 5265 7475 726e 733a 0a20  ..    Returns:. 
+00014400: 2020 2020 2054 6865 2069 6e69 7469 616c       The initial
+00014410: 697a 6564 2076 6172 6961 626c 6520 6469  ized variable di
+00014420: 6374 2e0a 2020 2020 2222 220a 2020 2020  ct..    """.    
+00014430: 4d6f 6475 6c65 2e5f 6d6f 6475 6c65 5f63  Module._module_c
+00014440: 6865 636b 7328 7365 6c66 290a 0a20 2020  hecks(self)..   
+00014450: 205f 2c20 765f 6f75 7420 3d20 7365 6c66   _, v_out = self
+00014460: 2e69 6e69 745f 7769 7468 5f6f 7574 7075  .init_with_outpu
+00014470: 7428 0a20 2020 2020 2072 6e67 732c 0a20  t(.      rngs,. 
+00014480: 2020 2020 202a 6172 6773 2c0a 2020 2020       *args,.    
+00014490: 2020 6d65 7468 6f64 3d6d 6574 686f 642c    method=method,
+000144a0: 0a20 2020 2020 206d 7574 6162 6c65 3d6d  .      mutable=m
+000144b0: 7574 6162 6c65 2c0a 2020 2020 2020 6361  utable,.      ca
+000144c0: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
+000144d0: 7465 733d 6361 7074 7572 655f 696e 7465  tes=capture_inte
+000144e0: 726d 6564 6961 7465 732c 0a20 2020 2020  rmediates,.     
+000144f0: 202a 2a6b 7761 7267 732c 0a20 2020 2029   **kwargs,.    )
+00014500: 0a20 2020 2072 6574 7572 6e20 765f 6f75  .    return v_ou
+00014510: 740a 0a20 2040 7472 6163 6562 6163 6b5f  t..  @traceback_
+00014520: 7574 696c 2e61 7069 5f62 6f75 6e64 6172  util.api_boundar
+00014530: 790a 2020 6465 6620 6c61 7a79 5f69 6e69  y.  def lazy_ini
+00014540: 7428 0a20 2020 2073 656c 662c 0a20 2020  t(.    self,.   
+00014550: 2072 6e67 733a 2055 6e69 6f6e 5b50 524e   rngs: Union[PRN
+00014560: 474b 6579 2c20 524e 4753 6571 7565 6e63  GKey, RNGSequenc
+00014570: 6573 5d2c 0a20 2020 202a 6172 6773 2c0a  es],.    *args,.
+00014580: 2020 2020 6d65 7468 6f64 3a20 4f70 7469      method: Opti
+00014590: 6f6e 616c 5b43 616c 6c61 626c 655b 2e2e  onal[Callable[..
+000145a0: 2e2c 2041 6e79 5d5d 203d 204e 6f6e 652c  ., Any]] = None,
+000145b0: 0a20 2020 206d 7574 6162 6c65 3a20 436f  .    mutable: Co
+000145c0: 6c6c 6563 7469 6f6e 4669 6c74 6572 203d  llectionFilter =
+000145d0: 2044 656e 794c 6973 7428 2769 6e74 6572   DenyList('inter
+000145e0: 6d65 6469 6174 6573 2729 2c0a 2020 2020  mediates'),.    
+000145f0: 2a2a 6b77 6172 6773 2c0a 2020 2920 2d3e  **kwargs,.  ) ->
+00014600: 2046 726f 7a65 6e56 6172 6961 626c 6544   FrozenVariableD
+00014610: 6963 743a 0a20 2020 2022 2222 496e 6974  ict:.    """Init
+00014620: 6961 6c69 7a65 7320 6120 6d6f 6475 6c65  ializes a module
+00014630: 2077 6974 686f 7574 2063 6f6d 7075 7469   without computi
+00014640: 6e67 206f 6e20 616e 2061 6374 7561 6c20  ng on an actual 
+00014650: 696e 7075 742e 0a0a 2020 2020 6c61 7a79  input...    lazy
+00014660: 5f69 6e69 7420 7769 6c6c 2069 6e69 7469  _init will initi
+00014670: 616c 697a 6520 7468 6520 7661 7269 6162  alize the variab
+00014680: 6c65 7320 7769 7468 6f75 7420 646f 696e  les without doin
+00014690: 6720 756e 6e65 6365 7373 6172 7920 636f  g unnecessary co
+000146a0: 6d70 7574 652e 0a20 2020 2054 6865 2069  mpute..    The i
+000146b0: 6e70 7574 2064 6174 6120 7368 6f75 6c64  nput data should
+000146c0: 2062 6520 7061 7373 6564 2061 7320 6120   be passed as a 
+000146d0: 6060 6a61 782e 5368 6170 6544 7479 7065  ``jax.ShapeDtype
+000146e0: 5374 7275 6374 6060 2077 6869 6368 0a20  Struct`` which. 
+000146f0: 2020 2073 7065 6369 6669 6573 2074 6865     specifies the
+00014700: 2073 6861 7065 2061 6e64 2064 7479 7065   shape and dtype
+00014710: 206f 6620 7468 6520 696e 7075 7420 6275   of the input bu
+00014720: 7420 6e6f 2063 6f6e 6372 6574 6520 6461  t no concrete da
+00014730: 7461 2e0a 0a20 2020 2045 7861 6d70 6c65  ta...    Example
+00014740: 3a3a 0a0a 2020 2020 2020 3e3e 3e20 6d6f  ::..      >>> mo
+00014750: 6465 6c20 3d20 6e6e 2e44 656e 7365 2866  del = nn.Dense(f
+00014760: 6561 7475 7265 733d 3235 3629 0a20 2020  eatures=256).   
+00014770: 2020 203e 3e3e 2076 6172 6961 626c 6573     >>> variables
+00014780: 203d 206d 6f64 656c 2e6c 617a 795f 696e   = model.lazy_in
+00014790: 6974 280a 2020 2020 2020 2e2e 2e20 2020  it(.      ...   
+000147a0: 2020 6a61 782e 7261 6e64 6f6d 2e6b 6579    jax.random.key
+000147b0: 2830 292c 206a 6178 2e53 6861 7065 4474  (0), jax.ShapeDt
+000147c0: 7970 6553 7472 7563 7428 2831 2c20 3132  ypeStruct((1, 12
+000147d0: 3829 2c20 6a6e 702e 666c 6f61 7433 3229  8), jnp.float32)
+000147e0: 290a 0a20 2020 2054 6865 2061 7267 7320  )..    The args 
+000147f0: 616e 6420 6b77 6172 6773 2061 7267 7320  and kwargs args 
+00014800: 7061 7373 6564 2074 6f20 6060 6c61 7a79  passed to ``lazy
+00014810: 5f69 6e69 7460 6020 6361 6e20 6265 2061  _init`` can be a
+00014820: 206d 6978 206f 660a 2020 2020 636f 6e63   mix of.    conc
+00014830: 7265 7465 2028 6a61 7820 6172 7261 7973  rete (jax arrays
+00014840: 2c20 7363 616c 6172 732c 2062 6f6f 6c73  , scalars, bools
+00014850: 2920 616e 6420 6162 7374 7261 6374 2028  ) and abstract (
+00014860: 5368 6170 6544 7479 7065 5374 7275 6374  ShapeDtypeStruct
+00014870: 290a 2020 2020 7661 6c75 6573 2e20 436f  ).    values. Co
+00014880: 6e63 7265 7465 2076 616c 7565 7320 6172  ncrete values ar
+00014890: 6520 6f6e 6c79 206e 6563 6573 7361 7279  e only necessary
+000148a0: 2066 6f72 2061 7267 756d 656e 7473 2074   for arguments t
+000148b0: 6861 7420 6166 6665 6374 0a20 2020 2074  hat affect.    t
+000148c0: 6865 2069 6e69 7469 616c 697a 6174 696f  he initializatio
+000148d0: 6e20 6f66 2076 6172 6961 626c 6573 2e20  n of variables. 
+000148e0: 466f 7220 6578 616d 706c 652c 2074 6865  For example, the
+000148f0: 206d 6f64 656c 206d 6967 6874 2065 7870   model might exp
+00014900: 6563 740a 2020 2020 6120 6b65 7977 6f72  ect.    a keywor
+00014910: 6420 6172 6720 7468 6174 2065 6e61 626c  d arg that enabl
+00014920: 6573 2f64 6973 6162 6c65 7320 6120 7375  es/disables a su
+00014930: 6270 6172 7420 6f66 2074 6865 206d 6f64  bpart of the mod
+00014940: 656c 2e0a 2020 2020 496e 2074 6869 7320  el..    In this 
+00014950: 6361 7365 2c20 616e 2065 7870 6c69 6369  case, an explici
+00014960: 7420 7661 6c75 6520 2854 7275 652f 466c  t value (True/Fl
+00014970: 6173 6529 2073 686f 756c 6420 6265 2070  ase) should be p
+00014980: 6173 7365 6420 6f74 6865 7277 6973 650a  assed otherwise.
+00014990: 2020 2020 6060 6c61 7a79 5f69 6e69 7460      ``lazy_init`
+000149a0: 6020 6361 6e6e 6f74 2069 6e66 6572 2077  ` cannot infer w
+000149b0: 6869 6368 2076 6172 6961 626c 6573 2073  hich variables s
+000149c0: 686f 756c 6420 6265 2069 6e69 7469 616c  hould be initial
+000149d0: 697a 6564 2e0a 0a20 2020 2041 7267 733a  ized...    Args:
+000149e0: 0a20 2020 2020 2072 6e67 733a 2054 6865  .      rngs: The
+000149f0: 2072 6e67 7320 666f 7220 7468 6520 7661   rngs for the va
+00014a00: 7269 6162 6c65 2063 6f6c 6c65 6374 696f  riable collectio
+00014a10: 6e73 2e0a 2020 2020 2020 2a61 7267 733a  ns..      *args:
+00014a20: 2061 7267 756d 656e 7473 2070 6173 7365   arguments passe
+00014a30: 6420 746f 2074 6865 2069 6e69 7420 6675  d to the init fu
+00014a40: 6e63 7469 6f6e 2e0a 2020 2020 2020 6d65  nction..      me
+00014a50: 7468 6f64 3a20 416e 206f 7074 696f 6e61  thod: An optiona
+00014a60: 6c20 6d65 7468 6f64 2e20 4966 2070 726f  l method. If pro
+00014a70: 7669 6465 642c 2061 7070 6c69 6573 2074  vided, applies t
+00014a80: 6869 7320 6d65 7468 6f64 2e20 4966 206e  his method. If n
+00014a90: 6f74 0a20 2020 2020 2020 2070 726f 7669  ot.        provi
+00014aa0: 6465 642c 2061 7070 6c69 6573 2074 6865  ded, applies the
+00014ab0: 2060 605f 5f63 616c 6c5f 5f60 6020 6d65   ``__call__`` me
+00014ac0: 7468 6f64 2e0a 2020 2020 2020 6d75 7461  thod..      muta
+00014ad0: 626c 653a 2043 616e 2062 6520 626f 6f6c  ble: Can be bool
+00014ae0: 2c20 7374 722c 206f 7220 6c69 7374 2e20  , str, or list. 
+00014af0: 5370 6563 6966 6965 7320 7768 6963 6820  Specifies which 
+00014b00: 636f 6c6c 6563 7469 6f6e 7320 7368 6f75  collections shou
+00014b10: 6c64 2062 650a 2020 2020 2020 2020 7472  ld be.        tr
+00014b20: 6561 7465 6420 6173 206d 7574 6162 6c65  eated as mutable
+00014b30: 3a20 6060 626f 6f6c 6060 3a20 616c 6c2f  : ``bool``: all/
+00014b40: 6e6f 2063 6f6c 6c65 6374 696f 6e73 2061  no collections a
+00014b50: 7265 206d 7574 6162 6c65 2e20 6060 7374  re mutable. ``st
+00014b60: 7260 603a 0a20 2020 2020 2020 2054 6865  r``:.        The
+00014b70: 206e 616d 6520 6f66 2061 2073 696e 676c   name of a singl
+00014b80: 6520 6d75 7461 626c 6520 636f 6c6c 6563  e mutable collec
+00014b90: 7469 6f6e 2e20 6060 6c69 7374 6060 3a20  tion. ``list``: 
+00014ba0: 4120 6c69 7374 206f 6620 6e61 6d65 7320  A list of names 
+00014bb0: 6f66 0a20 2020 2020 2020 206d 7574 6162  of.        mutab
+00014bc0: 6c65 2063 6f6c 6c65 6374 696f 6e73 2e20  le collections. 
+00014bd0: 4279 2064 6566 6175 6c74 2061 6c6c 2063  By default all c
+00014be0: 6f6c 6c65 6374 696f 6e73 2065 7863 6570  ollections excep
+00014bf0: 7420 2269 6e74 6572 6d65 6469 6174 6573  t "intermediates
+00014c00: 220a 2020 2020 2020 2020 6172 6520 6d75  ".        are mu
+00014c10: 7461 626c 652e 0a20 2020 2020 202a 2a6b  table..      **k
+00014c20: 7761 7267 733a 204b 6579 776f 7264 2061  wargs: Keyword a
+00014c30: 7267 756d 656e 7473 2070 6173 7365 6420  rguments passed 
+00014c40: 746f 2074 6865 2069 6e69 7420 6675 6e63  to the init func
+00014c50: 7469 6f6e 2e0a 0a20 2020 2052 6574 7572  tion...    Retur
+00014c60: 6e73 3a0a 2020 2020 2020 5468 6520 696e  ns:.      The in
+00014c70: 6974 6961 6c69 7a65 6420 7661 7269 6162  itialized variab
+00014c80: 6c65 2064 6963 742e 0a20 2020 2022 2222  le dict..    """
+00014c90: 0a20 2020 204d 6f64 756c 652e 5f6d 6f64  .    Module._mod
+00014ca0: 756c 655f 6368 6563 6b73 2873 656c 6629  ule_checks(self)
+00014cb0: 0a0a 2020 2020 6465 6620 6c61 7a79 5f77  ..    def lazy_w
+00014cc0: 7261 7070 6572 2872 6e67 732c 202a 6172  rapper(rngs, *ar
+00014cd0: 6773 2c20 2a2a 6b77 6172 6773 293a 0a20  gs, **kwargs):. 
+00014ce0: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
+00014cf0: 2e69 6e69 7428 726e 6773 2c20 2a61 7267  .init(rngs, *arg
+00014d00: 732c 206d 6574 686f 643d 6d65 7468 6f64  s, method=method
+00014d10: 2c20 6d75 7461 626c 653d 6d75 7461 626c  , mutable=mutabl
+00014d20: 652c 202a 2a6b 7761 7267 7329 0a0a 2020  e, **kwargs)..  
+00014d30: 2020 7265 7475 726e 2070 6172 7469 616c    return partial
+00014d40: 5f65 7661 6c2e 6c61 7a79 5f69 6e69 7428  _eval.lazy_init(
+00014d50: 6c61 7a79 5f77 7261 7070 6572 2928 726e  lazy_wrapper)(rn
+00014d60: 6773 2c20 2a61 7267 732c 202a 2a6b 7761  gs, *args, **kwa
+00014d70: 7267 7329 0a0a 2020 4070 726f 7065 7274  rgs)..  @propert
+00014d80: 790a 2020 6465 6620 7661 7269 6162 6c65  y.  def variable
+00014d90: 7328 7365 6c66 2920 2d3e 2056 6172 6961  s(self) -> Varia
+00014da0: 626c 6544 6963 743a 0a20 2020 2022 2222  bleDict:.    """
+00014db0: 5265 7475 726e 7320 7468 6520 7661 7269  Returns the vari
+00014dc0: 6162 6c65 7320 696e 2074 6869 7320 6d6f  ables in this mo
+00014dd0: 6475 6c65 2e22 2222 0a20 2020 2069 6620  dule.""".    if 
+00014de0: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
+00014df0: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
+00014e00: 5661 6c75 6545 7272 6f72 2822 4361 6e27  ValueError("Can'
+00014e10: 7420 6163 6365 7373 2076 6172 6961 626c  t access variabl
+00014e20: 6573 206f 6e20 756e 626f 756e 6420 6d6f  es on unbound mo
+00014e30: 6475 6c65 7322 290a 2020 2020 7265 7475  dules").    retu
+00014e40: 726e 2073 656c 662e 7363 6f70 652e 7661  rn self.scope.va
+00014e50: 7269 6162 6c65 7328 290a 0a20 2064 6566  riables()..  def
+00014e60: 2067 6574 5f76 6172 6961 626c 6528 7365   get_variable(se
+00014e70: 6c66 2c20 636f 6c3a 2073 7472 2c20 6e61  lf, col: str, na
+00014e80: 6d65 3a20 7374 722c 2064 6566 6175 6c74  me: str, default
+00014e90: 3a20 4f70 7469 6f6e 616c 5b54 5d20 3d20  : Optional[T] = 
+00014ea0: 4e6f 6e65 2920 2d3e 2054 3a0a 2020 2020  None) -> T:.    
+00014eb0: 2222 2252 6574 7269 6576 6573 2074 6865  """Retrieves the
+00014ec0: 2076 616c 7565 206f 6620 6120 5661 7269   value of a Vari
+00014ed0: 6162 6c65 2e0a 0a20 2020 2041 7267 733a  able...    Args:
+00014ee0: 0a20 2020 2020 2063 6f6c 3a20 7468 6520  .      col: the 
+00014ef0: 7661 7269 6162 6c65 2063 6f6c 6c65 6374  variable collect
+00014f00: 696f 6e2e 0a20 2020 2020 206e 616d 653a  ion..      name:
+00014f10: 2074 6865 206e 616d 6520 6f66 2074 6865   the name of the
+00014f20: 2076 6172 6961 626c 652e 0a20 2020 2020   variable..     
+00014f30: 2064 6566 6175 6c74 3a20 7468 6520 6465   default: the de
+00014f40: 6661 756c 7420 7661 6c75 6520 746f 2072  fault value to r
+00014f50: 6574 7572 6e20 6966 2074 6865 2076 6172  eturn if the var
+00014f60: 6961 626c 6520 646f 6573 206e 6f74 2065  iable does not e
+00014f70: 7869 7374 2069 6e0a 2020 2020 2020 2020  xist in.        
+00014f80: 7468 6973 2073 636f 7065 2e0a 0a20 2020  this scope...   
+00014f90: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
+00014fa0: 5468 6520 7661 6c75 6520 6f66 2074 6865  The value of the
+00014fb0: 2069 6e70 7574 2076 6172 6961 626c 652c   input variable,
+00014fc0: 206f 6620 7468 6520 6465 6661 756c 7420   of the default 
+00014fd0: 7661 6c75 6520 6966 2074 6865 2076 6172  value if the var
+00014fe0: 6961 626c 650a 2020 2020 2020 646f 6573  iable.      does
+00014ff0: 6e27 7420 6578 6973 7420 696e 2074 6869  n't exist in thi
+00015000: 7320 7363 6f70 652e 0a20 2020 2022 2222  s scope..    """
+00015010: 0a20 2020 2069 6620 7365 6c66 2e73 636f  .    if self.sco
+00015020: 7065 2069 7320 4e6f 6e65 3a0a 2020 2020  pe is None:.    
+00015030: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+00015040: 6f72 2822 4361 6e27 7420 6163 6365 7373  or("Can't access
+00015050: 2076 6172 6961 626c 6573 206f 6e20 756e   variables on un
+00015060: 626f 756e 6420 6d6f 6475 6c65 7322 290a  bound modules").
+00015070: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
+00015080: 7363 6f70 652e 6765 745f 7661 7269 6162  scope.get_variab
+00015090: 6c65 2863 6f6c 2c20 6e61 6d65 2c20 6465  le(col, name, de
+000150a0: 6661 756c 7429 0a0a 2020 6465 6620 7075  fault)..  def pu
+000150b0: 745f 7661 7269 6162 6c65 2873 656c 662c  t_variable(self,
+000150c0: 2063 6f6c 3a20 7374 722c 206e 616d 653a   col: str, name:
+000150d0: 2073 7472 2c20 7661 6c75 653a 2041 6e79   str, value: Any
+000150e0: 293a 0a20 2020 2022 2222 5570 6461 7465  ):.    """Update
+000150f0: 7320 7468 6520 7661 6c75 6520 6f66 2074  s the value of t
+00015100: 6865 2067 6976 656e 2076 6172 6961 626c  he given variabl
+00015110: 6520 6966 2069 7420 6973 206d 7574 6162  e if it is mutab
+00015120: 6c65 2c20 6f72 2061 6e20 6572 726f 7220  le, or an error 
+00015130: 6f74 6865 7277 6973 652e 0a0a 2020 2020  otherwise...    
+00015140: 4172 6773 3a0a 2020 2020 2020 636f 6c3a  Args:.      col:
+00015150: 2074 6865 2076 6172 6961 626c 6520 636f   the variable co
+00015160: 6c6c 6563 7469 6f6e 2e0a 2020 2020 2020  llection..      
+00015170: 6e61 6d65 3a20 7468 6520 6e61 6d65 206f  name: the name o
+00015180: 6620 7468 6520 7661 7269 6162 6c65 2e0a  f the variable..
+00015190: 2020 2020 2020 7661 6c75 653a 2074 6865        value: the
+000151a0: 206e 6577 2076 616c 7565 206f 6620 7468   new value of th
+000151b0: 6520 7661 7269 6162 6c65 2e0a 2020 2020  e variable..    
+000151c0: 2222 220a 2020 2020 6966 2073 656c 662e  """.    if self.
+000151d0: 7363 6f70 6520 6973 204e 6f6e 653a 0a20  scope is None:. 
+000151e0: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
+000151f0: 4572 726f 7228 2243 616e 2774 2061 6363  Error("Can't acc
+00015200: 6573 7320 7661 7269 6162 6c65 7320 6f6e  ess variables on
+00015210: 2075 6e62 6f75 6e64 206d 6f64 756c 6573   unbound modules
+00015220: 2229 0a20 2020 2073 656c 662e 7363 6f70  ").    self.scop
+00015230: 652e 7075 745f 7661 7269 6162 6c65 2863  e.put_variable(c
+00015240: 6f6c 2c20 6e61 6d65 2c20 7661 6c75 6529  ol, name, value)
+00015250: 0a0a 2020 406f 7665 726c 6f61 640a 2020  ..  @overload.  
+00015260: 6465 6620 736f 7728 7365 6c66 2c20 636f  def sow(self, co
+00015270: 6c3a 2073 7472 2c20 6e61 6d65 3a20 7374  l: str, name: st
+00015280: 722c 2076 616c 7565 3a20 416e 7929 202d  r, value: Any) -
+00015290: 3e20 626f 6f6c 3a0a 2020 2020 2e2e 2e0a  > bool:.    ....
+000152a0: 0a20 2040 6f76 6572 6c6f 6164 0a20 2064  .  @overload.  d
+000152b0: 6566 2073 6f77 280a 2020 2020 7365 6c66  ef sow(.    self
+000152c0: 2c0a 2020 2020 636f 6c3a 2073 7472 2c0a  ,.    col: str,.
+000152d0: 2020 2020 6e61 6d65 3a20 7374 722c 0a20      name: str,. 
+000152e0: 2020 2076 616c 7565 3a20 542c 0a20 2020     value: T,.   
+000152f0: 2072 6564 7563 655f 666e 3a20 4361 6c6c   reduce_fn: Call
+00015300: 6162 6c65 5b5b 4b2c 2054 5d2c 204b 5d20  able[[K, T], K] 
+00015310: 3d20 7475 706c 655f 7265 6475 6365 2c0a  = tuple_reduce,.
+00015320: 2020 2020 696e 6974 5f66 6e3a 2043 616c      init_fn: Cal
+00015330: 6c61 626c 655b 5b5d 2c20 4b5d 203d 2074  lable[[], K] = t
+00015340: 7570 6c65 5f69 6e69 742c 2020 2320 7479  uple_init,  # ty
+00015350: 7065 3a20 6967 6e6f 7265 0a20 2029 202d  pe: ignore.  ) -
+00015360: 3e20 626f 6f6c 3a0a 2020 2020 2e2e 2e0a  > bool:.    ....
+00015370: 0a20 2064 6566 2073 6f77 280a 2020 2020  .  def sow(.    
+00015380: 7365 6c66 2c0a 2020 2020 636f 6c3a 2073  self,.    col: s
+00015390: 7472 2c0a 2020 2020 6e61 6d65 3a20 7374  tr,.    name: st
+000153a0: 722c 0a20 2020 2076 616c 7565 3a20 542c  r,.    value: T,
+000153b0: 0a20 2020 2072 6564 7563 655f 666e 3a20  .    reduce_fn: 
+000153c0: 4361 6c6c 6162 6c65 5b5b 4b2c 2054 5d2c  Callable[[K, T],
+000153d0: 204b 5d20 3d20 7475 706c 655f 7265 6475   K] = tuple_redu
+000153e0: 6365 2c0a 2020 2020 696e 6974 5f66 6e3a  ce,.    init_fn:
+000153f0: 2043 616c 6c61 626c 655b 5b5d 2c20 4b5d   Callable[[], K]
+00015400: 203d 2074 7570 6c65 5f69 6e69 742c 2020   = tuple_init,  
+00015410: 2320 7479 7065 3a20 6967 6e6f 7265 0a20  # type: ignore. 
+00015420: 2029 202d 3e20 626f 6f6c 3a0a 2020 2020   ) -> bool:.    
+00015430: 2222 2253 746f 7265 7320 6120 7661 6c75  """Stores a valu
+00015440: 6520 696e 2061 2063 6f6c 6c65 6374 696f  e in a collectio
+00015450: 6e2e 0a0a 2020 2020 436f 6c6c 6563 7469  n...    Collecti
+00015460: 6f6e 7320 6361 6e20 6265 2075 7365 6420  ons can be used 
+00015470: 746f 2063 6f6c 6c65 6374 2069 6e74 6572  to collect inter
+00015480: 6d65 6469 6174 6520 7661 6c75 6573 2077  mediate values w
+00015490: 6974 686f 7574 0a20 2020 2074 6865 206f  ithout.    the o
+000154a0: 7665 7268 6561 6420 6f66 2065 7870 6c69  verhead of expli
+000154b0: 6369 746c 7920 7061 7373 696e 6720 6120  citly passing a 
+000154c0: 636f 6e74 6169 6e65 7220 7468 726f 7567  container throug
+000154d0: 6820 6561 6368 204d 6f64 756c 6520 6361  h each Module ca
+000154e0: 6c6c 2e0a 0a20 2020 2049 6620 7468 6520  ll...    If the 
+000154f0: 7461 7267 6574 2063 6f6c 6c65 6374 696f  target collectio
+00015500: 6e20 6973 206e 6f74 206d 7574 6162 6c65  n is not mutable
+00015510: 2060 6073 6f77 6060 2062 6568 6176 6573   ``sow`` behaves
+00015520: 206c 696b 6520 6120 6e6f 2d6f 700a 2020   like a no-op.  
+00015530: 2020 616e 6420 7265 7475 726e 7320 6060    and returns ``
+00015540: 4661 6c73 6560 602e 0a0a 2020 2020 4578  False``...    Ex
+00015550: 616d 706c 653a 3a0a 0a20 2020 2020 203e  ample::..      >
+00015560: 3e3e 2069 6d70 6f72 7420 6a61 780a 2020  >> import jax.  
+00015570: 2020 2020 3e3e 3e20 696d 706f 7274 206a      >>> import j
+00015580: 6178 2e6e 756d 7079 2061 7320 6a6e 700a  ax.numpy as jnp.
+00015590: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
+000155a0: 2066 6c61 782e 6c69 6e65 6e20 6173 206e   flax.linen as n
+000155b0: 6e0a 0a20 2020 2020 203e 3e3e 2063 6c61  n..      >>> cla
+000155c0: 7373 2046 6f6f 286e 6e2e 4d6f 6475 6c65  ss Foo(nn.Module
+000155d0: 293a 0a20 2020 2020 202e 2e2e 2020 2040  ):.      ...   @
+000155e0: 6e6e 2e63 6f6d 7061 6374 0a20 2020 2020  nn.compact.     
+000155f0: 202e 2e2e 2020 2064 6566 205f 5f63 616c   ...   def __cal
+00015600: 6c5f 5f28 7365 6c66 2c20 7829 3a0a 2020  l__(self, x):.  
+00015610: 2020 2020 2e2e 2e20 2020 2020 6820 3d20      ...     h = 
+00015620: 6e6e 2e44 656e 7365 2834 2928 7829 0a20  nn.Dense(4)(x). 
+00015630: 2020 2020 202e 2e2e 2020 2020 2073 656c       ...     sel
+00015640: 662e 736f 7728 2769 6e74 6572 6d65 6469  f.sow('intermedi
+00015650: 6174 6573 272c 2027 6827 2c20 6829 0a20  ates', 'h', h). 
+00015660: 2020 2020 202e 2e2e 2020 2020 2072 6574       ...     ret
+00015670: 7572 6e20 6e6e 2e44 656e 7365 2832 2928  urn nn.Dense(2)(
+00015680: 6829 0a0a 2020 2020 2020 3e3e 3e20 7820  h)..      >>> x 
+00015690: 3d20 6a6e 702e 6f6e 6573 2828 3136 2c20  = jnp.ones((16, 
+000156a0: 3929 290a 2020 2020 2020 3e3e 3e20 6d6f  9)).      >>> mo
+000156b0: 6465 6c20 3d20 466f 6f28 290a 2020 2020  del = Foo().    
+000156c0: 2020 3e3e 3e20 7661 7269 6162 6c65 7320    >>> variables 
+000156d0: 3d20 6d6f 6465 6c2e 696e 6974 286a 6178  = model.init(jax
+000156e0: 2e72 616e 646f 6d2e 6b65 7928 3029 2c20  .random.key(0), 
+000156f0: 7829 0a20 2020 2020 203e 3e3e 2079 2c20  x).      >>> y, 
+00015700: 7374 6174 6520 3d20 6d6f 6465 6c2e 6170  state = model.ap
+00015710: 706c 7928 7661 7269 6162 6c65 732c 2078  ply(variables, x
+00015720: 2c20 6d75 7461 626c 653d 5b27 696e 7465  , mutable=['inte
+00015730: 726d 6564 6961 7465 7327 5d29 0a20 2020  rmediates']).   
+00015740: 2020 203e 3e3e 206a 6178 2e74 7265 652e     >>> jax.tree.
+00015750: 6d61 7028 6a6e 702e 7368 6170 652c 2073  map(jnp.shape, s
+00015760: 7461 7465 5b27 696e 7465 726d 6564 6961  tate['intermedia
+00015770: 7465 7327 5d29 0a20 2020 2020 207b 2768  tes']).      {'h
+00015780: 273a 2028 2831 362c 2034 292c 297d 0a0a  ': ((16, 4),)}..
+00015790: 2020 2020 4279 2064 6566 6175 6c74 2074      By default t
+000157a0: 6865 2076 616c 7565 7320 6172 6520 7374  he values are st
+000157b0: 6f72 6564 2069 6e20 6120 7475 706c 6520  ored in a tuple 
+000157c0: 616e 6420 6561 6368 2073 746f 7265 6420  and each stored 
+000157d0: 7661 6c75 650a 2020 2020 6973 2061 7070  value.    is app
+000157e0: 656e 6465 6420 6174 2074 6865 2065 6e64  ended at the end
+000157f0: 2e20 5468 6973 2077 6179 2061 6c6c 2069  . This way all i
+00015800: 6e74 6572 6d65 6469 6174 6573 2063 616e  ntermediates can
+00015810: 2062 6520 7472 6163 6b65 6420 7768 656e   be tracked when
+00015820: 0a20 2020 2074 6865 2073 616d 6520 6d6f  .    the same mo
+00015830: 6475 6c65 2069 7320 6361 6c6c 6564 206d  dule is called m
+00015840: 756c 7469 706c 6520 7469 6d65 732e 2041  ultiple times. A
+00015850: 6c74 6572 6e61 7469 7665 6c79 2c20 6120  lternatively, a 
+00015860: 6375 7374 6f6d 0a20 2020 2069 6e69 742f  custom.    init/
+00015870: 7265 6475 6365 2066 756e 6374 696f 6e20  reduce function 
+00015880: 6361 6e20 6265 2070 6173 7365 643a 3a0a  can be passed::.
+00015890: 0a20 2020 2020 203e 3e3e 2063 6c61 7373  .      >>> class
+000158a0: 2046 6f6f 3228 6e6e 2e4d 6f64 756c 6529   Foo2(nn.Module)
+000158b0: 3a0a 2020 2020 2020 2e2e 2e20 2020 406e  :.      ...   @n
+000158c0: 6e2e 636f 6d70 6163 740a 2020 2020 2020  n.compact.      
+000158d0: 2e2e 2e20 2020 6465 6620 5f5f 6361 6c6c  ...   def __call
+000158e0: 5f5f 2873 656c 662c 2078 293a 0a20 2020  __(self, x):.   
+000158f0: 2020 202e 2e2e 2020 2020 2069 6e69 745f     ...     init_
+00015900: 666e 203d 206c 616d 6264 613a 2030 0a20  fn = lambda: 0. 
+00015910: 2020 2020 202e 2e2e 2020 2020 2072 6564       ...     red
+00015920: 7563 655f 666e 203d 206c 616d 6264 6120  uce_fn = lambda 
+00015930: 612c 2062 3a20 6120 2b20 620a 2020 2020  a, b: a + b.    
+00015940: 2020 2e2e 2e20 2020 2020 7365 6c66 2e73    ...     self.s
+00015950: 6f77 2827 696e 7465 726d 6564 6961 7465  ow('intermediate
+00015960: 7327 2c20 2768 272c 2078 2c0a 2020 2020  s', 'h', x,.    
+00015970: 2020 2e2e 2e20 2020 2020 2020 2020 2020    ...           
+00015980: 2020 2020 696e 6974 5f66 6e3d 696e 6974      init_fn=init
+00015990: 5f66 6e2c 2072 6564 7563 655f 666e 3d72  _fn, reduce_fn=r
+000159a0: 6564 7563 655f 666e 290a 2020 2020 2020  educe_fn).      
+000159b0: 2e2e 2e20 2020 2020 7365 6c66 2e73 6f77  ...     self.sow
+000159c0: 2827 696e 7465 726d 6564 6961 7465 7327  ('intermediates'
+000159d0: 2c20 2768 272c 2078 202a 2032 2c0a 2020  , 'h', x * 2,.  
+000159e0: 2020 2020 2e2e 2e20 2020 2020 2020 2020      ...         
+000159f0: 2020 2020 2020 696e 6974 5f66 6e3d 696e        init_fn=in
+00015a00: 6974 5f66 6e2c 2072 6564 7563 655f 666e  it_fn, reduce_fn
+00015a10: 3d72 6564 7563 655f 666e 290a 2020 2020  =reduce_fn).    
+00015a20: 2020 2e2e 2e20 2020 2020 7265 7475 726e    ...     return
+00015a30: 2078 0a0a 2020 2020 2020 3e3e 3e20 7820   x..      >>> x 
+00015a40: 3d20 6a6e 702e 6f6e 6573 2828 312c 2031  = jnp.ones((1, 1
+00015a50: 2929 0a20 2020 2020 203e 3e3e 206d 6f64  )).      >>> mod
+00015a60: 656c 203d 2046 6f6f 3228 290a 2020 2020  el = Foo2().    
+00015a70: 2020 3e3e 3e20 7661 7269 6162 6c65 7320    >>> variables 
+00015a80: 3d20 6d6f 6465 6c2e 696e 6974 286a 6178  = model.init(jax
+00015a90: 2e72 616e 646f 6d2e 6b65 7928 3029 2c20  .random.key(0), 
+00015aa0: 7829 0a20 2020 2020 203e 3e3e 2079 2c20  x).      >>> y, 
+00015ab0: 7374 6174 6520 3d20 6d6f 6465 6c2e 6170  state = model.ap
+00015ac0: 706c 7928 0a20 2020 2020 202e 2e2e 2020  ply(.      ...  
+00015ad0: 2020 2076 6172 6961 626c 6573 2c20 782c     variables, x,
+00015ae0: 206d 7574 6162 6c65 3d5b 2769 6e74 6572   mutable=['inter
+00015af0: 6d65 6469 6174 6573 275d 290a 2020 2020  mediates']).    
+00015b00: 2020 3e3e 3e20 7072 696e 7428 7374 6174    >>> print(stat
+00015b10: 655b 2769 6e74 6572 6d65 6469 6174 6573  e['intermediates
+00015b20: 275d 290a 2020 2020 2020 7b27 6827 3a20  ']).      {'h': 
+00015b30: 4172 7261 7928 5b5b 332e 5d5d 2c20 6474  Array([[3.]], dt
+00015b40: 7970 653d 666c 6f61 7433 3229 7d0a 0a20  ype=float32)}.. 
+00015b50: 2020 2041 7267 733a 0a20 2020 2020 2063     Args:.      c
+00015b60: 6f6c 3a20 5468 6520 6e61 6d65 206f 6620  ol: The name of 
+00015b70: 7468 6520 7661 7269 6162 6c65 2063 6f6c  the variable col
+00015b80: 6c65 6374 696f 6e2e 0a20 2020 2020 206e  lection..      n
+00015b90: 616d 653a 2054 6865 206e 616d 6520 6f66  ame: The name of
+00015ba0: 2074 6865 2076 6172 6961 626c 652e 0a20   the variable.. 
+00015bb0: 2020 2020 2076 616c 7565 3a20 5468 6520       value: The 
+00015bc0: 7661 6c75 6520 6f66 2074 6865 2076 6172  value of the var
+00015bd0: 6961 626c 652e 0a20 2020 2020 2072 6564  iable..      red
+00015be0: 7563 655f 666e 3a20 5468 6520 6675 6e63  uce_fn: The func
+00015bf0: 7469 6f6e 2075 7365 6420 746f 2063 6f6d  tion used to com
+00015c00: 6269 6e65 2074 6865 2065 7869 7374 696e  bine the existin
+00015c10: 6720 7661 6c75 6520 7769 7468 2074 6865  g value with the
+00015c20: 206e 6577 0a20 2020 2020 2020 2076 616c   new.        val
+00015c30: 7565 2e20 5468 6520 6465 6661 756c 7420  ue. The default 
+00015c40: 6973 2074 6f20 6170 7065 6e64 2074 6865  is to append the
+00015c50: 2076 616c 7565 2074 6f20 6120 7475 706c   value to a tupl
+00015c60: 652e 0a20 2020 2020 2069 6e69 745f 666e  e..      init_fn
+00015c70: 3a20 466f 7220 7468 6520 6669 7273 7420  : For the first 
+00015c80: 7661 6c75 6520 7374 6f72 6564 2c20 6060  value stored, ``
+00015c90: 7265 6475 6365 5f66 6e60 6020 7769 6c6c  reduce_fn`` will
+00015ca0: 2062 6520 7061 7373 6564 2074 6865 2072   be passed the r
+00015cb0: 6573 756c 740a 2020 2020 2020 2020 6f66  esult.        of
+00015cc0: 2060 6069 6e69 745f 666e 6060 2074 6f67   ``init_fn`` tog
+00015cd0: 6574 6865 7220 7769 7468 2074 6865 2076  ether with the v
+00015ce0: 616c 7565 2074 6f20 6265 2073 746f 7265  alue to be store
+00015cf0: 642e 2054 6865 2064 6566 6175 6c74 2069  d. The default i
+00015d00: 7320 616e 0a20 2020 2020 2020 2065 6d70  s an.        emp
+00015d10: 7479 2074 7570 6c65 2e0a 0a20 2020 2052  ty tuple...    R
+00015d20: 6574 7572 6e73 3a0a 2020 2020 2020 6060  eturns:.      ``
+00015d30: 5472 7565 6060 2069 6620 7468 6520 7661  True`` if the va
+00015d40: 6c75 6520 6861 7320 6265 656e 2073 746f  lue has been sto
+00015d50: 7265 6420 7375 6363 6573 7366 756c 6c79  red successfully
+00015d60: 2c20 6060 4661 6c73 6560 6020 6f74 6865  , ``False`` othe
+00015d70: 7277 6973 652e 0a20 2020 2022 2222 0a20  rwise..    """. 
+00015d80: 2020 2069 6620 7365 6c66 2e73 636f 7065     if self.scope
+00015d90: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
+00015da0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+00015db0: 2822 4361 6e27 7420 7374 6f72 6520 7661  ("Can't store va
+00015dc0: 7269 6162 6c65 7320 6f6e 2075 6e62 6f75  riables on unbou
+00015dd0: 6e64 206d 6f64 756c 6573 2229 0a20 2020  nd modules").   
+00015de0: 2069 6620 6e6f 7420 7365 6c66 2e73 636f   if not self.sco
+00015df0: 7065 2e69 735f 6d75 7461 626c 655f 636f  pe.is_mutable_co
+00015e00: 6c6c 6563 7469 6f6e 2863 6f6c 293a 0a20  llection(col):. 
+00015e10: 2020 2020 2072 6574 7572 6e20 4661 6c73       return Fals
+00015e20: 650a 2020 2020 6966 2073 656c 662e 7363  e.    if self.sc
+00015e30: 6f70 652e 6861 735f 7661 7269 6162 6c65  ope.has_variable
+00015e40: 2863 6f6c 2c20 6e61 6d65 293a 0a20 2020  (col, name):.   
+00015e50: 2020 2078 7320 3d20 7365 6c66 2e73 636f     xs = self.sco
+00015e60: 7065 2e67 6574 5f76 6172 6961 626c 6528  pe.get_variable(
+00015e70: 636f 6c2c 206e 616d 6529 0a20 2020 2065  col, name).    e
+00015e80: 6c73 653a 0a20 2020 2020 2073 656c 662e  lse:.      self.
+00015e90: 7363 6f70 652e 7265 7365 7276 6528 6e61  scope.reserve(na
+00015ea0: 6d65 2c20 636f 6c29 0a20 2020 2020 2073  me, col).      s
+00015eb0: 656c 662e 5f73 7461 7465 2e63 6869 6c64  elf._state.child
+00015ec0: 7265 6e5b 6e61 6d65 5d20 3d20 636f 6c0a  ren[name] = col.
+00015ed0: 2020 2020 2020 7873 203d 2069 6e69 745f        xs = init_
+00015ee0: 666e 2829 0a20 2020 2078 7320 3d20 7265  fn().    xs = re
+00015ef0: 6475 6365 5f66 6e28 7873 2c20 7661 6c75  duce_fn(xs, valu
+00015f00: 6529 0a20 2020 2073 656c 662e 7363 6f70  e).    self.scop
+00015f10: 652e 7075 745f 7661 7269 6162 6c65 2863  e.put_variable(c
+00015f20: 6f6c 2c20 6e61 6d65 2c20 7873 290a 2020  ol, name, xs).  
+00015f30: 2020 7265 7475 726e 2054 7275 650a 0a20    return True.. 
+00015f40: 2064 6566 2070 6572 7475 7262 280a 2020   def perturb(.  
+00015f50: 2020 7365 6c66 2c20 6e61 6d65 3a20 7374    self, name: st
+00015f60: 722c 2076 616c 7565 3a20 542c 2063 6f6c  r, value: T, col
+00015f70: 6c65 6374 696f 6e3a 2073 7472 203d 2027  lection: str = '
+00015f80: 7065 7274 7572 6261 7469 6f6e 7327 0a20  perturbations'. 
+00015f90: 2029 202d 3e20 543a 0a20 2020 2022 2222   ) -> T:.    """
+00015fa0: 4164 6420 616e 207a 6572 6f2d 7661 6c75  Add an zero-valu
+00015fb0: 6520 7661 7269 6162 6c65 2028 2770 6572  e variable ('per
+00015fc0: 7475 7262 6174 696f 6e27 2920 746f 2074  turbation') to t
+00015fd0: 6865 2069 6e74 6572 6d65 6469 6174 6520  he intermediate 
+00015fe0: 7661 6c75 652e 0a0a 2020 2020 5468 6520  value...    The 
+00015ff0: 6772 6164 6965 6e74 206f 6620 6060 7661  gradient of ``va
+00016000: 6c75 6560 6020 776f 756c 6420 6265 2074  lue`` would be t
+00016010: 6865 2073 616d 6520 6173 2074 6865 2067  he same as the g
+00016020: 7261 6469 656e 7420 6f66 2074 6869 730a  radient of this.
+00016030: 2020 2020 7065 7274 7572 6261 7469 6f6e      perturbation
+00016040: 2076 6172 6961 626c 652e 2054 6865 7265   variable. There
+00016050: 666f 7265 2c20 6966 2079 6f75 2064 6566  fore, if you def
+00016060: 696e 6520 796f 7572 206c 6f73 7320 6675  ine your loss fu
+00016070: 6e63 7469 6f6e 2077 6974 680a 2020 2020  nction with.    
+00016080: 626f 7468 2070 6172 616d 7320 616e 6420  both params and 
+00016090: 7065 7274 7572 6261 7469 6f6e 7320 6173  perturbations as
+000160a0: 2073 7461 6e64 616c 6f6e 6520 6172 6775   standalone argu
+000160b0: 6d65 6e74 732c 2079 6f75 2063 616e 2067  ments, you can g
+000160c0: 6574 2074 6865 0a20 2020 2069 6e74 6572  et the.    inter
+000160d0: 6d65 6469 6174 6520 6772 6164 6965 6e74  mediate gradient
+000160e0: 7320 6f66 2060 6076 616c 7565 6060 2062  s of ``value`` b
+000160f0: 7920 7275 6e6e 696e 6720 6060 6a61 782e  y running ``jax.
+00016100: 6772 6164 6060 206f 6e20 7468 6520 7065  grad`` on the pe
+00016110: 7274 7572 6261 7469 6f6e 0a20 2020 2061  rturbation.    a
+00016120: 7267 756d 656e 742e 0a0a 2020 2020 2e2e  rgument...    ..
+00016130: 206e 6f74 653a 3a0a 2020 2020 2020 5468   note::.      Th
+00016140: 6973 2069 7320 616e 2065 7870 6572 696d  is is an experim
+00016150: 656e 7461 6c20 4150 4920 616e 6420 6d61  ental API and ma
+00016160: 7920 6265 2074 7765 616b 6564 206c 6174  y be tweaked lat
+00016170: 6572 2066 6f72 2062 6574 7465 720a 2020  er for better.  
+00016180: 2020 2020 7065 7266 6f72 6d61 6e63 6520      performance 
+00016190: 616e 6420 7573 6162 696c 6974 792e 0a20  and usability.. 
+000161a0: 2020 2020 2041 7420 6974 7320 6375 7272       At its curr
+000161b0: 656e 7420 7374 6167 652c 2069 7420 6372  ent stage, it cr
+000161c0: 6561 7465 7320 6578 7472 6120 6475 6d6d  eates extra dumm
+000161d0: 7920 7661 7269 6162 6c65 7320 7468 6174  y variables that
+000161e0: 206f 6363 7570 6965 7320 6578 7472 610a   occupies extra.
+000161f0: 2020 2020 2020 6d65 6d6f 7279 2073 7061        memory spa
+00016200: 6365 2e20 5573 6520 6974 206f 6e6c 7920  ce. Use it only 
+00016210: 746f 2064 6562 7567 2067 7261 6469 656e  to debug gradien
+00016220: 7473 2069 6e20 7472 6169 6e69 6e67 2e0a  ts in training..
+00016230: 0a20 2020 2045 7861 6d70 6c65 3a3a 0a0a  .    Example::..
+00016240: 2020 2020 2020 3e3e 3e20 636c 6173 7320        >>> class 
+00016250: 466f 6f28 6e6e 2e4d 6f64 756c 6529 3a0a  Foo(nn.Module):.
+00016260: 2020 2020 2020 2e2e 2e20 2020 406e 6e2e        ...   @nn.
+00016270: 636f 6d70 6163 740a 2020 2020 2020 2e2e  compact.      ..
+00016280: 2e20 2020 6465 6620 5f5f 6361 6c6c 5f5f  .   def __call__
+00016290: 2873 656c 662c 2078 293a 0a20 2020 2020  (self, x):.     
+000162a0: 202e 2e2e 2020 2020 2078 203d 206e 6e2e   ...     x = nn.
+000162b0: 4465 6e73 6528 3329 2878 290a 2020 2020  Dense(3)(x).    
+000162c0: 2020 2e2e 2e20 2020 2020 7820 3d20 7365    ...     x = se
+000162d0: 6c66 2e70 6572 7475 7262 2827 6465 6e73  lf.perturb('dens
+000162e0: 6533 272c 2078 290a 2020 2020 2020 2e2e  e3', x).      ..
+000162f0: 2e20 2020 2020 7265 7475 726e 206e 6e2e  .     return nn.
+00016300: 4465 6e73 6528 3229 2878 290a 0a20 2020  Dense(2)(x)..   
+00016310: 2020 203e 3e3e 2064 6566 206c 6f73 7328     >>> def loss(
+00016320: 7661 7269 6162 6c65 732c 2069 6e70 7574  variables, input
+00016330: 732c 2074 6172 6765 7473 293a 0a20 2020  s, targets):.   
+00016340: 2020 202e 2e2e 2020 2070 7265 6473 203d     ...   preds =
+00016350: 206d 6f64 656c 2e61 7070 6c79 2876 6172   model.apply(var
+00016360: 6961 626c 6573 2c20 696e 7075 7473 290a  iables, inputs).
+00016370: 2020 2020 2020 2e2e 2e20 2020 7265 7475        ...   retu
+00016380: 726e 206a 6e70 2e73 7175 6172 6528 7072  rn jnp.square(pr
+00016390: 6564 7320 2d20 7461 7267 6574 7329 2e6d  eds - targets).m
+000163a0: 6561 6e28 290a 0a20 2020 2020 203e 3e3e  ean()..      >>>
+000163b0: 2078 203d 206a 6e70 2e6f 6e65 7328 2832   x = jnp.ones((2
+000163c0: 2c20 3929 290a 2020 2020 2020 3e3e 3e20  , 9)).      >>> 
+000163d0: 7920 3d20 6a6e 702e 6f6e 6573 2828 322c  y = jnp.ones((2,
+000163e0: 2032 2929 0a20 2020 2020 203e 3e3e 206d   2)).      >>> m
+000163f0: 6f64 656c 203d 2046 6f6f 2829 0a20 2020  odel = Foo().   
+00016400: 2020 203e 3e3e 2076 6172 6961 626c 6573     >>> variables
+00016410: 203d 206d 6f64 656c 2e69 6e69 7428 6a61   = model.init(ja
+00016420: 782e 7261 6e64 6f6d 2e6b 6579 2830 292c  x.random.key(0),
+00016430: 2078 290a 2020 2020 2020 3e3e 3e20 696e   x).      >>> in
+00016440: 746d 5f67 7261 6473 203d 206a 6178 2e67  tm_grads = jax.g
+00016450: 7261 6428 6c6f 7373 2c20 6172 676e 756d  rad(loss, argnum
+00016460: 733d 3029 2876 6172 6961 626c 6573 2c20  s=0)(variables, 
+00016470: 782c 2079 290a 2020 2020 2020 3e3e 3e20  x, y).      >>> 
+00016480: 7072 696e 7428 696e 746d 5f67 7261 6473  print(intm_grads
+00016490: 5b27 7065 7274 7572 6261 7469 6f6e 7327  ['perturbations'
+000164a0: 5d5b 2764 656e 7365 3327 5d29 0a20 2020  ]['dense3']).   
+000164b0: 2020 205b 5b2d 312e 3435 3639 3234 2020     [[-1.456924  
+000164c0: 202d 302e 3434 3333 3235 3337 2020 302e   -0.44332537  0.
+000164d0: 3032 3432 3238 3437 5d0a 2020 2020 2020  02422847].      
+000164e0: 205b 2d31 2e34 3536 3932 3420 2020 2d30   [-1.456924   -0
+000164f0: 2e34 3433 3332 3533 3720 2030 2e30 3234  .44332537  0.024
+00016500: 3232 3834 375d 5d0a 0a20 2020 2049 6620  22847]]..    If 
+00016510: 7065 7274 7572 6261 7469 6f6e 7320 6172  perturbations ar
+00016520: 6520 6e6f 7420 7061 7373 6564 2074 6f20  e not passed to 
+00016530: 6060 6170 706c 7960 602c 2060 6070 6572  ``apply``, ``per
+00016540: 7475 7262 6060 2062 6568 6176 6573 206c  turb`` behaves l
+00016550: 696b 6520 6120 6e6f 2d6f 700a 2020 2020  ike a no-op.    
+00016560: 736f 2079 6f75 2063 616e 2065 6173 696c  so you can easil
+00016570: 7920 6469 7361 626c 6520 7468 6520 6265  y disable the be
+00016580: 6861 7669 6f72 2077 6865 6e20 6e6f 7420  havior when not 
+00016590: 6e65 6564 6564 3a3a 0a0a 2020 2020 2020  needed::..      
+000165a0: 3e3e 3e20 6d6f 6465 6c2e 6170 706c 7928  >>> model.apply(
+000165b0: 7661 7269 6162 6c65 732c 2078 2920 2320  variables, x) # 
+000165c0: 776f 726b 7320 6173 2065 7870 6563 7465  works as expecte
+000165d0: 640a 2020 2020 2020 4172 7261 7928 5b5b  d.      Array([[
+000165e0: 2d31 2e30 3938 3031 3238 202c 202d 302e  -1.0980128 , -0.
+000165f0: 3637 3936 3137 3335 5d2c 0a20 2020 2020  67961735],.     
+00016600: 2020 2020 2020 2020 5b2d 312e 3039 3830          [-1.0980
+00016610: 3132 3820 2c20 2d30 2e36 3739 3631 3733  128 , -0.6796173
+00016620: 355d 5d2c 2064 7479 7065 3d66 6c6f 6174  5]], dtype=float
+00016630: 3332 290a 2020 2020 2020 3e3e 3e20 6d6f  32).      >>> mo
+00016640: 6465 6c2e 6170 706c 7928 7b27 7061 7261  del.apply({'para
+00016650: 6d73 273a 2076 6172 6961 626c 6573 5b27  ms': variables['
+00016660: 7061 7261 6d73 275d 7d2c 2078 2920 2320  params']}, x) # 
+00016670: 6265 6861 7665 7320 6c69 6b65 2061 206e  behaves like a n
+00016680: 6f2d 6f70 0a20 2020 2020 2041 7272 6179  o-op.      Array
+00016690: 285b 5b2d 312e 3039 3830 3132 3820 2c20  ([[-1.0980128 , 
+000166a0: 2d30 2e36 3739 3631 3733 355d 2c0a 2020  -0.67961735],.  
+000166b0: 2020 2020 2020 2020 2020 205b 2d31 2e30             [-1.0
+000166c0: 3938 3031 3238 202c 202d 302e 3637 3936  980128 , -0.6796
+000166d0: 3137 3335 5d5d 2c20 6474 7970 653d 666c  1735]], dtype=fl
+000166e0: 6f61 7433 3229 0a20 2020 2020 203e 3e3e  oat32).      >>>
+000166f0: 2069 6e74 6d5f 6772 6164 7320 3d20 6a61   intm_grads = ja
+00016700: 782e 6772 6164 286c 6f73 732c 2061 7267  x.grad(loss, arg
+00016710: 6e75 6d73 3d30 2928 7b27 7061 7261 6d73  nums=0)({'params
+00016720: 273a 2076 6172 6961 626c 6573 5b27 7061  ': variables['pa
+00016730: 7261 6d73 275d 7d2c 2078 2c20 7929 0a20  rams']}, x, y). 
+00016740: 2020 2020 203e 3e3e 2027 7065 7274 7572       >>> 'pertur
+00016750: 6261 7469 6f6e 7327 206e 6f74 2069 6e20  bations' not in 
+00016760: 696e 746d 5f67 7261 6473 0a20 2020 2020  intm_grads.     
+00016770: 2054 7275 650a 2020 2020 2222 220a 2020   True.    """.  
+00016780: 2020 6966 2073 656c 662e 7363 6f70 6520    if self.scope 
+00016790: 6973 204e 6f6e 653a 0a20 2020 2020 2072  is None:.      r
+000167a0: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
+000167b0: 2243 616e 2774 2073 746f 7265 2076 6172  "Can't store var
+000167c0: 6961 626c 6573 206f 6e20 756e 626f 756e  iables on unboun
+000167d0: 6420 6d6f 6475 6c65 7322 290a 0a20 2020  d modules")..   
+000167e0: 2069 6620 7365 6c66 2e69 735f 6d75 7461   if self.is_muta
+000167f0: 626c 655f 636f 6c6c 6563 7469 6f6e 2863  ble_collection(c
+00016800: 6f6c 6c65 6374 696f 6e29 3a0a 2020 2020  ollection):.    
+00016810: 2020 6966 206e 6f74 2073 656c 662e 7363    if not self.sc
+00016820: 6f70 652e 6861 735f 7661 7269 6162 6c65  ope.has_variable
+00016830: 2863 6f6c 6c65 6374 696f 6e2c 206e 616d  (collection, nam
+00016840: 6529 3a0a 2020 2020 2020 2020 7365 6c66  e):.        self
+00016850: 2e73 636f 7065 2e72 6573 6572 7665 286e  .scope.reserve(n
+00016860: 616d 652c 2063 6f6c 6c65 6374 696f 6e29  ame, collection)
+00016870: 0a20 2020 2020 2020 2073 656c 662e 5f73  .        self._s
+00016880: 7461 7465 2e63 6869 6c64 7265 6e5b 6e61  tate.children[na
+00016890: 6d65 5d20 3d20 636f 6c6c 6563 7469 6f6e  me] = collection
+000168a0: 0a20 2020 2020 2020 2073 656c 662e 7363  .        self.sc
+000168b0: 6f70 652e 7075 745f 7661 7269 6162 6c65  ope.put_variable
+000168c0: 2863 6f6c 6c65 6374 696f 6e2c 206e 616d  (collection, nam
+000168d0: 652c 206a 6e70 2e7a 6572 6f73 5f6c 696b  e, jnp.zeros_lik
+000168e0: 6528 7661 6c75 6529 2920 2023 2074 7970  e(value))  # typ
+000168f0: 653a 2069 676e 6f72 650a 0a20 2020 2069  e: ignore..    i
+00016900: 6620 636f 6c6c 6563 7469 6f6e 2069 6e20  f collection in 
+00016910: 7365 6c66 2e73 636f 7065 2e72 6f6f 742e  self.scope.root.
+00016920: 5f76 6172 6961 626c 6573 3a0a 2020 2020  _variables:.    
+00016930: 2020 6966 2073 656c 662e 7363 6f70 652e    if self.scope.
+00016940: 6861 735f 7661 7269 6162 6c65 2863 6f6c  has_variable(col
+00016950: 6c65 6374 696f 6e2c 206e 616d 6529 3a0a  lection, name):.
+00016960: 2020 2020 2020 2020 7661 6c75 6520 2b3d          value +=
+00016970: 2073 656c 662e 7363 6f70 652e 6765 745f   self.scope.get_
+00016980: 7661 7269 6162 6c65 2863 6f6c 6c65 6374  variable(collect
+00016990: 696f 6e2c 206e 616d 6529 2020 2320 7479  ion, name)  # ty
+000169a0: 7065 3a20 6967 6e6f 7265 0a20 2020 2020  pe: ignore.     
+000169b0: 2065 6c73 653a 0a20 2020 2020 2020 2072   else:.        r
+000169c0: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
+000169d0: 6622 5065 7274 7572 6261 7469 6f6e 2063  f"Perturbation c
+000169e0: 6f6c 6c65 6374 696f 6e20 7b63 6f6c 6c65  ollection {colle
+000169f0: 6374 696f 6e7d 2070 7265 7365 6e74 2c20  ction} present, 
+00016a00: 6275 7420 220a 2020 2020 2020 2020 2020  but ".          
+00016a10: 2020 2020 2020 2020 2020 2020 2020 2066                 f
+00016a20: 226d 6973 7369 6e67 2070 6572 7475 7262  "missing perturb
+00016a30: 6174 696f 6e20 7661 7269 6162 6c65 207b  ation variable {
+00016a40: 6e61 6d65 7d22 290a 0a20 2020 2072 6574  name}")..    ret
+00016a50: 7572 6e20 7661 6c75 650a 0a20 2064 6566  urn value..  def
+00016a60: 2074 6162 756c 6174 6528 0a20 2020 2073   tabulate(.    s
+00016a70: 656c 662c 0a20 2020 2072 6e67 733a 2055  elf,.    rngs: U
+00016a80: 6e69 6f6e 5b50 524e 474b 6579 2c20 524e  nion[PRNGKey, RN
+00016a90: 4753 6571 7565 6e63 6573 5d2c 0a20 2020  GSequences],.   
+00016aa0: 202a 6172 6773 2c0a 2020 2020 6465 7074   *args,.    dept
+00016ab0: 683a 204f 7074 696f 6e61 6c5b 696e 745d  h: Optional[int]
+00016ac0: 203d 204e 6f6e 652c 0a20 2020 2073 686f   = None,.    sho
+00016ad0: 775f 7265 7065 6174 6564 3a20 626f 6f6c  w_repeated: bool
+00016ae0: 203d 2046 616c 7365 2c0a 2020 2020 6d75   = False,.    mu
+00016af0: 7461 626c 653a 2043 6f6c 6c65 6374 696f  table: Collectio
+00016b00: 6e46 696c 7465 7220 3d20 4465 6e79 4c69  nFilter = DenyLi
+00016b10: 7374 2827 696e 7465 726d 6564 6961 7465  st('intermediate
+00016b20: 7327 292c 0a20 2020 2063 6f6e 736f 6c65  s'),.    console
+00016b30: 5f6b 7761 7267 733a 204f 7074 696f 6e61  _kwargs: Optiona
+00016b40: 6c5b 4d61 7070 696e 675b 7374 722c 2041  l[Mapping[str, A
+00016b50: 6e79 5d5d 203d 204e 6f6e 652c 0a20 2020  ny]] = None,.   
+00016b60: 2074 6162 6c65 5f6b 7761 7267 733a 204d   table_kwargs: M
+00016b70: 6170 7069 6e67 5b73 7472 2c20 416e 795d  apping[str, Any]
+00016b80: 203d 204d 6170 7069 6e67 5072 6f78 7954   = MappingProxyT
+00016b90: 7970 6528 7b7d 292c 0a20 2020 2063 6f6c  ype({}),.    col
+00016ba0: 756d 6e5f 6b77 6172 6773 3a20 4d61 7070  umn_kwargs: Mapp
+00016bb0: 696e 675b 7374 722c 2041 6e79 5d20 3d20  ing[str, Any] = 
+00016bc0: 4d61 7070 696e 6750 726f 7879 5479 7065  MappingProxyType
+00016bd0: 287b 7d29 2c0a 2020 2020 636f 6d70 7574  ({}),.    comput
+00016be0: 655f 666c 6f70 733a 2062 6f6f 6c20 3d20  e_flops: bool = 
+00016bf0: 4661 6c73 652c 0a20 2020 2063 6f6d 7075  False,.    compu
+00016c00: 7465 5f76 6a70 5f66 6c6f 7073 3a20 626f  te_vjp_flops: bo
+00016c10: 6f6c 203d 2046 616c 7365 2c0a 2020 2020  ol = False,.    
+00016c20: 2a2a 6b77 6172 6773 2c0a 2020 2920 2d3e  **kwargs,.  ) ->
+00016c30: 2073 7472 3a0a 2020 2020 2222 2243 7265   str:.    """Cre
+00016c40: 6174 6573 2061 2073 756d 6d61 7279 206f  ates a summary o
+00016c50: 6620 7468 6520 4d6f 6475 6c65 2072 6570  f the Module rep
+00016c60: 7265 7365 6e74 6564 2061 7320 6120 7461  resented as a ta
+00016c70: 626c 652e 0a0a 2020 2020 5468 6973 206d  ble...    This m
+00016c80: 6574 686f 6420 6861 7320 7468 6520 7361  ethod has the sa
+00016c90: 6d65 2073 6967 6e61 7475 7265 2061 6e64  me signature and
+00016ca0: 2069 6e74 6572 6e61 6c6c 7920 6361 6c6c   internally call
+00016cb0: 7320 6060 4d6f 6475 6c65 2e69 6e69 7460  s ``Module.init`
+00016cc0: 602c 0a20 2020 2062 7574 2069 6e73 7465  `,.    but inste
+00016cd0: 6164 206f 6620 7265 7475 726e 696e 6720  ad of returning 
+00016ce0: 7468 6520 7661 7269 6162 6c65 732c 2069  the variables, i
+00016cf0: 7420 7265 7475 726e 7320 7468 6520 7374  t returns the st
+00016d00: 7269 6e67 2073 756d 6d61 7269 7a69 6e67  ring summarizing
+00016d10: 0a20 2020 2074 6865 204d 6f64 756c 6520  .    the Module 
+00016d20: 696e 2061 2074 6162 6c65 2e20 6060 7461  in a table. ``ta
+00016d30: 6275 6c61 7465 6060 2075 7365 7320 6060  bulate`` uses ``
+00016d40: 6a61 782e 6576 616c 5f73 6861 7065 6060  jax.eval_shape``
+00016d50: 2074 6f20 7275 6e20 7468 6520 666f 7277   to run the forw
+00016d60: 6172 640a 2020 2020 636f 6d70 7574 6174  ard.    computat
+00016d70: 696f 6e20 7769 7468 6f75 7420 636f 6e73  ion without cons
+00016d80: 756d 696e 6720 616e 7920 464c 4f50 7320  uming any FLOPs 
+00016d90: 6f72 2061 6c6c 6f63 6174 696e 6720 6d65  or allocating me
+00016da0: 6d6f 7279 2e0a 0a20 2020 2041 6464 6974  mory...    Addit
+00016db0: 696f 6e61 6c20 6172 6775 6d65 6e74 7320  ional arguments 
+00016dc0: 6361 6e20 6265 2070 6173 7365 6420 696e  can be passed in
+00016dd0: 746f 2074 6865 2060 6063 6f6e 736f 6c65  to the ``console
+00016de0: 5f6b 7761 7267 7360 6020 6172 6775 6d65  _kwargs`` argume
+00016df0: 6e74 2c20 666f 720a 2020 2020 6578 616d  nt, for.    exam
+00016e00: 706c 652c 2060 607b 2777 6964 7468 273a  ple, ``{'width':
+00016e10: 2031 3230 7d60 602e 2046 6f72 2061 2066   120}``. For a f
+00016e20: 756c 6c20 6c69 7374 206f 6620 6060 636f  ull list of ``co
+00016e30: 6e73 6f6c 655f 6b77 6172 6773 6060 2061  nsole_kwargs`` a
+00016e40: 7267 756d 656e 7473 2c0a 2020 2020 7365  rguments,.    se
+00016e50: 653a 0a20 2020 2068 7474 7073 3a2f 2f72  e:.    https://r
+00016e60: 6963 682e 7265 6164 7468 6564 6f63 732e  ich.readthedocs.
+00016e70: 696f 2f65 6e2f 7374 6162 6c65 2f72 6566  io/en/stable/ref
+00016e80: 6572 656e 6365 2f63 6f6e 736f 6c65 2e68  erence/console.h
+00016e90: 746d 6c23 7269 6368 2e63 6f6e 736f 6c65  tml#rich.console
+00016ea0: 2e43 6f6e 736f 6c65 0a0a 2020 2020 4578  .Console..    Ex
+00016eb0: 616d 706c 653a 3a0a 0a20 2020 2020 203e  ample::..      >
+00016ec0: 3e3e 2069 6d70 6f72 7420 666c 6178 2e6c  >> import flax.l
+00016ed0: 696e 656e 2061 7320 6e6e 0a20 2020 2020  inen as nn.     
+00016ee0: 203e 3e3e 2069 6d70 6f72 7420 6a61 782c   >>> import jax,
+00016ef0: 206a 6178 2e6e 756d 7079 2061 7320 6a6e   jax.numpy as jn
+00016f00: 700a 0a20 2020 2020 203e 3e3e 2063 6c61  p..      >>> cla
+00016f10: 7373 2046 6f6f 286e 6e2e 4d6f 6475 6c65  ss Foo(nn.Module
+00016f20: 293a 0a20 2020 2020 202e 2e2e 2020 2040  ):.      ...   @
+00016f30: 6e6e 2e63 6f6d 7061 6374 0a20 2020 2020  nn.compact.     
+00016f40: 202e 2e2e 2020 2064 6566 205f 5f63 616c   ...   def __cal
+00016f50: 6c5f 5f28 7365 6c66 2c20 7829 3a0a 2020  l__(self, x):.  
+00016f60: 2020 2020 2e2e 2e20 2020 2020 6820 3d20      ...     h = 
+00016f70: 6e6e 2e44 656e 7365 2834 2928 7829 0a20  nn.Dense(4)(x). 
+00016f80: 2020 2020 202e 2e2e 2020 2020 2072 6574       ...     ret
+00016f90: 7572 6e20 6e6e 2e44 656e 7365 2832 2928  urn nn.Dense(2)(
+00016fa0: 6829 0a0a 2020 2020 2020 3e3e 3e20 7820  h)..      >>> x 
+00016fb0: 3d20 6a6e 702e 6f6e 6573 2828 3136 2c20  = jnp.ones((16, 
+00016fc0: 3929 290a 0a20 2020 2020 203e 3e3e 2023  9))..      >>> #
+00016fd0: 2070 7269 6e74 2846 6f6f 2829 2e74 6162   print(Foo().tab
+00016fe0: 756c 6174 6528 0a20 2020 2020 203e 3e3e  ulate(.      >>>
+00016ff0: 2023 2020 2020 206a 6178 2e72 616e 646f   #     jax.rando
+00017000: 6d2e 6b65 7928 3029 2c20 782c 2063 6f6d  m.key(0), x, com
+00017010: 7075 7465 5f66 6c6f 7073 3d54 7275 652c  pute_flops=True,
+00017020: 2063 6f6d 7075 7465 5f76 6a70 5f66 6c6f   compute_vjp_flo
+00017030: 7073 3d54 7275 6529 290a 0a20 2020 2054  ps=True))..    T
+00017040: 6869 7320 6769 7665 7320 7468 6520 666f  his gives the fo
+00017050: 6c6c 6f77 696e 6720 6f75 7470 7574 3a3a  llowing output::
+00017060: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
+00017070: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017080: 2020 2020 2020 2020 2020 2020 2020 466f                Fo
+00017090: 6f20 5375 6d6d 6172 790a 2020 2020 2020  o Summary.      
+000170a0: e294 8fe2 9481 e294 81e2 9481 e294 81e2  ................
+000170b0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+000170c0: b3e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+000170d0: e294 81e2 9481 e294 81e2 94b3 e294 81e2  ................
+000170e0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+000170f0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00017100: e294 81e2 9481 e294 81e2 94b3 e294 81e2  ................
+00017110: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00017120: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00017130: e294 81e2 9481 e294 81e2 94b3 e294 81e2  ................
+00017140: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00017150: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
+00017160: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00017170: 9481 e294 81e2 94b3 e294 81e2 9481 e294  ................
+00017180: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00017190: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+000171a0: 9481 e294 81e2 9481 e294 81e2 9493 0a20  ............... 
+000171b0: 2020 2020 20e2 9483 2070 6174 6820 2020       ... path   
+000171c0: 20e2 9483 206d 6f64 756c 6520 e294 8320   ... module ... 
+000171d0: 696e 7075 7473 2020 2020 2020 2020 e294  inputs        ..
+000171e0: 8320 6f75 7470 7574 7320 2020 2020 2020  . outputs       
+000171f0: e294 8320 666c 6f70 7320 e294 8320 766a  ... flops ... vj
+00017200: 705f 666c 6f70 7320 e294 8320 7061 7261  p_flops ... para
+00017210: 6d73 2020 2020 2020 2020 2020 e294 830a  ms          ....
+00017220: 2020 2020 2020 e294 a1e2 9481 e294 81e2        ..........
+00017230: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00017240: 81e2 9481 e295 87e2 9481 e294 81e2 9481  ................
+00017250: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00017260: 9587 e294 81e2 9481 e294 81e2 9481 e294  ................
+00017270: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00017280: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00017290: 9587 e294 81e2 9481 e294 81e2 9481 e294  ................
+000172a0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+000172b0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+000172c0: 9587 e294 81e2 9481 e294 81e2 9481 e294  ................
+000172d0: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
+000172e0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+000172f0: 9481 e294 81e2 9481 e294 81e2 9587 e294  ................
+00017300: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00017310: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00017320: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00017330: 81e2 94a9 0a20 2020 2020 20e2 9482 2020  .....      ...  
+00017340: 2020 2020 2020 20e2 9482 2046 6f6f 2020         ... Foo  
+00017350: 2020 e294 8220 666c 6f61 7433 325b 3136    ... float32[16
+00017360: 2c39 5d20 e294 8220 666c 6f61 7433 325b  ,9] ... float32[
+00017370: 3136 2c32 5d20 e294 8220 3135 3034 2020  16,2] ... 1504  
+00017380: e294 8220 3434 3630 2020 2020 2020 e294  ... 4460      ..
+00017390: 8220 2020 2020 2020 2020 2020 2020 2020  .               
+000173a0: 2020 e294 820a 2020 2020 2020 e294 9ce2    ....      ....
+000173b0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+000173c0: 80e2 9480 e294 80e2 9480 e294 bce2 9480  ................
+000173d0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+000173e0: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
+000173f0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017400: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017410: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
+00017420: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017430: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017440: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
+00017450: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00017460: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017470: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017480: 80e2 94bc e294 80e2 9480 e294 80e2 9480  ................
+00017490: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+000174a0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+000174b0: 80e2 9480 e294 80e2 94a4 0a20 2020 2020  ...........     
+000174c0: 20e2 9482 2044 656e 7365 5f30 20e2 9482   ... Dense_0 ...
+000174d0: 2044 656e 7365 2020 e294 8220 666c 6f61   Dense  ... floa
+000174e0: 7433 325b 3136 2c39 5d20 e294 8220 666c  t32[16,9] ... fl
+000174f0: 6f61 7433 325b 3136 2c34 5d20 e294 8220  oat32[16,4] ... 
+00017500: 3132 3136 2020 e294 8220 3336 3230 2020  1216  ... 3620  
+00017510: 2020 2020 e294 8220 6269 6173 3a20 2020      ... bias:   
+00017520: 2020 2020 2020 2020 e294 820a 2020 2020          ....    
+00017530: 2020 e294 8220 2020 2020 2020 2020 e294    ...         ..
+00017540: 8220 2020 2020 2020 20e2 9482 2020 2020  .        ...    
+00017550: 2020 2020 2020 2020 2020 20e2 9482 2020             ...  
+00017560: 2020 2020 2020 2020 2020 2020 20e2 9482               ...
+00017570: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
+00017580: 2020 2020 20e2 9482 2066 6c6f 6174 3332       ... float32
+00017590: 5b34 5d20 2020 2020 20e2 9482 0a20 2020  [4]      ....   
+000175a0: 2020 20e2 9482 2020 2020 2020 2020 20e2     ...         .
+000175b0: 9482 2020 2020 2020 2020 e294 8220 2020  ..        ...   
+000175c0: 2020 2020 2020 2020 2020 2020 e294 8220              ... 
+000175d0: 2020 2020 2020 2020 2020 2020 2020 e294                ..
+000175e0: 8220 2020 2020 2020 e294 8220 2020 2020  .       ...     
+000175f0: 2020 2020 2020 e294 8220 6b65 726e 656c        ... kernel
+00017600: 3a20 2020 2020 2020 2020 e294 820a 2020  :         ....  
+00017610: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
+00017620: e294 8220 2020 2020 2020 20e2 9482 2020  ...        ...  
+00017630: 2020 2020 2020 2020 2020 2020 20e2 9482               ...
+00017640: 2020 2020 2020 2020 2020 2020 2020 20e2                 .
+00017650: 9482 2020 2020 2020 20e2 9482 2020 2020  ..       ...    
+00017660: 2020 2020 2020 20e2 9482 2066 6c6f 6174         ... float
+00017670: 3332 5b39 2c34 5d20 2020 20e2 9482 0a20  32[9,4]    .... 
+00017680: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
+00017690: 20e2 9482 2020 2020 2020 2020 e294 8220   ...        ... 
+000176a0: 2020 2020 2020 2020 2020 2020 2020 e294                ..
+000176b0: 8220 2020 2020 2020 2020 2020 2020 2020  .               
+000176c0: e294 8220 2020 2020 2020 e294 8220 2020  ...       ...   
+000176d0: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
+000176e0: 2020 2020 2020 2020 2020 2020 e294 820a              ....
+000176f0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00017700: 2020 e294 8220 2020 2020 2020 20e2 9482    ...        ...
+00017710: 2020 2020 2020 2020 2020 2020 2020 20e2                 .
+00017720: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
+00017730: 20e2 9482 2020 2020 2020 20e2 9482 2020   ...       ...  
+00017740: 2020 2020 2020 2020 20e2 9482 2034 3020           ... 40 
+00017750: 2831 3630 2042 2920 2020 2020 20e2 9482  (160 B)      ...
+00017760: 0a20 2020 2020 20e2 949c e294 80e2 9480  .      .........
+00017770: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017780: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
+00017790: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+000177a0: e294 bce2 9480 e294 80e2 9480 e294 80e2  ................
+000177b0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+000177c0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+000177d0: e294 bce2 9480 e294 80e2 9480 e294 80e2  ................
+000177e0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+000177f0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017800: e294 bce2 9480 e294 80e2 9480 e294 80e2  ................
+00017810: 9480 e294 80e2 9480 e294 bce2 9480 e294  ................
+00017820: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017830: e294 80e2 9480 e294 80e2 9480 e294 bce2  ................
+00017840: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017850: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017860: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017870: 9480 e294 a40a 2020 2020 2020 e294 8220  ......      ... 
+00017880: 4465 6e73 655f 3120 e294 8220 4465 6e73  Dense_1 ... Dens
+00017890: 6520 20e2 9482 2066 6c6f 6174 3332 5b31  e  ... float32[1
+000178a0: 362c 345d 20e2 9482 2066 6c6f 6174 3332  6,4] ... float32
+000178b0: 5b31 362c 325d 20e2 9482 2032 3838 2020  [16,2] ... 288  
+000178c0: 20e2 9482 2038 3430 2020 2020 2020 20e2   ... 840       .
+000178d0: 9482 2062 6961 733a 2020 2020 2020 2020  .. bias:        
+000178e0: 2020 20e2 9482 0a20 2020 2020 20e2 9482     ....      ...
+000178f0: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
+00017900: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
+00017910: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00017920: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
+00017930: 2020 e294 8220 2020 2020 2020 2020 2020    ...           
+00017940: e294 8220 666c 6f61 7433 325b 325d 2020  ... float32[2]  
+00017950: 2020 2020 e294 820a 2020 2020 2020 e294      ....      ..
+00017960: 8220 2020 2020 2020 2020 e294 8220 2020  .         ...   
+00017970: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
+00017980: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
+00017990: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
+000179a0: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+000179b0: 20e2 9482 206b 6572 6e65 6c3a 2020 2020   ... kernel:    
+000179c0: 2020 2020 20e2 9482 0a20 2020 2020 20e2       ....      .
+000179d0: 9482 2020 2020 2020 2020 20e2 9482 2020  ..         ...  
+000179e0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+000179f0: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
+00017a00: 2020 2020 2020 2020 2020 e294 8220 2020            ...   
+00017a10: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
+00017a20: 2020 e294 8220 666c 6f61 7433 325b 342c    ... float32[4,
+00017a30: 325d 2020 2020 e294 820a 2020 2020 2020  2]    ....      
+00017a40: e294 8220 2020 2020 2020 2020 e294 8220  ...         ... 
+00017a50: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
+00017a60: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
+00017a70: 2020 2020 2020 2020 2020 20e2 9482 2020             ...  
+00017a80: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
+00017a90: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+00017aa0: 2020 2020 2020 20e2 9482 0a20 2020 2020         ....     
+00017ab0: 20e2 9482 2020 2020 2020 2020 20e2 9482   ...         ...
+00017ac0: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
+00017ad0: 2020 2020 2020 2020 2020 e294 8220 2020            ...   
+00017ae0: 2020 2020 2020 2020 2020 2020 e294 8220              ... 
+00017af0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00017b00: 2020 2020 e294 8220 3130 2028 3430 2042      ... 10 (40 B
+00017b10: 2920 2020 2020 2020 e294 820a 2020 2020  )       ....    
+00017b20: 2020 e294 9ce2 9480 e294 80e2 9480 e294    ..............
+00017b30: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017b40: e294 bce2 9480 e294 80e2 9480 e294 80e2  ................
+00017b50: 9480 e294 80e2 9480 e294 80e2 94bc e294  ................
+00017b60: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017b70: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017b80: 9480 e294 80e2 9480 e294 80e2 94bc e294  ................
+00017b90: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017ba0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017bb0: 9480 e294 80e2 9480 e294 80e2 94bc e294  ................
+00017bc0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017bd0: e294 80e2 94bc e294 80e2 9480 e294 80e2  ................
+00017be0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017bf0: 80e2 9480 e294 80e2 94bc e294 80e2 9480  ................
+00017c00: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017c10: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017c20: 80e2 9480 e294 80e2 9480 e294 80e2 94a4  ................
+00017c30: 0a20 2020 2020 20e2 9482 2020 2020 2020  .      ...      
+00017c40: 2020 20e2 9482 2020 2020 2020 2020 e294     ...        ..
+00017c50: 8220 2020 2020 2020 2020 2020 2020 2020  .               
+00017c60: e294 8220 2020 2020 2020 2020 2020 2020  ...             
+00017c70: 2020 e294 8220 2020 2020 2020 e294 8220    ...       ... 
+00017c80: 2020 2020 546f 7461 6c20 e294 8220 3530      Total ... 50
+00017c90: 2028 3230 3020 4229 2020 2020 2020 e294   (200 B)      ..
+00017ca0: 820a 2020 2020 2020 e294 94e2 9480 e294  ..      ........
+00017cb0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017cc0: e294 80e2 9480 e294 b4e2 9480 e294 80e2  ................
+00017cd0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017ce0: 80e2 94b4 e294 80e2 9480 e294 80e2 9480  ................
+00017cf0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017d00: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017d10: 80e2 94b4 e294 80e2 9480 e294 80e2 9480  ................
+00017d20: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017d30: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017d40: 80e2 94b4 e294 80e2 9480 e294 80e2 9480  ................
+00017d50: e294 80e2 9480 e294 80e2 94b4 e294 80e2  ................
+00017d60: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017d70: 80e2 9480 e294 80e2 9480 e294 80e2 94b4  ................
+00017d80: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017d90: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017da0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017db0: e294 80e2 9498 0a0a 2020 2020 2020 2020  ........        
+00017dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017dd0: 2020 2020 2020 2020 2020 2020 546f 7461              Tota
+00017de0: 6c20 5061 7261 6d65 7465 7273 3a20 3530  l Parameters: 50
+00017df0: 2028 3230 3020 4229 0a0a 2020 2020 2a2a   (200 B)..    **
+00017e00: 4e6f 7465 2a2a 3a20 726f 7773 206f 7264  Note**: rows ord
+00017e10: 6572 2069 6e20 7468 6520 7461 626c 6520  er in the table 
+00017e20: 646f 6573 206e 6f74 2072 6570 7265 7365  does not represe
+00017e30: 6e74 2065 7865 6375 7469 6f6e 206f 7264  nt execution ord
+00017e40: 6572 2c0a 2020 2020 696e 7374 6561 6420  er,.    instead 
+00017e50: 6974 2061 6c69 676e 7320 7769 7468 2074  it aligns with t
+00017e60: 6865 206f 7264 6572 206f 6620 6b65 7973  he order of keys
+00017e70: 2069 6e20 6060 7661 7269 6162 6c65 7360   in ``variables`
+00017e80: 6020 7768 6963 6820 6172 6520 736f 7274  ` which are sort
+00017e90: 6564 0a20 2020 2061 6c70 6861 6265 7469  ed.    alphabeti
+00017ea0: 6361 6c6c 792e 0a0a 2020 2020 2a2a 4e6f  cally...    **No
+00017eb0: 7465 2a2a 3a20 6060 766a 705f 666c 6f70  te**: ``vjp_flop
+00017ec0: 7360 6020 7265 7475 726e 7320 6060 3060  s`` returns ``0`
+00017ed0: 6020 6966 2074 6865 206d 6f64 756c 6520  ` if the module 
+00017ee0: 6973 206e 6f74 2064 6966 6665 7265 6e74  is not different
+00017ef0: 6961 626c 652e 0a0a 2020 2020 4172 6773  iable...    Args
+00017f00: 3a0a 2020 2020 2020 726e 6773 3a20 5468  :.      rngs: Th
+00017f10: 6520 726e 6773 2066 6f72 2074 6865 2076  e rngs for the v
+00017f20: 6172 6961 626c 6520 636f 6c6c 6563 7469  ariable collecti
+00017f30: 6f6e 7320 6173 2070 6173 7365 6420 746f  ons as passed to
+00017f40: 2060 604d 6f64 756c 652e 696e 6974 6060   ``Module.init``
+00017f50: 2e0a 2020 2020 2020 2a61 7267 733a 2054  ..      *args: T
+00017f60: 6865 2061 7267 756d 656e 7473 2074 6f20  he arguments to 
+00017f70: 7468 6520 666f 7277 6172 6420 636f 6d70  the forward comp
+00017f80: 7574 6174 696f 6e2e 0a20 2020 2020 2064  utation..      d
+00017f90: 6570 7468 3a20 636f 6e74 726f 6c73 2068  epth: controls h
+00017fa0: 6f77 206d 616e 7920 7375 626d 6f64 756c  ow many submodul
+00017fb0: 6520 6465 6570 2074 6865 2073 756d 6d61  e deep the summa
+00017fc0: 7279 2063 616e 2067 6f2e 2042 7920 6465  ry can go. By de
+00017fd0: 6661 756c 742c 0a20 2020 2020 2020 2069  fault,.        i
+00017fe0: 7473 2060 604e 6f6e 6560 6020 7768 6963  ts ``None`` whic
+00017ff0: 6820 6d65 616e 7320 6e6f 206c 696d 6974  h means no limit
+00018000: 2e20 4966 2061 2073 7562 6d6f 6475 6c65  . If a submodule
+00018010: 2069 7320 6e6f 7420 7368 6f77 6e20 6265   is not shown be
+00018020: 6361 7573 6520 6f66 0a20 2020 2020 2020  cause of.       
+00018030: 2074 6865 2064 6570 7468 206c 696d 6974   the depth limit
+00018040: 2c20 6974 7320 7061 7261 6d65 7465 7220  , its parameter 
+00018050: 636f 756e 7420 616e 6420 6279 7465 7320  count and bytes 
+00018060: 7769 6c6c 2062 6520 6164 6465 6420 746f  will be added to
+00018070: 2074 6865 2072 6f77 0a20 2020 2020 2020   the row.       
+00018080: 206f 6620 6974 7320 6669 7273 7420 7368   of its first sh
+00018090: 6f77 6e20 616e 6365 7374 6f72 2073 7563  own ancestor suc
+000180a0: 6820 7468 6174 2074 6865 2073 756d 206f  h that the sum o
+000180b0: 6620 616c 6c20 726f 7773 2061 6c77 6179  f all rows alway
+000180c0: 7320 6164 6473 0a20 2020 2020 2020 2075  s adds.        u
+000180d0: 7020 746f 2074 6865 2074 6f74 616c 206e  p to the total n
+000180e0: 756d 6265 7220 6f66 2070 6172 616d 6574  umber of paramet
+000180f0: 6572 7320 6f66 2074 6865 204d 6f64 756c  ers of the Modul
+00018100: 652e 0a20 2020 2020 2073 686f 775f 7265  e..      show_re
+00018110: 7065 6174 6564 3a20 4966 2060 6054 7275  peated: If ``Tru
+00018120: 6560 602c 2072 6570 6561 7465 6420 6361  e``, repeated ca
+00018130: 6c6c 7320 746f 2074 6865 2073 616d 6520  lls to the same 
+00018140: 6d6f 6475 6c65 2077 696c 6c20 6265 2073  module will be s
+00018150: 686f 776e 0a20 2020 2020 2020 2069 6e20  hown.        in 
+00018160: 7468 6520 7461 626c 652c 206f 7468 6572  the table, other
+00018170: 7769 7365 206f 6e6c 7920 7468 6520 6669  wise only the fi
+00018180: 7273 7420 6361 6c6c 2077 696c 6c20 6265  rst call will be
+00018190: 2073 686f 776e 2e20 4465 6661 756c 7420   shown. Default 
+000181a0: 6973 0a20 2020 2020 2020 2060 6046 616c  is.        ``Fal
+000181b0: 7365 6060 2e0a 2020 2020 2020 6d75 7461  se``..      muta
+000181c0: 626c 653a 2043 616e 2062 6520 626f 6f6c  ble: Can be bool
+000181d0: 2c20 7374 722c 206f 7220 6c69 7374 2e20  , str, or list. 
+000181e0: 5370 6563 6966 6965 7320 7768 6963 6820  Specifies which 
+000181f0: 636f 6c6c 6563 7469 6f6e 7320 7368 6f75  collections shou
+00018200: 6c64 2062 650a 2020 2020 2020 2020 7472  ld be.        tr
+00018210: 6561 7465 6420 6173 206d 7574 6162 6c65  eated as mutable
+00018220: 3a20 6060 626f 6f6c 6060 3a20 616c 6c2f  : ``bool``: all/
+00018230: 6e6f 2063 6f6c 6c65 6374 696f 6e73 2061  no collections a
+00018240: 7265 206d 7574 6162 6c65 2e20 6060 7374  re mutable. ``st
+00018250: 7260 603a 0a20 2020 2020 2020 2054 6865  r``:.        The
+00018260: 206e 616d 6520 6f66 2061 2073 696e 676c   name of a singl
+00018270: 6520 6d75 7461 626c 6520 636f 6c6c 6563  e mutable collec
+00018280: 7469 6f6e 2e20 6060 6c69 7374 6060 3a20  tion. ``list``: 
+00018290: 4120 6c69 7374 206f 6620 6e61 6d65 7320  A list of names 
+000182a0: 6f66 0a20 2020 2020 2020 206d 7574 6162  of.        mutab
+000182b0: 6c65 2063 6f6c 6c65 6374 696f 6e73 2e20  le collections. 
+000182c0: 4279 2064 6566 6175 6c74 2c20 616c 6c20  By default, all 
+000182d0: 636f 6c6c 6563 7469 6f6e 7320 6578 6365  collections exce
+000182e0: 7074 2027 696e 7465 726d 6564 6961 7465  pt 'intermediate
+000182f0: 7327 0a20 2020 2020 2020 2061 7265 206d  s'.        are m
+00018300: 7574 6162 6c65 2e0a 2020 2020 2020 636f  utable..      co
+00018310: 6e73 6f6c 655f 6b77 6172 6773 3a20 416e  nsole_kwargs: An
+00018320: 206f 7074 696f 6e61 6c20 6469 6374 696f   optional dictio
+00018330: 6e61 7279 2077 6974 6820 6164 6469 7469  nary with additi
+00018340: 6f6e 616c 206b 6579 776f 7264 2061 7267  onal keyword arg
+00018350: 756d 656e 7473 0a20 2020 2020 2020 2074  uments.        t
+00018360: 6861 7420 6172 6520 7061 7373 6564 2074  hat are passed t
+00018370: 6f20 6060 7269 6368 2e63 6f6e 736f 6c65  o ``rich.console
+00018380: 2e43 6f6e 736f 6c65 6060 2077 6865 6e20  .Console`` when 
+00018390: 7265 6e64 6572 696e 6720 7468 6520 7461  rendering the ta
+000183a0: 626c 652e 0a20 2020 2020 2020 2044 6566  ble..        Def
+000183b0: 6175 6c74 2061 7267 756d 656e 7473 2061  ault arguments a
+000183c0: 7265 2060 607b 2766 6f72 6365 5f74 6572  re ``{'force_ter
+000183d0: 6d69 6e61 6c27 3a20 5472 7565 2c20 2766  minal': True, 'f
+000183e0: 6f72 6365 5f6a 7570 7974 6572 273a 0a20  orce_jupyter':. 
+000183f0: 2020 2020 2020 2046 616c 7365 7d60 602e         False}``.
+00018400: 0a20 2020 2020 2074 6162 6c65 5f6b 7761  .      table_kwa
+00018410: 7267 733a 2041 6e20 6f70 7469 6f6e 616c  rgs: An optional
+00018420: 2064 6963 7469 6f6e 6172 7920 7769 7468   dictionary with
+00018430: 2061 6464 6974 696f 6e61 6c20 6b65 7977   additional keyw
+00018440: 6f72 6420 6172 6775 6d65 6e74 730a 2020  ord arguments.  
+00018450: 2020 2020 2020 7468 6174 2061 7265 2070        that are p
+00018460: 6173 7365 6420 746f 2060 6072 6963 682e  assed to ``rich.
+00018470: 7461 626c 652e 5461 626c 6560 6020 636f  table.Table`` co
+00018480: 6e73 7472 7563 746f 722e 0a20 2020 2020  nstructor..     
+00018490: 2063 6f6c 756d 6e5f 6b77 6172 6773 3a20   column_kwargs: 
+000184a0: 416e 206f 7074 696f 6e61 6c20 6469 6374  An optional dict
+000184b0: 696f 6e61 7279 2077 6974 6820 6164 6469  ionary with addi
+000184c0: 7469 6f6e 616c 206b 6579 776f 7264 2061  tional keyword a
+000184d0: 7267 756d 656e 7473 0a20 2020 2020 2020  rguments.       
+000184e0: 2074 6861 7420 6172 6520 7061 7373 6564   that are passed
+000184f0: 2074 6f20 6060 7269 6368 2e74 6162 6c65   to ``rich.table
+00018500: 2e54 6162 6c65 2e61 6464 5f63 6f6c 756d  .Table.add_colum
+00018510: 6e60 6020 7768 656e 2061 6464 696e 6720  n`` when adding 
+00018520: 636f 6c75 6d6e 7320 746f 0a20 2020 2020  columns to.     
+00018530: 2020 2074 6865 2074 6162 6c65 2e0a 2020     the table..  
+00018540: 2020 2020 636f 6d70 7574 655f 666c 6f70      compute_flop
+00018550: 733a 2077 6865 7468 6572 2074 6f20 696e  s: whether to in
+00018560: 636c 7564 6520 6120 6060 666c 6f70 7360  clude a ``flops`
+00018570: 6020 636f 6c75 6d6e 2069 6e20 7468 6520  ` column in the 
+00018580: 7461 626c 6520 6c69 7374 696e 670a 2020  table listing.  
+00018590: 2020 2020 2020 7468 6520 6573 7469 6d61        the estima
+000185a0: 7465 6420 464c 4f50 7320 636f 7374 206f  ted FLOPs cost o
+000185b0: 6620 6561 6368 206d 6f64 756c 6520 666f  f each module fo
+000185c0: 7277 6172 6420 7061 7373 2e20 446f 6573  rward pass. Does
+000185d0: 2069 6e63 7572 2061 6374 7561 6c0a 2020   incur actual.  
+000185e0: 2020 2020 2020 6f6e 2d64 6576 6963 6520        on-device 
+000185f0: 636f 6d70 7574 6174 696f 6e20 2f20 636f  computation / co
+00018600: 6d70 696c 6174 696f 6e20 2f20 6d65 6d6f  mpilation / memo
+00018610: 7279 2061 6c6c 6f63 6174 696f 6e2c 2062  ry allocation, b
+00018620: 7574 2073 7469 6c6c 0a20 2020 2020 2020  ut still.       
+00018630: 2069 6e74 726f 6475 6365 7320 6f76 6572   introduces over
+00018640: 6865 6164 2066 6f72 206c 6172 6765 206d  head for large m
+00018650: 6f64 756c 6573 2028 652e 672e 2065 7874  odules (e.g. ext
+00018660: 7261 2032 3020 7365 636f 6e64 7320 666f  ra 20 seconds fo
+00018670: 7220 610a 2020 2020 2020 2020 5374 6162  r a.        Stab
+00018680: 6c65 2044 6966 6675 7369 6f6e 2773 2055  le Diffusion's U
+00018690: 4e65 742c 2077 6865 7265 6173 206f 7468  Net, whereas oth
+000186a0: 6572 7769 7365 2074 6162 756c 6174 696f  erwise tabulatio
+000186b0: 6e20 776f 756c 6420 6669 6e69 7368 2069  n would finish i
+000186c0: 6e20 350a 2020 2020 2020 2020 7365 636f  n 5.        seco
+000186d0: 6e64 7329 2e0a 2020 2020 2020 636f 6d70  nds)..      comp
+000186e0: 7574 655f 766a 705f 666c 6f70 733a 2077  ute_vjp_flops: w
+000186f0: 6865 7468 6572 2074 6f20 696e 636c 7564  hether to includ
+00018700: 6520 6120 6060 766a 705f 666c 6f70 7360  e a ``vjp_flops`
+00018710: 6020 636f 6c75 6d6e 2069 6e20 7468 6520  ` column in the 
+00018720: 7461 626c 650a 2020 2020 2020 2020 6c69  table.        li
+00018730: 7374 696e 6720 7468 6520 6573 7469 6d61  sting the estima
+00018740: 7465 6420 464c 4f50 7320 636f 7374 206f  ted FLOPs cost o
+00018750: 6620 6561 6368 206d 6f64 756c 6520 6261  f each module ba
+00018760: 636b 7761 7264 2070 6173 732e 0a20 2020  ckward pass..   
+00018770: 2020 2020 2049 6e74 726f 6475 6365 7320       Introduces 
+00018780: 6120 636f 6d70 7574 6520 6f76 6572 6865  a compute overhe
+00018790: 6164 206f 6620 6162 6f75 7420 322d 3358  ad of about 2-3X
+000187a0: 206f 6620 6060 636f 6d70 7574 655f 666c   of ``compute_fl
+000187b0: 6f70 7360 602e 0a20 2020 2020 202a 2a6b  ops``..      **k
+000187c0: 7761 7267 733a 206b 6579 776f 7264 2061  wargs: keyword a
+000187d0: 7267 756d 656e 7473 2074 6f20 7061 7373  rguments to pass
+000187e0: 2074 6f20 7468 6520 666f 7277 6172 6420   to the forward 
+000187f0: 636f 6d70 7574 6174 696f 6e2e 0a0a 2020  computation...  
+00018800: 2020 5265 7475 726e 733a 0a20 2020 2020    Returns:.     
+00018810: 2041 2073 7472 696e 6720 7375 6d6d 6172   A string summar
+00018820: 697a 696e 6720 7468 6520 4d6f 6475 6c65  izing the Module
+00018830: 2e0a 2020 2020 2222 220a 2020 2020 6672  ..    """.    fr
+00018840: 6f6d 2066 6c61 782e 6c69 6e65 6e20 696d  om flax.linen im
+00018850: 706f 7274 2073 756d 6d61 7279 0a0a 2020  port summary..  
+00018860: 2020 7461 6275 6c61 7465 5f66 6e20 3d20    tabulate_fn = 
+00018870: 7375 6d6d 6172 792e 7461 6275 6c61 7465  summary.tabulate
+00018880: 280a 2020 2020 2020 7365 6c66 2c0a 2020  (.      self,.  
+00018890: 2020 2020 726e 6773 2c0a 2020 2020 2020      rngs,.      
+000188a0: 6465 7074 683d 6465 7074 682c 0a20 2020  depth=depth,.   
+000188b0: 2020 2073 686f 775f 7265 7065 6174 6564     show_repeated
+000188c0: 3d73 686f 775f 7265 7065 6174 6564 2c0a  =show_repeated,.
+000188d0: 2020 2020 2020 6d75 7461 626c 653d 6d75        mutable=mu
+000188e0: 7461 626c 652c 0a20 2020 2020 2063 6f6e  table,.      con
+000188f0: 736f 6c65 5f6b 7761 7267 733d 636f 6e73  sole_kwargs=cons
+00018900: 6f6c 655f 6b77 6172 6773 2c0a 2020 2020  ole_kwargs,.    
+00018910: 2020 7461 626c 655f 6b77 6172 6773 3d74    table_kwargs=t
+00018920: 6162 6c65 5f6b 7761 7267 732c 0a20 2020  able_kwargs,.   
+00018930: 2020 2063 6f6c 756d 6e5f 6b77 6172 6773     column_kwargs
+00018940: 3d63 6f6c 756d 6e5f 6b77 6172 6773 2c0a  =column_kwargs,.
+00018950: 2020 2020 2020 636f 6d70 7574 655f 666c        compute_fl
+00018960: 6f70 733d 636f 6d70 7574 655f 666c 6f70  ops=compute_flop
+00018970: 732c 0a20 2020 2020 2063 6f6d 7075 7465  s,.      compute
+00018980: 5f76 6a70 5f66 6c6f 7073 3d63 6f6d 7075  _vjp_flops=compu
+00018990: 7465 5f76 6a70 5f66 6c6f 7073 2c0a 2020  te_vjp_flops,.  
+000189a0: 2020 290a 2020 2020 7265 7475 726e 2074    ).    return t
+000189b0: 6162 756c 6174 655f 666e 282a 6172 6773  abulate_fn(*args
+000189c0: 2c20 2a2a 6b77 6172 6773 290a 0a20 2064  , **kwargs)..  d
+000189d0: 6566 206d 6f64 756c 655f 7061 7468 7328  ef module_paths(
+000189e0: 0a20 2020 2073 656c 662c 0a20 2020 2072  .    self,.    r
+000189f0: 6e67 733a 2055 6e69 6f6e 5b50 524e 474b  ngs: Union[PRNGK
+00018a00: 6579 2c20 524e 4753 6571 7565 6e63 6573  ey, RNGSequences
+00018a10: 5d2c 0a20 2020 202a 6172 6773 2c0a 2020  ],.    *args,.  
+00018a20: 2020 7368 6f77 5f72 6570 6561 7465 643a    show_repeated:
+00018a30: 2062 6f6f 6c20 3d20 4661 6c73 652c 0a20   bool = False,. 
+00018a40: 2020 206d 7574 6162 6c65 3a20 436f 6c6c     mutable: Coll
+00018a50: 6563 7469 6f6e 4669 6c74 6572 203d 2044  ectionFilter = D
+00018a60: 656e 794c 6973 7428 2769 6e74 6572 6d65  enyList('interme
+00018a70: 6469 6174 6573 2729 2c0a 2020 2020 2a2a  diates'),.    **
+00018a80: 6b77 6172 6773 2c0a 2020 2920 2d3e 2064  kwargs,.  ) -> d
+00018a90: 6963 745b 7374 722c 2027 4d6f 6475 6c65  ict[str, 'Module
+00018aa0: 275d 3a0a 2020 2020 2222 2252 6574 7572  ']:.    """Retur
+00018ab0: 6e73 2061 2064 6963 7469 6f6e 6172 7920  ns a dictionary 
+00018ac0: 6d61 7070 696e 6720 6d6f 6475 6c65 2070  mapping module p
+00018ad0: 6174 6873 2074 6f20 6d6f 6475 6c65 2069  aths to module i
+00018ae0: 6e73 7461 6e63 6573 2e0a 0a20 2020 2054  nstances...    T
+00018af0: 6869 7320 6d65 7468 6f64 2068 6173 2074  his method has t
+00018b00: 6865 2073 616d 6520 7369 676e 6174 7572  he same signatur
+00018b10: 6520 616e 6420 696e 7465 726e 616c 6c79  e and internally
+00018b20: 2063 616c 6c73 2060 604d 6f64 756c 652e   calls ``Module.
+00018b30: 696e 6974 6060 2c0a 2020 2020 6275 7420  init``,.    but 
+00018b40: 696e 7374 6561 6420 6f66 2072 6574 7572  instead of retur
+00018b50: 6e69 6e67 2074 6865 2076 6172 6961 626c  ning the variabl
+00018b60: 6573 2c20 6974 2072 6574 7572 6e73 2061  es, it returns a
+00018b70: 2064 6963 7469 6f6e 6172 7920 6d61 7070   dictionary mapp
+00018b80: 696e 670a 2020 2020 6d6f 6475 6c65 2070  ing.    module p
+00018b90: 6174 6873 2074 6f20 756e 626f 756e 6465  aths to unbounde
+00018ba0: 6420 636f 7069 6573 206f 6620 6d6f 6475  d copies of modu
+00018bb0: 6c65 2069 6e73 7461 6e63 6573 2074 6861  le instances tha
+00018bc0: 7420 7765 7265 2075 7365 640a 2020 2020  t were used.    
+00018bd0: 6174 2072 756e 7469 6d65 2e20 6060 6d6f  at runtime. ``mo
+00018be0: 6475 6c65 5f70 6174 6873 6060 2075 7365  dule_paths`` use
+00018bf0: 7320 6060 6a61 782e 6576 616c 5f73 6861  s ``jax.eval_sha
+00018c00: 7065 6060 2074 6f20 7275 6e20 7468 6520  pe`` to run the 
+00018c10: 666f 7277 6172 640a 2020 2020 636f 6d70  forward.    comp
+00018c20: 7574 6174 696f 6e20 7769 7468 6f75 7420  utation without 
+00018c30: 636f 6e73 756d 696e 6720 616e 7920 464c  consuming any FL
+00018c40: 4f50 7320 6f72 2061 6c6c 6f63 6174 696e  OPs or allocatin
+00018c50: 6720 6d65 6d6f 7279 2e0a 0a20 2020 2045  g memory...    E
+00018c60: 7861 6d70 6c65 3a3a 0a0a 2020 2020 2020  xample::..      
+00018c70: 3e3e 3e20 696d 706f 7274 2066 6c61 782e  >>> import flax.
+00018c80: 6c69 6e65 6e20 6173 206e 6e0a 2020 2020  linen as nn.    
+00018c90: 2020 3e3e 3e20 696d 706f 7274 206a 6178    >>> import jax
+00018ca0: 2c20 6a61 782e 6e75 6d70 7920 6173 206a  , jax.numpy as j
+00018cb0: 6e70 0a0a 2020 2020 2020 3e3e 3e20 636c  np..      >>> cl
+00018cc0: 6173 7320 466f 6f28 6e6e 2e4d 6f64 756c  ass Foo(nn.Modul
+00018cd0: 6529 3a0a 2020 2020 2020 2e2e 2e20 2020  e):.      ...   
+00018ce0: 406e 6e2e 636f 6d70 6163 740a 2020 2020  @nn.compact.    
+00018cf0: 2020 2e2e 2e20 2020 6465 6620 5f5f 6361    ...   def __ca
+00018d00: 6c6c 5f5f 2873 656c 662c 2078 293a 0a20  ll__(self, x):. 
+00018d10: 2020 2020 202e 2e2e 2020 2020 2068 203d       ...     h =
+00018d20: 206e 6e2e 4465 6e73 6528 3429 2878 290a   nn.Dense(4)(x).
+00018d30: 2020 2020 2020 2e2e 2e20 2020 2020 7265        ...     re
+00018d40: 7475 726e 206e 6e2e 4465 6e73 6528 3229  turn nn.Dense(2)
+00018d50: 2868 290a 0a20 2020 2020 203e 3e3e 2078  (h)..      >>> x
+00018d60: 203d 206a 6e70 2e6f 6e65 7328 2831 362c   = jnp.ones((16,
+00018d70: 2039 2929 0a20 2020 2020 203e 3e3e 206d   9)).      >>> m
+00018d80: 6f64 756c 6573 203d 2046 6f6f 2829 2e6d  odules = Foo().m
+00018d90: 6f64 756c 655f 7061 7468 7328 6a61 782e  odule_paths(jax.
+00018da0: 7261 6e64 6f6d 2e6b 6579 2830 292c 2078  random.key(0), x
+00018db0: 290a 2020 2020 2020 3e3e 3e20 7072 696e  ).      >>> prin
+00018dc0: 7428 7b0a 2020 2020 2020 2e2e 2e20 2020  t({.      ...   
+00018dd0: 2020 703a 2074 7970 6528 6d29 2e5f 5f6e    p: type(m).__n
+00018de0: 616d 655f 5f20 666f 7220 702c 206d 2069  ame__ for p, m i
+00018df0: 6e20 6d6f 6475 6c65 732e 6974 656d 7328  n modules.items(
+00018e00: 290a 2020 2020 2020 2e2e 2e20 7d29 0a20  ).      ... }). 
+00018e10: 2020 2020 207b 2727 3a20 2746 6f6f 272c       {'': 'Foo',
+00018e20: 2027 4465 6e73 655f 3027 3a20 2744 656e   'Dense_0': 'Den
+00018e30: 7365 272c 2027 4465 6e73 655f 3127 3a20  se', 'Dense_1': 
+00018e40: 2744 656e 7365 277d 0a0a 2020 2020 4172  'Dense'}..    Ar
+00018e50: 6773 3a0a 2020 2020 2020 726e 6773 3a20  gs:.      rngs: 
+00018e60: 5468 6520 726e 6773 2066 6f72 2074 6865  The rngs for the
+00018e70: 2076 6172 6961 626c 6520 636f 6c6c 6563   variable collec
+00018e80: 7469 6f6e 7320 6173 2070 6173 7365 6420  tions as passed 
+00018e90: 746f 2060 604d 6f64 756c 652e 696e 6974  to ``Module.init
+00018ea0: 6060 2e0a 2020 2020 2020 2a61 7267 733a  ``..      *args:
+00018eb0: 2054 6865 2061 7267 756d 656e 7473 2074   The arguments t
+00018ec0: 6f20 7468 6520 666f 7277 6172 6420 636f  o the forward co
+00018ed0: 6d70 7574 6174 696f 6e2e 0a20 2020 2020  mputation..     
+00018ee0: 2073 686f 775f 7265 7065 6174 6564 3a20   show_repeated: 
+00018ef0: 4966 2060 6054 7275 6560 602c 2072 6570  If ``True``, rep
+00018f00: 6561 7465 6420 6361 6c6c 7320 746f 2074  eated calls to t
+00018f10: 6865 2073 616d 6520 6d6f 6475 6c65 2077  he same module w
+00018f20: 696c 6c20 6265 0a20 2020 2020 2020 2073  ill be.        s
+00018f30: 686f 776e 2069 6e20 7468 6520 7461 626c  hown in the tabl
+00018f40: 652c 206f 7468 6572 7769 7365 206f 6e6c  e, otherwise onl
+00018f50: 7920 7468 6520 6669 7273 7420 6361 6c6c  y the first call
+00018f60: 2077 696c 6c20 6265 2073 686f 776e 2e0a   will be shown..
+00018f70: 2020 2020 2020 2020 4465 6661 756c 7420          Default 
+00018f80: 6973 2060 6046 616c 7365 6060 2e0a 2020  is ``False``..  
+00018f90: 2020 2020 6d75 7461 626c 653a 2043 616e      mutable: Can
+00018fa0: 2062 6520 626f 6f6c 2c20 7374 722c 206f   be bool, str, o
+00018fb0: 7220 6c69 7374 2e20 5370 6563 6966 6965  r list. Specifie
+00018fc0: 7320 7768 6963 6820 636f 6c6c 6563 7469  s which collecti
+00018fd0: 6f6e 7320 7368 6f75 6c64 0a20 2020 2020  ons should.     
+00018fe0: 2020 2062 6520 7472 6561 7465 6420 6173     be treated as
+00018ff0: 206d 7574 6162 6c65 3a20 6060 626f 6f6c   mutable: ``bool
+00019000: 6060 3a20 616c 6c2f 6e6f 2063 6f6c 6c65  ``: all/no colle
+00019010: 6374 696f 6e73 2061 7265 206d 7574 6162  ctions are mutab
+00019020: 6c65 2e0a 2020 2020 2020 2020 6060 7374  le..        ``st
+00019030: 7260 603a 2054 6865 206e 616d 6520 6f66  r``: The name of
+00019040: 2061 2073 696e 676c 6520 6d75 7461 626c   a single mutabl
+00019050: 6520 636f 6c6c 6563 7469 6f6e 2e20 6060  e collection. ``
+00019060: 6c69 7374 6060 3a20 4120 6c69 7374 206f  list``: A list o
+00019070: 660a 2020 2020 2020 2020 6e61 6d65 7320  f.        names 
+00019080: 6f66 206d 7574 6162 6c65 2063 6f6c 6c65  of mutable colle
+00019090: 6374 696f 6e73 2e20 4279 2064 6566 6175  ctions. By defau
+000190a0: 6c74 2c20 616c 6c20 636f 6c6c 6563 7469  lt, all collecti
+000190b0: 6f6e 7320 6578 6365 7074 0a20 2020 2020  ons except.     
+000190c0: 2020 2027 696e 7465 726d 6564 6961 7465     'intermediate
+000190d0: 7327 2061 7265 206d 7574 6162 6c65 2e0a  s' are mutable..
+000190e0: 2020 2020 2020 2a2a 6b77 6172 6773 3a20        **kwargs: 
+000190f0: 6b65 7977 6f72 6420 6172 6775 6d65 6e74  keyword argument
+00019100: 7320 746f 2070 6173 7320 746f 2074 6865  s to pass to the
+00019110: 2066 6f72 7761 7264 2063 6f6d 7075 7461   forward computa
+00019120: 7469 6f6e 2e0a 0a20 2020 2052 6574 7572  tion...    Retur
+00019130: 6e73 3a0a 2020 2020 2020 4120 6469 6374  ns:.      A dict
+00019140: 6069 6f6e 6172 7920 6d61 7070 696e 6720  `ionary mapping 
+00019150: 6d6f 6475 6c65 2070 6174 6873 2074 6f20  module paths to 
+00019160: 6d6f 6475 6c65 2069 6e73 7461 6e63 6573  module instances
+00019170: 2e0a 2020 2020 2222 220a 2020 2020 6672  ..    """.    fr
+00019180: 6f6d 2066 6c61 782e 6c69 6e65 6e20 696d  om flax.linen im
+00019190: 706f 7274 2073 756d 6d61 7279 0a0a 2020  port summary..  
+000191a0: 2020 7461 626c 6520 3d20 7375 6d6d 6172    table = summar
+000191b0: 792e 5f67 6574 5f6d 6f64 756c 655f 7461  y._get_module_ta
+000191c0: 626c 6528 0a20 2020 2020 206d 6f64 756c  ble(.      modul
+000191d0: 653d 7365 6c66 2c0a 2020 2020 2020 6465  e=self,.      de
+000191e0: 7074 683d 4e6f 6e65 2c0a 2020 2020 2020  pth=None,.      
+000191f0: 7368 6f77 5f72 6570 6561 7465 643d 7368  show_repeated=sh
+00019200: 6f77 5f72 6570 6561 7465 642c 0a20 2020  ow_repeated,.   
+00019210: 2020 2063 6f6d 7075 7465 5f66 6c6f 7073     compute_flops
+00019220: 3d46 616c 7365 2c0a 2020 2020 2020 636f  =False,.      co
+00019230: 6d70 7574 655f 766a 705f 666c 6f70 733d  mpute_vjp_flops=
+00019240: 4661 6c73 652c 0a20 2020 2029 2872 6e67  False,.    )(rng
+00019250: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
+00019260: 6773 2c20 6d75 7461 626c 653d 6d75 7461  gs, mutable=muta
+00019270: 626c 6529 0a0a 2020 2020 7265 7475 726e  ble)..    return
+00019280: 207b 272f 272e 6a6f 696e 2872 6f77 2e70   {'/'.join(row.p
+00019290: 6174 6829 3a20 726f 772e 6d6f 6475 6c65  ath): row.module
+000192a0: 5f63 6f70 7920 666f 7220 726f 7720 696e  _copy for row in
+000192b0: 2074 6162 6c65 7d0a 0a0a 5f50 6172 656e   table}..._Paren
+000192c0: 7454 7970 6520 3d20 556e 696f 6e5b 5479  tType = Union[Ty
+000192d0: 7065 5b4d 6f64 756c 655d 2c20 5363 6f70  pe[Module], Scop
+000192e0: 652c 2054 7970 655b 5f53 656e 7469 6e65  e, Type[_Sentine
+000192f0: 6c5d 2c20 4e6f 6e65 5d0a 0a0a 6465 6620  l], None]...def 
+00019300: 6d65 7267 655f 7061 7261 6d28 6e61 6d65  merge_param(name
+00019310: 3a20 7374 722c 2061 3a20 4f70 7469 6f6e  : str, a: Option
+00019320: 616c 5b54 5d2c 2062 3a20 4f70 7469 6f6e  al[T], b: Option
+00019330: 616c 5b54 5d29 202d 3e20 543a 0a20 2022  al[T]) -> T:.  "
+00019340: 2222 4d65 7267 6573 2063 6f6e 7374 7275  ""Merges constru
+00019350: 6374 696f 6e2d 2061 6e64 2063 616c 6c2d  ction- and call-
+00019360: 7469 6d65 2061 7267 756d 656e 742e 0a0a  time argument...
+00019370: 2020 5468 6973 2069 7320 6120 7574 696c    This is a util
+00019380: 6974 7920 666f 7220 7375 7070 6f72 7469  ity for supporti
+00019390: 6e67 2061 2070 6174 7465 726e 2077 6865  ng a pattern whe
+000193a0: 7265 2061 204d 6f64 756c 6520 6879 7065  re a Module hype
+000193b0: 7270 6172 616d 6574 6572 0a20 2063 616e  rparameter.  can
+000193c0: 2062 6520 7061 7373 6564 2065 6974 6865   be passed eithe
+000193d0: 7220 746f 2060 605f 5f69 6e69 745f 5f60  r to ``__init__`
+000193e0: 6020 6f72 2060 605f 5f63 616c 6c5f 5f60  ` or ``__call__`
+000193f0: 602c 2061 6e64 2074 6865 2076 616c 7565  `, and the value
+00019400: 2074 6861 7420 6973 0a20 206e 6f74 2060   that is.  not `
+00019410: 604e 6f6e 6560 6020 7769 6c6c 2062 6520  `None`` will be 
+00019420: 7573 6564 2e0a 0a20 2045 7861 6d70 6c65  used...  Example
+00019430: 3a3a 0a0a 2020 2020 3e3e 3e20 696d 706f  ::..    >>> impo
+00019440: 7274 2066 6c61 782e 6c69 6e65 6e20 6173  rt flax.linen as
+00019450: 206e 6e0a 2020 2020 3e3e 3e20 6672 6f6d   nn.    >>> from
+00019460: 2074 7970 696e 6720 696d 706f 7274 204f   typing import O
+00019470: 7074 696f 6e61 6c0a 0a20 2020 203e 3e3e  ptional..    >>>
+00019480: 2063 6c61 7373 2046 6f6f 286e 6e2e 4d6f   class Foo(nn.Mo
+00019490: 6475 6c65 293a 0a20 2020 202e 2e2e 2020  dule):.    ...  
+000194a0: 2074 7261 696e 3a20 4f70 7469 6f6e 616c   train: Optional
+000194b0: 5b62 6f6f 6c5d 203d 204e 6f6e 650a 0a20  [bool] = None.. 
+000194c0: 2020 202e 2e2e 2020 2064 6566 205f 5f63     ...   def __c
+000194d0: 616c 6c5f 5f28 7365 6c66 2c20 7472 6169  all__(self, trai
+000194e0: 6e3a 204f 7074 696f 6e61 6c5b 626f 6f6c  n: Optional[bool
+000194f0: 5d20 3d20 4e6f 6e65 293a 0a20 2020 202e  ] = None):.    .
+00019500: 2e2e 2020 2020 2074 7261 696e 203d 206e  ..     train = n
+00019510: 6e2e 6d65 7267 655f 7061 7261 6d28 2774  n.merge_param('t
+00019520: 7261 696e 272c 2073 656c 662e 7472 6169  rain', self.trai
+00019530: 6e2c 2074 7261 696e 290a 0a20 2041 6e20  n, train)..  An 
+00019540: 6572 726f 7220 6973 2074 6872 6f77 6e20  error is thrown 
+00019550: 7768 656e 2062 6f74 6820 6172 6775 6d65  when both argume
+00019560: 6e74 7320 6172 6520 6060 4e6f 6e65 6060  nts are ``None``
+00019570: 206f 7220 626f 7468 2076 616c 7565 7320   or both values 
+00019580: 6172 6520 6e6f 740a 2020 6060 4e6f 6e65  are not.  ``None
+00019590: 6060 2e0a 0a20 2041 7267 733a 0a20 2020  ``...  Args:.   
+000195a0: 206e 616d 653a 2074 6865 206e 616d 6520   name: the name 
+000195b0: 6f66 2074 6865 2070 6172 616d 6574 6572  of the parameter
+000195c0: 2e20 5573 6564 2066 6f72 2065 7272 6f72  . Used for error
+000195d0: 206d 6573 7361 6765 732e 0a20 2020 2061   messages..    a
+000195e0: 3a20 6f70 7469 6f6e 2061 0a20 2020 2062  : option a.    b
+000195f0: 3a20 6f70 7469 6f6e 2062 0a0a 2020 5265  : option b..  Re
+00019600: 7475 726e 733a 0a20 2020 2061 206f 7220  turns:.    a or 
+00019610: 6220 7768 6963 6865 7665 7220 6973 206e  b whichever is n
+00019620: 6f74 2060 604e 6f6e 6560 602e 0a20 2022  ot ``None``..  "
+00019630: 2222 0a20 2069 6620 6120 6973 204e 6f6e  "".  if a is Non
+00019640: 6520 616e 6420 6220 6973 204e 6f6e 653a  e and b is None:
+00019650: 0a20 2020 2072 6169 7365 2056 616c 7565  .    raise Value
+00019660: 4572 726f 7228 0a20 2020 2020 2066 2750  Error(.      f'P
+00019670: 6172 616d 6574 6572 2022 7b6e 616d 657d  arameter "{name}
+00019680: 2220 6d75 7374 2062 6520 7061 7373 6564  " must be passed
+00019690: 2074 6f20 7468 6520 636f 6e73 7472 7563   to the construc
+000196a0: 746f 7220 6f72 2061 7420 6361 6c6c 2074  tor or at call t
+000196b0: 696d 652e 270a 2020 2020 290a 2020 6966  ime.'.    ).  if
+000196c0: 2061 2069 7320 6e6f 7420 4e6f 6e65 2061   a is not None a
+000196d0: 6e64 2062 2069 7320 6e6f 7420 4e6f 6e65  nd b is not None
+000196e0: 3a0a 2020 2020 7261 6973 6520 5661 6c75  :.    raise Valu
+000196f0: 6545 7272 6f72 280a 2020 2020 2020 6627  eError(.      f'
+00019700: 5061 7261 6d65 7465 7220 227b 6e61 6d65  Parameter "{name
+00019710: 7d22 2077 6173 2070 6173 7365 6420 746f  }" was passed to
+00019720: 2074 6865 2063 6f6e 7374 7275 6374 6f72   the constructor
+00019730: 2061 6e64 2061 7420 6361 6c6c 2074 696d   and at call tim
+00019740: 652e 270a 2020 2020 2020 2720 5368 6f75  e.'.      ' Shou
+00019750: 6c64 2062 6520 7061 7373 6564 206a 7573  ld be passed jus
+00019760: 7420 6f6e 6365 2e27 0a20 2020 2029 0a20  t once.'.    ). 
+00019770: 2069 6620 6120 6973 204e 6f6e 653a 0a20   if a is None:. 
+00019780: 2020 2061 7373 6572 7420 6220 6973 206e     assert b is n
+00019790: 6f74 204e 6f6e 650a 2020 2020 7265 7475  ot None.    retu
+000197a0: 726e 2062 0a20 2072 6574 7572 6e20 610a  rn b.  return a.
+000197b0: 0a0a 4074 7261 6365 6261 636b 5f75 7469  ..@traceback_uti
+000197c0: 6c2e 6170 695f 626f 756e 6461 7279 0a64  l.api_boundary.d
+000197d0: 6566 2061 7070 6c79 280a 2020 666e 3a20  ef apply(.  fn: 
+000197e0: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
+000197f0: 795d 2c0a 2020 6d6f 6475 6c65 3a20 4d6f  y],.  module: Mo
+00019800: 6475 6c65 2c0a 2020 6d75 7461 626c 653a  dule,.  mutable:
+00019810: 2043 6f6c 6c65 6374 696f 6e46 696c 7465   CollectionFilte
+00019820: 7220 3d20 4661 6c73 652c 0a20 2063 6170  r = False,.  cap
+00019830: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
+00019840: 6573 3a20 556e 696f 6e5b 626f 6f6c 2c20  es: Union[bool, 
+00019850: 4361 6c6c 6162 6c65 5b5b 4d6f 6475 6c65  Callable[[Module
+00019860: 2c20 7374 725d 2c20 626f 6f6c 5d5d 203d  , str], bool]] =
+00019870: 2046 616c 7365 2c0a 2920 2d3e 2043 616c   False,.) -> Cal
+00019880: 6c61 626c 655b 2e2e 2e2c 2041 6e79 5d3a  lable[..., Any]:
+00019890: 0a20 2022 2222 4372 6561 7465 7320 616e  .  """Creates an
+000198a0: 2061 7070 6c79 2066 756e 6374 696f 6e20   apply function 
+000198b0: 746f 2063 616c 6c20 6060 666e 6060 2077  to call ``fn`` w
+000198c0: 6974 6820 6120 626f 756e 6420 6d6f 6475  ith a bound modu
+000198d0: 6c65 2e0a 0a20 2055 6e6c 696b 6520 6060  le...  Unlike ``
+000198e0: 4d6f 6475 6c65 2e61 7070 6c79 6060 2074  Module.apply`` t
+000198f0: 6869 7320 6675 6e63 7469 6f6e 2072 6574  his function ret
+00019900: 7572 6e73 2061 206e 6577 2066 756e 6374  urns a new funct
+00019910: 696f 6e20 7769 7468 2074 6865 0a20 2073  ion with the.  s
+00019920: 6967 6e61 7475 7265 2060 6028 7661 7269  ignature ``(vari
+00019930: 6162 6c65 732c 202a 6172 6773 2c20 726e  ables, *args, rn
+00019940: 6773 3d4e 6f6e 652c 202a 2a6b 7761 7267  gs=None, **kwarg
+00019950: 7329 202d 3e20 5460 6020 7768 6572 6520  s) -> T`` where 
+00019960: 6060 5460 6020 6973 2074 6865 0a20 2072  ``T`` is the.  r
+00019970: 6574 7572 6e20 7479 7065 206f 6620 6060  eturn type of ``
+00019980: 666e 6060 2e20 4966 2060 606d 7574 6162  fn``. If ``mutab
+00019990: 6c65 6060 2069 7320 6e6f 7420 6060 4661  le`` is not ``Fa
+000199a0: 6c73 6560 6020 7468 6520 7265 7475 726e  lse`` the return
+000199b0: 2074 7970 6520 6973 2061 0a20 2074 7570   type is a.  tup
+000199c0: 6c65 2077 6865 7265 2074 6865 2073 6563  le where the sec
+000199d0: 6f6e 6420 6974 656d 2069 7320 6120 6060  ond item is a ``
+000199e0: 4672 6f7a 656e 4469 6374 6060 2077 6974  FrozenDict`` wit
+000199f0: 6820 7468 6520 6d75 7461 7465 6420 7661  h the mutated va
+00019a00: 7269 6162 6c65 732e 0a0a 2020 5468 6520  riables...  The 
+00019a10: 6170 706c 7920 6675 6e63 7469 6f6e 2074  apply function t
+00019a20: 6861 7420 6973 2072 6574 7572 6e65 6420  hat is returned 
+00019a30: 6361 6e20 6265 2064 6972 6563 746c 7920  can be directly 
+00019a40: 636f 6d70 6f73 6564 2077 6974 680a 2020  composed with.  
+00019a50: 4a41 5820 7472 616e 7366 6f72 6d61 7469  JAX transformati
+00019a60: 6f6e 7320 6c69 6b65 2060 606a 6178 2e6a  ons like ``jax.j
+00019a70: 6974 6060 3a3a 0a0a 2020 2020 3e3e 3e20  it``::..    >>> 
+00019a80: 636c 6173 7320 466f 6f28 6e6e 2e4d 6f64  class Foo(nn.Mod
+00019a90: 756c 6529 3a0a 2020 2020 2e2e 2e20 2020  ule):.    ...   
+00019aa0: 6465 6620 656e 636f 6465 2873 656c 662c  def encode(self,
+00019ab0: 2078 293a 0a20 2020 202e 2e2e 2020 2020   x):.    ...    
+00019ac0: 202e 2e2e 0a20 2020 202e 2e2e 2020 2064   ....    ...   d
+00019ad0: 6566 2064 6563 6f64 6528 7365 6c66 2c20  ef decode(self, 
+00019ae0: 7829 3a0a 2020 2020 2e2e 2e20 2020 2020  x):.    ...     
+00019af0: 2e2e 2e0a 0a20 2020 203e 3e3e 2064 6566  .....    >>> def
+00019b00: 2066 2866 6f6f 2c20 7829 3a0a 2020 2020   f(foo, x):.    
+00019b10: 2e2e 2e20 2020 7a20 3d20 666f 6f2e 656e  ...   z = foo.en
+00019b20: 636f 6465 2878 290a 2020 2020 2e2e 2e20  code(x).    ... 
+00019b30: 2020 7920 3d20 666f 6f2e 6465 636f 6465    y = foo.decode
+00019b40: 287a 290a 2020 2020 2e2e 2e20 2020 2320  (z).    ...   # 
+00019b50: 2e2e 2e0a 2020 2020 2e2e 2e20 2020 7265  ....    ...   re
+00019b60: 7475 726e 2079 0a0a 2020 2020 3e3e 3e20  turn y..    >>> 
+00019b70: 7661 7269 6162 6c65 7320 3d20 7b7d 0a20  variables = {}. 
+00019b80: 2020 203e 3e3e 2066 6f6f 203d 2046 6f6f     >>> foo = Foo
+00019b90: 2829 0a20 2020 203e 3e3e 2066 5f6a 6974  ().    >>> f_jit
+00019ba0: 7465 6420 3d20 6a61 782e 6a69 7428 6e6e  ted = jax.jit(nn
+00019bb0: 2e61 7070 6c79 2866 2c20 666f 6f29 290a  .apply(f, foo)).
+00019bc0: 2020 2020 3e3e 3e20 665f 6a69 7474 6564      >>> f_jitted
+00019bd0: 2876 6172 6961 626c 6573 2c20 6a6e 702e  (variables, jnp.
+00019be0: 6f6e 6573 2828 312c 2033 2929 290a 0a20  ones((1, 3))).. 
+00019bf0: 2041 7267 733a 0a20 2020 2066 6e3a 2054   Args:.    fn: T
+00019c00: 6865 2066 756e 6374 696f 6e20 7468 6174  he function that
+00019c10: 2073 686f 756c 6420 6265 2061 7070 6c69   should be appli
+00019c20: 6564 2e20 5468 6520 6669 7273 7420 6172  ed. The first ar
+00019c30: 6775 6d65 6e74 2070 6173 7365 6420 7769  gument passed wi
+00019c40: 6c6c 2062 650a 2020 2020 2020 6120 6d6f  ll be.      a mo
+00019c50: 6475 6c65 2069 6e73 7461 6e63 6520 6f66  dule instance of
+00019c60: 2074 6865 2060 606d 6f64 756c 6560 6020   the ``module`` 
+00019c70: 7769 7468 2076 6172 6961 626c 6573 2061  with variables a
+00019c80: 6e64 2052 4e47 7320 626f 756e 6420 746f  nd RNGs bound to
+00019c90: 2069 742e 0a20 2020 206d 6f64 756c 653a   it..    module:
+00019ca0: 2054 6865 2060 604d 6f64 756c 6560 6020   The ``Module`` 
+00019cb0: 7468 6174 2077 696c 6c20 6265 2075 7365  that will be use
+00019cc0: 6420 746f 2062 696e 6420 7661 7269 6162  d to bind variab
+00019cd0: 6c65 7320 616e 6420 524e 4773 2074 6f2e  les and RNGs to.
+00019ce0: 2054 6865 0a20 2020 2020 2060 604d 6f64   The.      ``Mod
+00019cf0: 756c 6560 6020 7061 7373 6564 2061 7320  ule`` passed as 
+00019d00: 7468 6520 6669 7273 7420 6172 6775 6d65  the first argume
+00019d10: 6e74 2074 6f20 6060 666e 6060 2077 696c  nt to ``fn`` wil
+00019d20: 6c20 6265 2061 2063 6c6f 6e65 206f 660a  l be a clone of.
+00019d30: 2020 2020 2020 6d6f 6475 6c65 2e0a 2020        module..  
+00019d40: 2020 6d75 7461 626c 653a 2043 616e 2062    mutable: Can b
+00019d50: 6520 626f 6f6c 2c20 7374 722c 206f 7220  e bool, str, or 
+00019d60: 6c69 7374 2e20 5370 6563 6966 6965 7320  list. Specifies 
+00019d70: 7768 6963 6820 636f 6c6c 6563 7469 6f6e  which collection
+00019d80: 7320 7368 6f75 6c64 2062 650a 2020 2020  s should be.    
+00019d90: 2020 7472 6561 7465 6420 6173 206d 7574    treated as mut
+00019da0: 6162 6c65 3a20 6060 626f 6f6c 6060 3a20  able: ``bool``: 
+00019db0: 616c 6c2f 6e6f 2063 6f6c 6c65 6374 696f  all/no collectio
+00019dc0: 6e73 2061 7265 206d 7574 6162 6c65 2e20  ns are mutable. 
+00019dd0: 6060 7374 7260 603a 2054 6865 0a20 2020  ``str``: The.   
+00019de0: 2020 206e 616d 6520 6f66 2061 2073 696e     name of a sin
+00019df0: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
+00019e00: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
+00019e10: 3a20 4120 6c69 7374 206f 6620 6e61 6d65  : A list of name
+00019e20: 7320 6f66 206d 7574 6162 6c65 0a20 2020  s of mutable.   
+00019e30: 2020 2063 6f6c 6c65 6374 696f 6e73 2e0a     collections..
+00019e40: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
+00019e50: 726d 6564 6961 7465 733a 2049 6620 6060  rmediates: If ``
+00019e60: 5472 7565 6060 2c20 6361 7074 7572 6573  True``, captures
+00019e70: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
+00019e80: 7475 726e 2076 616c 7565 7320 6f66 2061  turn values of a
+00019e90: 6c6c 0a20 2020 2020 204d 6f64 756c 6573  ll.      Modules
+00019ea0: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
+00019eb0: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
+00019ec0: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
+00019ed0: 6c74 2c20 6f6e 6c79 2074 6865 2072 6574  lt, only the ret
+00019ee0: 7572 6e0a 2020 2020 2020 7661 6c75 6573  urn.      values
+00019ef0: 206f 6620 616c 6c20 605f 5f63 616c 6c5f   of all `__call_
+00019f00: 5f60 206d 6574 686f 6473 2061 7265 2073  _` methods are s
+00019f10: 746f 7265 642e 2041 2066 756e 6374 696f  tored. A functio
+00019f20: 6e20 6361 6e20 6265 2070 6173 7365 6420  n can be passed 
+00019f30: 746f 0a20 2020 2020 2063 6861 6e67 6520  to.      change 
+00019f40: 7468 6520 6669 6c74 6572 2062 6568 6176  the filter behav
+00019f50: 696f 722e 2054 6865 2066 696c 7465 7220  ior. The filter 
+00019f60: 6675 6e63 7469 6f6e 2074 616b 6573 2074  function takes t
+00019f70: 6865 204d 6f64 756c 6520 696e 7374 616e  he Module instan
+00019f80: 6365 0a20 2020 2020 2061 6e64 206d 6574  ce.      and met
+00019f90: 686f 6420 6e61 6d65 2061 6e64 2072 6574  hod name and ret
+00019fa0: 7572 6e73 2061 2062 6f6f 6c20 696e 6469  urns a bool indi
+00019fb0: 6361 7469 6e67 2077 6865 7468 6572 2074  cating whether t
+00019fc0: 6865 206f 7574 7075 7420 6f66 2074 6861  he output of tha
+00019fd0: 740a 2020 2020 2020 6d65 7468 6f64 2069  t.      method i
+00019fe0: 6e76 6f63 6174 696f 6e20 7368 6f75 6c64  nvocation should
+00019ff0: 2062 6520 7374 6f72 6564 2e0a 0a20 2052   be stored...  R
+0001a000: 6574 7572 6e73 3a0a 2020 2020 5468 6520  eturns:.    The 
+0001a010: 6170 706c 7920 6675 6e63 7469 6f6e 2077  apply function w
+0001a020: 7261 7070 696e 6720 6060 666e 6060 2e0a  rapping ``fn``..
+0001a030: 2020 2222 220a 0a20 2040 6675 6e63 746f    """..  @functo
+0001a040: 6f6c 732e 7772 6170 7328 666e 290a 2020  ols.wraps(fn).  
+0001a050: 6465 6620 7363 6f70 655f 666e 2873 636f  def scope_fn(sco
+0001a060: 7065 2c20 2a61 7267 732c 202a 2a6b 7761  pe, *args, **kwa
+0001a070: 7267 7329 3a0a 2020 2020 5f63 6f6e 7465  rgs):.    _conte
+0001a080: 7874 2e63 6170 7475 7265 5f73 7461 636b  xt.capture_stack
+0001a090: 2e61 7070 656e 6428 6361 7074 7572 655f  .append(capture_
+0001a0a0: 696e 7465 726d 6564 6961 7465 7329 0a20  intermediates). 
+0001a0b0: 2020 2074 7279 3a0a 2020 2020 2020 7265     try:.      re
+0001a0c0: 7475 726e 2066 6e28 6d6f 6475 6c65 2e63  turn fn(module.c
+0001a0d0: 6c6f 6e65 2870 6172 656e 743d 7363 6f70  lone(parent=scop
+0001a0e0: 652c 205f 6465 6570 5f63 6c6f 6e65 3d54  e, _deep_clone=T
+0001a0f0: 7275 6529 2c20 2a61 7267 732c 202a 2a6b  rue), *args, **k
+0001a100: 7761 7267 7329 0a20 2020 2066 696e 616c  wargs).    final
+0001a110: 6c79 3a0a 2020 2020 2020 5f63 6f6e 7465  ly:.      _conte
+0001a120: 7874 2e63 6170 7475 7265 5f73 7461 636b  xt.capture_stack
+0001a130: 2e70 6f70 2829 0a0a 2020 6966 2063 6170  .pop()..  if cap
+0001a140: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
+0001a150: 6573 2069 7320 5472 7565 3a20 2023 2070  es is True:  # p
+0001a160: 796c 696e 743a 2064 6973 6162 6c65 3d67  ylint: disable=g
+0001a170: 2d62 6f6f 6c2d 6964 2d63 6f6d 7061 7269  -bool-id-compari
+0001a180: 736f 6e0a 2020 2020 6361 7074 7572 655f  son.    capture_
+0001a190: 696e 7465 726d 6564 6961 7465 7320 3d20  intermediates = 
+0001a1a0: 6361 7074 7572 655f 6361 6c6c 5f69 6e74  capture_call_int
+0001a1b0: 6572 6d65 6469 6174 6573 0a20 2069 6620  ermediates.  if 
+0001a1c0: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
+0001a1d0: 6961 7465 733a 0a20 2020 206d 7574 6162  iates:.    mutab
+0001a1e0: 6c65 203d 2075 6e69 6f6e 5f66 696c 7465  le = union_filte
+0001a1f0: 7273 286d 7574 6162 6c65 2c20 2769 6e74  rs(mutable, 'int
+0001a200: 6572 6d65 6469 6174 6573 2729 0a20 2072  ermediates').  r
+0001a210: 6574 7572 6e20 636f 7265 2e61 7070 6c79  eturn core.apply
+0001a220: 2873 636f 7065 5f66 6e2c 206d 7574 6162  (scope_fn, mutab
+0001a230: 6c65 3d6d 7574 6162 6c65 290a 0a0a 4074  le=mutable)...@t
+0001a240: 7261 6365 6261 636b 5f75 7469 6c2e 6170  raceback_util.ap
+0001a250: 695f 626f 756e 6461 7279 0a64 6566 2069  i_boundary.def i
+0001a260: 6e69 745f 7769 7468 5f6f 7574 7075 7428  nit_with_output(
+0001a270: 0a20 2066 6e3a 2043 616c 6c61 626c 655b  .  fn: Callable[
+0001a280: 2e2e 2e2c 2041 6e79 5d2c 0a20 206d 6f64  ..., Any],.  mod
+0001a290: 756c 653a 204d 6f64 756c 652c 0a20 206d  ule: Module,.  m
+0001a2a0: 7574 6162 6c65 3a20 436f 6c6c 6563 7469  utable: Collecti
+0001a2b0: 6f6e 4669 6c74 6572 203d 2044 656e 794c  onFilter = DenyL
+0001a2c0: 6973 7428 2769 6e74 6572 6d65 6469 6174  ist('intermediat
+0001a2d0: 6573 2729 2c0a 2020 6361 7074 7572 655f  es'),.  capture_
+0001a2e0: 696e 7465 726d 6564 6961 7465 733a 2055  intermediates: U
+0001a2f0: 6e69 6f6e 5b62 6f6f 6c2c 2043 616c 6c61  nion[bool, Calla
+0001a300: 626c 655b 5b4d 6f64 756c 652c 2073 7472  ble[[Module, str
+0001a310: 5d2c 2062 6f6f 6c5d 5d20 3d20 4661 6c73  ], bool]] = Fals
+0001a320: 652c 0a29 202d 3e20 4361 6c6c 6162 6c65  e,.) -> Callable
+0001a330: 5b2e 2e2e 2c20 5475 706c 655b 416e 792c  [..., Tuple[Any,
+0001a340: 2055 6e69 6f6e 5b46 726f 7a65 6e56 6172   Union[FrozenVar
+0001a350: 6961 626c 6544 6963 742c 2044 6963 745b  iableDict, Dict[
+0001a360: 7374 722c 2041 6e79 5d5d 5d5d 3a0a 2020  str, Any]]]]:.  
+0001a370: 2222 2243 7265 6174 6573 2061 6e20 696e  """Creates an in
+0001a380: 6974 2066 756e 6374 696f 6e20 746f 2063  it function to c
+0001a390: 616c 6c20 6060 666e 6060 2077 6974 6820  all ``fn`` with 
+0001a3a0: 6120 626f 756e 6420 6d6f 6475 6c65 2074  a bound module t
+0001a3b0: 6861 7420 616c 736f 2072 6574 7572 6e73  hat also returns
+0001a3c0: 2074 6865 2066 756e 6374 696f 6e20 6f75   the function ou
+0001a3d0: 7470 7574 732e 0a0a 2020 556e 6c69 6b65  tputs...  Unlike
+0001a3e0: 2060 604d 6f64 756c 652e 696e 6974 5f77   ``Module.init_w
+0001a3f0: 6974 685f 6f75 7470 7574 6060 2074 6869  ith_output`` thi
+0001a400: 7320 6675 6e63 7469 6f6e 2072 6574 7572  s function retur
+0001a410: 6e73 2061 206e 6577 2066 756e 6374 696f  ns a new functio
+0001a420: 6e20 7769 7468 0a20 2074 6865 2073 6967  n with.  the sig
+0001a430: 6e61 7475 7265 2060 6028 726e 6773 2c20  nature ``(rngs, 
+0001a440: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
+0001a450: 202d 3e20 2854 2c20 7661 7269 6162 6c65   -> (T, variable
+0001a460: 7329 6060 2077 6865 7265 2060 6054 6060  s)`` where ``T``
+0001a470: 2069 7320 7468 650a 2020 7265 7475 726e   is the.  return
+0001a480: 2074 7970 6520 6f66 2060 6066 6e60 602e   type of ``fn``.
+0001a490: 2054 6865 2072 6e67 7320 6361 6e20 6265   The rngs can be
+0001a4a0: 2061 2064 6963 7420 6f66 2050 524e 474b   a dict of PRNGK
+0001a4b0: 6579 7320 6f72 2061 2073 696e 676c 650a  eys or a single.
+0001a4c0: 2020 6060 6050 524e 474b 6579 6060 2077    ```PRNGKey`` w
+0001a4d0: 6869 6368 2069 7320 6571 7569 7661 6c65  hich is equivale
+0001a4e0: 6e74 2074 6f20 7061 7373 696e 6720 6120  nt to passing a 
+0001a4f0: 6469 6374 2077 6974 6820 6f6e 6520 5052  dict with one PR
+0001a500: 4e47 4b65 7920 7769 7468 2074 6865 0a20  NGKey with the. 
+0001a510: 206e 616d 6520 2270 6172 616d 7322 2e0a   name "params"..
+0001a520: 0a20 2054 6865 2069 6e69 7420 6675 6e63  .  The init func
+0001a530: 7469 6f6e 2074 6861 7420 6973 2072 6574  tion that is ret
+0001a540: 7572 6e65 6420 6361 6e20 6265 2064 6972  urned can be dir
+0001a550: 6563 746c 7920 636f 6d70 6f73 6564 2077  ectly composed w
+0001a560: 6974 680a 2020 4a41 5820 7472 616e 7366  ith.  JAX transf
+0001a570: 6f72 6d61 7469 6f6e 7320 6c69 6b65 2060  ormations like `
+0001a580: 606a 6178 2e6a 6974 6060 3a3a 0a0a 2020  `jax.jit``::..  
+0001a590: 2020 3e3e 3e20 636c 6173 7320 466f 6f28    >>> class Foo(
+0001a5a0: 6e6e 2e4d 6f64 756c 6529 3a0a 2020 2020  nn.Module):.    
+0001a5b0: 2e2e 2e20 2020 6465 6620 656e 636f 6465  ...   def encode
+0001a5c0: 2873 656c 662c 2078 293a 0a20 2020 202e  (self, x):.    .
+0001a5d0: 2e2e 2020 2020 202e 2e2e 0a20 2020 202e  ..     ....    .
+0001a5e0: 2e2e 2020 2064 6566 2064 6563 6f64 6528  ..   def decode(
+0001a5f0: 7365 6c66 2c20 7829 3a0a 2020 2020 2e2e  self, x):.    ..
+0001a600: 2e20 2020 2020 2e2e 2e0a 0a20 2020 203e  .     .....    >
+0001a610: 3e3e 2064 6566 2066 2866 6f6f 2c20 7829  >> def f(foo, x)
+0001a620: 3a0a 2020 2020 2e2e 2e20 2020 7a20 3d20  :.    ...   z = 
+0001a630: 666f 6f2e 656e 636f 6465 2878 290a 2020  foo.encode(x).  
+0001a640: 2020 2e2e 2e20 2020 7920 3d20 666f 6f2e    ...   y = foo.
+0001a650: 6465 636f 6465 287a 290a 2020 2020 2e2e  decode(z).    ..
+0001a660: 2e20 2020 2320 2e2e 2e0a 2020 2020 2e2e  .   # ....    ..
+0001a670: 2e20 2020 7265 7475 726e 2079 0a0a 2020  .   return y..  
+0001a680: 2020 3e3e 3e20 666f 6f20 3d20 466f 6f28    >>> foo = Foo(
+0001a690: 290a 2020 2020 3e3e 3e20 665f 6a69 7474  ).    >>> f_jitt
+0001a6a0: 6564 203d 206a 6178 2e6a 6974 286e 6e2e  ed = jax.jit(nn.
+0001a6b0: 696e 6974 5f77 6974 685f 6f75 7470 7574  init_with_output
+0001a6c0: 2866 2c20 666f 6f29 290a 2020 2020 3e3e  (f, foo)).    >>
+0001a6d0: 3e20 792c 2076 6172 6961 626c 6573 203d  > y, variables =
+0001a6e0: 2066 5f6a 6974 7465 6428 6a61 782e 7261   f_jitted(jax.ra
+0001a6f0: 6e64 6f6d 2e6b 6579 2830 292c 206a 6e70  ndom.key(0), jnp
+0001a700: 2e6f 6e65 7328 2831 2c20 3329 2929 0a0a  .ones((1, 3)))..
+0001a710: 2020 4172 6773 3a0a 2020 2020 666e 3a20    Args:.    fn: 
+0001a720: 5468 6520 6675 6e63 7469 6f6e 2074 6861  The function tha
+0001a730: 7420 7368 6f75 6c64 2062 6520 6170 706c  t should be appl
+0001a740: 6965 642e 2054 6865 2066 6972 7374 2061  ied. The first a
+0001a750: 7267 756d 656e 7420 7061 7373 6564 2077  rgument passed w
+0001a760: 696c 6c20 6265 0a20 2020 2020 2061 206d  ill be.      a m
+0001a770: 6f64 756c 6520 696e 7374 616e 6365 206f  odule instance o
+0001a780: 6620 7468 6520 6060 6d6f 6475 6c65 6060  f the ``module``
+0001a790: 2077 6974 6820 7661 7269 6162 6c65 7320   with variables 
+0001a7a0: 616e 6420 524e 4773 2062 6f75 6e64 2074  and RNGs bound t
+0001a7b0: 6f20 6974 2e0a 2020 2020 6d6f 6475 6c65  o it..    module
+0001a7c0: 3a20 5468 6520 6060 4d6f 6475 6c65 6060  : The ``Module``
+0001a7d0: 2074 6861 7420 7769 6c6c 2062 6520 7573   that will be us
+0001a7e0: 6564 2074 6f20 6269 6e64 2076 6172 6961  ed to bind varia
+0001a7f0: 626c 6573 2061 6e64 2052 4e47 7320 746f  bles and RNGs to
+0001a800: 2e20 5468 650a 2020 2020 2020 6060 4d6f  . The.      ``Mo
+0001a810: 6475 6c65 6060 2070 6173 7365 6420 6173  dule`` passed as
+0001a820: 2074 6865 2066 6972 7374 2061 7267 756d   the first argum
+0001a830: 656e 7420 746f 2060 6066 6e60 6020 7769  ent to ``fn`` wi
+0001a840: 6c6c 2062 6520 6120 636c 6f6e 6520 6f66  ll be a clone of
+0001a850: 0a20 2020 2020 206d 6f64 756c 652e 0a20  .      module.. 
+0001a860: 2020 206d 7574 6162 6c65 3a20 4361 6e20     mutable: Can 
+0001a870: 6265 2062 6f6f 6c2c 2073 7472 2c20 6f72  be bool, str, or
+0001a880: 206c 6973 742e 2053 7065 6369 6669 6573   list. Specifies
+0001a890: 2077 6869 6368 2063 6f6c 6c65 6374 696f   which collectio
+0001a8a0: 6e73 2073 686f 756c 6420 6265 0a20 2020  ns should be.   
+0001a8b0: 2020 2074 7265 6174 6564 2061 7320 6d75     treated as mu
+0001a8c0: 7461 626c 653a 2060 6062 6f6f 6c60 603a  table: ``bool``:
+0001a8d0: 2061 6c6c 2f6e 6f20 636f 6c6c 6563 7469   all/no collecti
+0001a8e0: 6f6e 7320 6172 6520 6d75 7461 626c 652e  ons are mutable.
+0001a8f0: 2060 6073 7472 6060 3a20 5468 650a 2020   ``str``: The.  
+0001a900: 2020 2020 6e61 6d65 206f 6620 6120 7369      name of a si
+0001a910: 6e67 6c65 206d 7574 6162 6c65 2063 6f6c  ngle mutable col
+0001a920: 6c65 6374 696f 6e2e 2060 606c 6973 7460  lection. ``list`
+0001a930: 603a 2041 206c 6973 7420 6f66 206e 616d  `: A list of nam
+0001a940: 6573 206f 6620 6d75 7461 626c 650a 2020  es of mutable.  
+0001a950: 2020 2020 636f 6c6c 6563 7469 6f6e 732e      collections.
+0001a960: 2042 7920 6465 6661 756c 742c 2061 6c6c   By default, all
+0001a970: 2063 6f6c 6c65 6374 696f 6e73 2065 7863   collections exc
+0001a980: 6570 7420 2269 6e74 6572 6d65 6469 6174  ept "intermediat
+0001a990: 6573 2220 6172 650a 2020 2020 2020 6d75  es" are.      mu
+0001a9a0: 7461 626c 652e 0a20 2020 2063 6170 7475  table..    captu
+0001a9b0: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
+0001a9c0: 3a20 4966 2060 6054 7275 6560 602c 2063  : If ``True``, c
+0001a9d0: 6170 7475 7265 7320 696e 7465 726d 6564  aptures intermed
+0001a9e0: 6961 7465 2072 6574 7572 6e20 7661 6c75  iate return valu
+0001a9f0: 6573 206f 6620 616c 6c0a 2020 2020 2020  es of all.      
+0001aa00: 4d6f 6475 6c65 7320 696e 7369 6465 2074  Modules inside t
+0001aa10: 6865 2022 696e 7465 726d 6564 6961 7465  he "intermediate
+0001aa20: 7322 2063 6f6c 6c65 6374 696f 6e2e 2042  s" collection. B
+0001aa30: 7920 6465 6661 756c 742c 206f 6e6c 7920  y default, only 
+0001aa40: 7468 6520 7265 7475 726e 0a20 2020 2020  the return.     
+0001aa50: 2076 616c 7565 7320 6f66 2061 6c6c 2060   values of all `
+0001aa60: 5f5f 6361 6c6c 5f5f 6020 6d65 7468 6f64  __call__` method
+0001aa70: 7320 6172 6520 7374 6f72 6564 2e20 4120  s are stored. A 
+0001aa80: 6675 6e63 7469 6f6e 2063 616e 2062 6520  function can be 
+0001aa90: 7061 7373 6564 2074 6f0a 2020 2020 2020  passed to.      
+0001aaa0: 6368 616e 6765 2074 6865 2066 696c 7465  change the filte
+0001aab0: 7220 6265 6861 7669 6f72 2e20 5468 6520  r behavior. The 
+0001aac0: 6669 6c74 6572 2066 756e 6374 696f 6e20  filter function 
+0001aad0: 7461 6b65 7320 7468 6520 4d6f 6475 6c65  takes the Module
+0001aae0: 2069 6e73 7461 6e63 650a 2020 2020 2020   instance.      
+0001aaf0: 616e 6420 6d65 7468 6f64 206e 616d 6520  and method name 
+0001ab00: 616e 6420 7265 7475 726e 7320 6120 626f  and returns a bo
+0001ab10: 6f6c 2069 6e64 6963 6174 696e 6720 7768  ol indicating wh
+0001ab20: 6574 6865 7220 7468 6520 6f75 7470 7574  ether the output
+0001ab30: 206f 6620 7468 6174 0a20 2020 2020 206d   of that.      m
+0001ab40: 6574 686f 6420 696e 766f 6361 7469 6f6e  ethod invocation
+0001ab50: 2073 686f 756c 6420 6265 2073 746f 7265   should be store
+0001ab60: 642e 0a0a 2020 5265 7475 726e 733a 0a20  d...  Returns:. 
+0001ab70: 2020 2054 6865 2069 6e69 7420 6675 6e63     The init func
+0001ab80: 7469 6f6e 2077 7261 7070 696e 6720 6060  tion wrapping ``
+0001ab90: 666e 6060 2e0a 2020 2222 220a 0a20 2040  fn``..  """..  @
+0001aba0: 6675 6e63 746f 6f6c 732e 7772 6170 7328  functools.wraps(
+0001abb0: 666e 290a 2020 6465 6620 7363 6f70 655f  fn).  def scope_
+0001abc0: 666e 2873 636f 7065 2c20 2a61 7267 732c  fn(scope, *args,
+0001abd0: 202a 2a6b 7761 7267 7329 3a0a 2020 2020   **kwargs):.    
+0001abe0: 5f63 6f6e 7465 7874 2e63 6170 7475 7265  _context.capture
+0001abf0: 5f73 7461 636b 2e61 7070 656e 6428 6361  _stack.append(ca
+0001ac00: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
+0001ac10: 7465 7329 0a20 2020 2074 7279 3a0a 2020  tes).    try:.  
+0001ac20: 2020 2020 7265 7475 726e 2066 6e28 6d6f      return fn(mo
+0001ac30: 6475 6c65 2e63 6c6f 6e65 2870 6172 656e  dule.clone(paren
+0001ac40: 743d 7363 6f70 652c 205f 6465 6570 5f63  t=scope, _deep_c
+0001ac50: 6c6f 6e65 3d54 7275 6529 2c20 2a61 7267  lone=True), *arg
+0001ac60: 732c 202a 2a6b 7761 7267 7329 0a20 2020  s, **kwargs).   
+0001ac70: 2066 696e 616c 6c79 3a0a 2020 2020 2020   finally:.      
+0001ac80: 5f63 6f6e 7465 7874 2e63 6170 7475 7265  _context.capture
+0001ac90: 5f73 7461 636b 2e70 6f70 2829 0a0a 2020  _stack.pop()..  
+0001aca0: 6966 2063 6170 7475 7265 5f69 6e74 6572  if capture_inter
+0001acb0: 6d65 6469 6174 6573 2069 7320 5472 7565  mediates is True
+0001acc0: 3a20 2023 2070 796c 696e 743a 2064 6973  :  # pylint: dis
+0001acd0: 6162 6c65 3d67 2d62 6f6f 6c2d 6964 2d63  able=g-bool-id-c
+0001ace0: 6f6d 7061 7269 736f 6e0a 2020 2020 6361  omparison.    ca
+0001acf0: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
+0001ad00: 7465 7320 3d20 6361 7074 7572 655f 6361  tes = capture_ca
+0001ad10: 6c6c 5f69 6e74 6572 6d65 6469 6174 6573  ll_intermediates
+0001ad20: 0a20 2069 6620 6361 7074 7572 655f 696e  .  if capture_in
+0001ad30: 7465 726d 6564 6961 7465 733a 0a20 2020  termediates:.   
+0001ad40: 206d 7574 6162 6c65 203d 2075 6e69 6f6e   mutable = union
+0001ad50: 5f66 696c 7465 7273 286d 7574 6162 6c65  _filters(mutable
+0001ad60: 2c20 2769 6e74 6572 6d65 6469 6174 6573  , 'intermediates
+0001ad70: 2729 0a20 2072 6574 7572 6e20 636f 7265  ').  return core
+0001ad80: 2e69 6e69 7428 7363 6f70 655f 666e 2c20  .init(scope_fn, 
+0001ad90: 6d75 7461 626c 653d 6d75 7461 626c 6529  mutable=mutable)
+0001ada0: 0a0a 0a40 7472 6163 6562 6163 6b5f 7574  ...@traceback_ut
+0001adb0: 696c 2e61 7069 5f62 6f75 6e64 6172 790a  il.api_boundary.
+0001adc0: 6465 6620 696e 6974 280a 2020 666e 3a20  def init(.  fn: 
+0001add0: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
+0001ade0: 795d 2c0a 2020 6d6f 6475 6c65 3a20 4d6f  y],.  module: Mo
+0001adf0: 6475 6c65 2c0a 2020 6d75 7461 626c 653a  dule,.  mutable:
+0001ae00: 2043 6f6c 6c65 6374 696f 6e46 696c 7465   CollectionFilte
+0001ae10: 7220 3d20 4465 6e79 4c69 7374 2827 696e  r = DenyList('in
+0001ae20: 7465 726d 6564 6961 7465 7327 292c 0a20  termediates'),. 
+0001ae30: 2063 6170 7475 7265 5f69 6e74 6572 6d65   capture_interme
+0001ae40: 6469 6174 6573 3a20 556e 696f 6e5b 626f  diates: Union[bo
+0001ae50: 6f6c 2c20 4361 6c6c 6162 6c65 5b5b 4d6f  ol, Callable[[Mo
+0001ae60: 6475 6c65 2c20 7374 725d 2c20 626f 6f6c  dule, str], bool
+0001ae70: 5d5d 203d 2046 616c 7365 2c0a 2920 2d3e  ]] = False,.) ->
+0001ae80: 2043 616c 6c61 626c 655b 2e2e 2e2c 2055   Callable[..., U
+0001ae90: 6e69 6f6e 5b46 726f 7a65 6e56 6172 6961  nion[FrozenVaria
+0001aea0: 626c 6544 6963 742c 2044 6963 745b 7374  bleDict, Dict[st
+0001aeb0: 722c 2041 6e79 5d5d 5d3a 0a20 2022 2222  r, Any]]]:.  """
+0001aec0: 4372 6561 7465 7320 616e 2069 6e69 7420  Creates an init 
+0001aed0: 6675 6e63 7469 6f6e 2074 6f20 6361 6c6c  function to call
+0001aee0: 2060 6066 6e60 6020 7769 7468 2061 2062   ``fn`` with a b
+0001aef0: 6f75 6e64 206d 6f64 756c 652e 0a0a 2020  ound module...  
+0001af00: 556e 6c69 6b65 2060 604d 6f64 756c 652e  Unlike ``Module.
+0001af10: 696e 6974 6060 2074 6869 7320 6675 6e63  init`` this func
+0001af20: 7469 6f6e 2072 6574 7572 6e73 2061 206e  tion returns a n
+0001af30: 6577 2066 756e 6374 696f 6e20 7769 7468  ew function with
+0001af40: 2074 6865 2073 6967 6e61 7475 7265 0a20   the signature. 
+0001af50: 2060 6028 726e 6773 2c20 2a61 7267 732c   ``(rngs, *args,
+0001af60: 202a 2a6b 7761 7267 7329 202d 3e20 7661   **kwargs) -> va
+0001af70: 7269 6162 6c65 7360 602e 0a20 2054 6865  riables``..  The
+0001af80: 2072 6e67 7320 6361 6e20 6265 2061 2064   rngs can be a d
+0001af90: 6963 7420 6f66 2050 524e 474b 6579 7320  ict of PRNGKeys 
+0001afa0: 6f72 2061 2073 696e 676c 6520 6060 6050  or a single ```P
+0001afb0: 524e 474b 6579 6060 2077 6869 6368 2069  RNGKey`` which i
+0001afc0: 730a 2020 6571 7569 7661 6c65 6e74 2074  s.  equivalent t
+0001afd0: 6f20 7061 7373 696e 6720 6120 6469 6374  o passing a dict
+0001afe0: 2077 6974 6820 6f6e 6520 5052 4e47 4b65   with one PRNGKe
+0001aff0: 7920 7769 7468 2074 6865 206e 616d 6520  y with the name 
+0001b000: 2270 6172 616d 7322 2e0a 0a20 2054 6865  "params"...  The
+0001b010: 2069 6e69 7420 6675 6e63 7469 6f6e 2074   init function t
+0001b020: 6861 7420 6973 2072 6574 7572 6e65 6420  hat is returned 
+0001b030: 6361 6e20 6265 2064 6972 6563 746c 7920  can be directly 
+0001b040: 636f 6d70 6f73 6564 2077 6974 680a 2020  composed with.  
+0001b050: 4a41 5820 7472 616e 7366 6f72 6d61 7469  JAX transformati
+0001b060: 6f6e 7320 6c69 6b65 2060 606a 6178 2e6a  ons like ``jax.j
+0001b070: 6974 6060 3a3a 0a0a 2020 2020 3e3e 3e20  it``::..    >>> 
+0001b080: 636c 6173 7320 466f 6f28 6e6e 2e4d 6f64  class Foo(nn.Mod
+0001b090: 756c 6529 3a0a 2020 2020 2e2e 2e20 2020  ule):.    ...   
+0001b0a0: 6465 6620 656e 636f 6465 2873 656c 662c  def encode(self,
+0001b0b0: 2078 293a 0a20 2020 202e 2e2e 2020 2020   x):.    ...    
+0001b0c0: 202e 2e2e 0a20 2020 202e 2e2e 2020 2064   ....    ...   d
+0001b0d0: 6566 2064 6563 6f64 6528 7365 6c66 2c20  ef decode(self, 
+0001b0e0: 7829 3a0a 2020 2020 2e2e 2e20 2020 2020  x):.    ...     
+0001b0f0: 2e2e 2e0a 0a20 2020 203e 3e3e 2064 6566  .....    >>> def
+0001b100: 2066 2866 6f6f 2c20 7829 3a0a 2020 2020   f(foo, x):.    
+0001b110: 2e2e 2e20 2020 7a20 3d20 666f 6f2e 656e  ...   z = foo.en
+0001b120: 636f 6465 2878 290a 2020 2020 2e2e 2e20  code(x).    ... 
+0001b130: 2020 7920 3d20 666f 6f2e 6465 636f 6465    y = foo.decode
+0001b140: 287a 290a 2020 2020 2e2e 2e20 2020 2320  (z).    ...   # 
+0001b150: 2e2e 2e0a 2020 2020 2e2e 2e20 2020 7265  ....    ...   re
+0001b160: 7475 726e 2079 0a0a 2020 2020 3e3e 3e20  turn y..    >>> 
+0001b170: 666f 6f20 3d20 466f 6f28 290a 2020 2020  foo = Foo().    
+0001b180: 3e3e 3e20 665f 6a69 7474 6564 203d 206a  >>> f_jitted = j
+0001b190: 6178 2e6a 6974 286e 6e2e 696e 6974 2866  ax.jit(nn.init(f
+0001b1a0: 2c20 666f 6f29 290a 2020 2020 3e3e 3e20  , foo)).    >>> 
+0001b1b0: 7661 7269 6162 6c65 7320 3d20 665f 6a69  variables = f_ji
+0001b1c0: 7474 6564 286a 6178 2e72 616e 646f 6d2e  tted(jax.random.
+0001b1d0: 6b65 7928 3029 2c20 6a6e 702e 6f6e 6573  key(0), jnp.ones
+0001b1e0: 2828 312c 2033 2929 290a 0a20 2041 7267  ((1, 3)))..  Arg
+0001b1f0: 733a 0a20 2020 2066 6e3a 2054 6865 2066  s:.    fn: The f
+0001b200: 756e 6374 696f 6e20 7468 6174 2073 686f  unction that sho
+0001b210: 756c 6420 6265 2061 7070 6c69 6564 2e20  uld be applied. 
+0001b220: 5468 6520 6669 7273 7420 6172 6775 6d65  The first argume
+0001b230: 6e74 2070 6173 7365 6420 7769 6c6c 2062  nt passed will b
+0001b240: 650a 2020 2020 2020 6120 6d6f 6475 6c65  e.      a module
+0001b250: 2069 6e73 7461 6e63 6520 6f66 2074 6865   instance of the
+0001b260: 2060 606d 6f64 756c 6560 6020 7769 7468   ``module`` with
+0001b270: 2076 6172 6961 626c 6573 2061 6e64 2052   variables and R
+0001b280: 4e47 7320 626f 756e 6420 746f 2069 742e  NGs bound to it.
+0001b290: 0a20 2020 206d 6f64 756c 653a 2054 6865  .    module: The
+0001b2a0: 2060 604d 6f64 756c 6560 6020 7468 6174   ``Module`` that
+0001b2b0: 2077 696c 6c20 6265 2075 7365 6420 746f   will be used to
+0001b2c0: 2062 696e 6420 7661 7269 6162 6c65 7320   bind variables 
+0001b2d0: 616e 6420 524e 4773 2074 6f2e 2054 6865  and RNGs to. The
+0001b2e0: 0a20 2020 2020 2060 604d 6f64 756c 6560  .      ``Module`
+0001b2f0: 6020 7061 7373 6564 2061 7320 7468 6520  ` passed as the 
+0001b300: 6669 7273 7420 6172 6775 6d65 6e74 2074  first argument t
+0001b310: 6f20 6060 666e 6060 2077 696c 6c20 6265  o ``fn`` will be
+0001b320: 2061 2063 6c6f 6e65 206f 660a 2020 2020   a clone of.    
+0001b330: 2020 6d6f 6475 6c65 2e0a 2020 2020 6d75    module..    mu
+0001b340: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
+0001b350: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
+0001b360: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
+0001b370: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
+0001b380: 6f75 6c64 2062 650a 2020 2020 2020 7472  ould be.      tr
+0001b390: 6561 7465 6420 6173 206d 7574 6162 6c65  eated as mutable
+0001b3a0: 3a20 6060 626f 6f6c 6060 3a20 616c 6c2f  : ``bool``: all/
+0001b3b0: 6e6f 2063 6f6c 6c65 6374 696f 6e73 2061  no collections a
+0001b3c0: 7265 206d 7574 6162 6c65 2e20 6060 7374  re mutable. ``st
+0001b3d0: 7260 603a 2054 6865 0a20 2020 2020 206e  r``: The.      n
+0001b3e0: 616d 6520 6f66 2061 2073 696e 676c 6520  ame of a single 
+0001b3f0: 6d75 7461 626c 6520 636f 6c6c 6563 7469  mutable collecti
+0001b400: 6f6e 2e20 6060 6c69 7374 6060 3a20 4120  on. ``list``: A 
+0001b410: 6c69 7374 206f 6620 6e61 6d65 7320 6f66  list of names of
+0001b420: 206d 7574 6162 6c65 0a20 2020 2020 2063   mutable.      c
+0001b430: 6f6c 6c65 6374 696f 6e73 2e20 4279 2064  ollections. By d
+0001b440: 6566 6175 6c74 2c20 616c 6c20 636f 6c6c  efault, all coll
+0001b450: 6563 7469 6f6e 7320 6578 6365 7074 2022  ections except "
+0001b460: 696e 7465 726d 6564 6961 7465 7322 2061  intermediates" a
+0001b470: 7265 0a20 2020 2020 206d 7574 6162 6c65  re.      mutable
+0001b480: 2e0a 2020 2020 6361 7074 7572 655f 696e  ..    capture_in
+0001b490: 7465 726d 6564 6961 7465 733a 2049 6620  termediates: If 
+0001b4a0: 6054 7275 6560 2c20 6361 7074 7572 6573  `True`, captures
+0001b4b0: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
+0001b4c0: 7475 726e 2076 616c 7565 7320 6f66 2061  turn values of a
+0001b4d0: 6c6c 0a20 2020 2020 204d 6f64 756c 6573  ll.      Modules
+0001b4e0: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
+0001b4f0: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
+0001b500: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
+0001b510: 6c74 2c20 6f6e 6c79 2074 6865 2072 6574  lt, only the ret
+0001b520: 7572 6e0a 2020 2020 2020 7661 6c75 6573  urn.      values
+0001b530: 206f 6620 616c 6c20 605f 5f63 616c 6c5f   of all `__call_
+0001b540: 5f60 206d 6574 686f 6473 2061 7265 2073  _` methods are s
+0001b550: 746f 7265 642e 2041 2066 756e 6374 696f  tored. A functio
+0001b560: 6e20 6361 6e20 6265 2070 6173 7365 6420  n can be passed 
+0001b570: 746f 0a20 2020 2020 2063 6861 6e67 6520  to.      change 
+0001b580: 7468 6520 6669 6c74 6572 2062 6568 6176  the filter behav
+0001b590: 696f 722e 2054 6865 2066 696c 7465 7220  ior. The filter 
+0001b5a0: 6675 6e63 7469 6f6e 2074 616b 6573 2074  function takes t
+0001b5b0: 6865 204d 6f64 756c 6520 696e 7374 616e  he Module instan
+0001b5c0: 6365 0a20 2020 2020 2061 6e64 206d 6574  ce.      and met
+0001b5d0: 686f 6420 6e61 6d65 2061 6e64 2072 6574  hod name and ret
+0001b5e0: 7572 6e73 2061 2062 6f6f 6c20 696e 6469  urns a bool indi
+0001b5f0: 6361 7469 6e67 2077 6865 7468 6572 2074  cating whether t
+0001b600: 6865 206f 7574 7075 7420 6f66 2074 6861  he output of tha
+0001b610: 740a 2020 2020 2020 6d65 7468 6f64 2069  t.      method i
+0001b620: 6e76 6f63 6174 696f 6e20 7368 6f75 6c64  nvocation should
+0001b630: 2062 6520 7374 6f72 6564 2e0a 0a20 2052   be stored...  R
+0001b640: 6574 7572 6e73 3a0a 2020 2020 5468 6520  eturns:.    The 
+0001b650: 696e 6974 2066 756e 6374 696f 6e20 7772  init function wr
+0001b660: 6170 7069 6e67 2060 6066 6e60 602e 0a20  apping ``fn``.. 
+0001b670: 2022 2222 0a20 2069 6e69 745f 666e 203d   """.  init_fn =
+0001b680: 2069 6e69 745f 7769 7468 5f6f 7574 7075   init_with_outpu
+0001b690: 7428 666e 2c20 6d6f 6475 6c65 2c20 6d75  t(fn, module, mu
+0001b6a0: 7461 626c 652c 2063 6170 7475 7265 5f69  table, capture_i
+0001b6b0: 6e74 6572 6d65 6469 6174 6573 290a 0a20  ntermediates).. 
+0001b6c0: 2040 6675 6e63 746f 6f6c 732e 7772 6170   @functools.wrap
+0001b6d0: 7328 696e 6974 5f66 6e29 0a20 2064 6566  s(init_fn).  def
+0001b6e0: 2069 6e69 745f 7772 6170 7065 7228 2a61   init_wrapper(*a
+0001b6f0: 7267 732c 202a 2a6b 7761 7267 7329 3a0a  rgs, **kwargs):.
+0001b700: 2020 2020 7265 7475 726e 2069 6e69 745f      return init_
+0001b710: 666e 282a 6172 6773 2c20 2a2a 6b77 6172  fn(*args, **kwar
+0001b720: 6773 295b 315d 0a0a 2020 7265 7475 726e  gs)[1]..  return
+0001b730: 2069 6e69 745f 7772 6170 7065 720a 0a0a   init_wrapper...
+0001b740: 2320 544f 444f 2863 6761 7263 6961 6529  # TODO(cgarciae)
+0001b750: 3a20 7765 2061 7265 2064 6566 696e 696e  : we are definin
+0001b760: 6720 436f 6d70 6163 744e 616d 6553 636f  g CompactNameSco
+0001b770: 7065 206a 7573 7420 746f 0a23 2061 766f  pe just to.# avo
+0001b780: 6964 2061 2070 7974 7970 6520 6275 6720  id a pytype bug 
+0001b790: 7769 7468 2074 6865 2046 6c61 7820 6f76  with the Flax ov
+0001b7a0: 6572 6c61 792e 2057 6520 7368 6f75 6c64  erlay. We should
+0001b7b0: 2061 696d 2074 6f0a 2320 7265 6d6f 7665   aim to.# remove
+0001b7c0: 2069 6e20 7468 6520 6174 2073 6f6d 6520   in the at some 
+0001b7d0: 706f 696e 7420 6173 2069 7473 206e 6f74  point as its not
+0001b7e0: 2065 7267 6f6e 6f6d 6963 2e0a 6966 206e   ergonomic..if n
+0001b7f0: 6f74 2074 7970 696e 672e 5459 5045 5f43  ot typing.TYPE_C
+0001b800: 4845 434b 494e 473a 0a0a 2020 636c 6173  HECKING:..  clas
+0001b810: 7320 436f 6d70 6163 744e 616d 6553 636f  s CompactNameSco
+0001b820: 7065 284d 6f64 756c 6529 3a0a 2020 2020  pe(Module):.    
+0001b830: 666e 3a20 4361 6c6c 6162 6c65 0a20 2020  fn: Callable.   
+0001b840: 206d 6f64 756c 655f 666e 3a20 4361 6c6c   module_fn: Call
+0001b850: 6162 6c65 5b5b 5d2c 204d 6f64 756c 655d  able[[], Module]
+0001b860: 0a0a 2020 2020 4063 6f6d 7061 6374 0a20  ..    @compact. 
+0001b870: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
+0001b880: 7365 6c66 2c20 2a61 7267 732c 202a 2a6b  self, *args, **k
+0001b890: 7761 7267 7329 202d 3e20 416e 793a 0a20  wargs) -> Any:. 
+0001b8a0: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
+0001b8b0: 2e66 6e28 7365 6c66 2e6d 6f64 756c 655f  .fn(self.module_
+0001b8c0: 666e 2829 2c20 2a61 7267 732c 202a 2a6b  fn(), *args, **k
+0001b8d0: 7761 7267 7329 0a65 6c73 653a 0a0a 2020  wargs).else:..  
+0001b8e0: 4064 6174 6163 6c61 7373 6573 2e64 6174  @dataclasses.dat
+0001b8f0: 6163 6c61 7373 0a20 2063 6c61 7373 2043  aclass.  class C
+0001b900: 6f6d 7061 6374 4e61 6d65 5363 6f70 653a  ompactNameScope:
+0001b910: 0a20 2020 2066 6e3a 2043 616c 6c61 626c  .    fn: Callabl
+0001b920: 650a 2020 2020 6d6f 6475 6c65 5f66 6e3a  e.    module_fn:
+0001b930: 2043 616c 6c61 626c 650a 2020 2020 6e61   Callable.    na
+0001b940: 6d65 3a20 7374 720a 0a20 2020 2064 6566  me: str..    def
+0001b950: 205f 5f63 616c 6c5f 5f28 7365 6c66 2c20   __call__(self, 
+0001b960: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
+0001b970: 202d 3e20 416e 793a 0a20 2020 2020 202e   -> Any:.      .
+0001b980: 2e2e 0a                                  ...
```

### Comparing `flax-0.8.2/flax/linen/normalization.py` & `flax-0.8.3/flax/linen/normalization.py`

 * *Files 2% similar despite different names*

```diff
@@ -53,27 +53,29 @@
   if jnp.iscomplexobj(x):
     return lax.square(lax.real(x)) + lax.square(lax.imag(x))
   else:
     return lax.square(x)
 
 
 def _compute_stats(
-  x: Array,
-  axes: Axes,
-  dtype: Optional[Dtype],
-  axis_name: Optional[str] = None,
-  axis_index_groups: Any = None,
-  use_mean: bool = True,
-  use_fast_variance: bool = True,
-  mask: Optional[Array] = None,
+    x: Array,
+    axes: Axes,
+    dtype: Optional[Dtype],
+    axis_name: Optional[str] = None,
+    axis_index_groups: Any = None,
+    use_mean: bool = True,
+    use_fast_variance: bool = True,
+    mask: Optional[Array] = None,
+    force_float32_reductions=True,
 ):
   """Computes mean and variance statistics.
 
   This implementation takes care of a few important details:
-  - Computes in float32 precision for stability in half precision training.
+  - By default, computes in float32 precision for stability
+    in half precision training.
   - If `use_fast_variance` is `True`, mean and variance are computed using
     Var = E[|x|^2] - |E[x]|^2, instead of Var = E[|x - E[x]|^2]), in a single
     XLA fusion.
   - Clips negative variances to zero which can happen due to
     roundoff errors. This avoids downstream NaNs.
   - Supports averaging across a parallel axis and subgroups of a parallel axis
     with a single `lax.pmean` call to avoid latency.
@@ -89,25 +91,28 @@
       and XLA:SPMD will insert the necessary collectives.
     axis_index_groups: Optional axis indices.
     use_mean: If true, calculate the mean from the input and use it when
       computing the variance. If false, set the mean to zero and compute the
       variance without subtracting the mean.
     use_fast_variance: If true, use a faster, but less numerically stable,
       calculation for the variance.
-    mask: Binary array of shape broadcastable to `inputs` tensor, indicating
-      the positions for which the mean and variance should be computed.
+    mask: Binary array of shape broadcastable to `inputs` tensor, indicating the
+      positions for which the mean and variance should be computed.
+    force_float32_reductions: If false, this will skip float32 promotion and use
+      the input dtype or inherited dtype from ``x``.
 
   Returns:
     A pair ``(mean, var)``.
   """
   if dtype is None:
     dtype = jnp.result_type(x)
   # promote x to at least float32, this avoids half precision computation
   # but preserves double or complex floating points
-  dtype = jnp.promote_types(dtype, jnp.float32)
+  if force_float32_reductions:
+    dtype = jnp.promote_types(dtype, jnp.float32)
   x = jnp.asarray(x, dtype)
   axes = _canonicalize_axes(x.ndim, axes)
 
   def maybe_distributed_mean(*xs, mask=None):
     mus = tuple(x.mean(axes, where=mask) for x in xs)
     if axis_name is None:
       return mus if len(xs) > 1 else mus[0]
@@ -295,31 +300,32 @@
   use_bias: bool = True
   use_scale: bool = True
   bias_init: Initializer = initializers.zeros
   scale_init: Initializer = initializers.ones
   axis_name: Optional[str] = None
   axis_index_groups: Any = None
   use_fast_variance: bool = True
+  force_float32_reductions: bool = True
 
   @compact
   def __call__(
       self,
       x,
       use_running_average: Optional[bool] = None,
       *,
       mask: Optional[jax.Array] = None,
   ):
     """Normalizes the input using batch statistics.
 
-    NOTE:
-    During initialization (when ``self.is_initializing()`` is ``True``) the running
-    average of the batch statistics will not be updated. Therefore, the inputs
-    fed during initialization don't need to match that of the actual input
-    distribution and the reduction axis (set with ``axis_name``) does not have
-    to exist.
+    .. note::
+      During initialization (when ``self.is_initializing()`` is ``True``) the running
+      average of the batch statistics will not be updated. Therefore, the inputs
+      fed during initialization don't need to match that of the actual input
+      distribution and the reduction axis (set with ``axis_name``) does not have
+      to exist.
 
     Args:
       x: the input to be normalized.
       use_running_average: if true, the statistics stored in batch_stats will be
         used instead of computing the batch statistics on the input.
       mask: Binary array of shape broadcastable to ``inputs`` tensor, indicating
         the positions for which the mean and variance should be computed.
@@ -345,21 +351,22 @@
       'batch_stats', 'var', lambda s: jnp.ones(s, jnp.float32), feature_shape
     )
 
     if use_running_average:
       mean, var = ra_mean.value, ra_var.value
     else:
       mean, var = _compute_stats(
-        x,
-        reduction_axes,
-        dtype=self.dtype,
-        axis_name=self.axis_name if not self.is_initializing() else None,
-        axis_index_groups=self.axis_index_groups,
-        use_fast_variance=self.use_fast_variance,
-        mask=mask,
+          x,
+          reduction_axes,
+          dtype=self.dtype,
+          axis_name=self.axis_name if not self.is_initializing() else None,
+          axis_index_groups=self.axis_index_groups,
+          use_fast_variance=self.use_fast_variance,
+          mask=mask,
+          force_float32_reductions=self.force_float32_reductions,
       )
 
       if not self.is_initializing():
         ra_mean.value = (
           self.momentum * ra_mean.value + (1 - self.momentum) * mean
         )
         ra_var.value = self.momentum * ra_var.value + (1 - self.momentum) * var
@@ -385,17 +392,18 @@
   """Layer normalization (https://arxiv.org/abs/1607.06450).
 
   LayerNorm normalizes the activations of the layer for each given example in a
   batch independently, rather than across a batch like Batch Normalization.
   i.e. applies a transformation that maintains the mean activation within
   each example close to 0 and the activation standard deviation close to 1.
 
-  NOTE: This normalization operation is identical to InstanceNorm and GroupNorm;
-  the difference is simply which axes are reduced and the shape of the feature axes
-  (i.e. the shape of the learnable scale and bias parameters).
+  .. note::
+    This normalization operation is identical to InstanceNorm and GroupNorm;
+    the difference is simply which axes are reduced and the shape of the feature
+    axes (i.e. the shape of the learnable scale and bias parameters).
 
   Example usage::
 
     >>> import flax.linen as nn
     >>> import jax
     >>> import numpy as np
 
@@ -450,35 +458,37 @@
   bias_init: Initializer = initializers.zeros
   scale_init: Initializer = initializers.ones
   reduction_axes: Axes = -1
   feature_axes: Axes = -1
   axis_name: Optional[str] = None
   axis_index_groups: Any = None
   use_fast_variance: bool = True
+  force_float32_reductions: bool = True
 
   @compact
   def __call__(self, x, *, mask: Optional[jax.Array] = None):
     """Applies layer normalization on the input.
 
     Args:
       x: the inputs
       mask: Binary array of shape broadcastable to ``inputs`` tensor, indicating
         the positions for which the mean and variance should be computed.
 
     Returns:
       Normalized inputs (the same shape as inputs).
     """
     mean, var = _compute_stats(
-      x,
-      self.reduction_axes,
-      self.dtype,
-      self.axis_name,
-      self.axis_index_groups,
-      use_fast_variance=self.use_fast_variance,
-      mask=mask,
+        x,
+        self.reduction_axes,
+        self.dtype,
+        self.axis_name,
+        self.axis_index_groups,
+        use_fast_variance=self.use_fast_variance,
+        mask=mask,
+        force_float32_reductions=self.force_float32_reductions,
     )
 
     return _normalize(
       self,
       x,
       mean,
       var,
@@ -547,36 +557,38 @@
   use_scale: bool = True
   scale_init: Initializer = initializers.ones
   reduction_axes: Axes = -1
   feature_axes: Axes = -1
   axis_name: Optional[str] = None
   axis_index_groups: Any = None
   use_fast_variance: bool = True
+  force_float32_reductions: bool = True
 
   @compact
   def __call__(self, x, *, mask: Optional[jax.Array] = None):
     """Applies RMS layer normalization on the input.
 
     Args:
       x: the inputs
       mask: Binary array of shape broadcastable to ``inputs`` tensor, indicating
         the positions for which the mean and variance should be computed.
 
     Returns:
       Normalized inputs (the same shape as inputs).
     """
     mean, var = _compute_stats(
-      x,
-      self.reduction_axes,
-      self.dtype,
-      self.axis_name,
-      self.axis_index_groups,
-      use_mean=False,
-      use_fast_variance=self.use_fast_variance,
-      mask=mask,
+        x,
+        self.reduction_axes,
+        self.dtype,
+        self.axis_name,
+        self.axis_index_groups,
+        use_mean=False,
+        use_fast_variance=self.use_fast_variance,
+        mask=mask,
+        force_float32_reductions=self.force_float32_reductions,
     )
 
     return _normalize(
       self,
       x,
       mean,
       var,
@@ -598,16 +610,17 @@
   This op is similar to batch normalization, but statistics are shared across
   equally-sized groups of channels and not shared across batch dimension.
   Thus, group normalization does not depend on the batch composition and does
   not require maintaining internal state for storing statistics.
   The user should either specify the total number of channel groups or the
   number of channels per group.
 
-  NOTE: LayerNorm is a special case of GroupNorm where ``num_groups=1``, and
-  InstanceNorm is a special case of GroupNorm where ``group_size=1``.
+  .. note::
+    LayerNorm is a special case of GroupNorm where ``num_groups=1``, and
+    InstanceNorm is a special case of GroupNorm where ``group_size=1``.
 
   Example usage::
 
     >>> import flax.linen as nn
     >>> import jax
     >>> import numpy as np
 
@@ -669,14 +682,15 @@
   use_scale: bool = True
   bias_init: Initializer = initializers.zeros
   scale_init: Initializer = initializers.ones
   reduction_axes: Optional[Axes] = None
   axis_name: Optional[str] = None
   axis_index_groups: Any = None
   use_fast_variance: bool = True
+  force_float32_reductions: bool = True
 
   @compact
   def __call__(self, x, *, mask: Optional[jax.Array] = None):
     """Applies group normalization to the input (arxiv.org/abs/1803.08494).
 
     Args:
       x: the input of shape ``...C`` where ``C`` is a channels dimension and ``...``
@@ -736,21 +750,22 @@
     group_size = x.shape[-1] // num_groups
     group_shape = x.shape[:-1] + (num_groups, group_size)
 
     if mask is not None:
       mask = mask.reshape(mask.shape[:-1] + (num_groups, group_size))
 
     mean, var = _compute_stats(
-      x.reshape(group_shape),
-      list(reduction_axes[:-1]) + [-1],
-      self.dtype,
-      self.axis_name,
-      self.axis_index_groups,
-      use_fast_variance=self.use_fast_variance,
-      mask=mask,
+        x.reshape(group_shape),
+        list(reduction_axes[:-1]) + [-1],
+        self.dtype,
+        self.axis_name,
+        self.axis_index_groups,
+        use_fast_variance=self.use_fast_variance,
+        mask=mask,
+        force_float32_reductions=self.force_float32_reductions,
     )
     mean = jnp.repeat(mean, group_size, axis=-1)
     var = jnp.repeat(var, group_size, axis=-1)
 
     return _normalize(
       self,
       x,
@@ -774,17 +789,18 @@
   InstanceNorm normalizes the activations of the layer for each channel (rather
   than across all channels like Layer Normalization), and for each given example
   in a batch independently (rather than across an entire batch like Batch
   Normalization). i.e. applies a transformation that maintains the mean activation
   within each channel within each example close to 0 and the activation standard
   deviation close to 1.
 
-  NOTE: This normalization operation is identical to LayerNorm and GroupNorm; the
-  difference is simply which axes are reduced and the shape of the feature axes
-  (i.e. the shape of the learnable scale and bias parameters).
+  .. note::
+    This normalization operation is identical to LayerNorm and GroupNorm; the
+    difference is simply which axes are reduced and the shape of the feature axes
+    (i.e. the shape of the learnable scale and bias parameters).
 
   Example usage::
 
     >>> import flax.linen as nn
     >>> import jax
     >>> import numpy as np
 
@@ -839,14 +855,15 @@
   use_scale: bool = True
   bias_init: Initializer = initializers.zeros
   scale_init: Initializer = initializers.ones
   feature_axes: Axes = -1
   axis_name: Optional[str] = None
   axis_index_groups: Any = None
   use_fast_variance: bool = True
+  force_float32_reductions: bool = True
 
   @compact
   def __call__(self, x, *, mask: Optional[jax.Array] = None):
     """Applies instance normalization on the input.
 
     Args:
       x: the inputs
@@ -859,21 +876,22 @@
     feature_axes = _canonicalize_axes(x.ndim, self.feature_axes)
     if 0 in feature_axes:
       raise ValueError('The channel axes cannot include the leading dimension '
                        'as this is assumed to be the batch axis.')
     reduction_axes = [i for i in range(1, x.ndim) if i not in feature_axes]
 
     mean, var = _compute_stats(
-      x,
-      reduction_axes,
-      self.dtype,
-      self.axis_name,
-      self.axis_index_groups,
-      use_fast_variance=self.use_fast_variance,
-      mask=mask,
+        x,
+        reduction_axes,
+        self.dtype,
+        self.axis_name,
+        self.axis_index_groups,
+        use_fast_variance=self.use_fast_variance,
+        mask=mask,
+        force_float32_reductions=self.force_float32_reductions,
     )
 
     return _normalize(
       self,
       x,
       mean,
       var,
@@ -899,25 +917,25 @@
   - https://arxiv.org/abs/1809.11096
 
   Spectral normalization normalizes the weight params so that the spectral
   norm of the matrix is equal to 1. This is implemented as a layer wrapper
   where each wrapped layer will have its params spectral normalized before
   computing its ``__call__`` output.
 
-  Usage Note:
-  The initialized variables dict will contain, in addition to a 'params'
-  collection, a separate 'batch_stats' collection that will contain a
-  ``u`` vector and ``sigma`` value, which are intermediate values used
-  when performing spectral normalization. During training, we pass in
-  ``update_stats=True`` and ``mutable=['batch_stats']`` so that ``u``
-  and ``sigma`` are updated with the most recently computed values using
-  power iteration. This will help the power iteration method approximate
-  the true singular value more accurately over time. During eval, we pass
-  in ``update_stats=False`` to ensure we get deterministic behavior from
-  the model. For example::
+  .. note::
+    The initialized variables dict will contain, in addition to a 'params'
+    collection, a separate 'batch_stats' collection that will contain a
+    ``u`` vector and ``sigma`` value, which are intermediate values used
+    when performing spectral normalization. During training, we pass in
+    ``update_stats=True`` and ``mutable=['batch_stats']`` so that ``u``
+    and ``sigma`` are updated with the most recently computed values using
+    power iteration. This will help the power iteration method approximate
+    the true singular value more accurately over time. During eval, we pass
+    in ``update_stats=False`` to ensure we get deterministic behavior from
+    the model.
 
   Example usage::
 
     >>> import flax, flax.linen as nn
     >>> import jax, jax.numpy as jnp
     >>> import optax
```

### Comparing `flax-0.8.2/flax/linen/partitioning.py` & `flax-0.8.3/flax/linen/partitioning.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/linen/pooling.py` & `flax-0.8.3/flax/linen/pooling.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,17 +19,19 @@
 from jax import lax
 
 
 def pool(inputs, init, reduce_fn, window_shape, strides, padding):
   """Helper function to define pooling functions.
 
   Pooling functions are implemented using the ReduceWindow XLA op.
-  NOTE: Be aware that pooling is not generally differentiable.
-  That means providing a reduce_fn that is differentiable does not imply that
-  pool is differentiable.
+
+  .. note::
+    Be aware that pooling is not generally differentiable.
+    That means providing a reduce_fn that is differentiable does not imply that
+    pool is differentiable.
 
   Args:
     inputs: input data with dimensions (batch, window dims..., features).
     init: the initial value for the reduction
     reduce_fn: a reduce function of the form ``(T, T) -> T``.
     window_shape: a shape tuple defining the window to reduce over.
     strides: a sequence of ``n`` integers, representing the inter-window
```

### Comparing `flax-0.8.2/flax/linen/recurrent.py` & `flax-0.8.3/flax/linen/recurrent.py`

 * *Files 2% similar despite different names*

```diff
@@ -631,14 +631,24 @@
       f = \sigma(W_{if} x + b_{if} + W_{hf} h) \\
       n = \tanh(W_{in} x + b_{in} + f * (W_{hn} h + b_{hn})) \\
       h' = (1 - f) * n + f * h \\
       \end{array}
 
   where x is the input and h is the output of the previous time step.
 
+  If ``reset_gate`` is false, the above becomes
+
+  .. math::
+
+      \begin{array}{ll}
+      f = \sigma(W_{if} x + b_{if} + W_{hf} h) \\
+      n = \tanh(W_{in} x + b_{in} + W_{hn} h) \\
+      h' = (1 - f) * n + f * h \\
+      \end{array}
+
   Example usage::
 
     >>> import flax.linen as nn
     >>> import jax, jax.numpy as jnp
 
     >>> x = jax.random.normal(jax.random.key(0), (2, 3))
     >>> layer = nn.MGUCell(features=4)
@@ -659,26 +669,28 @@
       The default is set to initializers.ones_init() because this prevents
       vanishing gradients. See https://proceedings.mlr.press/v37/jozefowicz15.pdf,
       section 2.2 for more details.
     activation_bias_init: initializer for the bias parameters of the activation
       output (default: initializers.zeros_init()).
     dtype: the dtype of the computation (default: None).
     param_dtype: the dtype passed to parameter initializers (default: float32).
+    reset_gate: flag for applying reset gating.
   """
 
   features: int
   gate_fn: Callable[..., Any] = sigmoid
   activation_fn: Callable[..., Any] = tanh
   kernel_init: Initializer = default_kernel_init
   recurrent_kernel_init: Initializer = initializers.orthogonal()
   forget_bias_init: Initializer = initializers.ones_init()
   activation_bias_init: Initializer = initializers.zeros_init()
   dtype: Optional[Dtype] = None
   param_dtype: Dtype = jnp.float32
   carry_init: Initializer = initializers.zeros_init()
+  reset_gate: bool = True
 
   @compact
   def __call__(self, carry, inputs):
     """Minimal gated unit (MGU) cell.
 
     Args:
       carry: the hidden state of the MGU cell,
@@ -709,18 +721,20 @@
       param_dtype=self.param_dtype,
       kernel_init=self.kernel_init,
     )
     f = self.gate_fn(
       dense_i(name='if', bias_init=self.forget_bias_init)(inputs)
       + dense_h(name='hf')(h)
     )
-    # add bias because the linear transformations aren't directly summed.
+    # add bias when the linear transformations aren't directly summed.
+    x = dense_h(name="hn", use_bias=self.reset_gate)(h)
+    if self.reset_gate:
+      x *= f
     n = self.activation_fn(
-      dense_i(name='in', bias_init=self.activation_bias_init)(inputs)
-      + f * dense_h(name='hn', use_bias=True)(h)
+      dense_i(name="in", bias_init=self.activation_bias_init)(inputs) + x
     )
     new_h = (1.0 - f) * n + f * h
     return new_h, new_h
 
   @nowrap
   def initialize_carry(self, rng: PRNGKey, input_shape: Tuple[int, ...]):
     """Initialize the RNN cell carry.
@@ -759,15 +773,15 @@
      h_t = o_t \tanh(c_t)
      \end{array}
 
   where * denotes the convolution operator;
   i_t, f_t, o_t are input, forget and output gate activations,
   and g_t is a vector of cell updates.
 
-  Notes:
+  .. note::
     Forget gate initialization:
       Following jozefowicz2015empirical we add 1.0 to b_f
       after initialization in order to reduce the scale of forgetting in
       the beginning of the training.
 
   Example usage::
```

### Comparing `flax-0.8.2/flax/linen/spmd.py` & `flax-0.8.3/flax/linen/spmd.py`

 * *Files 1% similar despite different names*

```diff
@@ -30,15 +30,15 @@
 import enum
 import functools
 import threading
 from typing import Any, Callable, List, Optional, Sequence, Tuple, Union
 
 import jax
 from jax import lax
-from jax.experimental import maps
+from jax.interpreters import pxla
 
 from flax import struct
 from flax.core import meta
 from flax.typing import (
   Array,
   LogicalNames,
   LogicalRules,
@@ -103,17 +103,17 @@
   existing = set(jax.tree_util.tree_leaves(existing_assignments))
   if existing.intersection(new):
     return False
   return True
 
 
 def _logical_to_mesh_axes(
-  array_dim_names: Optional[Sequence[Optional[str]]],
-  rules: Optional[LogicalRules] = None,
-) -> Optional[List[Union[_UnassignedAxis, None, str, Tuple[str]]]]:
+    array_dim_names: Optional[Sequence[Optional[str]]],
+    rules: Optional[LogicalRules] = None,
+) -> Optional[List[Union[_UnassignedAxis, None, str, Tuple[str, ...]]]]:
   """Same as logical_to_mesh_axes, but doesn't fill in _unassigned_axis."""
   if array_dim_names is None:
     return None
   if rules is None:
     rules = _axis_rules.rules
   axis_name_counts = collections.Counter(array_dim_names)
   dups = tuple(
@@ -122,15 +122,15 @@
   if dups:
     raise ValueError(
       f'Unsupported: Dimensions {dups} occur more than once in array names.'
     )
   if not isinstance(rules, (tuple, list)):
     raise ValueError('Unknown axis rule specification type.')
   # We assign mesh axes using a priority based ruleset over logical axis names.
-  result: List[Union[_UnassignedAxis, None, str, Tuple[str]]]
+  result: List[Union[_UnassignedAxis, None, str, Tuple[str, ...]]]
   result = [_unassigned_axis] * len(array_dim_names)
   for rule_model_name, rule_mesh_names in rules:
     if rule_model_name in array_dim_names:
       pos = array_dim_names.index(rule_model_name)
       if (
         _mesh_assignment_free(rule_mesh_names, result)
         and result[pos] == _unassigned_axis
@@ -205,17 +205,17 @@
       lambda x: jax.sharding.NamedSharding(mesh, x),
       logical_to_mesh(tree, rules),
       is_leaf=lambda x: isinstance(x, jax.sharding.PartitionSpec),
   )
 
 
 def _global_mesh_defined() -> bool:
-  """Checks if global xmap/jit mesh resource environment is defined."""
-  maps_env = maps.thread_resources.env
-  return maps_env.physical_mesh.devices.shape != ()  # pylint: disable=g-explicit-bool-comparison
+  """Checks if global mesh resource environment is defined."""
+  env = pxla.thread_resources.env
+  return env.physical_mesh.devices.shape != ()  # pylint: disable=g-explicit-bool-comparison
 
 
 class RulesFallback(enum.Enum):
   """How a sharding constraint should behave when no matching rule is found."""
 
   AXIS_IS_UNSHARDED = 'axis_is_unsharded'
   RAISE_ERROR = 'raise_error'
```

### Comparing `flax-0.8.2/flax/linen/stochastic.py` & `flax-0.8.3/flax/linen/stochastic.py`

 * *Files 2% similar despite different names*

```diff
@@ -22,17 +22,20 @@
 from flax.linen.module import Module, compact, merge_param
 from flax.typing import PRNGKey
 
 
 class Dropout(Module):
   """Create a dropout layer.
 
-  Note: When using :meth:`Module.apply() <flax.linen.Module.apply>`, make sure
-  to include an RNG seed named ``'dropout'``. Dropout isn't necessary for
-  variable initialization. Example usage::
+  .. note::
+    When using :meth:`Module.apply() <flax.linen.Module.apply>`, make sure
+    to include an RNG seed named ``'dropout'``. Dropout isn't necessary for
+    variable initialization.
+
+  Example usage::
 
     >>> import flax.linen as nn
     >>> import jax, jax.numpy as jnp
 
     >>> class MLP(nn.Module):
     ...   @nn.compact
     ...   def __call__(self, x, train):
```

### Comparing `flax-0.8.2/flax/linen/summary.py` & `flax-0.8.3/flax/linen/summary.py`

 * *Files 0% similar despite different names*

```diff
@@ -456,15 +456,15 @@
     all_paths: Set[Tuple[str, ...]] = set(call.path for call in calls)
     visited_paths: Set[Tuple[str, ...]] = set()
 
     for c in calls:
       call_depth = len(c.path)
       inputs = _process_inputs(c.args, c.kwargs)
 
-      if c.path in visited_paths:
+      if c.path in visited_paths or not hasattr(c.module, c.method):
         if not show_repeated:
           continue
         module_vars = {}
         counted_vars = {}
       elif depth is not None:
         if call_depth > depth:
           continue
```

### Comparing `flax-0.8.2/flax/linen/transforms.py` & `flax-0.8.3/flax/linen/transforms.py`

 * *Files 1% similar despite different names*

```diff
@@ -1139,14 +1139,15 @@
   out_axes=0,
   length: Optional[int] = None,
   reverse: bool = False,
   unroll: int = 1,
   data_transform: Optional[Callable[..., Any]] = None,
   metadata_params: Mapping[Any, Any] = {},
   methods=None,
+  _split_transpose: bool = False,
 ) -> Target:
   """A lifted version of ``jax.lax.scan``.
 
   See ``jax.lax.scan`` for the unlifted scan in Jax.
 
   To improve consistency with ``vmap``, this version of scan
   uses ``in_axes`` and ``out_axes`` to determine which arguments
@@ -1276,14 +1277,16 @@
       loop (default: 1).
     data_transform: optional function to transform raw functional-core variable
       and rng groups inside lifted scan body_fn, intended for inline SPMD
       annotations.
     metadata_params: arguments dict passed to AxisMetadata instances in the
       variable tree.
     methods: If ``target`` is a ``Module``, the methods of ``Module`` to scan over.
+    _split_transpose: An experimental feature to split the transpose of a scan
+       into a scan and a map, backed by an experimental Jax lax.scan() feature.
 
   Returns:
     The scan function with the signature ``(module, carry, *xs) -> (carry,
     ys)``, where ``xs`` and ``ys`` are the scan values that go in and out of
     the loop.
   """
   return lift_transform(
@@ -1294,14 +1297,15 @@
     variable_carry=variable_carry,
     split_rngs=split_rngs,
     in_axes=in_axes,
     out_axes=out_axes,
     length=length,
     reverse=reverse,
     unroll=unroll,
+    _split_transpose=_split_transpose,
     data_transform=data_transform,
     metadata_params=metadata_params,
     methods=methods,
   )
 
 
 def map_variables(
@@ -1779,16 +1783,16 @@
     ...       return body_fn(self, c)
     ...     else:
     ...       return nn.while_loop(cond_fn, body_fn, self, c,
     ...                             carry_variables='state')
 
     >>> k = jax.random.key(0)
     >>> x = jnp.ones((2, 2))
-    >>> intial_vars = WhileLoopExample().init(k, x)
-    >>> result, state = WhileLoopExample().apply(intial_vars, x, mutable=['state'])
+    >>> initial_vars = WhileLoopExample().init(k, x)
+    >>> result, state = WhileLoopExample().apply(initial_vars, x, mutable=['state'])
 
   Args:
     cond_fn: Should return True as long as the loop should continue.
     body_fn: The body of the while loop.
     mdl: The Module which should be lifted into the loop.
     init: The initial state passed to the loop
     carry_variables: collections that are carried through the loop
```

### Comparing `flax-0.8.2/flax/metrics/__init__.py` & `flax-0.8.3/flax/metrics/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/metrics/tensorboard.py` & `flax-0.8.3/flax/metrics/tensorboard.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/serialization.py` & `flax-0.8.3/flax/serialization.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/struct.py` & `flax-0.8.3/flax/struct.py`

 * *Files 3% similar despite different names*

```diff
@@ -31,16 +31,17 @@
   return dataclasses.field(metadata={'pytree_node': pytree_node}, **kwargs)
 
 
 @dataclass_transform(field_specifiers=(field,))  # type: ignore[literal-required]
 def dataclass(clz: _T, **kwargs) -> _T:
   """Create a class which can be passed to functional transformations.
 
-  NOTE: Inherit from ``PyTreeNode`` instead to avoid type checking issues when
-  using PyType.
+  .. note::
+    Inherit from ``PyTreeNode`` instead to avoid type checking issues when
+    using PyType.
 
   Jax transformations such as ``jax.jit`` and ``jax.grad`` require objects that are
   immutable and can be mapped over using the ``jax.tree_util`` methods.
   The ``dataclass`` decorator makes it easy to define custom classes that can be
   passed safely to Jax. For example::
 
     >>> from flax import struct
@@ -118,35 +119,49 @@
 
   def replace(self, **updates):
     """ "Returns a new object replacing the specified fields with new values."""
     return dataclasses.replace(self, **updates)
 
   data_clz.replace = replace
 
-  def iterate_clz(x):
-    meta = tuple(getattr(x, name) for name in meta_fields)
-    data = tuple(getattr(x, name) for name in data_fields)
-    return data, meta
-
-  def iterate_clz_with_keys(x):
-    meta = tuple(getattr(x, name) for name in meta_fields)
-    data = tuple(
-      (jax.tree_util.GetAttrKey(name), getattr(x, name)) for name in data_fields
-    )
-    return data, meta
+  # Remove this guard once minimux JAX version is >0.4.26.
+  try:
+    if hasattr(jax.tree_util, 'register_dataclass'):
+      jax.tree_util.register_dataclass(
+          data_clz, data_fields, meta_fields
+      )
+    else:
+      raise NotImplementedError
+  except NotImplementedError:
 
-  def clz_from_iterable(meta, data):
-    meta_args = tuple(zip(meta_fields, meta))
-    data_args = tuple(zip(data_fields, data))
-    kwargs = dict(meta_args + data_args)
-    return data_clz(**kwargs)
+    def iterate_clz(x):
+      meta = tuple(getattr(x, name) for name in meta_fields)
+      data = tuple(getattr(x, name) for name in data_fields)
+      return data, meta
+
+    def iterate_clz_with_keys(x):
+      meta = tuple(getattr(x, name) for name in meta_fields)
+      data = tuple(
+          (jax.tree_util.GetAttrKey(name), getattr(x, name))
+          for name in data_fields
+      )
+      return data, meta
 
-  jax.tree_util.register_pytree_with_keys(
-    data_clz, iterate_clz_with_keys, clz_from_iterable, iterate_clz,
-  )
+    def clz_from_iterable(meta, data):
+      meta_args = tuple(zip(meta_fields, meta))
+      data_args = tuple(zip(data_fields, data))
+      kwargs = dict(meta_args + data_args)
+      return data_clz(**kwargs)
+
+    jax.tree_util.register_pytree_with_keys(
+        data_clz,
+        iterate_clz_with_keys,
+        clz_from_iterable,
+        iterate_clz,
+    )
 
   def to_state_dict(x):
     state_dict = {
       name: serialization.to_state_dict(getattr(x, name))
       for name in data_fields
     }
     return state_dict
@@ -222,16 +237,16 @@
     >>> # This class can now be used safely in Jax to compute gradients w.r.t. the
     >>> # parameters.
     >>> model = Model(params, apply_fn)
     >>> loss_fn = lambda model: 3.
     >>> model_grad = jax.grad(loss_fn)(model)
   """
 
-  def __init_subclass__(cls):
-    dataclass(cls)  # pytype: disable=wrong-arg-types
+  def __init_subclass__(cls, **kwargs):
+    dataclass(cls, **kwargs)  # pytype: disable=wrong-arg-types
 
   def __init__(self, *args, **kwargs):
     # stub for pytype
     raise NotImplementedError
 
   def replace(self: TNode, **overrides) -> TNode:
     # stub for pytype
```

### Comparing `flax-0.8.2/flax/testing/__init__.py` & `flax-0.8.3/flax/testing/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/testing/benchmark.py` & `flax-0.8.3/flax/testing/benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/traceback_util.py` & `flax-0.8.3/flax/traceback_util.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/training/__init__.py` & `flax-0.8.3/flax/training/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/training/checkpoints.py` & `flax-0.8.3/flax/training/checkpoints.py`

 * *Files 0% similar despite different names*

```diff
@@ -676,15 +676,15 @@
     if orbax_checkpointer and isinstance(
       orbax_checkpointer, ocp.AsyncCheckpointer
     ):
       orbax_checkpointer.wait_until_finished()
     # If no checkpointer provided, save synchronously with default setting.
     if not orbax_checkpointer:
       orbax_checkpointer = ocp.Checkpointer(
-        ocp.PyTreeCheckpointHandler(restore_with_serialized_types=False)
+        ocp.PyTreeCheckpointHandler()
       )
     # Check singular target.
     if jtu.treedef_is_leaf(jtu.tree_structure(target)) and not isinstance(
       orbax_checkpointer._handler,
       ocp.ArrayCheckpointHandler,  # pylint: disable=protected-access
     ):
       raise ValueError(
@@ -825,15 +825,15 @@
       orbax_checkpointer, ocp.AsyncCheckpointer
     ):
       orbax_checkpointer.wait_until_finished()
 
     # If no checkpointer provided, save synchronously with default setting.
     if not orbax_checkpointer:
       orbax_checkpointer = ocp.Checkpointer(
-        ocp.PyTreeCheckpointHandler(restore_with_serialized_types=False)
+        ocp.PyTreeCheckpointHandler()
       )
     # Check singular target.
     if jtu.treedef_is_leaf(jtu.tree_structure(target)) and not isinstance(
       orbax_checkpointer._handler,
       ocp.ArrayCheckpointHandler,  # pylint: disable=protected-access
     ):
       raise ValueError(
@@ -1108,15 +1108,15 @@
   # Restore the checkpoint with Orbax if needed.
   is_orbax = _is_orbax_checkpoint(ckpt_path)
   ckpt_type = 'orbax' if is_orbax else 'legacy Flax'
   logging.info(f'Restoring {ckpt_type} checkpoint from {ckpt_path}')
   if is_orbax:
     if not orbax_checkpointer:
       orbax_checkpointer = ocp.Checkpointer(
-        ocp.PyTreeCheckpointHandler(restore_with_serialized_types=False)
+        ocp.PyTreeCheckpointHandler()
       )
 
     restore_kwargs = {}
     if target is not None:
       restore_kwargs['restore_args'] = orbax_utils.restore_args_from_target(
         target
       )
```

### Comparing `flax-0.8.2/flax/training/common_utils.py` & `flax-0.8.3/flax/training/common_utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/training/dynamic_scale.py` & `flax-0.8.3/flax/training/dynamic_scale.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/training/early_stopping.py` & `flax-0.8.3/flax/training/early_stopping.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/training/lr_schedule.py` & `flax-0.8.3/flax/training/lr_schedule.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/training/orbax_utils.py` & `flax-0.8.3/flax/training/orbax_utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/training/prefetch_iterator.py` & `flax-0.8.3/flax/training/prefetch_iterator.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/training/train_state.py` & `flax-0.8.3/flax/training/train_state.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,18 +8,19 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Any, Callable
+from typing import Any, Callable, Union
 
 import optax
 
+import jax
 from flax import core, struct
 from flax.linen.fp8_ops import OVERWRITE_WITH_GRADIENT
 
 
 class TrainState(struct.PyTreeNode):
   """Simple train state for the common case with a single Optax optimizer.
 
@@ -66,15 +67,15 @@
       convenience to have a shorter params list for the ``train_step()`` function
       in your training loop.
     params: The parameters to be updated by ``tx`` and used by ``apply_fn``.
     tx: An Optax gradient transformation.
     opt_state: The state for ``tx``.
   """
 
-  step: int
+  step: Union[int, jax.Array]
   apply_fn: Callable = struct.field(pytree_node=False)
   params: core.FrozenDict[str, Any] = struct.field(pytree_node=True)
   tx: optax.GradientTransformation = struct.field(pytree_node=False)
   opt_state: optax.OptState = struct.field(pytree_node=True)
 
   def apply_gradients(self, *, grads, **kwargs):
     """Updates ``step``, ``params``, ``opt_state`` and ``**kwargs`` in return value.
```

### Comparing `flax-0.8.2/flax/traverse_util.py` & `flax-0.8.3/flax/traverse_util.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/flax/typing.py` & `flax-0.8.3/flax/typing.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,16 +13,18 @@
 # limitations under the License.
 
 from typing import (
   Any,
   Callable,
   Dict,
   Generic,
+  Hashable,
   Mapping,
   Optional,
+  Protocol,
   Sequence,
   Tuple,
   TypeVar,
   Union,
 )
 
 import jax
@@ -34,17 +36,24 @@
 # General
 
 Array = Union[jax.Array, Any]
 PRNGKey = jax.Array
 RNGSequences = Dict[str, PRNGKey]
 Dtype = Union[jax.typing.DTypeLike, Any]
 Shape = Sequence[int]
+K = TypeVar('K')
+
+
+class Key(Hashable, Protocol):
+  def __lt__(self: K, value: K, /) -> bool:
+    ...
+
 
 Path = str
-PathParts = Tuple[str, ...]
+PathParts = Tuple[Key, ...]
 
 Leaf = Any
 
 
 # Linear
 
 PrecisionLike = Union[
@@ -106,14 +115,16 @@
 Axes = Union[int, Sequence[int]]
 
 
 # SPMD
 
 LogicalNames = Tuple[Union[str, None], ...]
 
-LogicalRules = Sequence[Tuple[str, Union[str, Tuple[str], None]]]
+# Maps each logical axis  to physical mesh, can be either None (replicated),
+# one physical axis or a tuple of physical axes.
+LogicalRules = Sequence[Tuple[str, Union[str, Tuple[str, ...], None]]]
 ArrayPytree = Any  # pylint: disable=invalid-name
 LogicalPartitionSpec = Any  # pylint: disable=invalid-name
 LogicalPartitionSpecPytree = Any  # pylint: disable=invalid-name
 PartitionSpecPytree = Any  # pylint: disable=invalid-name
 
-Sharding = Tuple[Optional[str], ...]
+Sharding = Tuple[Optional[str], ...]
```

### Comparing `flax-0.8.2/flax/version.py` & `flax-0.8.3/flax/version.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,8 +9,8 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Current Flax version at head on Github."""
-__version__ = '0.8.2'
+__version__ = '0.8.3'
```

### Comparing `flax-0.8.2/flax.egg-info/PKG-INFO` & `flax-0.8.3/flax.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flax
-Version: 0.8.2
+Version: 0.8.3
 Summary: Flax: A neural network library for JAX designed for flexibility
 Author-email: Flax team <flax-dev@google.com>
 Project-URL: homepage, https://github.com/google/flax
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
@@ -47,14 +47,15 @@
 Requires-Dist: sentencepiece; extra == "testing"
 Requires-Dist: tensorflow_text>=2.11.0; extra == "testing"
 Requires-Dist: tensorflow_datasets; extra == "testing"
 Requires-Dist: tensorflow; extra == "testing"
 Requires-Dist: torch; extra == "testing"
 Requires-Dist: nbstripout; extra == "testing"
 Requires-Dist: black[jupyter]==23.7.0; extra == "testing"
+Requires-Dist: penzai; python_version >= "3.10" and extra == "testing"
 
 <div align="center">
 <img src="https://raw.githubusercontent.com/google/flax/main/images/flax_logo_250px.png" alt="logo"></img>
 </div>
 
 # Flax: A neural network library and ecosystem for JAX designed for flexibility
 
@@ -247,15 +248,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.8.1},
+  version = {0.8.2},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.8.2/flax.egg-info/SOURCES.txt` & `flax-0.8.3/flax.egg-info/SOURCES.txt`

 * *Files 13% similar despite different names*

```diff
@@ -34,14 +34,15 @@
 docs/index.rst
 docs/linen_intro.ipynb
 docs/linen_intro.md
 docs/philosophy.md
 docs/quick_start.ipynb
 docs/quick_start.md
 docs/requirements.txt
+docs/robots.txt
 docs/_ext/codediff.py
 docs/_ext/codediff_test.py
 docs/_ext/flax_module.py
 docs/_static/css/flax_theme.css
 docs/_templates/autosummary/flax_module.rst
 docs/api_reference/flax.config.rst
 docs/api_reference/flax.core.frozen_dict.rst
@@ -50,14 +51,32 @@
 docs/api_reference/flax.jax_utils.rst
 docs/api_reference/flax.serialization.rst
 docs/api_reference/flax.struct.rst
 docs/api_reference/flax.traceback_util.rst
 docs/api_reference/flax.training.rst
 docs/api_reference/flax.traverse_util.rst
 docs/api_reference/index.rst
+docs/api_reference/flax.experimental.nnx/helpers.rst
+docs/api_reference/flax.experimental.nnx/index.rst
+docs/api_reference/flax.experimental.nnx/module.rst
+docs/api_reference/flax.experimental.nnx/rnglib.rst
+docs/api_reference/flax.experimental.nnx/spmd.rst
+docs/api_reference/flax.experimental.nnx/transforms.rst
+docs/api_reference/flax.experimental.nnx/variables.rst
+docs/api_reference/flax.experimental.nnx/visualization.rst
+docs/api_reference/flax.experimental.nnx/nn/activations.rst
+docs/api_reference/flax.experimental.nnx/nn/attention.rst
+docs/api_reference/flax.experimental.nnx/nn/index.rst
+docs/api_reference/flax.experimental.nnx/nn/initializers.rst
+docs/api_reference/flax.experimental.nnx/nn/linear.rst
+docs/api_reference/flax.experimental.nnx/nn/normalization.rst
+docs/api_reference/flax.experimental.nnx/nn/stochastic.rst
+docs/api_reference/flax.experimental.nnx/training/index.rst
+docs/api_reference/flax.experimental.nnx/training/metrics.rst
+docs/api_reference/flax.experimental.nnx/training/optimizer.rst
 docs/api_reference/flax.linen/activation_functions.rst
 docs/api_reference/flax.linen/decorators.rst
 docs/api_reference/flax.linen/index.rst
 docs/api_reference/flax.linen/init_apply.rst
 docs/api_reference/flax.linen/initializers.rst
 docs/api_reference/flax.linen/inspection.rst
 docs/api_reference/flax.linen/layers.rst
@@ -76,20 +95,22 @@
 docs/examples/repositories_that_use_flax.rst
 docs/experimental/index.rst
 docs/experimental/nnx/index.rst
 docs/experimental/nnx/mnist_tutorial.ipynb
 docs/experimental/nnx/mnist_tutorial.md
 docs/experimental/nnx/nnx_basics.ipynb
 docs/experimental/nnx/nnx_basics.md
+docs/experimental/nnx/transforms.rst
 docs/flip/0000-template.md
 docs/flip/1009-optimizer-api.md
 docs/flip/1777-default-dtype.md
 docs/flip/2396-rnn.md
 docs/flip/2434-general-metadata.md
 docs/flip/2974-kw-only-dataclasses.md
+docs/flip/3099-rnnbase-refactor.md
 docs/flip/README.md
 docs/guides/flax_sharp_bits.ipynb
 docs/guides/flax_sharp_bits.md
 docs/guides/index.rst
 docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst
 docs/guides/converting_and_upgrading/haiku_migration_guide.rst
 docs/guides/converting_and_upgrading/index.rst
@@ -307,73 +328,72 @@
 flax/experimental/nnx/examples/lm1b/temperature_sampler.py
 flax/experimental/nnx/examples/lm1b/temperature_sampler_test.py
 flax/experimental/nnx/examples/lm1b/tokenizer.py
 flax/experimental/nnx/examples/lm1b/train.py
 flax/experimental/nnx/examples/lm1b/train_test.py
 flax/experimental/nnx/examples/lm1b/utils.py
 flax/experimental/nnx/examples/lm1b/configs/default.py
-flax/experimental/nnx/examples/toy_examples/00_demo.ipynb
 flax/experimental/nnx/examples/toy_examples/01_functional_api.py
 flax/experimental/nnx/examples/toy_examples/02_lifted_transforms.py
-flax/experimental/nnx/examples/toy_examples/03_train_state.py
 flax/experimental/nnx/examples/toy_examples/05_vae.py
 flax/experimental/nnx/examples/toy_examples/06_scan_over_layers.py
-flax/experimental/nnx/examples/toy_examples/07_transformer.py
 flax/experimental/nnx/examples/toy_examples/08_save_load_checkpoints.py
 flax/experimental/nnx/examples/toy_examples/09_parameter_surgery.py
-flax/experimental/nnx/examples/toy_examples/10_quantization.py
 flax/experimental/nnx/examples/toy_examples/requirements.txt
-flax/experimental/nnx/ideas/shape_inference.py
 flax/experimental/nnx/nnx/__init__.py
 flax/experimental/nnx/nnx/compatibility.py
 flax/experimental/nnx/nnx/errors.py
 flax/experimental/nnx/nnx/filterlib.py
-flax/experimental/nnx/nnx/graph_utils.py
+flax/experimental/nnx/nnx/graph.py
 flax/experimental/nnx/nnx/helpers.py
 flax/experimental/nnx/nnx/ids.py
 flax/experimental/nnx/nnx/module.py
 flax/experimental/nnx/nnx/proxy_caller.py
-flax/experimental/nnx/nnx/pytreelib.py
 flax/experimental/nnx/nnx/reprlib.py
 flax/experimental/nnx/nnx/rnglib.py
 flax/experimental/nnx/nnx/spmd.py
 flax/experimental/nnx/nnx/state.py
 flax/experimental/nnx/nnx/tracers.py
 flax/experimental/nnx/nnx/transforms.py
 flax/experimental/nnx/nnx/variables.py
+flax/experimental/nnx/nnx/visualization.py
 flax/experimental/nnx/nnx/nn/__init__.py
 flax/experimental/nnx/nnx/nn/activations.py
 flax/experimental/nnx/nnx/nn/attention.py
 flax/experimental/nnx/nnx/nn/dtypes.py
 flax/experimental/nnx/nnx/nn/initializers.py
 flax/experimental/nnx/nnx/nn/linear.py
 flax/experimental/nnx/nnx/nn/normalization.py
 flax/experimental/nnx/nnx/nn/stochastic.py
+flax/experimental/nnx/nnx/training/metrics.py
+flax/experimental/nnx/nnx/training/optimizer.py
 flax/experimental/nnx/scripts/requirements.txt
 flax/experimental/nnx/scripts/run-all-examples.bash
 flax/experimental/nnx/tests/__init__.py
 flax/experimental/nnx/tests/test_compatibility.py
 flax/experimental/nnx/tests/test_containers.py
 flax/experimental/nnx/tests/test_graph_utils.py
 flax/experimental/nnx/tests/test_helpers.py
 flax/experimental/nnx/tests/test_ids.py
 flax/experimental/nnx/tests/test_integration.py
+flax/experimental/nnx/tests/test_metrics.py
 flax/experimental/nnx/tests/test_module.py
+flax/experimental/nnx/tests/test_optimizer.py
 flax/experimental/nnx/tests/test_partitioning.py
-flax/experimental/nnx/tests/test_pytree.py
 flax/experimental/nnx/tests/test_rngs.py
 flax/experimental/nnx/tests/test_spmd.py
 flax/experimental/nnx/tests/test_state.py
 flax/experimental/nnx/tests/test_transforms.py
 flax/experimental/nnx/tests/test_variable.py
 flax/experimental/nnx/tests/nn/test_attention.py
 flax/experimental/nnx/tests/nn/test_conv.py
 flax/experimental/nnx/tests/nn/test_embed.py
 flax/experimental/nnx/tests/nn/test_linear.py
 flax/experimental/nnx/tests/nn/test_normalization.py
+flax/experimental/nnx/tests/nn/test_stochastic.py
 flax/linen/README.md
 flax/linen/__init__.py
 flax/linen/activation.py
 flax/linen/attention.py
 flax/linen/batch_apply.py
 flax/linen/combinators.py
 flax/linen/dtypes.py
```

### Comparing `flax-0.8.2/flax.egg-info/requires.txt` & `flax-0.8.3/flax.egg-info/requires.txt`

 * *Files 7% similar despite different names*

```diff
@@ -37,7 +37,10 @@
 tensorflow
 torch
 nbstripout
 black[jupyter]==23.7.0
 
 [testing:python_version < "3.10"]
 clu<=0.0.9
+
+[testing:python_version >= "3.10"]
+penzai
```

### Comparing `flax-0.8.2/images/flax_logo.png` & `flax-0.8.3/images/flax_logo.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/images/flax_logo.svg` & `flax-0.8.3/images/flax_logo.svg`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/images/flax_logo_250px.png` & `flax-0.8.3/images/flax_logo_250px.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/images/flax_logo_500px.png` & `flax-0.8.3/images/flax_logo_500px.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/pylintrc` & `flax-0.8.3/pylintrc`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/pyproject.toml` & `flax-0.8.3/pyproject.toml`

 * *Files 17% similar despite different names*

```diff
@@ -58,14 +58,15 @@
     "tensorflow_text>=2.11.0",  # WMT/LM1B examples
     "tensorflow_datasets",
     "tensorflow",
     "torch",
     "nbstripout",
     "black[jupyter]==23.7.0",
     # "pyink==23.5.0", # disabling pyink fow now
+    "penzai; python_version>='3.10'",
 ]
 
 [project.urls]
 homepage = "https://github.com/google/flax"
 
 [tool.setuptools.dynamic]
 readme = {file = ["README.md"], content-type = "text/markdown"}
@@ -140,14 +141,16 @@
     "ignore:.*jax.random.KeyArray is deprecated.*:DeprecationWarning",
     # DeprecationWarning: jax.core.Shape is deprecated.
     "ignore:.*jax.core.Shape is deprecated.*:DeprecationWarning",
     # DeprecationWarning: pkg_resources is deprecated as an API.
     "ignore:.*pkg_resources is deprecated as an API.*:DeprecationWarning",
     # DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
     "ignore:.*Deprecated call to.*pkg_resources.declare_namespace.*:DeprecationWarning",
+    # DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).
+    "ignore:.*jax.tree_map is deprecated.*:DeprecationWarning",
 ]
 
 [tool.coverage.report]
 exclude_lines = [
     "@abc.abstractmethod",
     "raise NotImplementedError",
 ]
@@ -160,15 +163,15 @@
 
 [tool.ruff]
 # Exclude a variety of commonly ignored directories.
 exclude = [
   "__init__.py",
   "activation.py",
   "partitioning.py",
-  "variables.py",
+  "flax/core/variables.py",
   "examples/",
 ]
 
 line-length = 80
 indent-width = 2
 
 [tool.ruff.lint]
```

### Comparing `flax-0.8.2/tests/checkpoints_test.py` & `flax-0.8.3/tests/checkpoints_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/colab_tpu_jax_version.ipynb` & `flax-0.8.3/tests/colab_tpu_jax_version.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/configurations_test.py` & `flax-0.8.3/tests/configurations_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/core_frozen_dict_test.py` & `flax-0.8.3/tests/core/core_frozen_dict_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/core_lift_test.py` & `flax-0.8.3/tests/core/core_lift_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,14 +19,18 @@
 from absl.testing import absltest
 from jax import numpy as jnp
 from jax import random
 
 from flax import errors
 from flax.core import FrozenDict, apply, copy, init, lift, nn
 
+# TODO(jakevdp): use jax.debug_key_reuse directly once min jax version is 0.4.26
+jax_debug_key_reuse = (jax.debug_key_reuse if hasattr(jax, 'debug_key_reuse')
+                       else jax.enable_key_reuse_checks)
+
 
 class LiftTest(absltest.TestCase):
   def test_aliasing(self):
     def f(scope):
       a = scope.push('a')
 
       def g(scopes, _):
@@ -174,15 +178,15 @@
       {}, x, rngs={'params': random.key(1), 'loop': random.key(2)}
     )
     self.assertEqual(vars['state']['acc'], x)
     self.assertEqual(c, 2 * x)
     np.testing.assert_array_equal(
       vars['state']['rng_params'][0], vars['state']['rng_params'][1]
     )
-    with jax.enable_key_reuse_checks(False):
+    with jax_debug_key_reuse(False):
       np.testing.assert_array_compare(
         operator.__ne__,
         vars['state']['rng_loop'][0],
         vars['state']['rng_loop'][1],
       )
 
   def test_cond(self):
```

### Comparing `flax-0.8.2/tests/core/core_meta_test.py` & `flax-0.8.3/tests/core/core_meta_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/core_scope_test.py` & `flax-0.8.3/tests/core/core_scope_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/design/core_attention_test.py` & `flax-0.8.3/tests/core/design/core_attention_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/design/core_auto_encoder_test.py` & `flax-0.8.3/tests/core/design/core_auto_encoder_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/design/core_big_resnets_test.py` & `flax-0.8.3/tests/core/design/core_big_resnets_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/design/core_custom_vjp_test.py` & `flax-0.8.3/tests/core/design/core_custom_vjp_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/design/core_dense_test.py` & `flax-0.8.3/tests/core/design/core_dense_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/design/core_flow_test.py` & `flax-0.8.3/tests/core/design/core_flow_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/design/core_resnet_test.py` & `flax-0.8.3/tests/core/design/core_resnet_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/design/core_scan_test.py` & `flax-0.8.3/tests/core/design/core_scan_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/design/core_tied_autoencoder_test.py` & `flax-0.8.3/tests/core/design/core_tied_autoencoder_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/design/core_vmap_test.py` & `flax-0.8.3/tests/core/design/core_vmap_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/core/design/core_weight_std_test.py` & `flax-0.8.3/tests/core/design/core_weight_std_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/cursor_test.py` & `flax-0.8.3/tests/cursor_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/download_dataset_metadata.sh` & `flax-0.8.3/tests/download_dataset_metadata.sh`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/early_stopping_test.py` & `flax-0.8.3/tests/early_stopping_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/import_test.ipynb` & `flax-0.8.3/tests/import_test.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/io_test.py` & `flax-0.8.3/tests/io_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/jax_utils_test.py` & `flax-0.8.3/tests/jax_utils_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/linen/initializers_test.py` & `flax-0.8.3/tests/linen/initializers_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/linen/kw_only_dataclasses_test.py` & `flax-0.8.3/tests/linen/kw_only_dataclasses_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/linen/linen_activation_test.py` & `flax-0.8.3/tests/linen/linen_activation_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/linen/linen_attention_test.py` & `flax-0.8.3/tests/linen/linen_attention_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,24 +10,24 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for flax.linen.attention."""
 
-import jax
-import jax.numpy as jnp
-import numpy as np
+import functools
 from absl.testing import absltest, parameterized
-from jax import lax, random
-from jax.nn import initializers
-
 from flax import errors, jax_utils
 from flax import linen as nn
 from flax.core import pop
+import jax
+from jax import lax, random
+from jax.nn import initializers
+import jax.numpy as jnp
+import numpy as np
 
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
 
 class AttentionTest(parameterized.TestCase):
   def test_multihead_self_attention(self):
@@ -522,10 +522,40 @@
     self.assertTrue((out1 == out2).all())
     self.assertTrue(
         jax.tree_util.tree_all(
             jax.tree_util.tree_map(lambda x, y: (x == y).all(), v1, v2)
         )
     )
 
+  @parameterized.parameters(
+      {'force_fp32': True, 'attn_weights_dtype': jnp.float32},
+      {'force_fp32': False, 'attn_weights_dtype': jnp.bfloat16},
+  )
+  def test_mixed_precision_multihead_attention(
+      self, force_fp32, attn_weights_dtype
+  ):
+    input_key, params_key, dropout_key = random.split(random.key(0), 3)
+    x = random.uniform(input_key, (2, 4))
+    attention_kwargs = dict(
+        num_heads=2,
+        qkv_features=4,
+        kernel_init=initializers.lecun_normal(),
+        bias_init=initializers.uniform(),
+        attention_fn=functools.partial(
+            nn.dot_product_attention, force_fp32_for_softmax=force_fp32
+        ),
+        deterministic=False,
+        dtype=jnp.bfloat16,
+    )
+    mha = nn.MultiHeadDotProductAttention(**attention_kwargs)
+    init_vars = mha.init({'params': params_key, 'dropout': dropout_key}, x)
+    _, updated_vars = mha.apply(
+        init_vars, x, mutable=['intermediates'], sow_weights=True
+    )
+    self.assertEqual(
+        updated_vars['intermediates']['attention_weights'][0].dtype,
+        attn_weights_dtype,
+    )
+
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.8.2/tests/linen/linen_batch_apply_test.py` & `flax-0.8.3/tests/linen/linen_batch_apply_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/linen/linen_combinators_test.py` & `flax-0.8.3/tests/linen/linen_combinators_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/linen/linen_dtypes_test.py` & `flax-0.8.3/tests/linen/linen_dtypes_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/linen/linen_linear_test.py` & `flax-0.8.3/tests/linen/linen_linear_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/linen/linen_meta_test.py` & `flax-0.8.3/tests/linen/linen_meta_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/linen/linen_module_test.py` & `flax-0.8.3/tests/linen/linen_module_test.py`

 * *Files 0% similar despite different names*

```diff
@@ -2542,14 +2542,25 @@
       bar(x)
 
     # bar.__call__ and foo.__call__
     self.assertLen(called, 2)
     self.assertIs(called[0], bar)
     self.assertIs(called[1], foo)
 
+  def test_cloudpickle_class(self):
+    import cloudpickle
+
+    class MyModule(nn.Module):
+      pass
+
+    a = MyModule()
+
+    UnpickledMyModule = cloudpickle.loads(cloudpickle.dumps(MyModule))
+    b = UnpickledMyModule()
+
   def test_cloudpickle_module(self):
     from cloudpickle import cloudpickle_fast
 
     class NNModuleWithProperty(nn.Module):
       a: int
       b: str
```

### Comparing `flax-0.8.2/tests/linen/linen_recurrent_test.py` & `flax-0.8.3/tests/linen/linen_recurrent_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/linen/linen_test.py` & `flax-0.8.3/tests/linen/linen_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -916,15 +916,15 @@
     prev_loss = float('inf')
     for _ in range(10):
       state, loss = train_step(state, {'image': x, 'label': y})
       self.assertLess(loss, prev_loss)
       prev_loss = loss
 
 
-class StochasticTest(absltest.TestCase):
+class StochasticTest(parameterized.TestCase):
   def test_dropout(self):
     rng = random.key(0)
     key1, key2 = random.split(rng)
     module = nn.Dropout(rate=0.5)
     y1 = module.apply(
       {}, jnp.ones((20, 20)), deterministic=False, rngs={'dropout': key1}
     )
@@ -971,14 +971,78 @@
     y2 = d1.apply({}, inputs, deterministic=False, rngs={'dropout': key2})
     np.testing.assert_array_equal(y2, np.zeros_like(inputs))
     # ensure gradient of rate==1.0 case is non-NaN
     fn = lambda x, k: d1.apply({}, x, rngs={'dropout': k}, deterministic=False)
     res = jax.grad(lambda x, k: jnp.sum(fn(x, k)))(inputs, key3)
     self.assertFalse(np.isnan(res).any())
 
+  @parameterized.parameters(
+    {
+      'num_dims': 2,
+      'broadcast_dims': (1,),
+      'slice_fn': lambda out, i: out[i, :],
+      'summed_total': 2 * 10,
+    },
+    {
+      'num_dims': 2,
+      'broadcast_dims': (0,),
+      'slice_fn': lambda out, i: out[:, i],
+      'summed_total': 2 * 10,
+    },
+    {
+      'num_dims': 3,
+      'broadcast_dims': (1, 2),
+      'slice_fn': lambda out, i: out[i, :, :],
+      'summed_total': 2 * 10 * 10,
+    },
+    {
+      'num_dims': 3,
+      'broadcast_dims': (1,),
+      'slice_fn': lambda out, i, j: out[i, :, j],
+      'summed_total': 2 * 10,
+    },
+    {
+      'num_dims': 4,
+      'broadcast_dims': (0, 2, 3),
+      'slice_fn': lambda out, i: out[:, i, :, :],
+      'summed_total': 2 * 10 * 10 * 10,
+    },
+    {
+      'num_dims': 4,
+      'broadcast_dims': (0, 1),
+      'slice_fn': lambda out, i, j: out[:, :, i, j],
+      'summed_total': 2 * 10 * 10,
+    },
+    {
+      'num_dims': 4,
+      'broadcast_dims': (3,),
+      'slice_fn': lambda out, i, j, k: out[i, j, k, :],
+      'summed_total': 2 * 10,
+    },
+  )
+  def test_dropout_broadcast(
+    self, num_dims, broadcast_dims, slice_fn, summed_total
+  ):
+    module = nn.Dropout(
+      rate=0.5, broadcast_dims=broadcast_dims, deterministic=False
+    )
+    x = jnp.ones((10,) * num_dims)
+    out = module.apply({}, x, rngs={'dropout': random.key(0)})
+
+    for i in range(10):
+      if num_dims - len(broadcast_dims) >= 2:
+        for j in range(10):
+          if num_dims - len(broadcast_dims) >= 3:
+            for k in range(10):
+              self.assertTrue(slice_fn(out, i, j, k).sum() in (0, summed_total))
+          else:
+            self.assertTrue(slice_fn(out, i, j).sum() in (0, summed_total))
+      else:
+        self.assertTrue(slice_fn(out, i).sum() in (0, summed_total))
+
   def test_dropout_manual_rng(self):
     def clone(key):
       if hasattr(jax.random, 'clone'):
         # JAX v0.4.26+
         return jax.random.clone(key)
       return key
     class Foo(nn.Module):
@@ -1138,29 +1202,53 @@
     self.assertEqual(c0.shape, (2, 4))
     self.assertEqual(h0.shape, (2, 4))
     (_, y_opt), lstm_opt_params = lstm_opt.init_with_output(key2, (c0, h0), x)
 
     np.testing.assert_allclose(y, y_opt, rtol=1e-6)
     check_eq(lstm_params, lstm_opt_params)
 
+  def test_mgu_reset_gate(self):
+    module = nn.MGUCell(features=4, reset_gate=False)
+    rng = random.key(0)
+    rng, key1, key2 = random.split(rng, 3)
+    x = random.normal(key1, (2, 3))
+    carry0 = module.initialize_carry(rng, x.shape)
+    (carry, y), v = module.init_with_output(key2, carry0, x)
+
+    self.assertIn('kernel', v['params']['hn'])
+    self.assertNotIn('bias', v['params']['hn'])
+
+    f = jax.nn.sigmoid(
+      jnp.dot(x, v['params']['if']['kernel'])
+      + v['params']['if']['bias'].reshape(1, -1)
+      + jnp.dot(carry0, v['params']['hf']['kernel'])
+    )
+    n = jax.nn.tanh(
+      jnp.dot(x, v['params']['in']['kernel'])
+      + v['params']['in']['bias'].reshape(1, -1)
+      + jnp.dot(carry0, v['params']['hn']['kernel'])
+    )
+    expected_out = (1 - f) * n + f * carry0
+    np.testing.assert_allclose(y, expected_out)
+
 
 class IdsTest(absltest.TestCase):
   def test_hashable(self):
     id1 = ids.uuid()
     id2 = ids.uuid()
     self.assertEqual(id1, id1)
     self.assertNotEqual(id1, id2)
     self.assertNotEqual(hash(id1), hash(id2))
     id1c = copy.copy(id1)
     id1dc = copy.deepcopy(id1)
     self.assertNotEqual(hash(id1), hash(id1c))
     self.assertNotEqual(hash(id1), hash(id1dc))
 
 
-class Fp8Test(absltest.TestCase):
+class Fp8Test(parameterized.TestCase):
   def test_fp8_dot_general_injection(self):
     # Used to cast the inputs to be representable in FP8, so that the difference
     # of the results from the original gemm and fp8 gemm is small.
     cast_to_representable = functools.partial(
       fp8_ops.quantize_dequantize,
       scale=jnp.ones((1,)),
       compute_dtype=jnp.float32,
@@ -1297,15 +1385,18 @@
         atol=atol,
       )
 
       np.testing.assert_allclose(fp8_vars['input_scale'][0], scale_x)
       np.testing.assert_allclose(fp8_vars['kernel_scale'][0], scale_k)
       np.testing.assert_allclose(fp8_vars['output_grad_scale'][0], scale_g)
 
-  def test_fp8_meta_dtype(self):
+  @parameterized.parameters([True, False])
+  def test_fp8_meta_dtype(self, use_jit):
+    if not use_jit and not fp8_ops.CAN_USE_EARRAY:
+      self.skipTest("TODO: requires newer jax that has earray")
     f32 = jnp.dtype('float32')
     fm32 = fp8_ops.fm32
 
     # Create a scan loop with reused ah_f32 and sf_f32. So, the autograd will
     # accumulate the grads of them. We expect the max op (rather than add op)
     # for the accumulation by converting them to fm32 dtype.
     def outer(x, ah_f32, sf_f32):
@@ -1314,15 +1405,17 @@
       array_x = jnp.array([x], f32)
       def body_fun(carry, _):
         carry = fp8_ops.in_qdq(f32, carry, sf_fm32, ah_fm32)
         return carry, None
       array_x, _ = jax.lax.scan(body_fun, array_x, None, length=3)
       return array_x[0]
 
-    outer_fn = jax.jit(jax.grad(outer, (0, 1, 2)))
+    outer_fn = jax.grad(outer, (0, 1, 2))
+    if use_jit:
+      outer_fn = jax.jit(outer_fn)
     ah = jnp.array([0., 0., 0.], f32)
     sf = jnp.array([1.], f32)
     # 1st iteration
     grads, new_ah, new_sf = outer_fn(2.0, ah, sf)
     np.testing.assert_allclose(new_ah, [2., 0., 0.])
     np.testing.assert_allclose(new_sf, [1.])
     # 2nd iteration
```

### Comparing `flax-0.8.2/tests/linen/linen_transforms_test.py` & `flax-0.8.3/tests/linen/linen_transforms_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -26,14 +26,18 @@
 from flax.core import copy, freeze
 from flax.linen.transforms import _HashableProxy
 import jax
 from jax import random
 import jax.numpy as jnp
 import numpy as np
 
+# TODO(jakevdp): use jax.debug_key_reuse directly once min jax version is 0.4.26
+jax_debug_key_reuse = (jax.debug_key_reuse if hasattr(jax, 'debug_key_reuse')
+                       else jax.enable_key_reuse_checks)
+
 # Parse absl flags test_srcdir and test_tmpdir.
 jax.config.parse_flags_with_absl()
 
 # pylint: disable=attribute-defined-outside-init,unused-variable,g-wrong-blank-lines,g-bare-generic
 
 
 def tree_equals(x, y):
@@ -2034,15 +2038,15 @@
         mutable=True,
         rngs={'params': random.key(1), 'loop': random.key(2)},
     )
     self.assertEqual(vars['state']['acc'], x)
     np.testing.assert_array_equal(
         vars['state']['rng_params'][0], vars['state']['rng_params'][1]
     )
-    with jax.enable_key_reuse_checks(False):
+    with jax_debug_key_reuse(False):
       np.testing.assert_array_compare(
           operator.__ne__,
           vars['state']['rng_loop'][0],
           vars['state']['rng_loop'][1],
       )
 
   def test_cond(self):
```

### Comparing `flax-0.8.2/tests/linen/partitioning_test.py` & `flax-0.8.3/tests/linen/partitioning_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/linen/summary_test.py` & `flax-0.8.3/tests/linen/summary_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -752,10 +752,61 @@
     lines = Net().tabulate(jax.random.key(0), inputs).split('\n')
     self.assertIn('x: \x1b[2mfloat32\x1b[0m[1,1]', lines[5])
     # test kwargs
     lines = Net().tabulate(jax.random.key(0), inputs=inputs).split('\n')
     self.assertIn('inputs:', lines[5])
     self.assertIn('x: \x1b[2mfloat32\x1b[0m[1,1]', lines[6])
 
+  def test_tabulate_norm_wrapper(self):
+    class SubModel(nn.Module):
+      @nn.compact
+      def __call__(self, x):
+        x = nn.SpectralNorm(nn.Dense(5))(x, update_stats=False)
+        x = nn.Dense(6)(x)
+        x = nn.WeightNorm(nn.Dense(7))(x)
+        return x
+
+    class Model(nn.Module):
+      @nn.compact
+      def __call__(self, x):
+        x = nn.WeightNorm(nn.Dense(3))(x)
+        x = nn.Dense(4)(x)
+        x = SubModel()(x)
+        x = nn.Dense(8)(x)
+        x = nn.SpectralNorm(nn.Dense(9))(x, update_stats=False)
+        return x
+
+    x = jnp.ones((1, 2))
+    key = jax.random.key(0)
+    model = Model()
+
+    lines = model.tabulate(
+      key,
+      x,
+      console_kwargs=CONSOLE_TEST_KWARGS,
+      compute_flops=True,
+      compute_vjp_flops=True,
+    ).splitlines()
+
+    self.assertIn('Model', lines[5])
+    self.assertIn('WeightNorm_0', lines[7])
+    self.assertIn('Dense_0/kernel/scale', lines[7])
+    self.assertIn('Dense_0', lines[11])
+    self.assertIn('Dense_1', lines[16])
+    self.assertIn('SubModel_0', lines[21])
+    self.assertIn('SubModel_0/SpectralNorm_0', lines[23])
+    self.assertIn('Dense_0/kernel/sigma', lines[23])
+    self.assertIn('Dense_0/kernel/u', lines[24])
+    self.assertIn('SubModel_0/Dense_0', lines[28])
+    self.assertIn('SubModel_0/Dense_1', lines[33])
+    self.assertIn('SubModel_0/WeightNorm_0', lines[38])
+    self.assertIn('Dense_2/kernel/scale', lines[38])
+    self.assertIn('SubModel_0/Dense_2', lines[42])
+    self.assertIn('Dense_2', lines[47])
+    self.assertIn('SpectralNorm_0', lines[52])
+    self.assertIn('Dense_3/kernel/sigma', lines[52])
+    self.assertIn('Dense_3/kernel/u', lines[53])
+    self.assertIn('Dense_3', lines[57])
+
 
 if __name__ == '__main__':
   absltest.main()
```

### Comparing `flax-0.8.2/tests/run_all_tests.sh` & `flax-0.8.3/tests/run_all_tests.sh`

 * *Files 6% similar despite different names*

```diff
@@ -81,15 +81,15 @@
   sphinx-build -M doctest docs docs/_build -T
   # test build html
   sphinx-build -M html docs docs/_build -T
   # test docstrings
   pytest -n auto flax \
     --doctest-modules \
     --suppress-no-test-exit-code \
-    --ignore=flax/experimental/nnx
+    --ignore=flax/experimental/nnx/examples
 fi
 
 # check that flax is running on editable mode
 # (i.e. no notebook installed flax from pypi)
 echo "=== CHECKING FLAX IS EDITABLE ==="
 assert_error="flax is not running on editable mode."
 (cd docs; python -c "import flax; assert 'site-packages' not in flax.__file__, \"$assert_error\"")
@@ -107,26 +107,38 @@
       echo "pytest -n auto $file $PYTEST_OPTS"
       pytest -n auto $file $PYTEST_OPTS
       PYTEST_IGNORE+=" --ignore=$file"
   done
   # Run battery of core FLAX API tests.
   echo "pytest -n auto tests $PYTEST_OPTS $PYTEST_IGNORE"
   pytest -n auto tests $PYTEST_OPTS $PYTEST_IGNORE
+  # Run nnx tests
+  pytest -n auto flax/experimental/nnx/tests $PYTEST_OPTS $PYTEST_IGNORE
 
   # Per-example tests.
   #
   # we apply pytest within each example to avoid pytest's annoying test-filename collision.
   # In pytest foo/bar/baz_test.py and baz/bleep/baz_test.py will collide and error out when
   # /foo/bar and /baz/bleep aren't set up as packages.
   for egd in $(find examples -maxdepth 1 -mindepth 1 -type d); do
-      pytest $egd
+    # skip if folder starts with "_"
+    if [[ $egd == *"_"* ]]; then
+      continue
+    fi
+    pytest $egd
+  done
+
+  for egd in $(find flax/experimental/nnx/examples -maxdepth 1 -mindepth 1 -type d); do
+    # skip if folder starts with "_" or is "toy_examples"
+    if [[ $egd == *"_"* ]] || [[ $egd == *"toy_examples"* ]]; then
+      continue
+    fi
+    pytest $egd
   done
 
-  # Run nnx tests
-  pytest -n auto flax/experimental/nnx/tests $PYTEST_OPTS $PYTEST_IGNORE
 fi
 
 if $RUN_PYTYPE; then
   echo "=== RUNNING PYTYPE ==="
   # Validate types in library code.
   pytype --jobs auto --config pyproject.toml flax/ --exclude flax/experimental/nnx
```

### Comparing `flax-0.8.2/tests/serialization_test.py` & `flax-0.8.3/tests/serialization_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/tensorboard_test.py` & `flax-0.8.3/tests/tensorboard_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/traceback_util_test.py` & `flax-0.8.3/tests/traceback_util_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.2/tests/traverse_util_test.py` & `flax-0.8.3/tests/traverse_util_test.py`

 * *Files identical despite different names*

