# Comparing `tmp/tensordict_nightly-2024.4.8-cp39-cp39-win_amd64.whl.zip` & `tmp/tensordict_nightly-2024.4.9-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,40 +1,40 @@
-Zip file size: 286928 bytes, number of entries: 38
--rw-rw-rw-  2.0 fat     1652 b- defN 24-Apr-08 13:49 tensordict/__init__.py
--rw-rw-rw-  2.0 fat     6156 b- defN 24-Apr-08 13:49 tensordict/_contextlib.py
--rw-rw-rw-  2.0 fat   134377 b- defN 24-Apr-08 13:49 tensordict/_lazy.py
--rw-rw-rw-  2.0 fat     4693 b- defN 24-Apr-08 13:49 tensordict/_pytree.py
--rw-rw-rw-  2.0 fat   128209 b- defN 24-Apr-08 13:49 tensordict/_td.py
--rw-rw-rw-  2.0 fat   114176 b- defN 24-Apr-08 13:52 tensordict/_tensordict.pyd
--rw-rw-rw-  2.0 fat    21209 b- defN 24-Apr-08 13:49 tensordict/_torch_func.py
--rw-rw-rw-  2.0 fat   227083 b- defN 24-Apr-08 13:49 tensordict/base.py
--rw-rw-rw-  2.0 fat    16915 b- defN 24-Apr-08 13:49 tensordict/functional.py
--rw-rw-rw-  2.0 fat    27011 b- defN 24-Apr-08 13:49 tensordict/memmap.py
--rw-rw-rw-  2.0 fat    33049 b- defN 24-Apr-08 13:49 tensordict/memmap_deprec.py
--rw-rw-rw-  2.0 fat    46522 b- defN 24-Apr-08 13:49 tensordict/persistent.py
--rw-rw-rw-  2.0 fat    93206 b- defN 24-Apr-08 13:49 tensordict/tensorclass.py
--rw-rw-rw-  2.0 fat     1093 b- defN 24-Apr-08 13:49 tensordict/tensordict.py
--rw-rw-rw-  2.0 fat    75463 b- defN 24-Apr-08 13:49 tensordict/utils.py
--rw-rw-rw-  2.0 fat       86 b- defN 24-Apr-08 13:51 tensordict/version.py
--rw-rw-rw-  2.0 fat     1634 b- defN 24-Apr-08 13:49 tensordict/nn/__init__.py
--rw-rw-rw-  2.0 fat    54882 b- defN 24-Apr-08 13:49 tensordict/nn/common.py
--rw-rw-rw-  2.0 fat     5940 b- defN 24-Apr-08 13:49 tensordict/nn/ensemble.py
--rw-rw-rw-  2.0 fat    25890 b- defN 24-Apr-08 13:49 tensordict/nn/functional_modules.py
--rw-rw-rw-  2.0 fat    36844 b- defN 24-Apr-08 13:49 tensordict/nn/params.py
--rw-rw-rw-  2.0 fat    25553 b- defN 24-Apr-08 13:49 tensordict/nn/probabilistic.py
--rw-rw-rw-  2.0 fat    19947 b- defN 24-Apr-08 13:49 tensordict/nn/sequence.py
--rw-rw-rw-  2.0 fat    13231 b- defN 24-Apr-08 13:49 tensordict/nn/utils.py
--rw-rw-rw-  2.0 fat      795 b- defN 24-Apr-08 13:49 tensordict/nn/distributions/__init__.py
--rw-rw-rw-  2.0 fat     6629 b- defN 24-Apr-08 13:49 tensordict/nn/distributions/composite.py
--rw-rw-rw-  2.0 fat     9924 b- defN 24-Apr-08 13:49 tensordict/nn/distributions/continuous.py
--rw-rw-rw-  2.0 fat     2667 b- defN 24-Apr-08 13:49 tensordict/nn/distributions/discrete.py
--rw-rw-rw-  2.0 fat     6694 b- defN 24-Apr-08 13:49 tensordict/nn/distributions/truncated_normal.py
--rw-rw-rw-  2.0 fat     1266 b- defN 24-Apr-08 13:49 tensordict/nn/distributions/utils.py
--rw-rw-rw-  2.0 fat      393 b- defN 24-Apr-08 13:49 tensordict/prototype/__init__.py
--rw-rw-rw-  2.0 fat     7889 b- defN 24-Apr-08 13:49 tensordict/prototype/fx.py
--rw-rw-rw-  2.0 fat      796 b- defN 24-Apr-08 13:49 tensordict/prototype/tensorclass.py
--rw-rw-rw-  2.0 fat     1119 b- defN 24-Apr-08 13:52 tensordict_nightly-2024.4.8.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    22528 b- defN 24-Apr-08 13:52 tensordict_nightly-2024.4.8.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-Apr-08 13:52 tensordict_nightly-2024.4.8.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 24-Apr-08 13:52 tensordict_nightly-2024.4.8.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     3255 b- defN 24-Apr-08 13:52 tensordict_nightly-2024.4.8.dist-info/RECORD
-38 files, 1178887 bytes uncompressed, 281760 bytes compressed:  76.1%
+Zip file size: 290140 bytes, number of entries: 38
+-rw-rw-rw-  2.0 fat     1652 b- defN 24-Apr-09 13:49 tensordict/__init__.py
+-rw-rw-rw-  2.0 fat     6156 b- defN 24-Apr-09 13:49 tensordict/_contextlib.py
+-rw-rw-rw-  2.0 fat   133285 b- defN 24-Apr-09 13:49 tensordict/_lazy.py
+-rw-rw-rw-  2.0 fat     4693 b- defN 24-Apr-09 13:49 tensordict/_pytree.py
+-rw-rw-rw-  2.0 fat   133639 b- defN 24-Apr-09 13:49 tensordict/_td.py
+-rw-rw-rw-  2.0 fat   114176 b- defN 24-Apr-09 13:51 tensordict/_tensordict.pyd
+-rw-rw-rw-  2.0 fat    21209 b- defN 24-Apr-09 13:49 tensordict/_torch_func.py
+-rw-rw-rw-  2.0 fat   258288 b- defN 24-Apr-09 13:49 tensordict/base.py
+-rw-rw-rw-  2.0 fat    16915 b- defN 24-Apr-09 13:49 tensordict/functional.py
+-rw-rw-rw-  2.0 fat    27011 b- defN 24-Apr-09 13:49 tensordict/memmap.py
+-rw-rw-rw-  2.0 fat    33049 b- defN 24-Apr-09 13:49 tensordict/memmap_deprec.py
+-rw-rw-rw-  2.0 fat    46650 b- defN 24-Apr-09 13:49 tensordict/persistent.py
+-rw-rw-rw-  2.0 fat    93288 b- defN 24-Apr-09 13:49 tensordict/tensorclass.py
+-rw-rw-rw-  2.0 fat     1093 b- defN 24-Apr-09 13:49 tensordict/tensordict.py
+-rw-rw-rw-  2.0 fat    75531 b- defN 24-Apr-09 13:49 tensordict/utils.py
+-rw-rw-rw-  2.0 fat       86 b- defN 24-Apr-09 13:51 tensordict/version.py
+-rw-rw-rw-  2.0 fat     1634 b- defN 24-Apr-09 13:49 tensordict/nn/__init__.py
+-rw-rw-rw-  2.0 fat    54882 b- defN 24-Apr-09 13:49 tensordict/nn/common.py
+-rw-rw-rw-  2.0 fat     5940 b- defN 24-Apr-09 13:49 tensordict/nn/ensemble.py
+-rw-rw-rw-  2.0 fat    25890 b- defN 24-Apr-09 13:49 tensordict/nn/functional_modules.py
+-rw-rw-rw-  2.0 fat    37320 b- defN 24-Apr-09 13:49 tensordict/nn/params.py
+-rw-rw-rw-  2.0 fat    25553 b- defN 24-Apr-09 13:49 tensordict/nn/probabilistic.py
+-rw-rw-rw-  2.0 fat    19947 b- defN 24-Apr-09 13:49 tensordict/nn/sequence.py
+-rw-rw-rw-  2.0 fat    13231 b- defN 24-Apr-09 13:49 tensordict/nn/utils.py
+-rw-rw-rw-  2.0 fat      795 b- defN 24-Apr-09 13:49 tensordict/nn/distributions/__init__.py
+-rw-rw-rw-  2.0 fat     6629 b- defN 24-Apr-09 13:49 tensordict/nn/distributions/composite.py
+-rw-rw-rw-  2.0 fat     9924 b- defN 24-Apr-09 13:49 tensordict/nn/distributions/continuous.py
+-rw-rw-rw-  2.0 fat     2667 b- defN 24-Apr-09 13:49 tensordict/nn/distributions/discrete.py
+-rw-rw-rw-  2.0 fat     6694 b- defN 24-Apr-09 13:49 tensordict/nn/distributions/truncated_normal.py
+-rw-rw-rw-  2.0 fat     1266 b- defN 24-Apr-09 13:49 tensordict/nn/distributions/utils.py
+-rw-rw-rw-  2.0 fat      393 b- defN 24-Apr-09 13:49 tensordict/prototype/__init__.py
+-rw-rw-rw-  2.0 fat     7889 b- defN 24-Apr-09 13:49 tensordict/prototype/fx.py
+-rw-rw-rw-  2.0 fat      796 b- defN 24-Apr-09 13:49 tensordict/prototype/tensorclass.py
+-rw-rw-rw-  2.0 fat     1119 b- defN 24-Apr-09 13:51 tensordict_nightly-2024.4.9.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    22831 b- defN 24-Apr-09 13:51 tensordict_nightly-2024.4.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 24-Apr-09 13:51 tensordict_nightly-2024.4.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 24-Apr-09 13:51 tensordict_nightly-2024.4.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     3255 b- defN 24-Apr-09 13:51 tensordict_nightly-2024.4.9.dist-info/RECORD
+38 files, 1215487 bytes uncompressed, 284972 bytes compressed:  76.6%
```

## zipnote {}

```diff
@@ -93,23 +93,23 @@
 
 Filename: tensordict/prototype/fx.py
 Comment: 
 
 Filename: tensordict/prototype/tensorclass.py
 Comment: 
 
-Filename: tensordict_nightly-2024.4.8.dist-info/LICENSE
+Filename: tensordict_nightly-2024.4.9.dist-info/LICENSE
 Comment: 
 
-Filename: tensordict_nightly-2024.4.8.dist-info/METADATA
+Filename: tensordict_nightly-2024.4.9.dist-info/METADATA
 Comment: 
 
-Filename: tensordict_nightly-2024.4.8.dist-info/WHEEL
+Filename: tensordict_nightly-2024.4.9.dist-info/WHEEL
 Comment: 
 
-Filename: tensordict_nightly-2024.4.8.dist-info/top_level.txt
+Filename: tensordict_nightly-2024.4.9.dist-info/top_level.txt
 Comment: 
 
-Filename: tensordict_nightly-2024.4.8.dist-info/RECORD
+Filename: tensordict_nightly-2024.4.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensordict/_lazy.py

```diff
@@ -22,14 +22,15 @@
 import torch
 import torch.distributed as dist
 from functorch import dim as ftdim
 from tensordict._td import _SubTensorDict, _TensorDictKeysView, TensorDict
 from tensordict._tensordict import _unravel_key_to_tuple, unravel_key_list
 from tensordict.base import (
     _is_tensor_collection,
+    _NESTED_TENSORS_AS_LISTS,
     _register_tensor_class,
     BEST_ATTEMPT_INPLACE,
     CompatibleType,
     is_tensor_collection,
     NO_DEFAULT,
     T,
     TensorDictBase,
@@ -57,14 +58,15 @@
     is_tensorclass,
     KeyedJaggedTensor,
     lock_blocked,
     NestedKey,
 )
 from torch import Tensor
 
+
 _has_functorch = False
 try:
     try:
         from torch._C._functorch import (
             _add_batch_dim,
             _remove_batch_dim,
             is_batchedtensor,
@@ -84,15 +86,22 @@
 class _LazyStackedTensorDictKeysView(_TensorDictKeysView):
     tensordict: LazyStackedTensorDict
 
     def __len__(self) -> int:
         return len(self._keys())
 
     def _keys(self) -> list[str]:
-        return self.tensordict._key_list()
+        result = self.tensordict._key_list()
+        if self.is_leaf is _NESTED_TENSORS_AS_LISTS:
+            return [
+                (key, str(i))
+                for key in result
+                for i in range(len(self.tensordict.tensordicts))
+            ]
+        return result
 
     def __contains__(self, item):
         item = _unravel_key_to_tuple(item)
         if item[0] in self.tensordict._iterate_over_keys():
             if self.leaves_only:
                 return not _is_tensor_collection(self.tensordict.entry_class(item[0]))
             has_first_key = True
@@ -1409,14 +1418,45 @@
             self,
             include_nested=include_nested,
             leaves_only=leaves_only,
             is_leaf=is_leaf,
         )
         return keys
 
+    def values(self, include_nested=False, leaves_only=False, is_leaf=None):
+        if is_leaf is not _NESTED_TENSORS_AS_LISTS:
+            yield from super().values(
+                include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
+            )
+        else:
+            for td in self.tensordicts:
+                yield from td.values(
+                    include_nested=include_nested,
+                    leaves_only=leaves_only,
+                    is_leaf=is_leaf,
+                )
+
+    def items(self, include_nested=False, leaves_only=False, is_leaf=None):
+        if is_leaf is not _NESTED_TENSORS_AS_LISTS:
+            yield from super().items(
+                include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
+            )
+        else:
+            for i, td in enumerate(self.tensordicts):
+                for key, val in td.items(
+                    include_nested=include_nested,
+                    leaves_only=leaves_only,
+                    is_leaf=is_leaf,
+                ):
+                    if isinstance(key, str):
+                        key = (str(i), key)
+                    else:
+                        key = (str(i), *key)
+                    yield key, val
+
     valid_keys = keys
 
     # def _iterate_over_keys(self) -> None:
     #     for key in self.tensordicts[0].keys():
     #         if all(key in td.keys() for td in self.tensordicts):
     #             yield key
     def _iterate_over_keys(self) -> None:
@@ -1508,15 +1548,17 @@
                 *oth,
                 checked=checked,
                 device=device,
                 call_on_nested=call_on_nested,
                 default=default,
                 named=named,
                 nested_keys=nested_keys,
-                prefix=prefix,  # + (i,),
+                prefix=prefix + (str(i),)
+                if is_leaf is _NESTED_TENSORS_AS_LISTS
+                else prefix,
                 inplace=inplace,
                 filter_empty=filter_empty,
                 is_leaf=is_leaf,
             )
             for i, (td, *oth) in enumerate(zip(self.tensordicts, *others))
         ]
         if filter_empty and all(r is None for r in results):
@@ -1762,112 +1804,42 @@
                         result.append(self.tensordicts[i][_idx])
                 result = LazyStackedTensorDict.lazy_stack(
                     result, new_stack_dim, stack_dim_name=self._td_dim_name
                 )
                 return result
 
     def __eq__(self, other):
-        if is_tensorclass(other):
-            return other == self
-        if isinstance(other, (dict,)):
-            # we may want to broadcast it instead
-            other = TensorDict.from_dict(other, batch_size=self.batch_size)
-        if _is_tensor_collection(other.__class__):
-            if other.batch_size != self.batch_size:
-                if self.ndim < other.ndim:
-                    self_expand = self.expand(other.batch_size)
-                elif self.ndim > other.ndim:
-                    other = other.expand(self.batch_size)
-                    self_expand = self
-                else:
-                    raise RuntimeError(
-                        f"Could not compare tensordicts with shapes {self.shape} and {other.shape}"
-                    )
-            else:
-                self_expand = self
-            out = []
-            for td0, td1 in zip(
-                self_expand.tensordicts, other.unbind(self_expand.stack_dim)
-            ):
-                out.append(td0 == td1)
-            return LazyStackedTensorDict.lazy_stack(out, self.stack_dim)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return LazyStackedTensorDict.lazy_stack(
-                [td == other for td in self.tensordicts],
-                self.stack_dim,
-            )
-        return False
+        return self._dispatch_comparison(other, "__eq__", "__eq__", default=False)
 
     def __ne__(self, other):
-        if is_tensorclass(other):
-            return other != self
-        if isinstance(other, (dict,)):
-            # we may want to broadcast it instead
-            other = TensorDict.from_dict(other, batch_size=self.batch_size)
-        if _is_tensor_collection(other.__class__):
-            if other.batch_size != self.batch_size:
-                if self.ndim < other.ndim:
-                    self_expand = self.expand(other.batch_size)
-                elif self.ndim > other.ndim:
-                    other = other.expand(self.batch_size)
-                    self_expand = self
-                else:
-                    raise RuntimeError(
-                        f"Could not compare tensordicts with shapes {self.shape} and {other.shape}"
-                    )
-            else:
-                self_expand = self
-            out = []
-            for td0, td1 in zip(
-                self_expand.tensordicts, other.unbind(self_expand.stack_dim)
-            ):
-                out.append(td0 != td1)
-            return LazyStackedTensorDict.lazy_stack(out, self.stack_dim)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return LazyStackedTensorDict.lazy_stack(
-                [td != other for td in self.tensordicts],
-                self.stack_dim,
-            )
-        return True
+        return self._dispatch_comparison(other, "__ne__", "__ne__", default=True)
+
+    def __or__(self, other):
+        return self._dispatch_comparison(other, "__or__", "__or__", default=NO_DEFAULT)
 
     def __xor__(self, other):
-        if is_tensorclass(other):
-            return other == self
-        if isinstance(other, (dict,)):
-            # we may want to broadcast it instead
-            other = TensorDict.from_dict(other, batch_size=self.batch_size)
-        if _is_tensor_collection(other.__class__):
-            if other.batch_size != self.batch_size:
-                if self.ndim < other.ndim:
-                    self_expand = self.expand(other.batch_size)
-                elif self.ndim > other.ndim:
-                    other = other.expand(self.batch_size)
-                    self_expand = self
-                else:
-                    raise RuntimeError(
-                        f"Could not compare tensordicts with shapes {self.shape} and {other.shape}"
-                    )
-            else:
-                self_expand = self
-            out = []
-            for td0, td1 in zip(
-                self_expand.tensordicts, other.unbind(self_expand.stack_dim)
-            ):
-                out.append(td0 ^ td1)
-            return LazyStackedTensorDict.lazy_stack(out, self.stack_dim)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return LazyStackedTensorDict.lazy_stack(
-                [td ^ other for td in self.tensordicts],
-                self.stack_dim,
-            )
-        return False
+        return self._dispatch_comparison(
+            other, "__xor__", "__xor__", default=NO_DEFAULT
+        )
 
-    def __or__(self, other):
+    def __ge__(self, other):
+        return self._dispatch_comparison(other, "__ge__", "__le__", default=NO_DEFAULT)
+
+    def __gt__(self, other):
+        return self._dispatch_comparison(other, "__gt__", "__lt__", default=NO_DEFAULT)
+
+    def __le__(self, other):
+        return self._dispatch_comparison(other, "__le__", "__ge__", default=NO_DEFAULT)
+
+    def __lt__(self, other):
+        return self._dispatch_comparison(other, "__lt__", "__gt__", default=NO_DEFAULT)
+
+    def _dispatch_comparison(self, other, comparison_str, inverse_str, default):
         if is_tensorclass(other):
-            return other == self
+            return getattr(other, inverse_str)(self)
         if isinstance(other, (dict,)):
             # we may want to broadcast it instead
             other = TensorDict.from_dict(other, batch_size=self.batch_size)
         if _is_tensor_collection(other.__class__):
             if other.batch_size != self.batch_size:
                 if self.ndim < other.ndim:
                     self_expand = self.expand(other.batch_size)
@@ -1880,22 +1852,26 @@
                     )
             else:
                 self_expand = self
             out = []
             for td0, td1 in zip(
                 self_expand.tensordicts, other.unbind(self_expand.stack_dim)
             ):
-                out.append(td0 | td1)
+                out.append(getattr(td0, comparison_str)(td1))
             return LazyStackedTensorDict.lazy_stack(out, self.stack_dim)
         if isinstance(other, (numbers.Number, Tensor)):
             return LazyStackedTensorDict.lazy_stack(
-                [td | other for td in self.tensordicts],
+                [getattr(td, comparison_str)(other) for td in self.tensordicts],
                 self.stack_dim,
             )
-        return False
+        if default is NO_DEFAULT:
+            raise ValueError(
+                f"Incompatible value {type(other)} for op {comparison_str}."
+            )
+        return default
 
     def all(self, dim: int = None) -> bool | TensorDictBase:
         if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
             raise RuntimeError(
                 "dim must be greater than or equal to -tensordict.batch_dims and "
                 "smaller than tensordict.batch_dims"
             )
@@ -3155,14 +3131,18 @@
             "Cannot call `unsqueeze` on a lazy tensordict. Make it dense before calling this method by calling `to_tensordict`."
         )
 
     __xor__ = TensorDict.__xor__
     __or__ = TensorDict.__or__
     __eq__ = TensorDict.__eq__
     __ne__ = TensorDict.__ne__
+    __ge__ = TensorDict.__ge__
+    __gt__ = TensorDict.__gt__
+    __le__ = TensorDict.__le__
+    __lt__ = TensorDict.__lt__
     __setitem__ = TensorDict.__setitem__
     _add_batch_dim = TensorDict._add_batch_dim
     _check_device = TensorDict._check_device
     _check_is_shared = TensorDict._check_is_shared
     _convert_to_tensordict = TensorDict._convert_to_tensordict
     _index_tensordict = TensorDict._index_tensordict
     masked_select = TensorDict.masked_select
```

## tensordict/_td.py

```diff
@@ -556,14 +556,134 @@
             return TensorDict(
                 {key: value == other for key, value in self.items()},
                 self.batch_size,
                 device=self.device,
             )
         return False
 
+    def __ge__(self, other: object) -> T | bool:
+        if is_tensorclass(other):
+            return other <= self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                keys1 = sorted(
+                    keys1,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                keys2 = sorted(
+                    keys2,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 >= other.get(key)
+            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value >= other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return False
+
+    def __gt__(self, other: object) -> T | bool:
+        if is_tensorclass(other):
+            return other < self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                keys1 = sorted(
+                    keys1,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                keys2 = sorted(
+                    keys2,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 > other.get(key)
+            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value > other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return False
+
+    def __le__(self, other: object) -> T | bool:
+        if is_tensorclass(other):
+            return other >= self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                keys1 = sorted(
+                    keys1,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                keys2 = sorted(
+                    keys2,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 <= other.get(key)
+            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value <= other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return False
+
+    def __lt__(self, other: object) -> T | bool:
+        if is_tensorclass(other):
+            return other > self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                keys1 = sorted(
+                    keys1,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                keys2 = sorted(
+                    keys2,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 < other.get(key)
+            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value < other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return False
+
     def __setitem__(
         self,
         index: IndexType,
         value: T | dict | numbers.Number | CompatibleType,
     ) -> None:
         istuple = isinstance(index, tuple)
         if istuple or isinstance(index, str):
@@ -754,15 +874,17 @@
                     is_leaf=is_leaf,
                     **constructor_kwargs,
                 )
             else:
                 _others = [_other._get_str(key, default=default) for _other in others]
                 if named:
                     if nested_keys:
-                        item_trsf = fn(unravel_key(prefix + (key,)), item, *_others)
+                        item_trsf = fn(
+                            prefix + (key,) if prefix != () else key, item, *_others
+                        )
                     else:
                         item_trsf = fn(key, item, *_others)
                 else:
                     item_trsf = fn(item, *_others)
             if item_trsf is not None:
                 if not any_set:
                     if result is None:
@@ -3072,14 +3194,18 @@
         )
         # the id of out changes
         return self._get_str(key, default=NO_DEFAULT)
 
     # TODO: check these implementations
     __eq__ = TensorDict.__eq__
     __ne__ = TensorDict.__ne__
+    __ge__ = TensorDict.__ge__
+    __gt__ = TensorDict.__gt__
+    __le__ = TensorDict.__le__
+    __lt__ = TensorDict.__lt__
     __setitem__ = TensorDict.__setitem__
     __xor__ = TensorDict.__xor__
     __or__ = TensorDict.__or__
     _check_device = TensorDict._check_device
     _check_is_shared = TensorDict._check_is_shared
     all = TensorDict.all
     any = TensorDict.any
```

## tensordict/base.py

```diff
@@ -86,14 +86,32 @@
 
     def __bool__(self):
         return False
 
 
 NO_DEFAULT = _NoDefault()
 
+
+class _NestedTensorsAsLists:
+    """Class used to iterate over leaves of lazily stacked tensordicts."""
+
+    def __new__(cls):
+        if not hasattr(cls, "instance"):
+            cls.instance = super(_NestedTensorsAsLists, cls).__new__(cls)
+        return cls.instance
+
+    def __bool__(self):
+        return False
+
+    def __call__(self, val):
+        return _default_is_leaf(val)
+
+
+_NESTED_TENSORS_AS_LISTS = _NestedTensorsAsLists()
+
 T = TypeVar("T", bound="TensorDictBase")
 
 
 class _BEST_ATTEMPT_INPLACE:
     def __bool__(self):
         # we use an exception to exit when running `inplace = BEST_ATTEMPT_INPLACE if inplace else False`
         # more than once
@@ -142,15 +160,15 @@
             a new TensorDict instance with all tensors are boolean
             tensors of the same shape as the original tensors.
 
         """
         ...
 
     @abc.abstractmethod
-    def __xor__(self, other):
+    def __xor__(self, other: TensorDictBase | float):
         """XOR operation over two tensordicts, for evey key.
 
         The two tensordicts must have the same key set.
 
         Args:
             other (TensorDictBase, dict, or float): the value to compare against.
 
@@ -158,15 +176,15 @@
             a new TensorDict instance with all tensors are boolean
             tensors of the same shape as the original tensors.
 
         """
         ...
 
     @abc.abstractmethod
-    def __or__(self, other):
+    def __or__(self, other: TensorDictBase | float) -> T:
         """OR operation over two tensordicts, for evey key.
 
         The two tensordicts must have the same key set.
 
         Args:
             other (TensorDictBase, dict, or float): the value to compare against.
 
@@ -184,14 +202,58 @@
         Returns:
             a new TensorDict instance with all tensors are boolean
             tensors of the same shape as the original tensors.
 
         """
         ...
 
+    @abc.abstractmethod
+    def __ge__(self, other: object) -> T:
+        """Compares two tensordicts against each other using the "greater or equal" operator, for every key. The two tensordicts must have the same key set.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def __gt__(self, other: object) -> T:
+        """Compares two tensordicts against each other using the "greater than" operator, for every key. The two tensordicts must have the same key set.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def __le__(self, other: object) -> T:
+        """Compares two tensordicts against each other using the "lower or equal" operator, for every key. The two tensordicts must have the same key set.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def __lt__(self, other: object) -> T:
+        """Compares two tensordicts against each other using the "lower than" operator, for every key. The two tensordicts must have the same key set.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
     def __repr__(self) -> str:
         fields = _td_fields(self)
         field_str = indent(f"fields={{{fields}}}", 4 * " ")
         batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
         device_str = indent(f"device={self.device}", 4 * " ")
         is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
         string = ",\n".join([field_str, batch_size_str, device_str, is_shared_str])
@@ -726,14 +788,40 @@
         If ``dim`` is not specified, returns the ``batch_size`` attribute of the TensorDict.
 
         """
         if dim is None:
             return self.batch_size
         return self.batch_size[dim]
 
+    @property
+    def data(self):
+        """Returns a tensordict containing the .data attributes of the leaf tensors."""
+        return self._data()
+
+    @property
+    def grad(self):
+        """Returns a tensordict containing the .grad attributes of the leaf tensors."""
+        return self._grad()
+
+    @cache  # noqa
+    def _dtype(self):
+        dtype = None
+        for val in self.values(True, True):
+            val_dtype = getattr(val, "dtype", None)
+            if dtype is None and val_dtype is not None:
+                dtype = val_dtype
+            elif dtype is not None and val_dtype is not None and dtype != val_dtype:
+                return None
+        return dtype
+
+    @property
+    def dtype(self):
+        """Returns the dtype of the values in the tensordict, if it is unique."""
+        return self._dtype()
+
     def _batch_size_setter(self, new_batch_size: torch.Size) -> None:
         if new_batch_size == self.batch_size:
             return
         if self._lazy:
             raise RuntimeError(
                 "modifying the batch size of a lazy representation of a "
                 "tensordict is not permitted. Consider instantiating the "
@@ -3244,14 +3332,59 @@
                 val = self._get_str(k, NO_DEFAULT)
                 if is_leaf(val.__class__):
                     yield val
         else:
             for k in self.keys():
                 yield self._get_str(k, NO_DEFAULT)
 
+    @cache  # noqa: B019
+    def _values_list(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+    ) -> List:
+        return list(
+            self.values(
+                include_nested=include_nested,
+                leaves_only=leaves_only,
+                is_leaf=_NESTED_TENSORS_AS_LISTS,
+            )
+        )
+
+    @cache  # noqa: B019
+    def _items_list(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+    ) -> Tuple[List, List]:
+        return tuple(
+            list(key_or_val)
+            for key_or_val in zip(
+                *self.items(
+                    include_nested=include_nested,
+                    leaves_only=leaves_only,
+                    is_leaf=_NESTED_TENSORS_AS_LISTS,
+                )
+            )
+        )
+
+    @cache  # noqa: B019
+    def _grad(self):
+        result = self._fast_apply(lambda x: x.grad)
+        if self.is_locked:
+            return result.lock_()
+        return result
+
+    @cache  # noqa: B019
+    def _data(self):
+        result = self._fast_apply(lambda x: x.data)
+        if self.is_locked:
+            return result.lock_()
+        return result
+
     @abc.abstractmethod
     def keys(
         self,
         include_nested: bool = False,
         leaves_only: bool = False,
         is_leaf: Callable[[Type], bool] = None,
     ):
@@ -4548,14 +4681,811 @@
         if imaplist:
             if chunksize == 0:
                 out = torch.stack(imaplist, dim)
             else:
                 out = torch.cat(imaplist, dim)
         return out
 
+    # point-wise arithmetic ops
+    def __add__(self, other: TensorDictBase | float) -> T:
+        return self.add(other)
+
+    def __iadd__(self, other: TensorDictBase | float) -> T:
+        return self.add_(other)
+
+    def __abs__(self):
+        return self.abs()
+
+    def __truediv__(self, other: TensorDictBase | float) -> T:
+        return self.div(other)
+
+    def __itruediv__(self, other: TensorDictBase | float) -> T:
+        return self.div_(other)
+
+    def __mul__(self, other: TensorDictBase | float) -> T:
+        return self.mul(other)
+
+    def __imul__(self, other: TensorDictBase | float) -> T:
+        return self.mul_(other)
+
+    def __sub__(self, other: TensorDictBase | float) -> T:
+        return self.sub(other)
+
+    def __isub__(self, other: TensorDictBase | float) -> T:
+        return self.sub_(other)
+
+    def __pow__(self, other: TensorDictBase | float) -> T:
+        return self.pow(other)
+
+    def __ipow__(self, other: TensorDictBase | float) -> T:
+        return self.pow_(other)
+
+    def abs(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_abs(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def abs_(self) -> T:
+        torch._foreach_abs_(self._values_list(True, True))
+        return self
+
+    def acos(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_acos(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def acos_(self) -> T:
+        torch._foreach_acos_(self._values_list(True, True))
+        return self
+
+    def exp(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_exp(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def exp_(self) -> T:
+        torch._foreach_exp_(self._values_list(True, True))
+        return self
+
+    def neg(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_neg(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def neg_(self) -> T:
+        torch._foreach_neg_(self._values_list(True, True))
+        return self
+
+    def reciprocal(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_reciprocal(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def reciprocal_(self) -> T:
+        torch._foreach_reciprocal_(self._values_list(True, True))
+        return self
+
+    def sigmoid(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_sigmoid(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def sigmoid_(self) -> T:
+        torch._foreach_sigmoid_(self._values_list(True, True))
+        return self
+
+    def sign(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_sign(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def sign_(self) -> T:
+        torch._foreach_sign_(self._values_list(True, True))
+        return self
+
+    def sin(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_sin(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def sin_(self) -> T:
+        torch._foreach_sin_(self._values_list(True, True))
+        return self
+
+    def sinh(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_sinh(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def sinh_(self) -> T:
+        torch._foreach_sinh_(self._values_list(True, True))
+        return self
+
+    def tan(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_tan(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def tan_(self) -> T:
+        torch._foreach_tan_(self._values_list(True, True))
+        return self
+
+    def tanh(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_tanh(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def tanh_(self) -> T:
+        torch._foreach_tanh_(self._values_list(True, True))
+        return self
+
+    def trunc(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_trunc(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def trunc_(self) -> T:
+        torch._foreach_trunc_(self._values_list(True, True))
+        return self
+
+    def norm(self, p="fro", dim=None, keepdim=False, out=None, dtype=None):
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_norm(vals, p=p, dim=dim, keepdim=keepdim, dtype=dtype)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            batch_size=[],
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def lgamma(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_lgamma(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def lgamma_(self) -> T:
+        torch._foreach_lgamma_(self._values_list(True, True))
+        return self
+
+    def frac(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_frac(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def frac_(self) -> T:
+        torch._foreach_frac_(self._values_list(True, True))
+        return self
+
+    def expm1(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_expm1(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def expm1_(self) -> T:
+        torch._foreach_expm1_(self._values_list(True, True))
+        return self
+
+    def log(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_log(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def log_(self) -> T:
+        torch._foreach_log_(self._values_list(True, True))
+        return self
+
+    def log10(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_log10(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def log10_(self) -> T:
+        torch._foreach_log10_(self._values_list(True, True))
+        return self
+
+    def log1p(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_log1p(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def log1p_(self) -> T:
+        torch._foreach_log1p_(self._values_list(True, True))
+        return self
+
+    def log2(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_log2(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def log2_(self) -> T:
+        torch._foreach_log2_(self._values_list(True, True))
+        return self
+
+    def ceil(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_ceil(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def ceil_(self) -> T:
+        torch._foreach_ceil_(self._values_list(True, True))
+        return self
+
+    def floor(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_floor(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def floor_(self) -> T:
+        torch._foreach_floor_(self._values_list(True, True))
+        return self
+
+    def round(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_round(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def round_(self) -> T:
+        torch._foreach_round_(self._values_list(True, True))
+        return self
+
+    def erf(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_erf(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def erf_(self) -> T:
+        torch._foreach_erf_(self._values_list(True, True))
+        return self
+
+    def erfc(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_erfc(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def erfc_(self) -> T:
+        torch._foreach_erfc_(self._values_list(True, True))
+        return self
+
+    def asin(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_asin(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def asin_(self) -> T:
+        torch._foreach_asin_(self._values_list(True, True))
+        return self
+
+    def atan(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_atan(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def atan_(self) -> T:
+        torch._foreach_atan_(self._values_list(True, True))
+        return self
+
+    def cos(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_cos(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def cos_(self) -> T:
+        torch._foreach_cos_(self._values_list(True, True))
+        return self
+
+    def cosh(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_cosh(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def cosh_(self) -> T:
+        torch._foreach_cosh_(self._values_list(True, True))
+        return self
+
+    def add(self, other: TensorDictBase | float, alpha: float | None = None):
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        if alpha is not None:
+            vals = torch._foreach_add(vals, other_val, alpha=alpha)
+        else:
+            vals = torch._foreach_add(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def add_(self, other: TensorDictBase | float, alpha: float | None = None):
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        if alpha is not None:
+            torch._foreach_add_(self._values_list(True, True), other_val, alpha=alpha)
+        else:
+            torch._foreach_add_(self._values_list(True, True), other_val)
+        return self
+
+    def lerp(self, end: TensorDictBase | float, weight: TensorDictBase | float):
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(end)):
+            end_val = end._values_list(True, True)
+        else:
+            end_val = end
+        if _is_tensor_collection(type(weight)):
+            weight_val = weight._values_list(True, True)
+        else:
+            weight_val = weight
+        vals = torch._foreach_lerp(vals, end_val, weight_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def lerp_(self, end: TensorDictBase | float, weight: TensorDictBase | float):
+        if _is_tensor_collection(type(end)):
+            end_val = end._values_list(True, True)
+        else:
+            end_val = end
+        if _is_tensor_collection(type(weight)):
+            weight_val = weight._values_list(True, True)
+        else:
+            weight_val = weight
+        torch._foreach_lerp_(self._values_list(True, True), end_val, weight_val)
+        return self
+
+    def addcdiv(self, other1, other2, value: float | None = 1):
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other1)):
+            other1_val = other1._values_list(True, True)
+        else:
+            other1_val = other1
+        if _is_tensor_collection(type(other2)):
+            other2_val = other2._values_list(True, True)
+        else:
+            other2_val = other2
+        vals = torch._foreach_addcdiv(vals, other1_val, other2_val, value=value)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def addcdiv_(self, other1, other2, value: float | None = 1):
+        if _is_tensor_collection(type(other1)):
+            other1_val = other1._values_list(True, True)
+        else:
+            other1_val = other1
+        if _is_tensor_collection(type(other2)):
+            other2_val = other2._values_list(True, True)
+        else:
+            other2_val = other2
+        torch._foreach_addcdiv_(
+            self._values_list(True, True), other1_val, other2_val, value=value
+        )
+        return self
+
+    def addcmul(self, other1, other2, value: float | None = 1):
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other1)):
+            other1_val = other1._values_list(True, True)
+        else:
+            other1_val = other1
+        if _is_tensor_collection(type(other2)):
+            other2_val = other2._values_list(True, True)
+        else:
+            other2_val = other2
+        vals = torch._foreach_addcmul(vals, other1_val, other2_val, value=value)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def addcmul_(self, other1, other2, value: float | None = 1):
+        if _is_tensor_collection(type(other1)):
+            other1_val = other1._values_list(True, True)
+        else:
+            other1_val = other1
+        if _is_tensor_collection(type(other2)):
+            other2_val = other2._values_list(True, True)
+        else:
+            other2_val = other2
+        torch._foreach_addcmul_(
+            self._values_list(True, True), other1_val, other2_val, value=value
+        )
+        return self
+
+    def sub(self, other: TensorDictBase | float, alpha: float | None = None):
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        if alpha is not None:
+            vals = torch._foreach_sub(vals, other_val, alpha=alpha)
+        else:
+            vals = torch._foreach_sub(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def sub_(self, other: TensorDictBase | float, alpha: float | None = None):
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        if alpha is not None:
+            torch._foreach_sub_(self._values_list(True, True), other_val, alpha=alpha)
+        else:
+            torch._foreach_sub_(self._values_list(True, True), other_val)
+        return self
+
+    def mul_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_mul_(self._values_list(True, True), other_val)
+        return self
+
+    def mul(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_mul(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def maximum_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_maximum_(self._values_list(True, True), other_val)
+        return self
+
+    def maximum(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_maximum(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def minimum_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_minimum_(self._values_list(True, True), other_val)
+        return self
+
+    def minimum(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_minimum(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def clamp_max_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_clamp_max_(self._values_list(True, True), other_val)
+        return self
+
+    def clamp_max(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_clamp_max(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def clamp_min_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_clamp_min_(self._values_list(True, True), other_val)
+        return self
+
+    def clamp_min(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_clamp_min(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def pow_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_pow_(self._values_list(True, True), other_val)
+        return self
+
+    def pow(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_pow(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def div_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_div_(self._values_list(True, True), other_val)
+        return self
+
+    def div(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_div(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
+    def sqrt_(self):
+        torch._foreach_sqrt_(self._values_list(True, True))
+        return self
+
+    def sqrt(self):
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_sqrt(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+        )
+
     # Functorch compatibility
     @abc.abstractmethod
     @cache  # noqa: B019
     def _add_batch_dim(self, *, in_dim, vmap_level):
         ...
 
     @abc.abstractmethod
```

## tensordict/persistent.py

```diff
@@ -1229,14 +1229,18 @@
             "Cannot call `unsqueeze` on a persistent tensordict. Make it dense before calling this method by calling `to_tensordict`."
         )
 
     __eq__ = TensorDict.__eq__
     __ne__ = TensorDict.__ne__
     __xor__ = TensorDict.__xor__
     __or__ = TensorDict.__or__
+    __ge__ = TensorDict.__ge__
+    __gt__ = TensorDict.__gt__
+    __le__ = TensorDict.__le__
+    __lt__ = TensorDict.__lt__
     _apply_nest = TensorDict._apply_nest
     _check_device = TensorDict._check_device
     _check_is_shared = TensorDict._check_is_shared
     _convert_to_tensordict = TensorDict._convert_to_tensordict
     _index_tensordict = TensorDict._index_tensordict
     all = TensorDict.all
     any = TensorDict.any
```

## tensordict/tensorclass.py

```diff
@@ -168,15 +168,15 @@
 
     _is_non_tensor = getattr(cls, "_is_non_tensor", False)
 
     cls = dataclass(cls)
     expected_keys = set(cls.__dataclass_fields__)
 
     for attr in cls.__dataclass_fields__:
-        if attr in dir(TensorDict) and attr != "_is_non_tensor":
+        if attr in dir(TensorDict) and attr not in ("_is_non_tensor", "data"):
             raise AttributeError(
                 f"Attribute name {attr} can't be used with @tensorclass"
             )
 
     cls.__init__ = _init_wrapper(cls.__init__)
     cls._from_tensordict = classmethod(_from_tensordict_wrapper(expected_keys))
     cls.from_tensordict = cls._from_tensordict
@@ -2462,14 +2462,18 @@
             )
             if memmap:
                 self._memmap_(prefix=self._path_to_memmap, inplace=True)
         finally:
             _BREAK_ON_MEMMAP = True
         return self
 
+    @property
+    def data(self):
+        raise AttributeError
+
 
 _register_tensor_class(NonTensorStack)
 
 
 def _share_memory_nontensor(data, manager: Manager):
     if isinstance(data, int):
         return mp.Value(ctypes.c_int, data)
```

## tensordict/utils.py

```diff
@@ -1518,15 +1518,17 @@
 
 def _expand_to_match_shape(
     parent_batch_size: torch.Size,
     tensor: Tensor,
     self_batch_dims: int,
     self_device: DeviceType,
 ) -> Tensor | TensorDictBase:
-    if hasattr(tensor, "dtype"):
+    from tensordict.base import _is_tensor_collection
+
+    if not _is_tensor_collection(type(tensor)):
         return torch.zeros(
             (
                 *parent_batch_size,
                 *_shape(tensor)[self_batch_dims:],
             ),
             dtype=tensor.dtype,
             device=self_device,
@@ -1538,19 +1540,19 @@
             [*parent_batch_size, *_shape(tensor)[self_batch_dims:]]
         )
         return out
 
 
 def _set_max_batch_size(source: T, batch_dims=None):
     """Updates a tensordict with its maximium batch size."""
+    from tensordict.base import _is_tensor_collection
+
     tensor_data = [val for val in source.values() if not is_non_tensor(val)]
 
     for val in tensor_data:
-        from tensordict.base import _is_tensor_collection
-
         if _is_tensor_collection(val.__class__):
             _set_max_batch_size(val, batch_dims=batch_dims)
 
     batch_size = []
     if not tensor_data:  # when source is empty
         if batch_dims:
             source.batch_size = source.batch_size[:batch_dims]
```

## tensordict/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '2024.04.08'
-git_version = 'f622b2f973320f769b6c09793ca827f27e47d603'
+__version__ = '2024.04.09'
+git_version = 'eabefcc886f2022f752553b65c0262e0adf731b2'
```

## tensordict/nn/params.py

```diff
@@ -598,14 +598,38 @@
     def __eq__(self, other: object) -> TensorDictBase:
         ...
 
     @_fallback
     def __ne__(self, other: object) -> TensorDictBase:
         ...
 
+    @_fallback
+    def __xor__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __or__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __ge__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __gt__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __le__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __lt__(self, other: object) -> TensorDictBase:
+        ...
+
     def __getattr__(self, item: str) -> Any:
         if not item.startswith("_"):
             try:
                 return getattr(self.__dict__["_param_td"], item)
             except AttributeError:
                 return super().__getattr__(item)
         else:
@@ -807,15 +831,19 @@
             return self._param_td._propagate_unlock()
 
     unlock_ = TensorDict.unlock_
     lock_ = TensorDict.lock_
 
     @property
     def data(self):
-        return self._param_td.detach()
+        return self._param_td._data()
+
+    @property
+    def grad(self):
+        return self._param_td._grad()
 
     @_unlock_and_set(inplace=True)
     def flatten_keys(
         self, separator: str = ".", inplace: bool = False
     ) -> TensorDictBase:
         ...
 
@@ -877,22 +905,14 @@
     def _unsqueeze(self, dim: int) -> TensorDictBase:
         ...
 
     @_carry_over
     def _legacy_unsqueeze(self, dim: int) -> TensorDictBase:
         ...
 
-    @_fallback
-    def __xor__(self, other):
-        ...
-
-    @_fallback
-    def __or__(self, other):
-        ...
-
     _check_device = TensorDict._check_device
     _check_is_shared = TensorDict._check_is_shared
 
     @_fallback
     def all(self, dim: int = None) -> bool | TensorDictBase:
         ...
```

## Comparing `tensordict_nightly-2024.4.8.dist-info/LICENSE` & `tensordict_nightly-2024.4.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tensordict_nightly-2024.4.8.dist-info/METADATA` & `tensordict_nightly-2024.4.9.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tensordict-nightly
-Version: 2024.4.8
+Version: 2024.4.9
 Summary: UNKNOWN
 Home-page: https://github.com/pytorch/tensordict
 Author: tensordict contributors
 Author-email: vmoens@fb.com
 License: BSD
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.8
@@ -166,14 +166,22 @@
 >>> data = TensorDict({"a-tensor": torch.randn(1, 2)}, batch_size=[1, 2])
 >>> data["non-tensor"] = "a string!"
 >>> assert data["non-tensor"] == "a string!"
 ```
 
 ### Tensor-like features
 
+**\[Nightly feature\]** TensorDict supports many common point-wise arithmetic operations such as `==` or `+`, `+=`
+and similar (provided that the underlying tensors support the said operation):
+```python
+>>> td = TensorDict.fromkeys(["a", "b", "c"], 0)
+>>> td += 1
+>>> assert (td==1).all()
+```
+
 TensorDict objects can be indexed exactly like tensors. The resulting of indexing
 a TensorDict is another TensorDict containing tensors indexed along the required dimension:
 ```python
 >>> data = TensorDict({
 ...     "key 1": torch.ones(3, 4, 5),
 ...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
 ... }, batch_size=[3, 4])
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: tensordict-nightly Version: 2024.4.8 Summary:
+Metadata-Version: 2.1 Name: tensordict-nightly Version: 2024.4.9 Summary:
 UNKNOWN Home-page: https://github.com/pytorch/tensordict Author: tensordict
 contributors Author-email: vmoens@fb.com License: BSD Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.8 Classifier: Programming
 Language :: Python :: 3.9 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11 Classifier: Development
 Status :: 4 - Beta Description-Content-Type: text/markdown License-File:
 LICENSE Requires-Dist: torch >=2.2.0.dev Requires-Dist: numpy Requires-Dist:
@@ -120,16 +120,20 @@
 programmatically: ```python >>> data["nested", ("supernested", ("key",))] =
 torch.zeros(3, 4) # the batch-size must match >>> assert (data["nested",
 "supernested", "key"] == 0).all() >>> assert (("nested",), "supernested", (
 ("key",),)) in data.keys(include_nested=True) # this works too! ``` You can
 also store non-tensor data in tensordicts: ```python >>> data = TensorDict({"a-
 tensor": torch.randn(1, 2)}, batch_size=[1, 2]) >>> data["non-tensor"] = "a
 string!" >>> assert data["non-tensor"] == "a string!" ``` ### Tensor-like
-features TensorDict objects can be indexed exactly like tensors. The resulting
-of indexing a TensorDict is another TensorDict containing tensors indexed along
+features **\[Nightly feature\]** TensorDict supports many common point-wise
+arithmetic operations such as `==` or `+`, `+=` and similar (provided that the
+underlying tensors support the said operation): ```python >>> td =
+TensorDict.fromkeys(["a", "b", "c"], 0) >>> td += 1 >>> assert (td==1).all()
+``` TensorDict objects can be indexed exactly like tensors. The resulting of
+indexing a TensorDict is another TensorDict containing tensors indexed along
 the required dimension: ```python >>> data = TensorDict({ ... "key 1":
 torch.ones(3, 4, 5), ... "key 2": torch.zeros(3, 4, 5, dtype=torch.bool), ...
 }, batch_size=[3, 4]) >>> sub_tensordict = data[..., :2] >>> assert
 sub_tensordict.shape == torch.Size([3, 2]) >>> assert sub_tensordict["key
 1"].shape == torch.Size([3, 2, 5]) ``` Similarly, one can build tensordicts by
 stacking or concatenating single tensordicts: ```python >>> tensordicts =
 [TensorDict({ ... "key 1": torch.ones(3, 4, 5), ... "key 2": torch.zeros(3, 4,
```

## Comparing `tensordict_nightly-2024.4.8.dist-info/RECORD` & `tensordict_nightly-2024.4.9.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,38 +1,38 @@
 tensordict/__init__.py,sha256=wHCnoagVm3PGPXxdlS54wZgtYm5JS5mde8MqHkhwhaQ,1652
 tensordict/_contextlib.py,sha256=yao_SZSgKUJt6dXHtAc5ZFJzAmm_NQQFYgR1rjDg0k8,6156
-tensordict/_lazy.py,sha256=a8T-XcGvhwpJrS9M2Qc6eMwzkVa9lXz4KOVmLAJqHR8,134377
+tensordict/_lazy.py,sha256=IYLm-aQlzKmviuGthc81KCN9QXEQyMLbDom-7FwNzjY,133285
 tensordict/_pytree.py,sha256=qUbtPRqQLJ6_adDSYrIVQ8FoPcSY62EUDvL-pqd3KkI,4693
-tensordict/_td.py,sha256=nzOPlI6VY4Q7-aP695uXGkksgUPp_T4X7inxVFoxx7A,128209
-tensordict/_tensordict.pyd,sha256=GVPcoyRsMjFaVvLbZe3QCSsnikIpDpZAAsts3wsnBh0,114176
+tensordict/_td.py,sha256=3WdfIg-ApDNGTOym_eo3KLRj-6LrpacLcnacmeXYvNc,133639
+tensordict/_tensordict.pyd,sha256=xNW3_P82Rjqah8RHfZbNzBHPwmrNAL863ueWdvr476g,114176
 tensordict/_torch_func.py,sha256=L4dDRNWvXa5_PewtRnUsR3vGQpSajpcu_5XPINf-dTE,21209
-tensordict/base.py,sha256=vG551zg7rKPqX2qly0T_WbRpDtGo1J2T2eOjQ9U6jOs,227083
+tensordict/base.py,sha256=PTMCUXdQ-IwDAGZiPcRw6x14ggIwmEjXmQ_nut_xn28,258288
 tensordict/functional.py,sha256=dEDwr-tNEVYIoNpOQ1FoSM1f37rMzipxIxN_Z8JfQh0,16915
 tensordict/memmap.py,sha256=8HEbz9o7FCJoV6nXpxIlZSMffYaVN4wl4H2NDB5BevE,27011
 tensordict/memmap_deprec.py,sha256=nmPXn2GQ3Krg1GAXLoYPpFZNTpZoaBl8DpkwS-H32dk,33049
-tensordict/persistent.py,sha256=zsFCnM1VbGmnM2HE1qgrpNtOT44aIygpcBQ-67JpTyA,46522
-tensordict/tensorclass.py,sha256=tuW1KmSYEs_03VEBHZxOtcbvqIdEdttCDLdVAeWhUl0,93206
+tensordict/persistent.py,sha256=9ypMYy0x2uLGeDhW3EQd40_P3jNHzXC0Jq6YqKtgYkY,46650
+tensordict/tensorclass.py,sha256=iaI6sqv4qlKE8wlbtXihNuty4_T9rxMQWfZTrsIb67I,93288
 tensordict/tensordict.py,sha256=1OhkuiahFu9Ctz4X5GpjXscKNR-uje5CFLYvlko-TC0,1093
-tensordict/utils.py,sha256=HXECx2ynVjwICtyzMmXmTPiSzL0Yb8zg__ZiuvSCDmk,75463
-tensordict/version.py,sha256=XSLfXYvkT3RGYsPokdvD51MuE10fqm5WkuJMuTVLJzo,86
+tensordict/utils.py,sha256=aKADnOKqD25ddnwFfdCFxvXRgsSbiYVZA-GP-qFSPdg,75531
+tensordict/version.py,sha256=GxX1dJAS332K0HP4ynwtNYBsDo1orOKKDxFauk21oCo,86
 tensordict/nn/__init__.py,sha256=nWPo4TqDb1hYILbEc73EHVyv3iy2kumYcoUp1MaXS1g,1634
 tensordict/nn/common.py,sha256=RqjFAF4bm5_B9BB0m7xxgAVWjIXdxrUq-Y3imiNkBYk,54882
 tensordict/nn/ensemble.py,sha256=CWyHKGsN7hmr3HjynEPj5xrCvZggeFy_C7iSpZxHlS4,5940
 tensordict/nn/functional_modules.py,sha256=PVZO3Emvzu_XRjNrjdVVTvgj97GF7eQdZoEgyvlQJO4,25890
-tensordict/nn/params.py,sha256=nVpoF3OiF026l9q-dQA1s35-AjNoxDWsNVNVOEwfr28,36844
+tensordict/nn/params.py,sha256=Bppr9MMWLPD48CKM7m2nZVWVGx6D2Zf2UQ2OIXaGHhk,37320
 tensordict/nn/probabilistic.py,sha256=HBiibBQPBBWnV9MWb1k2dJ7WiWaddTZ6CBq3zT4Uv-8,25553
 tensordict/nn/sequence.py,sha256=mD0oJplRLlKflEGSopeSjc9MzdOfH00FM8Tm17_T7Sw,19947
 tensordict/nn/utils.py,sha256=UCK2w6QoOCouDoxP08CvokffxfnwTtWv3Wl5Ls8rgxs,13231
 tensordict/nn/distributions/__init__.py,sha256=kfuBq-yJHh9OkaQHaOJUpvam4KjKLIirE8UbpYF3BuM,795
 tensordict/nn/distributions/composite.py,sha256=bsvnghcdoj2Ak3r3eNgs2GpBxWIV9-z8IIwdZiOM7oM,6629
 tensordict/nn/distributions/continuous.py,sha256=Ge1qivY1uB3sWBVTFD6wKQbBrvU3tAa5GV28aqCs9l4,9924
 tensordict/nn/distributions/discrete.py,sha256=VrcrSPr9YyBDKagOphYTAgpiQqlwfpf4-8t3TsrJyPQ,2667
 tensordict/nn/distributions/truncated_normal.py,sha256=f--2ISj15TTUlLcUzMh1e50yADuh-vKMFbrzLqwUv7I,6694
 tensordict/nn/distributions/utils.py,sha256=3vEDATr12hUk9OYYKf4dzPmNMmLzKHQuZPsdWcPVfi4,1266
 tensordict/prototype/__init__.py,sha256=XBECOVFLLCiUsvNwwByWkz-U87nt7oJXPp2iIIA5Ysk,393
 tensordict/prototype/fx.py,sha256=AD6zDHD2g24iuP9v-yzWN_NX1xInK5E8RHvkend0amc,7889
 tensordict/prototype/tensorclass.py,sha256=bzbSJ7uaQVxfUS6uzwyh5SOBrhufxsW_dLzjMbQJLvw,796
-tensordict_nightly-2024.4.8.dist-info/LICENSE,sha256=PGO-oZsq4EzhE1-WQS2xGiEF3UCVb9YawfQ09cIMV_8,1119
-tensordict_nightly-2024.4.8.dist-info/METADATA,sha256=9NFCHCVujetUp70QXLG8sCqzkDW0HRVlaW057zAhu5I,22528
-tensordict_nightly-2024.4.8.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
-tensordict_nightly-2024.4.8.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
-tensordict_nightly-2024.4.8.dist-info/RECORD,,
+tensordict_nightly-2024.4.9.dist-info/LICENSE,sha256=PGO-oZsq4EzhE1-WQS2xGiEF3UCVb9YawfQ09cIMV_8,1119
+tensordict_nightly-2024.4.9.dist-info/METADATA,sha256=oWGR5n5Hw0zR20AnNKf5awk27_JMRWjBG-9ZDY8UpjU,22831
+tensordict_nightly-2024.4.9.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
+tensordict_nightly-2024.4.9.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
+tensordict_nightly-2024.4.9.dist-info/RECORD,,
```

